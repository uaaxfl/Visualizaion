2020.acl-main.45,D19-1606,0,0.0721148,"imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class O. Specifically, the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Koˇcisk`y et al., 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or"
2020.acl-main.45,P18-1246,0,0.0721139,"tion recently. Pang et al. (2019) proposed a novel method called IoU-balanced sampling and Chen et al. (2019) designed a ranking model to replace the conventional classification task with an average-precision loss 466 to alleviate the class imbalance issue. The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP. Sudre et al. (2017) addressed the severe class imbalance issue for the image segmentation task. They proposed to use the class re-balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks. Shen et al. (2018) investigated the influence of Dice-based loss for multi-class organ segmentation using a dataset of abdominal CT volumes. Kodym et al. (2018) proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk (OAR) of medical images. Shamir et al. (2019) extended the definition of the classical Dice coefficient to facilitate the direct comparison of a ground truth binary image with a probabilistic map. In this paper, we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address"
2020.acl-main.45,P17-1171,0,0.0765395,"stics, pages 465–476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues. To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the Fβ score, and thus comes with more flexibi"
2020.acl-main.45,Q18-1023,0,0.0517121,"Missing"
2020.acl-main.45,N16-1030,0,0.0283015,"mber of negative examples also 465 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 465–476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues. To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trad"
2020.acl-main.45,N18-1202,0,0.0748734,"Missing"
2020.acl-main.45,W06-0115,0,0.0872774,"Quoref (Dasigi et al., 2019). 71.56 80.35 81.40 81.25 82.97 73.88 79.16 80.62 82.11 83.30 (+1.19) 83.97 84.05 84.01 (+1.90) 84.22 84.72 84.47 (+2.36) Table 5: Experimental results for NER task. 4.2 Machine Reading Comprehension Named Entity Recognition Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 470 Baselines We used the following baselines: • QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. • BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. • XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that SQuAD v1.1 EM F1 Mod"
2020.acl-main.45,W13-3516,0,0.0210083,"Missing"
2020.acl-main.45,P16-1101,0,0.0293711,"omputational Linguistics, pages 465–476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues. To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the Fβ score, and thus com"
2020.acl-main.45,P18-2124,0,0.0420015,"Missing"
2020.acl-main.45,D16-1264,0,0.271008,"y narrows down the gap between the F1 score in evaluation and the dice loss in training. Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class O. Specifically, the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Koˇcisk`y et al., 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SO"
2020.acl-main.45,D11-1141,0,0.0306078,"Tagger (Devlin et al., 2018) 92.33 91.98 92.34 BERT-Tagger+FL 91.24 93.22 92.47 (+0.13) BERT-Tagger+DL 91.44 92.88 92.52 (+0.18) BERT-Tagger+DSC 92.87 93.54 92.58 (+0.24) Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation. Baselines We used the following baselines: • Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. • Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. • Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task. Table 4: Experimental results for English POS datasets. In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after p exc"
2020.acl-main.45,W03-0419,0,0.289135,"Missing"
2020.acl-main.45,I17-1018,0,0.0122977,".58 (+0.24) Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation. Baselines We used the following baselines: • Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. • Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. • Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task. Table 4: Experimental results for English POS datasets. In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after p exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the"
2020.acl-main.45,P18-1144,0,0.0723501,"speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation. Baselines We used the following baselines: • Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. • Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. • Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task. Table 4: Experimental results for English POS datasets. In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after p exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push p to 1 as much as possible. 4 Expe"
2020.acl-main.519,W07-1009,0,0.201314,"tural languages. The task of flat NER is commonly formalized as a sequence labeling task: a sequence labeling model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a single tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER (Huang et al., 2015; Chiu and Nichols, 2015), where multiple categories need to be assigned to a single token if the token participates in multiple entities. Many attempts have been made to reconcile sequence labeling models with nested NER (Alex et al., 2007; Byrne, 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Katiyar and Cardie, 2018), mostly based on the pipelined systems. However, pipelined systems suffer from the disadvantages of error propagation, long running time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing 5849 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–5859 c July 5 - 10, 2020. 2020 Association for Computational Linguistics NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al"
2020.acl-main.519,P17-1171,0,0.0263152,"duced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entities. Shibuya and Hovy (2019) provided inference model that extracts entities iteratively from outermost ones to inner ones. Strakov´a et al. (2019) viewed nested NER as a sequence-tosequence generation problem, in which the input sequence is a list of tokens and the target sequence is a list of labels. 2.3 Machine Reading Comprehension (MRC) MRC models (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016, 2017; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) extract answer spans from a passage through a given question. The task can be formalized as two multi-class classification tasks, i.e., predicting the starting and ending positions of the answer spans. Over the past one or two years, there has been a trend of transforming NLP tasks to MRC question answering. For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R(x, y) can be parameterized as a question q(x) whose answer is y. For example, the relation EDUCATED - AT can be mapped to “Where did x study?”. Given a question q(x), if a non-nu"
2020.acl-main.519,Q16-1026,0,0.448371,"ested entities from GENIA and ACE04 corpora. 1 Introduction Named Entity Recognition (NER) refers to the task of detecting the span and the semantic category of entities from a chunk of text. The task can be further divided into two sub-categories, nested NER and flat NER, depending on whether entities are nested or not. Nested NER refers to a phenomenon that the spans of entities (mentions) are nested, as shown in Figure 1. Entity overlapping is a fairly common phenomenon in natural languages. The task of flat NER is commonly formalized as a sequence labeling task: a sequence labeling model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a single tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER (Huang et al., 2015; Chiu and Nichols, 2015), where multiple categories need to be assigned to a single token if the token participates in multiple entities. Many attempts have been made to reconcile sequence labeling models with nested NER (Alex et al., 2007; Byrne, 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Katiyar and Cardie, 2018), mostly based on the pipelined syste"
2020.acl-main.519,D18-1217,0,0.0378837,", etc). MSRA (Levow, 2006) is a Chinese dataset and performs as a benchmark dataset. Data in MSRA is collected from news domain and is used as shared task on SIGNAN backoff 2006. There are three types of named entities. OntoNotes 4.0 (Pradhan et al., 2011) is a Chinese dataset and consists of text from news domain. OntoNotes 4.0 annotates 18 named entity types. In this paper, we take the same data split as Wu et al. (2019). Baselines For English datasets, we use the following models as baselines. • BiLSTM-CRF from Ma and Hovy (2016). • ELMo tagging model from Peters et al. (2018b). • CVT from Clark et al. (2018), which uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder. • Bert-Tagger from Devlin et al. (2018), which treats NER as a tagging task. For Chinese datasets, we use the following models as baselines: • Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice. • Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task. • Glyce-BERT: The current SOTA model in Chinese NER developed by Wu et al. (2019), which combines glyph information with BERT pretraining. 4.2.3 Results and Discussions Table 3 presents comparisons between the proposed model an"
2020.acl-main.519,D09-1015,0,0.142718,"Missing"
2020.acl-main.519,P19-1585,0,0.0353673,"he Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b). Fisher and Vlachos (2019) introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entities. Shibuya and Hovy (2019) provided inference model that extracts entities iteratively from outermost ones to inner ones. Strakov´a et al. (2019) viewed nested NER as a sequence-tosequence generation problem, in which the input sequence is a list of tokens and the target sequence is a list of labels. 2.3 Machine Reading Comprehension (MRC) MRC models (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016, 2017; Wang et al., 2016; Shen et al.,"
2020.acl-main.519,W03-0426,0,0.252105,"s on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively on ACE04, ACE05, GENIA and KBP17, as well as flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0. We wish that our work would inspire the introduction of new paradigms for the entity recognition task. 2 2.1 Related Work Named Entity Recognition (NER) Traditional sequence labeling models use CRFs (Lafferty et al., 2001; Sutton et al., 2007) as a backbone for NER. The first work using neural models for NER goes back to 2003, when Hammerton (2003) attempted to solve the problem using unidirectional LSTMs. Collobert et al. (2011) presented a CNN-CRF structure, augmented with character embeddings by Santos and Guimaraes (2015). Lample et al. (2016) explored neural structures for NER, in which the bidirectional LSTMs are combined with CRFs with features based on character-based word representations and unsupervised word representations. Ma and Hovy (2016) and Chiu and Nichols (2016) used a character CNN to extract features from characters. Recent large-scale language model pretraining methods such as BERT (Devlin et al., 2018) and ELMo (P"
2020.acl-main.519,N18-1131,0,0.0383373,"tifies outermost entities, and then successive CRFs would identify increasingly nested entities. Finkel and Manning (2009) built a model to extract nested entity mentions based on parse trees. They made the assumption that one mention is fully contained by the other when they overlap. Lu and Roth (2015) proposed to use mention hyper-graphs for recognizing overlapping mentions. Xu et al. (2017) utilized a local classifier that runs on every possible span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-t"
2020.acl-main.519,N18-1079,0,0.156608,"beling task: a sequence labeling model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a single tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER (Huang et al., 2015; Chiu and Nichols, 2015), where multiple categories need to be assigned to a single token if the token participates in multiple entities. Many attempts have been made to reconcile sequence labeling models with nested NER (Alex et al., 2007; Byrne, 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Katiyar and Cardie, 2018), mostly based on the pipelined systems. However, pipelined systems suffer from the disadvantages of error propagation, long running time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing 5849 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–5859 c July 5 - 10, 2020. 2020 Association for Computational Linguistics NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of handling both flat and nested"
2020.acl-main.519,N16-1030,0,0.15209,"2003, English OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0. We wish that our work would inspire the introduction of new paradigms for the entity recognition task. 2 2.1 Related Work Named Entity Recognition (NER) Traditional sequence labeling models use CRFs (Lafferty et al., 2001; Sutton et al., 2007) as a backbone for NER. The first work using neural models for NER goes back to 2003, when Hammerton (2003) attempted to solve the problem using unidirectional LSTMs. Collobert et al. (2011) presented a CNN-CRF structure, augmented with character embeddings by Santos and Guimaraes (2015). Lample et al. (2016) explored neural structures for NER, in which the bidirectional LSTMs are combined with CRFs with features based on character-based word representations and unsupervised word representations. Ma and Hovy (2016) and Chiu and Nichols (2016) used a character CNN to extract features from characters. Recent large-scale language model pretraining methods such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018a) further enhanced the performance of NER, yielding state-of-the-art performances. 2.2 Nested Named Entity Recognition The overlapping between entities (mentions) was first noticed by"
2020.acl-main.519,W06-0115,0,0.092749,"Table 3: Results for flat NER tasks. 4.2.2 Table 2: Results for nested NER tasks. micro-averaged precision, recall and F1 scores for evaluation. CoNLL2003 (Sang and Meulder, 2003) is an English dataset with four types of named entities: Location, Organization, Person and Miscellaneous. We followed data processing protocols in Ma and Hovy (2016). OntoNotes 5.0 (Pradhan et al., 2013) is an English dataset and consists of text from a wide variety of sources. The dataset includes 18 types of named entity, consisting of 11 types (Person, Organization, etc) and 7 values (Date, Percent, etc). MSRA (Levow, 2006) is a Chinese dataset and performs as a benchmark dataset. Data in MSRA is collected from news domain and is used as shared task on SIGNAN backoff 2006. There are three types of named entities. OntoNotes 4.0 (Pradhan et al., 2011) is a Chinese dataset and consists of text from news domain. OntoNotes 4.0 annotates 18 named entity types. In this paper, we take the same data split as Wu et al. (2019). Baselines For English datasets, we use the following models as baselines. • BiLSTM-CRF from Ma and Hovy (2016). • ELMo tagging model from Peters et al. (2018b). • CVT from Clark et al. (2018), which"
2020.acl-main.519,K17-1034,0,0.129311,"Missing"
2020.acl-main.519,P19-1129,1,0.748393,"e other when they overlap. Lu and Roth (2015) proposed to use mention hyper-graphs for recognizing overlapping mentions. Xu et al. (2017) utilized a local classifier that runs on every possible span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b). Fisher and Vlachos (2019) introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entit"
2020.acl-main.519,P19-1511,0,0.132201,"Missing"
2020.acl-main.519,D15-1102,0,0.311145,"ed as a sequence labeling task: a sequence labeling model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a single tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER (Huang et al., 2015; Chiu and Nichols, 2015), where multiple categories need to be assigned to a single token if the token participates in multiple entities. Many attempts have been made to reconcile sequence labeling models with nested NER (Alex et al., 2007; Byrne, 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Katiyar and Cardie, 2018), mostly based on the pipelined systems. However, pipelined systems suffer from the disadvantages of error propagation, long running time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing 5849 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–5859 c July 5 - 10, 2020. 2020 Association for Computational Linguistics NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of ha"
2020.acl-main.519,N19-1308,0,0.0259177,"tained by the other when they overlap. Lu and Roth (2015) proposed to use mention hyper-graphs for recognizing overlapping mentions. Xu et al. (2017) utilized a local classifier that runs on every possible span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b). Fisher and Vlachos (2019) introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entit"
2020.acl-main.519,P16-1101,0,0.417806,"A and ACE04 corpora. 1 Introduction Named Entity Recognition (NER) refers to the task of detecting the span and the semantic category of entities from a chunk of text. The task can be further divided into two sub-categories, nested NER and flat NER, depending on whether entities are nested or not. Nested NER refers to a phenomenon that the spans of entities (mentions) are nested, as shown in Figure 1. Entity overlapping is a fairly common phenomenon in natural languages. The task of flat NER is commonly formalized as a sequence labeling task: a sequence labeling model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a single tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER (Huang et al., 2015; Chiu and Nichols, 2015), where multiple categories need to be assigned to a single token if the token participates in multiple entities. Many attempts have been made to reconcile sequence labeling models with nested NER (Alex et al., 2007; Byrne, 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Katiyar and Cardie, 2018), mostly based on the pipelined systems. However, pipeli"
2020.acl-main.519,D17-1276,0,0.103843,"very possible span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b). Fisher and Vlachos (2019) introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entities. Shibuya and Hovy (2019) provided inference model that extracts entities iteratively from outermost ones to inner ones. Strakov´a et al. (2019) viewed nested NER as a sequence-tosequence"
2020.acl-main.519,N18-1202,0,0.103197,") attempted to solve the problem using unidirectional LSTMs. Collobert et al. (2011) presented a CNN-CRF structure, augmented with character embeddings by Santos and Guimaraes (2015). Lample et al. (2016) explored neural structures for NER, in which the bidirectional LSTMs are combined with CRFs with features based on character-based word representations and unsupervised word representations. Ma and Hovy (2016) and Chiu and Nichols (2016) used a character CNN to extract features from characters. Recent large-scale language model pretraining methods such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018a) further enhanced the performance of NER, yielding state-of-the-art performances. 2.2 Nested Named Entity Recognition The overlapping between entities (mentions) was first noticed by Kim et al. (2003), who developed handcrafted rules to identify overlapping mentions. Alex et al. (2007) proposed two multi-layer CRF models for nested NER. The first model is the inside-out model, in which the first CRF identifies the innermost entities, and the successive layer CRF is built over words and the innermost entities extracted from the previous CRF to identify second-level entities, etc. The other is"
2020.acl-main.519,W13-3516,0,0.0628713,"Missing"
2020.acl-main.519,P18-2124,0,0.0616493,"Missing"
2020.acl-main.519,D16-1264,0,0.0714443,", long running time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing 5849 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–5859 c July 5 - 10, 2020. 2020 Association for Computational Linguistics NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of handling both flat and nested NER. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a SQuADstyle (Rajpurkar et al., 2016, 2018) machine reading comprehension (MRC) task. Each entity type is characterized by a natural language query, and entities are extracted by answering these queries given the contexts. For example, the task of assigning the PER( PERSON ) label to “[Washington] was born into slavery on the farm of James Burroughs” is formalized as answering the question “which person is mentioned in the text?”. This strategy naturally tackles the entity overlapping issue in nested NER: the extraction of two entities with different categories that overlap requires answering two independent questions. The MRC f"
2020.acl-main.519,W03-0419,0,0.413409,"Missing"
2020.acl-main.519,W15-3904,0,0.0343162,"respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0. We wish that our work would inspire the introduction of new paradigms for the entity recognition task. 2 2.1 Related Work Named Entity Recognition (NER) Traditional sequence labeling models use CRFs (Lafferty et al., 2001; Sutton et al., 2007) as a backbone for NER. The first work using neural models for NER goes back to 2003, when Hammerton (2003) attempted to solve the problem using unidirectional LSTMs. Collobert et al. (2011) presented a CNN-CRF structure, augmented with character embeddings by Santos and Guimaraes (2015). Lample et al. (2016) explored neural structures for NER, in which the bidirectional LSTMs are combined with CRFs with features based on character-based word representations and unsupervised word representations. Ma and Hovy (2016) and Chiu and Nichols (2016) used a character CNN to extract features from characters. Recent large-scale language model pretraining methods such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018a) further enhanced the performance of NER, yielding state-of-the-art performances. 2.2 Nested Named Entity Recognition The overlapping between entities (mentions)"
2020.acl-main.519,D18-1309,0,0.177686,"Missing"
2020.acl-main.519,P19-1527,0,0.109133,"Missing"
2020.acl-main.519,P17-1114,0,0.0247982,"entities, and the successive layer CRF is built over words and the innermost entities extracted from the previous CRF to identify second-level entities, etc. The other is the outsidein model, in which the first CRF identifies outermost entities, and then successive CRFs would identify increasingly nested entities. Finkel and Manning (2009) built a model to extract nested entity mentions based on parse trees. They made the assumption that one mention is fully contained by the other when they overlap. Lu and Roth (2015) proposed to use mention hyper-graphs for recognizing overlapping mentions. Xu et al. (2017) utilized a local classifier that runs on every possible span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted rel"
2020.acl-main.519,P18-1144,0,0.135967,"Missing"
2020.acl-main.519,D19-1034,0,0.132711,"entions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. Ju et al. (2018) dynamically stacked flat NER layers in a hierarchical manner. Lin et al. 5850 (2019a) proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. Luan et al. (2019) built a span enumeration approach by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. Other works (Muis and Lu, 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) also proposed various methods to tackle the nested NER problem. Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b). Fisher and Vlachos (2019) introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entities. Shibuya and Hovy (2019) provided inference model that extracts entities iteratively from outermost ones to inner ones. Strakov´a et al. (2019) viewed nested NER as a sequence-tosequence generation problem, in which the input seque"
2020.acl-main.519,D17-1283,0,0.0660113,"Missing"
2020.acl-main.519,D18-1019,0,0.0723133,"spectively. GENIA (Ohta et al., 2002) For the GENIA dataset, we use GENIAcorpus3.02p. We follow the protocols in Katiyar and Cardie (2018). KBP2017 We follow Katiyar and Cardie (2018) and evaluate our model on the 2017 English evaluation dataset (LDC2017D55). Training set consists of RichERE annotated datasets, which include LDC2015E29, LDC2015E68, LDC2016E31 and LDC2017E02. We follow the dataset split strategy in Lin et al. (2019b). 4.1.2 Baselines We use the following models as baselines: • Hyper-Graph: Katiyar and Cardie (2018) proposes a hypergraph-based model based on LSTMs. • Seg-Graph: Wang and Lu (2018) proposes a segmental hypergargh representation to model overlapping entity mentions. • ARN: Lin et al. (2019a) proposes AnchorRegion Networks by modeling and levraging the head-driven phrase structures of entity mentions. • KBP17-Best: Ji et al. (2017) gives an overview of the Entity Discovery task at the Knowledge Base Population (KBP) track at TAC2017 and also reports previous best results for the task of nested NER. • Seq2Seq-BERT: Strakov´a et al. (2019) views the nested NER as a sequence-tosequence problem. Input to the model is word tokens and the output sequence consists of labels. • P"
2020.acl-main.622,P14-1005,0,0.173467,"Missing"
2020.acl-main.622,P15-1136,0,0.111435,"al., 2015; Lee et al., 2017) that learn to select the antecedent of each anaphoric mention. Our CorefQA model is essentially a mention-ranking model, but we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) showed that semantic role labeling annotations could be solicited by using question-answer pairs to represent the predicate-argument structure. Levy et al. (2017) reduced relation extraction to answering simple reading comprehension questions, yielding models that generalize better in the 6954 Mention Proposal Module I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat jumped up into the record booth and started meowing along, meowing to me. Question: I … my cat … I was hired to do some Chr"
2020.acl-main.622,P16-1061,0,0.339673,"Missing"
2020.acl-main.622,D19-1606,0,0.259787,"he output layer of contextualization, span prediction requires a more thorough and deeper examination of the lexical, semantic and syntactic cues within the context, which will potentially lead to better performance. Moreover, the proposed question answering formulation allows us to take advantage of existing question answering datasets. Coreference annotation is expensive, cumbersome and often requires linguistic expertise from annotators. Under the proposed formulation, the coreference resolution has the same format as the existing question answering datasets (Rajpurkar et al., 2016a, 2018; Dasigi et al., 2019a). Those datasets can thus readily be used for data augmentation. We show that pre-training on existing question answering datasets improves the model’s generalization and 2 This is an illustration of the question formulation. The actual operation is described in Section 3.4. transferability, leading to additional performance boost. Experiments show that the proposed framework significantly outperforms previous models on two widely-used datasets. Specifically, we achieve new state-of-the-art scores of 83.1 (+3.5) on the CoNLL-2012 benchmark and 87.5 (+2.5) on the GAP benchmark. 2 2.1 Related"
2020.acl-main.622,P19-1386,0,0.0110556,"ed at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models. Data augmentation techniques have been explored in various fields such as question answering (Talmor and Berant, 2019), text classification (Kobayashi, 2018) and dialogue language understanding (Hou et al., 2018). In coreference resolution, Zhao et al. (2018); Emami et al. (2019); Zhao et al. (2019) focused on debiasing the gender bias problem; Aralikatte et al. (2019) explored the effectiveness of joint modeling of ellipsis and coreference resolution. To the best of our knowledge, we are the first to use existing question answering datasets as data augmentation for coreference resolution. 3 Model In this section, we describe our CorefQA model in detail. The overall architecture is illustrated in Figure 2. 3.1 Notations Given a sequence of input tokens X = {x1 , x2 , ..., xn } in a document, where n denotes the length of the document. N = n ∗ (n + 1)/2 denotes the num"
2020.acl-main.622,D15-1076,0,0.0387119,"iseman et al., 2015; Lee et al., 2017) that learn to select the antecedent of each anaphoric mention. Our CorefQA model is essentially a mention-ranking model, but we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) showed that semantic role labeling annotations could be solicited by using question-answer pairs to represent the predicate-argument structure. Levy et al. (2017) reduced relation extraction to answering simple reading comprehension questions, yielding models that generalize better in the 6954 Mention Proposal Module I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat jumped up into the record booth and started meowing along, meowing to me. Question: I … my cat … I was hired to do some Chr"
2020.acl-main.622,C18-1105,0,0.0248802,"odels are built under the assumption that gold mentions are provided at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models. Data augmentation techniques have been explored in various fields such as question answering (Talmor and Berant, 2019), text classification (Kobayashi, 2018) and dialogue language understanding (Hou et al., 2018). In coreference resolution, Zhao et al. (2018); Emami et al. (2019); Zhao et al. (2019) focused on debiasing the gender bias problem; Aralikatte et al. (2019) explored the effectiveness of joint modeling of ellipsis and coreference resolution. To the best of our knowledge, we are the first to use existing question answering datasets as data augmentation for coreference resolution. 3 Model In this section, we describe our CorefQA model in detail. The overall architecture is illustrated in Figure 2. 3.1 Notations Given a sequence of input tokens X = {x1 , x2 , ..., xn } in a document, where n d"
2020.acl-main.622,2020.tacl-1.5,0,0.178945,"Missing"
2020.acl-main.622,D19-1588,0,0.416343,"Missing"
2020.acl-main.622,P19-1066,0,0.275848,"nt mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark. 1 1 Figure 1: An illustration of the paradigm shift from coreference resolution to query-based span prediction. Spans with the same format represent coreferent mentions. Introduction Recent coreference resolution systems (Lee et al., 2017, 2018; Zhang et al., 2018a; Kantor and Globerson, 2019) consider all text spans in a document as potential mentions and learn to find an antecedent for each possible mention. There are two key issues with this paradigm, in terms of task formalization and the algorithm. At the task formalization level, mentions left out at the mention proposal stage can never be recovered since the downstream module only operates on the proposed mentions. Existing models often suffer from mention proposal (Zhang et al., 1 https://github.com/ShannonAI/CorefQA 2018a). The coreference datasets can only provide a weak signal for spans that correspond to entity mentions"
2020.acl-main.622,N18-2072,0,0.0158034,"fits of training joint models for these tasks. Their models are built under the assumption that gold mentions are provided at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models. Data augmentation techniques have been explored in various fields such as question answering (Talmor and Berant, 2019), text classification (Kobayashi, 2018) and dialogue language understanding (Hou et al., 2018). In coreference resolution, Zhao et al. (2018); Emami et al. (2019); Zhao et al. (2019) focused on debiasing the gender bias problem; Aralikatte et al. (2019) explored the effectiveness of joint modeling of ellipsis and coreference resolution. To the best of our knowledge, we are the first to use existing question answering datasets as data augmentation for coreference resolution. 3 Model In this section, we describe our CorefQA model in detail. The overall architecture is illustrated in Figure 2. 3.1 Notations Given a sequence of input t"
2020.acl-main.622,D17-1018,0,0.0579966,"n of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark. 1 1 Figure 1: An illustration of the paradigm shift from coreference resolution to query-based span prediction. Spans with the same format represent coreferent mentions. Introduction Recent coreference resolution systems (Lee et al., 2017, 2018; Zhang et al., 2018a; Kantor and Globerson, 2019) consider all text spans in a document as potential mentions and learn to find an antecedent for each possible mention. There are two key issues with this paradigm, in terms of task formalization and the algorithm. At the task formalization level, mentions left out at the mention proposal stage can never be recovered since the downstream module only operates on the proposed mentions. Existing models often suffer from mention proposal (Zhang et al., 1 https://github.com/ShannonAI/CorefQA 2018a). The coreference datasets can only provide a"
2020.acl-main.622,N18-2108,0,0.768457,"ectly model the representation of real-world entities and (2) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Lee et al., 2017) that learn to select the antecedent of each anaphoric mention. Our CorefQA model is essentially a mention-ranking model, but we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) showed that semantic role labeling annotations could be solicited by using question-answer pairs to represent the predicate-argument structure. Levy et al. (2017) reduced relation extraction to answering simple reading comprehension questions, yielding models that generalize better in the 6954 Mention Proposal Module I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat j"
2020.acl-main.622,K17-1034,0,0.0246564,"ut we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) showed that semantic role labeling annotations could be solicited by using question-answer pairs to represent the predicate-argument structure. Levy et al. (2017) reduced relation extraction to answering simple reading comprehension questions, yielding models that generalize better in the 6954 Mention Proposal Module I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat jumped up into the record booth and started meowing along, meowing to me. Question: I … my cat … I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat jumped up into the record booth"
2020.acl-main.622,P16-1094,1,0.873523,"Missing"
2020.acl-main.622,P19-1129,1,0.879507,"to the studio, and I was working on the song and the cat jumped up into the record booth and started meowing along, meowing to me. Coreference Clusters [I, I, my, me, I, me] [my cat, the cat] [Jingle Bells, the song] Figure 2: The overall architecture of our CorefQA model. The input passage is first fed into the Mention Proposal Module 3.3 to obtain candidate mentions. Then the Mention Linking Module 3.4 is used to extract coreferent mentions from the passage for each proposed mention. The coreference clusters are obtained using the scores produced in the above two stages. zero-shot setting. Li et al. (2019a,b) cast the tasks of named entity extraction and relation extraction as a reading comprehension problem. In parallel to our work, Aralikatte et al. (2019) converted coreference and ellipsis resolution in a question answering format, and showed the benefits of training joint models for these tasks. Their models are built under the assumption that gold mentions are provided at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a st"
2020.acl-main.622,H05-1004,0,0.0317466,"s. • EE + BERT-large (Kantor and Globerson, 2019) represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. • c2f-coref + SpanBERT-large (Joshi et al., 2019a) focuses on pre-training span representations to better represent and predict spans of text. 4.3 Results on CoNLL-2012 Shared Task The English data of CoNLL-2012 shared task (Pradhan et al., 2012) contains 2,802/343/348 train/development/test documents in 7 different genres. The main evaluation is the average of three metrics – MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) on the test set according to the official CoNLL-2012 evaluation scripts 7 . 6 https://github.com/lessw2020/ Ranger-Deep-Learning-Optimizer 7 http://conll.cemantix.org/2012/ software.html We compare the CorefQA model with several baseline models in Table 1. Our CorefQA system achieves a huge performance boost over existing systems: With SpanBERT-base, it achieves an F1 score of 79.9, which already outperforms the previous SOTA model using SpanBERT-large by 0.3. With SpanBERT-large, it achieves an F1 score of 83.1, with a 3.5 performance boost over the previous SOTA system. 4.4 Results on GAP T"
2020.acl-main.622,D18-1298,0,0.0435821,"Missing"
2020.acl-main.622,D14-1162,0,0.0815855,"al = 10 and the maximum number of antecedents kept for each mention C = 50. The SpanBERT parameters are updated by the Adam optimizer (Kingma and Ba, 2015) with initial learning rate 1 × 10−5 and the task parameters are updated by the Range optimizer 6 with initial learning rate 2 × 10−4 . 4.2 Baselines We compare the CorefQA model with previous neural models that are trained end-to-end: • e2e-coref (Lee et al., 2017) is the first endto-end coreference system that learns which spans are entity mentions and how to best cluster them jointly. Their token representations are built upon the GLoVe (Pennington et al., 2014) and Turian (Turian et al., 2010) embeddings. • c2f-coref + ELMo (Lee et al., 2018) extends Lee et al. (2017) by combining a coarse-tofine pruning with a higher-order inference mechanism. Their representations are built upon ELMo embeddings (Peters et al., 2018). • c2f-coref + BERT-large(Joshi et al., 2019b) builds the c2f-coref system on top of BERT (Devlin et al., 2019) token representations. • EE + BERT-large (Kantor and Globerson, 2019) represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. • c2f-coref + SpanBERT-large (Joshi et al., 2019a) foc"
2020.acl-main.622,N18-1202,0,0.0508539,"learning rate 2 × 10−4 . 4.2 Baselines We compare the CorefQA model with previous neural models that are trained end-to-end: • e2e-coref (Lee et al., 2017) is the first endto-end coreference system that learns which spans are entity mentions and how to best cluster them jointly. Their token representations are built upon the GLoVe (Pennington et al., 2014) and Turian (Turian et al., 2010) embeddings. • c2f-coref + ELMo (Lee et al., 2018) extends Lee et al. (2017) by combining a coarse-tofine pruning with a higher-order inference mechanism. Their representations are built upon ELMo embeddings (Peters et al., 2018). • c2f-coref + BERT-large(Joshi et al., 2019b) builds the c2f-coref system on top of BERT (Devlin et al., 2019) token representations. • EE + BERT-large (Kantor and Globerson, 2019) represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. • c2f-coref + SpanBERT-large (Joshi et al., 2019a) focuses on pre-training span representations to better represent and predict spans of text. 4.3 Results on CoNLL-2012 Shared Task The English data of CoNLL-2012 shared task (Pradhan et al., 2012) contains 2,802/343/348 train/development/test documents in 7 differen"
2020.acl-main.622,W12-4501,0,0.750664,"Missing"
2020.acl-main.622,P18-2124,0,0.0574605,"Missing"
2020.acl-main.622,D16-1264,0,0.368547,"only superficially modeled at the output layer of contextualization, span prediction requires a more thorough and deeper examination of the lexical, semantic and syntactic cues within the context, which will potentially lead to better performance. Moreover, the proposed question answering formulation allows us to take advantage of existing question answering datasets. Coreference annotation is expensive, cumbersome and often requires linguistic expertise from annotators. Under the proposed formulation, the coreference resolution has the same format as the existing question answering datasets (Rajpurkar et al., 2016a, 2018; Dasigi et al., 2019a). Those datasets can thus readily be used for data augmentation. We show that pre-training on existing question answering datasets improves the model’s generalization and 2 This is an illustration of the question formulation. The actual operation is described in Section 3.4. transferability, leading to additional performance boost. Experiments show that the proposed framework significantly outperforms previous models on two widely-used datasets. Specifically, we achieve new state-of-the-art scores of 83.1 (+3.5) on the CoNLL-2012 benchmark and 87.5 (+2.5) on the G"
2020.acl-main.622,P19-1485,0,0.0128017,"question answering format, and showed the benefits of training joint models for these tasks. Their models are built under the assumption that gold mentions are provided at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models. Data augmentation techniques have been explored in various fields such as question answering (Talmor and Berant, 2019), text classification (Kobayashi, 2018) and dialogue language understanding (Hou et al., 2018). In coreference resolution, Zhao et al. (2018); Emami et al. (2019); Zhao et al. (2019) focused on debiasing the gender bias problem; Aralikatte et al. (2019) explored the effectiveness of joint modeling of ellipsis and coreference resolution. To the best of our knowledge, we are the first to use existing question answering datasets as data augmentation for coreference resolution. 3 Model In this section, we describe our CorefQA model in detail. The overall architecture is illustrated in Figure 2. 3."
2020.acl-main.622,P10-1040,0,0.0302167,"ecedents kept for each mention C = 50. The SpanBERT parameters are updated by the Adam optimizer (Kingma and Ba, 2015) with initial learning rate 1 × 10−5 and the task parameters are updated by the Range optimizer 6 with initial learning rate 2 × 10−4 . 4.2 Baselines We compare the CorefQA model with previous neural models that are trained end-to-end: • e2e-coref (Lee et al., 2017) is the first endto-end coreference system that learns which spans are entity mentions and how to best cluster them jointly. Their token representations are built upon the GLoVe (Pennington et al., 2014) and Turian (Turian et al., 2010) embeddings. • c2f-coref + ELMo (Lee et al., 2018) extends Lee et al. (2017) by combining a coarse-tofine pruning with a higher-order inference mechanism. Their representations are built upon ELMo embeddings (Peters et al., 2018). • c2f-coref + BERT-large(Joshi et al., 2019b) builds the c2f-coref system on top of BERT (Devlin et al., 2019) token representations. • EE + BERT-large (Kantor and Globerson, 2019) represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. • c2f-coref + SpanBERT-large (Joshi et al., 2019a) focuses on pre-training span represe"
2020.acl-main.622,N18-2003,0,0.0236201,"mentions are provided at inference time, whereas our model does not need that assumption – it jointly trains the mention proposal model and the coreference resolution model in an end-to-end manner. 2.3 Data Augmentation Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models. Data augmentation techniques have been explored in various fields such as question answering (Talmor and Berant, 2019), text classification (Kobayashi, 2018) and dialogue language understanding (Hou et al., 2018). In coreference resolution, Zhao et al. (2018); Emami et al. (2019); Zhao et al. (2019) focused on debiasing the gender bias problem; Aralikatte et al. (2019) explored the effectiveness of joint modeling of ellipsis and coreference resolution. To the best of our knowledge, we are the first to use existing question answering datasets as data augmentation for coreference resolution. 3 Model In this section, we describe our CorefQA model in detail. The overall architecture is illustrated in Figure 2. 3.1 Notations Given a sequence of input tokens X = {x1 , x2 , ..., xn } in a document, where n denotes the length of the document. N = n ∗ (n +"
2020.acl-main.622,M95-1005,0,0.70145,"system on top of BERT (Devlin et al., 2019) token representations. • EE + BERT-large (Kantor and Globerson, 2019) represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. • c2f-coref + SpanBERT-large (Joshi et al., 2019a) focuses on pre-training span representations to better represent and predict spans of text. 4.3 Results on CoNLL-2012 Shared Task The English data of CoNLL-2012 shared task (Pradhan et al., 2012) contains 2,802/343/348 train/development/test documents in 7 different genres. The main evaluation is the average of three metrics – MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) on the test set according to the official CoNLL-2012 evaluation scripts 7 . 6 https://github.com/lessw2020/ Ranger-Deep-Learning-Optimizer 7 http://conll.cemantix.org/2012/ software.html We compare the CorefQA model with several baseline models in Table 1. Our CorefQA system achieves a huge performance boost over existing systems: With SpanBERT-base, it achieves an F1 score of 79.9, which already outperforms the previous SOTA model using SpanBERT-large by 0.3. With SpanBERT-large, it achieves an F1 score of 83.1, with a 3.5 performance boo"
2020.acl-main.622,Q18-1042,0,0.0295395,"according to the official CoNLL-2012 evaluation scripts 7 . 6 https://github.com/lessw2020/ Ranger-Deep-Learning-Optimizer 7 http://conll.cemantix.org/2012/ software.html We compare the CorefQA model with several baseline models in Table 1. Our CorefQA system achieves a huge performance boost over existing systems: With SpanBERT-base, it achieves an F1 score of 79.9, which already outperforms the previous SOTA model using SpanBERT-large by 0.3. With SpanBERT-large, it achieves an F1 score of 83.1, with a 3.5 performance boost over the previous SOTA system. 4.4 Results on GAP The GAP dataset (Webster et al., 2018) is a gender-balanced dataset that targets the challenges of resolving naturally occurring ambiguous pronouns. It comprises 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name) sampled from Wikipedia. We follow the protocols in Webster et al. (2018); Joshi et al. (2019b) and use the off-theshelf resolver trained on the CoNLL-2012 dataset to get the performance of the GAP dataset. Table 2 presents the results. We can see that the proposed CorefQA model achieves state-of-the-art performance on all metrics on the GAP dataset. 5 Ablation Study and Analysis We perform comprehensi"
2020.acl-main.622,N16-1114,0,0.241383,"Missing"
2020.acl-main.622,P15-1137,0,0.167055,"seman et al., 2016; Clark and Manning, 2015, 2016) rely on parsers and hand-engineered mention proposal algorithms. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019) tackled the problem in an end-to-end fashion by jointly detecting mentions and predicting coreferences. Based on how entity-level information is incorporated, they can be further categorized as (1) entity-level models (Bj¨orkelund and Kuhn, 2014; Clark and Manning, 2015, 2016; Wiseman et al., 2016) that directly model the representation of real-world entities and (2) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Lee et al., 2017) that learn to select the antecedent of each anaphoric mention. Our CorefQA model is essentially a mention-ranking model, but we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) sh"
2020.acl-main.622,P18-2017,0,0.142461,"ectly model the representation of real-world entities and (2) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Lee et al., 2017) that learn to select the antecedent of each anaphoric mention. Our CorefQA model is essentially a mention-ranking model, but we identify coreference using question answering. 2.2 Formalizing NLP Tasks as question answering Machine reading comprehension is a general and extensible task form. Many tasks in natural language processing can be framed as reading comprehension while abstracting away the taskspecific modeling constraints. McCann et al. (2018) introduced the decaNLP challenge, which converts a set of 10 core tasks in NLP to reading comprehension. He et al. (2015) showed that semantic role labeling annotations could be solicited by using question-answer pairs to represent the predicate-argument structure. Levy et al. (2017) reduced relation extraction to answering simple reading comprehension questions, yielding models that generalize better in the 6954 Mention Proposal Module I was hired to do some Christmas music, and it was just “Jingle Bells” and I brought my cat with me to the studio, and I was working on the song and the cat j"
2020.acl-main.622,P18-1205,0,0.0389047,"Missing"
2020.acl-main.622,N19-1064,0,0.0214523,"Missing"
2021.acl-long.161,C10-3004,0,0.053935,"Missing"
2021.acl-long.161,D18-1536,0,0.0159343,"nd THUCNews, the improvement from ChineseBERT is marginal as baselines have already achieved quite high results on these two datasets. On the TNEWS dataset, ChineseBERT outperforms all other models. We can see that the ERNIE model only performs slightly worse than ChineseBERT. This is because ERNIE is trained on additional web data, which is beneficial to model web news text that covers a wide range of domains. 5.4 Sentence Pair Matching (SPM) For SPM, the model is asked to determine whether a given sentence pair expresses the same semantics. We use the LCQMC (Liu et al., 2018) and BQ Corpus (Chen et al., 2018) datasets for evaluation. 8 https://github.com/pengming617/bert_ classification/tree/master/data 9 http://thuctc.thunlp.org/ 10 https://github.com/gaussic/ text-classification-cnn-rnn 11 https://github.com/CLUEbenchmark/CLUE 2070 Model ChnSentiCorp Dev Test ERNIE BERT BERT◦ RoBERTa◦ MacBERT ChineseBERT 95.4 95.1 95.4 95.0 95.2 95.6 95.5 95.4 95.3 95.6 95.6 95.7 RoBERTa◦ MacBERT ChineseBERT 95.8 95.7 95.8 95.8 95.9 95.9 THUCNews Dev Test Base 97.6 97.5 98.0 97.8 97.7 97.7 98.3 97.8 98.2 97.7 98.1 97.9 Large 98.3 97.8 98.1 97.9 98.3 97.9 TNEWS Dev Test 58.24 56.09 56.77 57.51 – 58.64 58.33 56.58"
2021.acl-long.161,Q16-1026,0,0.0435987,"aining/dev/test. BQ Corpus is another large-scale Chinese dataset containing 100K/10K/10K sentence pairs for training/dev/test. Results are shown in Table 6. We can see that ChineseBERT generally outperforms MacBERT on LCQMC but slightly underperforms BERT-wwm. We hypothesis this is because the domain of BQ Corpus more fits the pretraining data of BERTwwm than that of ChineseBERT. 5.5 OntoNotes 4.0 R F BERT RoBERTa◦ ChineseBERT BQ Corpus Dev Test Base 87.2 86.3 87.0 86.1 87.1 86.4 86.4 86.0 87.0 86.0 87.4 86.4 Large 87.0 86.3 87.6 86.2 87.8 86.5 P Named Entity Recognition (NER) For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task. We use OntoNotes 4.0 (Weischedel et al., 2011) and Weibo (Peng and Dredze, 2015) for this task. We use OntoNotes 4.0 and Weibo NER for this task. OntoNotes has 18 named entity types and Weibo has 4 named entity types. OntoNotes and Weibo respectively contain 15K/4K/4K and 1,350/270/270 instances for training/dev/test. Results are shown in Table 7. As we can see, ChineseBERT significantly outperforms BERT and RoBERTa in terms of F1. In"
2021.acl-long.161,D18-1269,0,0.0257898,"el CJRC Dev Test EM F1 EM F1 BERT BERT◦ RoBERTa◦ ChineseBERT 59.8 60.8 62.9 65.2 RoBERTa◦ ChineseBERT 65.6 66.5 Base 73.0 60.2 74.0 61.4 76.6 63.8 77.8 66.2 Large 77.5 66.4 77.9 67.0 73.0 73.9 76.6 77.9 77.6 78.3 Table 3: Performances of different models on the MRC dataset CJRC. We report results for baseline models based on their released models. ◦ represents models pretrained on extended data. 5.2 Natural Language Inference (NLI) The goal of NLI is to determine the entailment relationship between a hypothesis and a premise. We use the Cross-lingual Natural Language Inference (XNLI) dataset (Conneau et al., 2018) for evaluation. The corpus is a crowd-sourced collection of 5K test and 2.5K dev pairs for the MultiNLI corpus. Each sentence pair is annotated with the “entailment”, “neutral” or “contradiction” label. We use the official machine translated Chinese data for training.7 Results are present in Table 4, which shows that ChineseBERT is able to achieve the best performances for both base and large setups. 5.3 Text Classification (TC) In text classification the model is required to categorize a piece of text into one of the specified classes. We follow Cui et al. (2019a) to use THUC7 https://github"
2021.acl-long.161,2020.findings-emnlp.58,0,0.0626275,". We also use the dynamic masking strategy to avoid duplicate training instances (Liu et al., 2019b). We use https://pypi.org/project/pypinyin/ https://commoncrawl.org/ 5 2068 http://ltp.ai/ Data Source Vocab Size Input Unit Masking Task Training Steps Init Checkpoint # Token ERNIE BERT-wwm MacBERT ChineseBERT Heterogeneous 18K Char T/P/E MLM/NSP - Wikipedia 21K Char WWM MLM 2M BERT 0.4B Heterogeneous 21K Char WWM/N MAC/SOP 1M BERT 5.4B CommonCrawl 21K Char WWM/CM MLM 1M random 5B – Table 1: Comparison of data statistics between ERNIE (Sun et al., 2019), BERT-wwm (Cui et al., 2019a), MacBERT (Cui et al., 2020) and our proposed ChineseBERT. T: Token, P: Phrase, E: Entity, WWM: Whole Word Masking, N: N-gram, CM: Char Masking, MLM: Masked Language Model, NSP: Next Sentence Prediction, MAC: MLM-As-Correlation. SOP: Sentence Order Prediction. two model setups: base and large, respectively consisting of 12/24 Transformer layers, with input dimensionality of 768/1,024 and 12/16 heads per layer. This makes our models comparable to other BERT-style models in terms of model size. Upon the submission of the paper, we have trained the base model 500K steps with a maximum learning rate 1e-4, warmup of 20K steps"
2021.acl-long.161,D19-1600,0,0.0966287,"ce, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.1 1 Introduction Large-scale pretrained models have become a fundamental backbone for various natural language processing tasks such as natural language understanding (Liu et al., 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al., 2020). Apart from English NLP tasks, pretrained models have also demonstrated their effectiveness for various Chinese NLP tasks (Sun et al., 2019, 2020; Cui et al., 2019a, 2020). 1 The code and pretrained models are publicly available at https://github.com/ShannonAI/ChineseBert. Since pretraining models are originally designed for English, two important aspects specific to the Chinese language are missing in current large-scale pretraining: glyph-based information and pinyinbased information. For the former, a key aspect that makes Chinese distinguishable from languages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the"
2021.acl-long.161,W17-4109,0,0.0156613,"inguishable from languages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the radical “氵(water)”, which indicates that they are all related to water in semantics. Intuitively, the rich semantics behind Chinese character glyphs should enhance the expressiveness of Chinese NLP models. This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining. For the latter, pinyin, the Romanized sequence of a Chinese character representing its pronunciation(s), is crucial in modeling both semantic and syntax information that can not be captured by contextualized or glyph embeddings. This aspect is especially important considering the highly prevalent heteronym phenomenon in Chinese2 , where the same character have multiple pronunciations, each of which is associated with a specific meaning. Each pronunciation is associated with a specific pinyin expression. At the semanti"
2021.acl-long.161,2020.tacl-1.5,0,0.0255875,"the first large-scale Chinese Language Understanding Evaluation benchmark CLUE, facilitating researches in large-scale Chinese pretraining. Large-Scale Pretraining in NLP Recent years has witnessed substantial work on large-scale pretraining in NLP. BERT (Devlin et al., 2018), which is built on top of the Transformer architecture (Vaswani et al., 2017), is pretrained on large-scale unlabeled text corpus in the manner of Masked Language Model (MLM) and Next Sentence Prediction (NSP). Following this trend, considerable progress has been made by modifying the masking strategy (Yang et al., 2019; Joshi et al., 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al., 2020; Lample et al., 2019; Choromanski et al., 2020). Specifically, RoBERTa (Liu et al., 2019b) proposed to remove the NSP pretraining task since it has been proved to offer no benefits for improving down2.2 Learning Glyph Information Learning glyph information from surface Chinese character forms has gained attractions since the prevalence of deep neural networks. Inspired by word embeddings (Mikolov et al., 2013b,a), Sun et al. (2014); Shi et al. (2015); Li et al. (2015); Yin et al. (2016) used indexe"
2021.acl-long.161,N16-1030,0,0.025658,"us is another large-scale Chinese dataset containing 100K/10K/10K sentence pairs for training/dev/test. Results are shown in Table 6. We can see that ChineseBERT generally outperforms MacBERT on LCQMC but slightly underperforms BERT-wwm. We hypothesis this is because the domain of BQ Corpus more fits the pretraining data of BERTwwm than that of ChineseBERT. 5.5 OntoNotes 4.0 R F BERT RoBERTa◦ ChineseBERT BQ Corpus Dev Test Base 87.2 86.3 87.0 86.1 87.1 86.4 86.4 86.0 87.0 86.0 87.4 86.4 Large 87.0 86.3 87.6 86.2 87.8 86.5 P Named Entity Recognition (NER) For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task. We use OntoNotes 4.0 (Weischedel et al., 2011) and Weibo (Peng and Dredze, 2015) for this task. We use OntoNotes 4.0 and Weibo NER for this task. OntoNotes has 18 named entity types and Weibo has 4 named entity types. OntoNotes and Weibo respectively contain 15K/4K/4K and 1,350/270/270 instances for training/dev/test. Results are shown in Table 7. As we can see, ChineseBERT significantly outperforms BERT and RoBERTa in terms of F1. In spite of a slight los"
2021.acl-long.161,2020.acl-main.703,0,0.0808091,"Missing"
2021.acl-long.161,D07-1081,0,0.0605999,"Missing"
2021.acl-long.161,P19-1314,1,0.939104,"Brown et al., 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining. Unlike the English language, Chinese has its particular characteristics in terms of syntax, lexicon and pronunciation. Hence, pretraining Chinese models should fit the Chinese features correspondingly. Li et al. (2019b) proposed to use Chinese character as the basic unit instead of word or subword that is used in English (Wu et al., 2016; Sennrich et al., 2016). ERNIE (Sun et al., 2019, 2020) applied three types of masking strategies – charlevel masking, phrase-level masking and entitylevel masking – to enhance the ability of capturing multi-granularity semantics. Cui et al. (2019a, 2020) pretrained models using the Whole Word Masking strategy, where all characters within a Chinese word are masked altogether. In this way, the model is learning to address a more challenging task as opposed to predicting wor"
2021.acl-long.161,D15-1098,0,0.0300434,"trategy (Yang et al., 2019; Joshi et al., 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al., 2020; Lample et al., 2019; Choromanski et al., 2020). Specifically, RoBERTa (Liu et al., 2019b) proposed to remove the NSP pretraining task since it has been proved to offer no benefits for improving down2.2 Learning Glyph Information Learning glyph information from surface Chinese character forms has gained attractions since the prevalence of deep neural networks. Inspired by word embeddings (Mikolov et al., 2013b,a), Sun et al. (2014); Shi et al. (2015); Li et al. (2015); Yin et al. (2016) used indexed radical embeddings to capture character semantics, improving model performances on a wide range of Chinese NLP tasks. Another way of incorporating glyph information is to view characters in the form of image, by which glyph information can be naturally learned through image modeling. However, early work on learning visual features is not smooth. Liu et al. (2017); Shao et al. (2017); Zhang and LeCun (2017); Dai 2066 Fusion Layer 我 很 [M] [M] 猫 Char embedding 我 很 0 0 猫 Glyph embedding wo3 hen3 0 mao1 Pinyin Output embedding 我 很 [M] i very 0 Position embedding Fus"
2021.acl-long.161,P17-1188,0,0.0988083,"makes Chinese distinguishable from languages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the radical “氵(water)”, which indicates that they are all related to water in semantics. Intuitively, the rich semantics behind Chinese character glyphs should enhance the expressiveness of Chinese NLP models. This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining. For the latter, pinyin, the Romanized sequence of a Chinese character representing its pronunciation(s), is crucial in modeling both semantic and syntax information that can not be captured by contextualized or glyph embeddings. This aspect is especially important considering the highly prevalent heteronym phenomenon in Chinese2 , where the same character have multiple pronunciations, each of which is associated with a specific meaning. Each pronunciation is associated with a specific pinyin express"
2021.acl-long.161,P19-1441,0,0.152989,"rge-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks，including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.1 1 Introduction Large-scale pretrained models have become a fundamental backbone for various natural language processing tasks such as natural language understanding (Liu et al., 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al., 2020). Apart from English NLP tasks, pretrained models have also demonstrated their effectiveness for various Chinese NLP tasks (Sun et al., 2019, 2020; Cui et al., 2019a, 2020). 1 The code and pretrained models are publicly available at https://github.com/ShannonAI/ChineseBert. Since pretraining models are originally designed for English, two important aspects specific to the Chinese language are missing in current large-scale pretraining: glyph-based informat"
2021.acl-long.161,C18-1166,0,0.035119,"Missing"
2021.acl-long.161,2021.ccl-1.108,0,0.0887475,"Missing"
2021.acl-long.161,D15-1064,0,0.0310794,"erperforms BERT-wwm. We hypothesis this is because the domain of BQ Corpus more fits the pretraining data of BERTwwm than that of ChineseBERT. 5.5 OntoNotes 4.0 R F BERT RoBERTa◦ ChineseBERT BQ Corpus Dev Test Base 87.2 86.3 87.0 86.1 87.1 86.4 86.4 86.0 87.0 86.0 87.4 86.4 Large 87.0 86.3 87.6 86.2 87.8 86.5 P Named Entity Recognition (NER) For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task. We use OntoNotes 4.0 (Weischedel et al., 2011) and Weibo (Peng and Dredze, 2015) for this task. We use OntoNotes 4.0 and Weibo NER for this task. OntoNotes has 18 named entity types and Weibo has 4 named entity types. OntoNotes and Weibo respectively contain 15K/4K/4K and 1,350/270/270 instances for training/dev/test. Results are shown in Table 7. As we can see, ChineseBERT significantly outperforms BERT and RoBERTa in terms of F1. In spite of a slight loss on precision for the base version, the gains on recall are particularly high, leading to a final performance boost on F1. 5.6 Chinese Word Segmentation The task divides text into words and is formalized as a character-"
2021.acl-long.161,D19-1410,0,0.0240327,"roposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks，including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.1 1 Introduction Large-scale pretrained models have become a fundamental backbone for various natural language processing tasks such as natural language understanding (Liu et al., 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al., 2020). Apart from English NLP tasks, pretrained models have also demonstrated their effectiveness for various Chinese NLP tasks (Sun et al., 2019, 2020; Cui et al., 2019a, 2020). 1 The code and pretrained models are publicly available at https://github.com/ShannonAI/ChineseBert. Since pretraining models are originally designed for English, two important aspects specific to the Chinese language are missing in current large-scale pretraining: glyph-based information and pinyinbased information. For the former, a"
2021.acl-long.161,P16-1162,0,0.0960248,", 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining. Unlike the English language, Chinese has its particular characteristics in terms of syntax, lexicon and pronunciation. Hence, pretraining Chinese models should fit the Chinese features correspondingly. Li et al. (2019b) proposed to use Chinese character as the basic unit instead of word or subword that is used in English (Wu et al., 2016; Sennrich et al., 2016). ERNIE (Sun et al., 2019, 2020) applied three types of masking strategies – charlevel masking, phrase-level masking and entitylevel masking – to enhance the ability of capturing multi-granularity semantics. Cui et al. (2019a, 2020) pretrained models using the Whole Word Masking strategy, where all characters within a Chinese word are masked altogether. In this way, the model is learning to address a more challenging task as opposed to predicting word components. More recently, Zhang et al. (2020) developed the largest Chinese pretrained language model to date – CPM. It is pretrained on 100GB"
2021.acl-long.161,I17-1018,0,0.0236454,"inese character forms has gained attractions since the prevalence of deep neural networks. Inspired by word embeddings (Mikolov et al., 2013b,a), Sun et al. (2014); Shi et al. (2015); Li et al. (2015); Yin et al. (2016) used indexed radical embeddings to capture character semantics, improving model performances on a wide range of Chinese NLP tasks. Another way of incorporating glyph information is to view characters in the form of image, by which glyph information can be naturally learned through image modeling. However, early work on learning visual features is not smooth. Liu et al. (2017); Shao et al. (2017); Zhang and LeCun (2017); Dai 2066 Fusion Layer 我 很 [M] [M] 猫 Char embedding 我 很 0 0 猫 Glyph embedding wo3 hen3 0 mao1 Pinyin Output embedding 我 很 [M] i very 0 Position embedding Fusion embedding Char embedding Glyph embedding Pinyin embedding 我 Output 很 猫 mao1 猫 m a o 1 - - - - Fusion embedding 猫 m a o 1 - - - - 猫 Char embedding 0 猫 Glyph embedding 0 mao1 Pinyin embedding 很 0 hen3 0 很 [M] 3 [M] 猫 4 0 猫 hen3 0 0 mao1 cats māo and Cai (2017) used CNNs to extract glyph fea[M] images but 猫 tures [M] from character cats did not achieve consistent performance boost over all tasks. Su and Lee (2017)"
2021.acl-long.161,P15-2098,0,0.157078,"a key aspect that makes Chinese distinguishable from languages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the radical “氵(water)”, which indicates that they are all related to water in semantics. Intuitively, the rich semantics behind Chinese character glyphs should enhance the expressiveness of Chinese NLP models. This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining. For the latter, pinyin, the Romanized sequence of a Chinese character representing its pronunciation(s), is crucial in modeling both semantic and syntax information that can not be captured by contextualized or glyph embeddings. This aspect is especially important considering the highly prevalent heteronym phenomenon in Chinese2 , where the same character have multiple pronunciations, each of which is associated with a specific meaning. Each pronunciation is associated with a speci"
2021.acl-long.161,D17-1025,0,0.0716161,"nguages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the radical “氵(water)”, which indicates that they are all related to water in semantics. Intuitively, the rich semantics behind Chinese character glyphs should enhance the expressiveness of Chinese NLP models. This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining. For the latter, pinyin, the Romanized sequence of a Chinese character representing its pronunciation(s), is crucial in modeling both semantic and syntax information that can not be captured by contextualized or glyph embeddings. This aspect is especially important considering the highly prevalent heteronym phenomenon in Chinese2 , where the same character have multiple pronunciations, each of which is associated with a specific meaning. Each pronunciation is associated with a specific pinyin expression. At the semantic level, for examp"
2021.acl-long.161,D16-1100,0,0.0256835,"l., 2019; Joshi et al., 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al., 2020; Lample et al., 2019; Choromanski et al., 2020). Specifically, RoBERTa (Liu et al., 2019b) proposed to remove the NSP pretraining task since it has been proved to offer no benefits for improving down2.2 Learning Glyph Information Learning glyph information from surface Chinese character forms has gained attractions since the prevalence of deep neural networks. Inspired by word embeddings (Mikolov et al., 2013b,a), Sun et al. (2014); Shi et al. (2015); Li et al. (2015); Yin et al. (2016) used indexed radical embeddings to capture character semantics, improving model performances on a wide range of Chinese NLP tasks. Another way of incorporating glyph information is to view characters in the form of image, by which glyph information can be naturally learned through image modeling. However, early work on learning visual features is not smooth. Liu et al. (2017); Shao et al. (2017); Zhang and LeCun (2017); Dai 2066 Fusion Layer 我 很 [M] [M] 猫 Char embedding 我 很 0 0 猫 Glyph embedding wo3 hen3 0 mao1 Pinyin Output embedding 我 很 [M] i very 0 Position embedding Fusion embedding Char"
2021.emnlp-main.199,P05-1074,0,0.709273,"ning examples by corrupting a sentence and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2551–2562 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences, the generated paraphrases tend to be highly similar to the input sentences in terms of both wording and word orders. The issue in Hegde and Patil (2020) can be viewed as a microcosm of problems in existing unsupervised methods for par"
2021.emnlp-main.199,P14-1133,0,0.0378879,"model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2551–2562 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences, the generated paraphrases tend to be highly similar to the input sentences in terms of both wording and word orders. The issue in Hegde and Patil (2020) can be viewed as a microcosm of problems in existing unsupervised methods for paraphrase: we wish sentences to be diverse in expressions, but do not have a reliable measurement to avoid meaning change when expressions change. Add"
2021.emnlp-main.199,K16-1002,0,0.623381,"; Lan et al., 2017) are either of small sizes or restricted in narrow domains. For example, the Quora dataset1 contains 140K paraphrase pairs, the size of which is insufficient to build a large neural model. As another example, paraphrases in the larger MSCOCO (Lin et al., 2014) dataset are originally collected as image captions for object recognition, and repurposed for paraphrase generation. The domain for the MSCOCO dataset is thus restricted to captions depicting visual scenes. Unsupervised methods, such as reinforcement learning (Li et al., 2018; Siddique et al., 2020) and auto-encoders (Bowman et al., 2016; Roy and Grangier, 2019), on the other hand, have exhibited their ability for paraphrase generation in the absence of annotated datasets. The core problem with existing unsupervised methods for paraphrase is the lack of an objective (or reward function in RL) that reliably measures the semantic relatedness between two diverse expressions in an unsupervised manner, with which the model can be trained to promote pairs with the same meaning but diverse expressions. For example, Hegde and Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupte"
2021.emnlp-main.199,P11-1020,0,0.0606868,"nce and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2551–2562 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences, the generated paraphrases tend to be highly similar to the input sentences in terms of both wording and word orders. The issue in Hegde and Patil (2020) can be viewed as a microcosm of problems in existing unsupervised methods for paraphrase: we wish sentenc"
2021.emnlp-main.199,P19-1599,0,0.0273835,"Missing"
2021.emnlp-main.199,D17-1091,0,0.0177361,"ng the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; and (2) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the effectiveness of the proposed model in both supervised and unsupervised setups. 1 Introduction guage model pretraining (Lewis et al., 2020) and question answering (Dong et al., 2017). A long-standing challenge with paraphrase generation is to obtain reliable supervision signals. One way to resolve this issue is to manually annotate paraphrase pairs, which is both labor-intensive and expensive. Existing labeled paraphrase datasets (Lin et al., 2014; Fader et al., 2013; Lan et al., 2017) are either of small sizes or restricted in narrow domains. For example, the Quora dataset1 contains 140K paraphrase pairs, the size of which is insufficient to build a large neural model. As another example, paraphrases in the larger MSCOCO (Lin et al., 2014) dataset are originally collecte"
2021.emnlp-main.199,P13-1158,0,0.51059,"h the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the effectiveness of the proposed model in both supervised and unsupervised setups. 1 Introduction guage model pretraining (Lewis et al., 2020) and question answering (Dong et al., 2017). A long-standing challenge with paraphrase generation is to obtain reliable supervision signals. One way to resolve this issue is to manually annotate paraphrase pairs, which is both labor-intensive and expensive. Existing labeled paraphrase datasets (Lin et al., 2014; Fader et al., 2013; Lan et al., 2017) are either of small sizes or restricted in narrow domains. For example, the Quora dataset1 contains 140K paraphrase pairs, the size of which is insufficient to build a large neural model. As another example, paraphrases in the larger MSCOCO (Lin et al., 2014) dataset are originally collected as image captions for object recognition, and repurposed for paraphrase generation. The domain for the MSCOCO dataset is thus restricted to captions depicting visual scenes. Unsupervised methods, such as reinforcement learning (Li et al., 2018; Siddique et al., 2020) and auto-encoders ("
2021.emnlp-main.199,N13-1092,0,0.111672,"Missing"
2021.emnlp-main.199,2020.acl-main.22,0,0.0139533,"proposed diverse paraphrasing by warping the input’s meaning through attribute transfer. Regarding soliciting large-scale paraphrase datasets, Bannard and Callison-Burch (2005b) used statistical machine translation methods obtain 2 Related Work paraphrases in parallel text, the technique of Supervised Methods for paraphrase generation which is scaled up by Ganitkevitch et al. (2013) to rely on annotated paraphrase pairs to train the produce the Paraphrase Database (PPDB). Wieting model. Iyyer et al. (2018); Li et al. (2019); Chen et al. (2017) translate the non-English side of et al. (2019); Goyal and Durrett (2020) leveraged parallel text to obtain paraphrase pairs. Wieting syntactic structures to generate diverse paraphrases and Gimpel (2017) collected paraphrase dataset with different syntax. Xu et al. (2018); Qian with million of pairs via machine translation. et al. (2019) used different semantic embeddings or Hu et al. (2019a,b) produced paraphrases from generators to produce more diverse paraphrases. a bilingual corpus based on the techniques of Kazemnejad et al. (2020) proposed a retrieval- negative constraints, inference sampling, and based approach to retrieve paraphrase from a large clustering"
2021.emnlp-main.199,N19-1090,0,0.0218403,"Missing"
2021.emnlp-main.199,N16-1014,1,0.832277,"sentenceposition embedding, token-position embedding and the word embedding. Predicting ci follows a wordby-word fashion. We consider the style of both left-to-right generation and right-to-left generation to optimize p(ci |c&lt;i , c>i ), which is respectively given by the following objective: − p(→ c i |c&lt;i , c>i ) = n Y p(wi,j |c&lt;i , c>i , wi,&lt;j ) j=1 − p(← c i |c&lt;i , c>i ) = 1 Y j=n p(wi,j |c&lt;i , c>i , wi,>j ) same, which correspond to the backward probability given from sentences to contexts. This is akin to the bi-directional mutual-information based generation strategy (Fang et al., 2015; Li et al., 2016a; Li and Jurafsky, 2016; Wang et al., 2021). The backward probability can be modeled by predicting preceding contexts given subsequent contexts p(c&lt;i |ci , c>i ) and to predict subsequent contexts given preceding contexts p(c>i |c&lt;i , ci ). We implement the above models, i.e. − − p(→ c i |c&lt;i , c>i ), p(← c i |c&lt;i , c>i ), p(c&lt;i |ci , c>i ), p(c>i |c&lt;i , ci ) based on the S EQ 2S EQ structure on a subset of CommonCrawl containing 10 billion tokens in total. We use Transformers as the backbone (Vaswani et al., 2017)2 with the number of encoder blocks, decoder blocks, the number of heads, dmode"
2021.emnlp-main.199,D18-1421,0,0.0346699,"Missing"
2021.emnlp-main.199,N18-1170,0,0.0178965,"ning (Witteveen and Andrews, 2019) and unsupervised learning (Hegde and Patil, 2020). Krishna et al. (2020) proposed diverse paraphrasing by warping the input’s meaning through attribute transfer. Regarding soliciting large-scale paraphrase datasets, Bannard and Callison-Burch (2005b) used statistical machine translation methods obtain 2 Related Work paraphrases in parallel text, the technique of Supervised Methods for paraphrase generation which is scaled up by Ganitkevitch et al. (2013) to rely on annotated paraphrase pairs to train the produce the Paraphrase Database (PPDB). Wieting model. Iyyer et al. (2018); Li et al. (2019); Chen et al. (2017) translate the non-English side of et al. (2019); Goyal and Durrett (2020) leveraged parallel text to obtain paraphrase pairs. Wieting syntactic structures to generate diverse paraphrases and Gimpel (2017) collected paraphrase dataset with different syntax. Xu et al. (2018); Qian with million of pairs via machine translation. et al. (2019) used different semantic embeddings or Hu et al. (2019a,b) produced paraphrases from generators to produce more diverse paraphrases. a bilingual corpus based on the techniques of Kazemnejad et al. (2020) proposed a retrie"
2021.emnlp-main.199,P19-1332,0,0.0623738,"ndrews, 2019) and unsupervised learning (Hegde and Patil, 2020). Krishna et al. (2020) proposed diverse paraphrasing by warping the input’s meaning through attribute transfer. Regarding soliciting large-scale paraphrase datasets, Bannard and Callison-Burch (2005b) used statistical machine translation methods obtain 2 Related Work paraphrases in parallel text, the technique of Supervised Methods for paraphrase generation which is scaled up by Ganitkevitch et al. (2013) to rely on annotated paraphrase pairs to train the produce the Paraphrase Database (PPDB). Wieting model. Iyyer et al. (2018); Li et al. (2019); Chen et al. (2017) translate the non-English side of et al. (2019); Goyal and Durrett (2020) leveraged parallel text to obtain paraphrase pairs. Wieting syntactic structures to generate diverse paraphrases and Gimpel (2017) collected paraphrase dataset with different syntax. Xu et al. (2018); Qian with million of pairs via machine translation. et al. (2019) used different semantic embeddings or Hu et al. (2019a,b) produced paraphrases from generators to produce more diverse paraphrases. a bilingual corpus based on the techniques of Kazemnejad et al. (2020) proposed a retrieval- negative cons"
2021.emnlp-main.199,2020.acl-main.535,0,0.0805729,"(PPDB). Wieting model. Iyyer et al. (2018); Li et al. (2019); Chen et al. (2017) translate the non-English side of et al. (2019); Goyal and Durrett (2020) leveraged parallel text to obtain paraphrase pairs. Wieting syntactic structures to generate diverse paraphrases and Gimpel (2017) collected paraphrase dataset with different syntax. Xu et al. (2018); Qian with million of pairs via machine translation. et al. (2019) used different semantic embeddings or Hu et al. (2019a,b) produced paraphrases from generators to produce more diverse paraphrases. a bilingual corpus based on the techniques of Kazemnejad et al. (2020) proposed a retrieval- negative constraints, inference sampling, and based approach to retrieve paraphrase from a large clustering. A relevant work to ours is Sun et al. corpus. Mallinson et al. (2017); Sokolov and Fil- (2021), which harnesses context to obtain sentence imonov (2020) casted paraphrase generation as the similarity. Sun et al. (2021) focuses on sentence 2552 similarity rather than paraphrase generation. 3 Model The key point of the proposed paradigm is to generate paraphrases based on the same context. This can be done in the following pipelined system: (1) we first train a cont"
2021.emnlp-main.199,2020.emnlp-main.55,0,0.0856333,"Missing"
2021.emnlp-main.199,D17-1126,0,0.0894794,"n process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the effectiveness of the proposed model in both supervised and unsupervised setups. 1 Introduction guage model pretraining (Lewis et al., 2020) and question answering (Dong et al., 2017). A long-standing challenge with paraphrase generation is to obtain reliable supervision signals. One way to resolve this issue is to manually annotate paraphrase pairs, which is both labor-intensive and expensive. Existing labeled paraphrase datasets (Lin et al., 2014; Fader et al., 2013; Lan et al., 2017) are either of small sizes or restricted in narrow domains. For example, the Quora dataset1 contains 140K paraphrase pairs, the size of which is insufficient to build a large neural model. As another example, paraphrases in the larger MSCOCO (Lin et al., 2014) dataset are originally collected as image captions for object recognition, and repurposed for paraphrase generation. The domain for the MSCOCO dataset is thus restricted to captions depicting visual scenes. Unsupervised methods, such as reinforcement learning (Li et al., 2018; Siddique et al., 2020) and auto-encoders (Bowman et al., 2016"
2021.emnlp-main.199,W04-1013,0,0.023143,"ointer, Transformer on various datasets are copied from Li et al. (2019). For reference purposes, we also implement the BT baseline inspired by the idea of back-translation (Sennrich et al., 2016; Wieting et al., 2017). We use Transformer-large as the backbone. BT is trained end-to-end on WMT’14 En↔Fr.6 A paraphrase pair is obtained by pairing the English sentence in the original dataset and the translation of the French sentence. Next we train a Transformer-large model on paraphrase pairs. We evaluate all models using BLEU (Papineni et al., 2002), iBLEU (Sun and Zhou, 2012) and ROUGE scores (Lin, 2004) . The iBLEU score penalizes the similarity of the generated paraphrase with respect to the original input sentence. Concretely, the iBLEU score of a triple of sentences (s, r, c) is given by: iBLEU(s, r, c) = αBLEU(c, r) − (1 − α)BLEU(c, s) (6) where s is the input sentence, r is the reference paraphrase and c is generated paraphrase. α is set to 0.8 following prior works. 4.3 In-domain Results We first show the in-domain results in Table 1. As can be seen, across all datasets, the proposed ConRPG model significantly outperforms baselines in 6 Wieting et al. (2017); Wieting and Gimpel (2017)"
2021.emnlp-main.199,N18-1018,0,0.02237,"ack of an objective (or reward function in RL) that reliably measures the semantic relatedness between two diverse expressions in an unsupervised manner, with which the model can be trained to promote pairs with the same meaning but diverse expressions. For example, Hegde and Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Proc"
2021.emnlp-main.199,E17-1083,0,0.101338,"offers the following merits over existing methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; and (2) using human-interpretable ranking scores to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable paraphrase model. Extensive experiments across different datasets under both supervised and unsupervised setups demonstrate the effectiveness of the proposed model. task of machine translation. Mallinson et al. (2017); Wieting et al. (2017) extended the idea of bilingual pivoting for paraphrase generation where the input sentence is first translated into a foreign language, and then translated back as the paraphrase. Sokolov and Filimonov (2020) trained a MT model using multilingual parallel data and then finetuned the model using parallel paraphrase data. Unsupervised Methods Li et al. (2018); Siddique et al. (2020) proposed to generate paraphrases using reinforcement learning, where certain rewarding criteria such as BLEU and ROUGE are optimized. Bowman et al. (2016); Yang et al. (2019) used the generati"
2021.emnlp-main.199,P79-1016,0,0.585449,"ple, Hegde and Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2551–2562 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences, the generated paraphrases tend to be highly similar to the input sentences in terms of both wording and word orders. The issue in Hegde and Patil (2020) ca"
2021.emnlp-main.199,C88-2088,0,0.760887,"Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2551–2562 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences, the generated paraphrases tend to be highly similar to the input sentences in terms of both wording and word orders. The issue in Hegde and Patil (2020) can be viewed as a microcos"
2021.emnlp-main.199,P02-1040,0,0.116226,"ble in the statistical sense. Results for ResidualLSTM, VAE-SVG-eq, Pointer, Transformer on various datasets are copied from Li et al. (2019). For reference purposes, we also implement the BT baseline inspired by the idea of back-translation (Sennrich et al., 2016; Wieting et al., 2017). We use Transformer-large as the backbone. BT is trained end-to-end on WMT’14 En↔Fr.6 A paraphrase pair is obtained by pairing the English sentence in the original dataset and the translation of the French sentence. Next we train a Transformer-large model on paraphrase pairs. We evaluate all models using BLEU (Papineni et al., 2002), iBLEU (Sun and Zhou, 2012) and ROUGE scores (Lin, 2004) . The iBLEU score penalizes the similarity of the generated paraphrase with respect to the original input sentence. Concretely, the iBLEU score of a triple of sentences (s, r, c) is given by: iBLEU(s, r, c) = αBLEU(c, r) − (1 − α)BLEU(c, s) (6) where s is the input sentence, r is the reference paraphrase and c is generated paraphrase. α is set to 0.8 following prior works. 4.3 In-domain Results We first show the in-domain results in Table 1. As can be seen, across all datasets, the proposed ConRPG model significantly outperforms baselin"
2021.emnlp-main.199,C16-1275,0,0.302554,"upervised methods for paraphrase is the lack of an objective (or reward function in RL) that reliably measures the semantic relatedness between two diverse expressions in an unsupervised manner, with which the model can be trained to promote pairs with the same meaning but diverse expressions. For example, Hegde and Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupted one to a pretrained model as the input with the original sentence as the output. Since the model is restricted to learning to reconstruct corrupted Paraphrase generation (Prakash et al., 2016a; Cao et al., 2016; Ma et al., 2018; Wang et al., 2018) is the task of generating an output sentence which is semantically identical to a given input sentence but with variations in lexicon or syntax. It is a long-standing problem in the field of natural language processing (NLP) (McKeown, 1979; Meteer and Shaked, 1988; Quirk et al., 2004; Bannard and Callison-Burch, 2005a; Chen and Dolan, 2011) and 1 has fundamental applications on end tasks such as https://www.kaggle.com/c/ semantic parsing (Berant and Liang, 2014), lan- quora-question-pairs 2551 Proceedings of the 2021 Conference on Empiri"
2021.emnlp-main.199,D19-1313,0,0.0294827,"Missing"
2021.emnlp-main.199,W04-3219,0,0.381934,"Missing"
2021.emnlp-main.199,P19-1605,0,0.129974,"re either of small sizes or restricted in narrow domains. For example, the Quora dataset1 contains 140K paraphrase pairs, the size of which is insufficient to build a large neural model. As another example, paraphrases in the larger MSCOCO (Lin et al., 2014) dataset are originally collected as image captions for object recognition, and repurposed for paraphrase generation. The domain for the MSCOCO dataset is thus restricted to captions depicting visual scenes. Unsupervised methods, such as reinforcement learning (Li et al., 2018; Siddique et al., 2020) and auto-encoders (Bowman et al., 2016; Roy and Grangier, 2019), on the other hand, have exhibited their ability for paraphrase generation in the absence of annotated datasets. The core problem with existing unsupervised methods for paraphrase is the lack of an objective (or reward function in RL) that reliably measures the semantic relatedness between two diverse expressions in an unsupervised manner, with which the model can be trained to promote pairs with the same meaning but diverse expressions. For example, Hegde and Patil (2020) crafted unsupervised pseudo training examples by corrupting a sentence and then fed the corrupted one to a pretrained mod"
2021.emnlp-main.199,P17-1099,0,0.0184719,"ely used for valgeneration process, as developers can develop their idation and test. own scoring functions to generate paraphrases of 4 specific features. This leads to a more controllable https://www.kaggle.com/c/ paraphrase model. quora-question-pairs 2555 • ResidualLSTM: Prakash et al. (2016b) deepened the LSTM network by stacking multiple layers with residual connection. • VAE-SVG-eq: Gupta et al. (2018) combined VAEs with LSTMs for paraphrase generation. Both encoder and decoder are conditioned on the source input sentence so that more consistent paraphrases can be generated. • Pointer: See et al. (2017) augmented the standard S EQ 2S EQ model by using a pointer mechanism which can copy source words in the input rather than decode from scratch. • Transformer: Vaswani et al. (2017) proposed the Transformer architecture which is based on the self-attention mechanism. • DNPG: Li et al. (2019) proposed a Transformer-based model that can learn and generate paraphrases at different granularities. • Wikianswers: The Wikianswers dataset (Fader et al., 2013) contains 2.3M paraphrase pairs extracted from the Wikianswers website. We follow Liu et al. (2019) to randomly pick 5K pairs for validation and 2"
2021.emnlp-main.199,P16-1009,0,0.0971345,"Missing"
2021.emnlp-main.199,P12-2008,0,0.0185129,"esults for ResidualLSTM, VAE-SVG-eq, Pointer, Transformer on various datasets are copied from Li et al. (2019). For reference purposes, we also implement the BT baseline inspired by the idea of back-translation (Sennrich et al., 2016; Wieting et al., 2017). We use Transformer-large as the backbone. BT is trained end-to-end on WMT’14 En↔Fr.6 A paraphrase pair is obtained by pairing the English sentence in the original dataset and the translation of the French sentence. Next we train a Transformer-large model on paraphrase pairs. We evaluate all models using BLEU (Papineni et al., 2002), iBLEU (Sun and Zhou, 2012) and ROUGE scores (Lin, 2004) . The iBLEU score penalizes the similarity of the generated paraphrase with respect to the original input sentence. Concretely, the iBLEU score of a triple of sentences (s, r, c) is given by: iBLEU(s, r, c) = αBLEU(c, r) − (1 − α)BLEU(c, s) (6) where s is the input sentence, r is the reference paraphrase and c is generated paraphrase. α is set to 0.8 following prior works. 4.3 In-domain Results We first show the in-domain results in Table 1. As can be seen, across all datasets, the proposed ConRPG model significantly outperforms baselines in 6 Wieting et al. (2017"
2021.emnlp-main.199,D19-5623,0,0.0492047,"mpled through the VAE’s decoder can be regarded as paraphrases for an input sentence due to the reconstruction optimization target. Fu et al. (2019) similarly adopted a generative method but worked at the bagof-words level. Other works explored paraphrase generation in an unsupervised manner by using vector quantised VAE (VQ-VAE) (Roy and Grangier, 2019), simulated annealing (Liu et al., 2019) or disentangled syntactic and semantic spaces (Bao et al., 2019). More recently, large-scale language model pretraining has also been proven to benefit paraphrase generation in both supervised learning (Witteveen and Andrews, 2019) and unsupervised learning (Hegde and Patil, 2020). Krishna et al. (2020) proposed diverse paraphrasing by warping the input’s meaning through attribute transfer. Regarding soliciting large-scale paraphrase datasets, Bannard and Callison-Burch (2005b) used statistical machine translation methods obtain 2 Related Work paraphrases in parallel text, the technique of Supervised Methods for paraphrase generation which is scaled up by Ganitkevitch et al. (2013) to rely on annotated paraphrase pairs to train the produce the Paraphrase Database (PPDB). Wieting model. Iyyer et al. (2018); Li et al. (20"
2021.emnlp-main.199,D19-1309,0,0.0183624,"translation. Mallinson et al. (2017); Wieting et al. (2017) extended the idea of bilingual pivoting for paraphrase generation where the input sentence is first translated into a foreign language, and then translated back as the paraphrase. Sokolov and Filimonov (2020) trained a MT model using multilingual parallel data and then finetuned the model using parallel paraphrase data. Unsupervised Methods Li et al. (2018); Siddique et al. (2020) proposed to generate paraphrases using reinforcement learning, where certain rewarding criteria such as BLEU and ROUGE are optimized. Bowman et al. (2016); Yang et al. (2019) used the generative framework for paraphrase generation by training a variational autoencoder (VAE) (Kingma and Welling, 2013) to optimize the lower bound of the reconstruction likelihood for an input sentence. Sentences sampled through the VAE’s decoder can be regarded as paraphrases for an input sentence due to the reconstruction optimization target. Fu et al. (2019) similarly adopted a generative method but worked at the bagof-words level. Other works explored paraphrase generation in an unsupervised manner by using vector quantised VAE (VQ-VAE) (Roy and Grangier, 2019), simulated annealin"
2021.emnlp-main.246,D15-1075,0,0.0194988,"Base Tiny 2,048 1,024 512 256 8,192 4,096 2,048 1,024 8 6 6 6 16 16 8 8 1.1B 275M 93M 35M Table 1: Model statistics. dmodel , dff , L and H respectively denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answering (Rajpurkar et al., 2016, 2018), natural language inference (Bowman et al., 2015; Williams et al., 2017) and text classification (Socher et al., 2013; Tang et al., 2014; Howard and Ruder, 2018; Chai et al., 2020; Lin et al., 2021). We use BERT (Devlin et al., 2018) as the backbone, and fine-tune BERT on different datasets. Adam (Kingma and Ba, 2014) is used for all models, with batch size, learning rate and the number of epochs treated as hyper-parameters to be tuned on the dev set. We compare the proposed strategy with the following weight based pruning models: • Magnitude Pruning (Han et al., 2015b): removing weights based on their absolute weight values. • Movement Pru"
2021.emnlp-main.246,D18-1045,0,0.033162,"Missing"
2021.emnlp-main.246,2020.repl4nlp-1.18,0,0.105356,"smaller but dense counterparts, leading to greater speedup; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning). 1 (Joulin et al., 2016; Ganesh et al., 2020; Gordon et al., 2020). Among pruning techniques, weight based pruning is a widely-used group of methods. It focuses on removing weights according to their importance under different specific criteria, e.g., the magnitude (Han et al., 2015b,a), first-order derivative (Lee et al., 2018; Sanh et al., 2020) and second-order derivative information (LeCun et al., 1990; Hassibi and Stork, 1993), and it has been successfully applied to a large variety of model architectures (Guo et al., 2016; Gale et al., 2019; Molchanov et al., 2019) and downstream tasks (McCarley, 2019; Gordon et al., 2020). While weight-based methods h"
2021.emnlp-main.246,P18-1031,0,0.01713,"istics. dmodel , dff , L and H respectively denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answering (Rajpurkar et al., 2016, 2018), natural language inference (Bowman et al., 2015; Williams et al., 2017) and text classification (Socher et al., 2013; Tang et al., 2014; Howard and Ruder, 2018; Chai et al., 2020; Lin et al., 2021). We use BERT (Devlin et al., 2018) as the backbone, and fine-tune BERT on different datasets. Adam (Kingma and Ba, 2014) is used for all models, with batch size, learning rate and the number of epochs treated as hyper-parameters to be tuned on the dev set. We compare the proposed strategy with the following weight based pruning models: • Magnitude Pruning (Han et al., 2015b): removing weights based on their absolute weight values. • Movement Pruning (Sanh et al., 2020): removing weights based on the first-order derivative. • L0 Pruning (Louizos et al., 20"
2021.emnlp-main.246,2020.findings-emnlp.372,0,0.0828652,"Missing"
2021.emnlp-main.246,2021.findings-acl.126,1,0.682689,"denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answering (Rajpurkar et al., 2016, 2018), natural language inference (Bowman et al., 2015; Williams et al., 2017) and text classification (Socher et al., 2013; Tang et al., 2014; Howard and Ruder, 2018; Chai et al., 2020; Lin et al., 2021). We use BERT (Devlin et al., 2018) as the backbone, and fine-tune BERT on different datasets. Adam (Kingma and Ba, 2014) is used for all models, with batch size, learning rate and the number of epochs treated as hyper-parameters to be tuned on the dev set. We compare the proposed strategy with the following weight based pruning models: • Magnitude Pruning (Han et al., 2015b): removing weights based on their absolute weight values. • Movement Pruning (Sanh et al., 2020): removing weights based on the first-order derivative. • L0 Pruning (Louizos et al., 2017): using the L0 loss to regularize t"
2021.emnlp-main.246,P18-2124,0,0.0207746,"Missing"
2021.emnlp-main.246,D16-1264,0,0.0839632,"ter performances at the same sparsity level. We conduct extensive experiments on both generative tasks (MT) and discriminative tasks (question answering) in NLP to examine the effectiveness of the proposed strategy. We show that compared to weight-based pruning methods including magnitude pruning (Han et al., 2015b), movement pruning (Sanh et al., 2020) and L0 pruning (Louizos et al., 2017), the proposed method yields greater speedup along with better performances for the same sparsity levels on generative NLP tasks of WMT’14 En→Fr and WMT’14 En→De, and discriminative NLP tasks of SQuAD v1.1 (Rajpurkar et al., 2016), MNLI (Williams et al., 2017) and SST5 (Socher et al., 2013). In addition, we also show that the proposed method serves the feature selection purposes, where we observe significant performance boosts when fixing preserved neurons and relearning the pruned ones, leading to a state-of-theart performance of 43.9 BLEU score for En→Fr translation in setups without back-translation or external data. 2 2.1 Related Work Model Pruning Han et al. (2015b) removed all parameters with weight values below a threshold, and then retrained the remaining sparse network. Guo et al. (2016) proposed dynamic netwo"
2021.emnlp-main.246,K16-1029,0,0.0228998,"g refers 2.2 Mutual Information Feature Selection to reducing the model size by dropping a fraction of the model parameters, which dates back to early Feature selection is the process of selecting a works of Optimal Brain Damage (PBD) (LeCun proper subset of features for better model perforet al., 1990) and Optimal Brain Surgeon (OBS) mances (Kira and Rendell, 1992; Guyon and Elisse(Hassibi and Stork, 1993). One major branch of eff, 2003; Chandrashekar and Sahin, 2014; Bolónneural model pruning methods is magnitude prun- Canedo et al., 2016; Cai et al., 2018). A widely ing (Han et al., 2015b; See et al., 2016; Narang used method for feature selection is Mutual Inforet al., 2017; Molchanov et al., 2019; Gale et al., mation Based Feature Selection (Vergara and Es2019; Frankle et al., 2020), which prunes model tévez, 2014; Liu et al., 2009; Beraha et al., 2019), parameters measured by their importance scores. which selects features that minimize the redun3080 dancy and maximize the relevance w.r.t. the target variable. Various approaches including minimumRedundancy-Maximum-Relevance (mRMR) (Estévez et al., 2009; Brown et al., 2012; Bennasar et al., 2015) are proposed to accurately select features. 3"
2021.emnlp-main.246,P16-1009,0,0.0345523,"or L2 regularizers for feature selection (Ng, 2004; Ravikumar et al., 2010). MI-based pruning method is comparable to MI based feature selection, which attaches attentions to the features by measuring feature-label correlations (Kuncheva, 2007; Yu et al., 2008). 4 Experiments We conduct experiments on both generative and discriminative NLP tasks. For generative tasks, we conduct experiments on WMT14 En-Fr and WMT14 En-DE. The WMT14 En-Fr dataset consist of 36M and is split into 32000 word-piece vocabulary. The WMT 2014 En-DE dataset consisting of about 4.5 million sentence pairs. We use BPE (Sennrich et al., 2016b) to maintain a sourcetarget vocabulary of 37,000. We use Transformers (Vaswani et al., 2017) as the model backbone. We use En-Fr to perform comprehensive analysis where we use four model setups: extra-large, large, base and tiny. The model statistics are shown in Table 1. It is worth noting that the large and base models are identical to models in Vaswani et al. (2017). We train different models with 16 V100 GPUs with 32G memories. We follow protocols in Vaswani et al. (2017). Adam (Kingma and Ba, 2014) is used for all models with β1 = 0.9, β2 = 0.98 and  = 10−6 . A dropout rate of 0.1 is a"
2021.emnlp-main.246,P16-1162,0,0.0417262,"or L2 regularizers for feature selection (Ng, 2004; Ravikumar et al., 2010). MI-based pruning method is comparable to MI based feature selection, which attaches attentions to the features by measuring feature-label correlations (Kuncheva, 2007; Yu et al., 2008). 4 Experiments We conduct experiments on both generative and discriminative NLP tasks. For generative tasks, we conduct experiments on WMT14 En-Fr and WMT14 En-DE. The WMT14 En-Fr dataset consist of 36M and is split into 32000 word-piece vocabulary. The WMT 2014 En-DE dataset consisting of about 4.5 million sentence pairs. We use BPE (Sennrich et al., 2016b) to maintain a sourcetarget vocabulary of 37,000. We use Transformers (Vaswani et al., 2017) as the model backbone. We use En-Fr to perform comprehensive analysis where we use four model setups: extra-large, large, base and tiny. The model statistics are shown in Table 1. It is worth noting that the large and base models are identical to models in Vaswani et al. (2017). We train different models with 16 V100 GPUs with 32G memories. We follow protocols in Vaswani et al. (2017). Adam (Kingma and Ba, 2014) is used for all models with β1 = 0.9, β2 = 0.98 and  = 10−6 . A dropout rate of 0.1 is a"
2021.emnlp-main.246,D13-1170,0,0.0177786,"e experiments on both generative tasks (MT) and discriminative tasks (question answering) in NLP to examine the effectiveness of the proposed strategy. We show that compared to weight-based pruning methods including magnitude pruning (Han et al., 2015b), movement pruning (Sanh et al., 2020) and L0 pruning (Louizos et al., 2017), the proposed method yields greater speedup along with better performances for the same sparsity levels on generative NLP tasks of WMT’14 En→Fr and WMT’14 En→De, and discriminative NLP tasks of SQuAD v1.1 (Rajpurkar et al., 2016), MNLI (Williams et al., 2017) and SST5 (Socher et al., 2013). In addition, we also show that the proposed method serves the feature selection purposes, where we observe significant performance boosts when fixing preserved neurons and relearning the pruned ones, leading to a state-of-theart performance of 43.9 BLEU score for En→Fr translation in setups without back-translation or external data. 2 2.1 Related Work Model Pruning Han et al. (2015b) removed all parameters with weight values below a threshold, and then retrained the remaining sparse network. Guo et al. (2016) proposed dynamic network surgery, allowing for model connection recovery from incor"
2021.emnlp-main.246,P19-1355,0,0.0293689,"spite of impressive results of neural networks, and logistic regression, we propose MI based layerthe huge model size has hindered their applications wise pruning, to address the aforementioned drawin cases where computation and memory resources backs of weight-based pruning methods in NLP. are limited.1 As a result, training and using existFor each layer of a multi-layer neural network, neuing huge models not only requires rich hardware rons with higher values of MI with respect to the resources, but also consumes high environmental preserved neurons in the upper layer are preserved. costs (Strubell et al., 2019). Starting from the top softmax layer, layer-wise pruning proceeds until reaching the bottom input Model pruning, reduces model sizes by dropping a fraction of the model parameters, to reduce com- word embedding layer in a top-down fashion. Once the preserved neurons in each layer are selected, the putation intensity and memory footprint of large redundant dimensions along with the correspondmodels at the lowest cost of accuracy on end tasks ing rows and columns of the weight matrices can 1 For example, the GPT-3 model (Brown et al., 2020) has be pruned or squeezed, inducing model sparsity at"
2021.emnlp-main.246,2021.acl-long.161,1,0.722321,"et al., same size as the model before pruning. We as view 2021; Zheng et al., 2021) are used. 3083 Model dmodel dff L H # Params Extra-Large Large Base Tiny 2,048 1,024 512 256 8,192 4,096 2,048 1,024 8 6 6 6 16 16 8 8 1.1B 275M 93M 35M Table 1: Model statistics. dmodel , dff , L and H respectively denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answering (Rajpurkar et al., 2016, 2018), natural language inference (Bowman et al., 2015; Williams et al., 2017) and text classification (Socher et al., 2013; Tang et al., 2014; Howard and Ruder, 2018; Chai et al., 2020; Lin et al., 2021). We use BERT (Devlin et al., 2018) as the backbone, and fine-tune BERT on different datasets. Adam (Kingma and Ba, 2014) is used for all models, with batch size, learning rate and the number of epochs treated as hyper-parameters to be tuned on the dev set. We compare the proposed strategy with the f"
2021.emnlp-main.246,P14-1146,0,0.0172005,"Table 1: Model statistics. dmodel , dff , L and H respectively denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answering (Rajpurkar et al., 2016, 2018), natural language inference (Bowman et al., 2015; Williams et al., 2017) and text classification (Socher et al., 2013; Tang et al., 2014; Howard and Ruder, 2018; Chai et al., 2020; Lin et al., 2021). We use BERT (Devlin et al., 2018) as the backbone, and fine-tune BERT on different datasets. Adam (Kingma and Ba, 2014) is used for all models, with batch size, learning rate and the number of epochs treated as hyper-parameters to be tuned on the dev set. We compare the proposed strategy with the following weight based pruning models: • Magnitude Pruning (Han et al., 2015b): removing weights based on their absolute weight values. • Movement Pruning (Sanh et al., 2020): removing weights based on the first-order derivative. • L0 Pru"
2021.emnlp-main.246,2021.acl-short.47,0,0.0220831,"erformances of different prundimensions. It is worth noting that the strategy of ing techniques in the vanilla supervised setup, no advanced retraining pruned dimensions does not serve as the MT techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), self-learning (He et al., 2020; Sun et al., goal of speedup and model compressing, as pruned 2020), data noising (Xie et al., 2017; Bengio et al., 2015), dimensions are relearned, making the model of the nearest neighbor search (Khandelwal et al., 2020; Meng et al., same size as the model before pruning. We as view 2021; Zheng et al., 2021) are used. 3083 Model dmodel dff L H # Params Extra-Large Large Base Tiny 2,048 1,024 512 256 8,192 4,096 2,048 1,024 8 6 6 6 16 16 8 8 1.1B 275M 93M 35M Table 1: Model statistics. dmodel , dff , L and H respectively denote input/output dimensionality, inner-layer dimensionality, # layers and # heads. For discriminative tasks, we followed the current trend of LM pretraining (Devlin et al., 2018; Liu et al., 2019a; Jiao et al., 2019; Radford et al., 2019; Lan et al., 2019; Brown et al., 2020; Clark et al., 2020; Sun et al., 2021). We test different pruning models on the tasks of question answer"
2021.emnlp-main.246,P19-1580,0,0.0186775,"l., 2016; He et al., 2017), sparsity regularization (Li et al., 2016; Liu et al., 2017; Huang and Wang, 2018; Gordon et al., 2018) and automatic network searching (He et al., 2018; Yu and Huang, 2019; Dong and Yang, 2019; Ding et al., 2019). Pruning Transformers Pruning Transformer based models has been of growing interest (Guo et al., 2019; Chen et al., 2020; Li et al., 2020). Fan et al. (2019) proposed LayerDrop to reduce Transformer depth. Michel et al. (2019) proposed to use head importance score to prune BERT attention heads. Attention heads can also be pruned by using L0 regularization (Voita et al., 2019) and cascade pruning (Wang et al., 2021). Wang et al. (2020) combined L0 regularization with matrix factorization to prune BERT. Gordon et al. (2020) proposed that BERT can be pruned once during pre-training rather than separately for each task without sacrificing performance. Generic Model Pruning Model pruning refers 2.2 Mutual Information Feature Selection to reducing the model size by dropping a fraction of the model parameters, which dates back to early Feature selection is the process of selecting a works of Optimal Brain Damage (PBD) (LeCun proper subset of features for better model per"
2021.emnlp-main.246,2020.emnlp-main.496,0,0.0689764,"Missing"
2021.findings-acl.126,D17-1209,0,0.0606144,"Missing"
2021.findings-acl.126,W18-6501,0,0.0237153,"etworks (De Cao and Kipf, 2018; Li et al., 2018b) and graph spatialtemporal networks (Li et al., 2017; Yu et al., 2017). GNNs serve as powerful tools to utilize the relationship between different objects, and have been applied to various domains such as traffic prediction (Yu et al., 2018; Zhang et al., 2018a) and recommendation (Zhang et al., 2020; Monti et al., 2017). In the context of NLP, GNNs have achieved remarkable successes across a wide range of end tasks such as relation extraction (Zhang et al., 2018b), semantic role labeling (Marcheggiani and Titov, 2017), data-to-text generation (Marcheggiani and Perez-Beltrachini, 2018), machine translation (Bastings et al., 2017) and question answering (Song et al., 2018; De Cao et al., 2018). The prevalence of neural networks has motivated a diverse array of works on developing neural models for text classification. Different neural model architectures (Kim, 2014; Zhou et al., 2015; Radford et al., 2018; Chai et al., 2020) have demonstrated their effectiveness against traditional statistical feature based methods (Wallach, 2006). Other works leverage label embeddings and jointly train them along with input texts (Wang et al., 2018; Pappas and Henderson, 2019). More recentl"
2021.findings-acl.126,D17-1159,0,0.0245608,"al., 2016; Kipf and Welling, 2016b), graph generative networks (De Cao and Kipf, 2018; Li et al., 2018b) and graph spatialtemporal networks (Li et al., 2017; Yu et al., 2017). GNNs serve as powerful tools to utilize the relationship between different objects, and have been applied to various domains such as traffic prediction (Yu et al., 2018; Zhang et al., 2018a) and recommendation (Zhang et al., 2020; Monti et al., 2017). In the context of NLP, GNNs have achieved remarkable successes across a wide range of end tasks such as relation extraction (Zhang et al., 2018b), semantic role labeling (Marcheggiani and Titov, 2017), data-to-text generation (Marcheggiani and Perez-Beltrachini, 2018), machine translation (Bastings et al., 2017) and question answering (Song et al., 2018; De Cao et al., 2018). The prevalence of neural networks has motivated a diverse array of works on developing neural models for text classification. Different neural model architectures (Kim, 2014; Zhou et al., 2015; Radford et al., 2018; Chai et al., 2020) have demonstrated their effectiveness against traditional statistical feature based methods (Wallach, 2006). Other works leverage label embeddings and jointly train them along with input"
2021.findings-acl.126,P05-1015,0,0.176459,"Missing"
2021.findings-acl.126,Q19-1009,0,0.011605,"Marcheggiani and Perez-Beltrachini, 2018), machine translation (Bastings et al., 2017) and question answering (Song et al., 2018; De Cao et al., 2018). The prevalence of neural networks has motivated a diverse array of works on developing neural models for text classification. Different neural model architectures (Kim, 2014; Zhou et al., 2015; Radford et al., 2018; Chai et al., 2020) have demonstrated their effectiveness against traditional statistical feature based methods (Wallach, 2006). Other works leverage label embeddings and jointly train them along with input texts (Wang et al., 2018; Pappas and Henderson, 2019). More recently, the success achieved by large-scale pretraining models has spurred great interests in adapting the largescale pretraining framework (Devlin et al., 2018) into text classification (Reimers and Gurevych, 2019), leading to remarkable progressive on fewshot (Mukherjee and Awadallah, 2020) and zeroshot (Ye et al., 2020) learning. Our work is inspired by the work of using graph neural networks for text classification (Yao et al., 2019; Huang et al., 2019; Zhang and Zhang, 2020). But different from these works, we focus on combining large-scale pretrained models and GNNs, and show th"
2021.findings-acl.126,D19-1410,0,0.0318162,"on developing neural models for text classification. Different neural model architectures (Kim, 2014; Zhou et al., 2015; Radford et al., 2018; Chai et al., 2020) have demonstrated their effectiveness against traditional statistical feature based methods (Wallach, 2006). Other works leverage label embeddings and jointly train them along with input texts (Wang et al., 2018; Pappas and Henderson, 2019). More recently, the success achieved by large-scale pretraining models has spurred great interests in adapting the largescale pretraining framework (Devlin et al., 2018) into text classification (Reimers and Gurevych, 2019), leading to remarkable progressive on fewshot (Mukherjee and Awadallah, 2020) and zeroshot (Ye et al., 2020) learning. Our work is inspired by the work of using graph neural networks for text classification (Yao et al., 2019; Huang et al., 2019; Zhang and Zhang, 2020). But different from these works, we focus on combining large-scale pretrained models and GNNs, and show that GNNs can significantly benefit from large-scale pretraining. Existing works that combine BERT and GNNs uses graph to model relationships between tokens within a single document sample (Lu et al., 2020; He et al., 2020b),"
2021.findings-acl.126,P18-1216,0,0.028863,"o-text generation (Marcheggiani and Perez-Beltrachini, 2018), machine translation (Bastings et al., 2017) and question answering (Song et al., 2018; De Cao et al., 2018). The prevalence of neural networks has motivated a diverse array of works on developing neural models for text classification. Different neural model architectures (Kim, 2014; Zhou et al., 2015; Radford et al., 2018; Chai et al., 2020) have demonstrated their effectiveness against traditional statistical feature based methods (Wallach, 2006). Other works leverage label embeddings and jointly train them along with input texts (Wang et al., 2018; Pappas and Henderson, 2019). More recently, the success achieved by large-scale pretraining models has spurred great interests in adapting the largescale pretraining framework (Devlin et al., 2018) into text classification (Reimers and Gurevych, 2019), leading to remarkable progressive on fewshot (Mukherjee and Awadallah, 2020) and zeroshot (Ye et al., 2020) learning. Our work is inspired by the work of using graph neural networks for text classification (Yao et al., 2019; Huang et al., 2019; Zhang and Zhang, 2020). But different from these works, we focus on combining large-scale pretrained"
2021.findings-acl.126,2020.emnlp-main.668,0,0.0375045,"her works leverage label embeddings and jointly train them along with input texts (Wang et al., 2018; Pappas and Henderson, 2019). More recently, the success achieved by large-scale pretraining models has spurred great interests in adapting the largescale pretraining framework (Devlin et al., 2018) into text classification (Reimers and Gurevych, 2019), leading to remarkable progressive on fewshot (Mukherjee and Awadallah, 2020) and zeroshot (Ye et al., 2020) learning. Our work is inspired by the work of using graph neural networks for text classification (Yao et al., 2019; Huang et al., 2019; Zhang and Zhang, 2020). But different from these works, we focus on combining large-scale pretrained models and GNNs, and show that GNNs can significantly benefit from large-scale pretraining. Existing works that combine BERT and GNNs uses graph to model relationships between tokens within a single document sample (Lu et al., 2020; He et al., 2020b), which fall into the category of inductive learning. Different from these works, we use graph to model relationships between different samples from the whole corpus to utilize the similarity between labeled and unlabeled documents, and uses GNNs to learn their relations"
2021.findings-acl.126,D18-1244,0,0.0152294,"sets. 1456 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1456–1462 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work Graph neural networks (GNNs) are connectionist models that capture dependencies and relations between graph nodes via message passing through edges that connect nodes (Scarselli et al., 2008; Hamilton et al., 2017; Xu et al., 2018). GNNs are practically categorized into (Wu et al., 2020): graph convolutional networks (Kipf and Welling, 2016a; Wu et al., 2019), graph attention networks (Veliˇckovi´c et al., 2017; Zhang et al., 2018a), graph auto-encoder (Cao et al., 2016; Kipf and Welling, 2016b), graph generative networks (De Cao and Kipf, 2018; Li et al., 2018b) and graph spatialtemporal networks (Li et al., 2017; Yu et al., 2017). GNNs serve as powerful tools to utilize the relationship between different objects, and have been applied to various domains such as traffic prediction (Yu et al., 2018; Zhang et al., 2018a) and recommendation (Zhang et al., 2020; Monti et al., 2017). In the context of NLP, GNNs have achieved remarkable successes across a wide range of end tasks such as relation extraction (Zhang et al., 20"
2021.findings-acl.238,D18-1403,0,0.0995522,"ic sub-tasks, namely, aspect term extraction (ATE), opinion term extraction (OTE) and aspect sentiment classification (ASC). For example, consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some r"
2021.findings-acl.238,D17-1047,0,0.0244849,"ce “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li e"
2021.findings-acl.238,2020.acl-main.582,0,0.562907,"ASC in a pipeline or an integrated model. The pipeline models (Hu et al., 2019; Phan and Ogunbona, 2020) are extract-then-classify processes and were proposed to solve the two tasks successively. However, they can still derive error accumulations. For integrated models, (Wang et al., 2018; Li et al., 2019) solved ATE and ASC by collapsed tagging that is a unified tagging scheme to link the two tasks. (Luo et al., 2019a) considered the relationship between the two tasks and attempted to investigate useful information from one task to another. Some works (Wang et al., 2017; Dai and Song, 2019; Chen et al., 2020; Zhao et al., 2020) integrated ATE and OTE in the same framework to illustrate these two tasks can benefit from each other. Then emerging methods (He et al., 2019; Chen and Qian, 2020; Peng et al., 2020) proposed to inject OTE as an auxiliary task to further improve the performance of ABSA. However, the number of sub-tasks and interactions among them in existing integrated methods are fixed, which can be restricted when sub-tasks vary in practice. 3 3.1 The Multiplex Interaction Network Task Definition All the sub-tasks related to ABSA are categorized into extractive and classification sub-ta"
2021.findings-acl.238,P19-1052,0,0.0598821,"Angelidis and Lapata, 2018; Ma et al., 2019), they were usually formulated as a sequence tagging problem, and various neutral networks with attention mechanisms were proposed to solve the task. For ASC, (Liu and Zhang, 2017; Cheng et al., 2017; He et al., 2018; Tang et al., 2019; Liang et al., 2019; Lei et al., 2019) attempted to exploit contextual and positional proximity of aspect terms for prediction by attentional neural networks. And (Tian et al., 2020) proposed to learn a unified sentiment representation for different sentiment analysis tasks. Recently, capsule network (Du et al., 2019; Chen and Qian, 2019), and graph convolution networks (Zhang et al., 2019) were also utilized in ASC and achieved SOTA performance. These separate learning approaches may have disadvantages in practical applications as they need to be pipelined and the interactions between different sub-tasks are totally neglected. Joint learning strives to combine sub-tasks of ABSA into a unified learning process. For example, some studies proposed to handle ATE and ASC in a pipeline or an integrated model. The pipeline models (Hu et al., 2019; Phan and Ogunbona, 2020) are extract-then-classify processes and were proposed to solv"
2021.findings-acl.238,2020.acl-main.340,0,0.729925,"019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo et al., 2019a; Chen and Qian, 2020; Peng et al., 2020) and achieved better performance than pipeline solutions. To name some, (Li et al., 2019) incorporated ATE and ASC and formulated the problem as a single sequence 2695 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2695–2705 August 1–6, 2021. ©2021 Association for Computational Linguistics Sentence Food is pretty good but the orange juice is horrific . ATE OTE ASC B O POS O O - O O - O B - O O - O O - B O NEG I O NEG O O - O B - O O - Table 1: A training instance sentence with gold ATE, OTE and ASC labels. labeling task with a collapsed ta"
2021.findings-acl.238,P19-1520,0,0.159119,"ed to handle ATE and ASC in a pipeline or an integrated model. The pipeline models (Hu et al., 2019; Phan and Ogunbona, 2020) are extract-then-classify processes and were proposed to solve the two tasks successively. However, they can still derive error accumulations. For integrated models, (Wang et al., 2018; Li et al., 2019) solved ATE and ASC by collapsed tagging that is a unified tagging scheme to link the two tasks. (Luo et al., 2019a) considered the relationship between the two tasks and attempted to investigate useful information from one task to another. Some works (Wang et al., 2017; Dai and Song, 2019; Chen et al., 2020; Zhao et al., 2020) integrated ATE and OTE in the same framework to illustrate these two tasks can benefit from each other. Then emerging methods (He et al., 2019; Chen and Qian, 2020; Peng et al., 2020) proposed to inject OTE as an auxiliary task to further improve the performance of ABSA. However, the number of sub-tasks and interactions among them in existing integrated methods are fixed, which can be restricted when sub-tasks vary in practice. 3 3.1 The Multiplex Interaction Network Task Definition All the sub-tasks related to ABSA are categorized into extractive and cl"
2021.findings-acl.238,D19-1551,0,0.508212,"ferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo et al., 2019a; Che"
2021.findings-acl.238,C18-1096,0,0.099017,"asonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He e"
2021.findings-acl.238,P19-1048,0,0.518593,"gelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo et al., 2019a; Chen and Qian, 2020; Peng et al., 2020) and achieved better performance than pipeline solutions. To name some, (Li et al., 2019) incorporated ATE and ASC and formulated the problem as a single sequence 2695 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2695–2705 August 1–6, 2021. ©2021 Association for Computational Linguistics Sentence Food is pretty good b"
2021.findings-acl.238,P19-1051,0,0.478417,"sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo et al., 2019a; Chen and Qian, 2020; Peng et al., 2020) and achieved better performance than pipeline solutions. To name some, (Li et al., 2019) incorporated ATE and ASC and formulated the pro"
2021.findings-acl.238,P18-1087,0,0.341064,"the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo"
2021.findings-acl.238,P19-1056,0,0.20864,"TE), opinion term extraction (OTE) and aspect sentiment classification (ASC). For example, consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to r"
2021.findings-acl.238,P19-1344,0,0.36406,"xtraction (OTE) and aspect sentiment classification (ASC). For example, consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues"
2021.findings-acl.238,D17-1310,0,0.190399,"ts of three specific sub-tasks, namely, aspect term extraction (ATE), opinion term extraction (OTE) and aspect sentiment classification (ASC). For example, consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tas"
2021.findings-acl.238,D14-1162,0,0.0850503,"Qian, 2020), we combine CMLA (Xu et al., 2018) for ATE, TNet and TCaps for ASC to construct two pipeline baselines. 2700 OTE is integrated into ATE. (2) unified methods: IMN (He et al., 2019) is an interactive multi-task model jointly trained on ATE and ASC where OTE is also integrated into ATE. While RACL (Chen and Qian, 2020) is trained on ATE, OTE and ASC in parallel that considers four relations among the three sub-tasks. 4.2.1 Settings Following (He et al., 2019), we adopt double embedding in word embedding layer of MIN, where each word embedding is a concatenation of general embedding (Pennington et al., 2014) with 300 dimensions and domain embedding (Xu et al., 2018) with 100 dimensions. We set the hidden size de = 400, dh = 300, dp = 300, and the kernel size, number of shared CNN layers to 5 and 2, individually. And the numbers of information feedback in Res14 and Lap14 are set to 2 and 3 respectively. The layers of multi-layer CNN for ATE, OTE and ASC are set to {2, 2, 1} and the kernel size is 5 in two dataset. Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-4 and a batch size of 8 are utilized for all datasets. We also combine MIN with BERTLarge to get MIN-BERT and {de , dh , d"
2021.findings-acl.238,P19-1462,0,0.325943,". (Luo et al., 2019b) presented a neural framework that leverages sememes to enhance lexical semantics for long-tailed aspect extraction. (Liao et al., 2019) utilized the capability of coupling global and local representation to discover aspect terms. For supervised methods (Wang et al., 2016b; Li 2696 and Lam, 2017; Angelidis and Lapata, 2018; Ma et al., 2019), they were usually formulated as a sequence tagging problem, and various neutral networks with attention mechanisms were proposed to solve the task. For ASC, (Liu and Zhang, 2017; Cheng et al., 2017; He et al., 2018; Tang et al., 2019; Liang et al., 2019; Lei et al., 2019) attempted to exploit contextual and positional proximity of aspect terms for prediction by attentional neural networks. And (Tian et al., 2020) proposed to learn a unified sentiment representation for different sentiment analysis tasks. Recently, capsule network (Du et al., 2019; Chen and Qian, 2019), and graph convolution networks (Zhang et al., 2019) were also utilized in ASC and achieved SOTA performance. These separate learning approaches may have disadvantages in practical applications as they need to be pipelined and the interactions between different sub-tasks are to"
2021.findings-acl.238,2020.acl-main.293,0,0.750146,"y for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wang et al., 2018; Li et al., 2019; He et al., 2019; Luo et al., 2019a; Chen and Qian, 2020; Peng et al., 2020) and achieved better performance than pipeline solutions. To name some, (Li et al., 2019) incorporated ATE and ASC and formulated the problem as a single sequence"
2021.findings-acl.238,D19-1465,0,0.0607373,"term extraction (ATE), opinion term extraction (OTE) and aspect sentiment classification (ASC). For example, consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have"
2021.findings-acl.238,E17-2091,0,0.019861,"dency relations between opinion words and aspects for aspect terms extraction. (Luo et al., 2019b) presented a neural framework that leverages sememes to enhance lexical semantics for long-tailed aspect extraction. (Liao et al., 2019) utilized the capability of coupling global and local representation to discover aspect terms. For supervised methods (Wang et al., 2016b; Li 2696 and Lam, 2017; Angelidis and Lapata, 2018; Ma et al., 2019), they were usually formulated as a sequence tagging problem, and various neutral networks with attention mechanisms were proposed to solve the task. For ASC, (Liu and Zhang, 2017; Cheng et al., 2017; He et al., 2018; Tang et al., 2019; Liang et al., 2019; Lei et al., 2019) attempted to exploit contextual and positional proximity of aspect terms for prediction by attentional neural networks. And (Tian et al., 2020) proposed to learn a unified sentiment representation for different sentiment analysis tasks. Recently, capsule network (Du et al., 2019; Chen and Qian, 2019), and graph convolution networks (Zhang et al., 2019) were also utilized in ASC and achieved SOTA performance. These separate learning approaches may have disadvantages in practical applications as they"
2021.findings-acl.238,P19-1053,0,0.472876,"Missing"
2021.findings-acl.238,2020.acl-main.374,0,0.115421,"zed the capability of coupling global and local representation to discover aspect terms. For supervised methods (Wang et al., 2016b; Li 2696 and Lam, 2017; Angelidis and Lapata, 2018; Ma et al., 2019), they were usually formulated as a sequence tagging problem, and various neutral networks with attention mechanisms were proposed to solve the task. For ASC, (Liu and Zhang, 2017; Cheng et al., 2017; He et al., 2018; Tang et al., 2019; Liang et al., 2019; Lei et al., 2019) attempted to exploit contextual and positional proximity of aspect terms for prediction by attentional neural networks. And (Tian et al., 2020) proposed to learn a unified sentiment representation for different sentiment analysis tasks. Recently, capsule network (Du et al., 2019; Chen and Qian, 2019), and graph convolution networks (Zhang et al., 2019) were also utilized in ASC and achieved SOTA performance. These separate learning approaches may have disadvantages in practical applications as they need to be pipelined and the interactions between different sub-tasks are totally neglected. Joint learning strives to combine sub-tasks of ABSA into a unified learning process. For example, some studies proposed to handle ATE and ASC in a"
2021.findings-acl.238,D16-1059,0,0.529877,"consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wan"
2021.findings-acl.238,D16-1058,0,0.764514,"consider the sentence “The price is reasonable although the service is inferior.”, ATE aims to extract a set of aspect terms from the sentence, i.e. {price, service}, OTE extracts the opinion words, i.e. {reasonable, inferior} , and ASC predicts sentiment polarity for each aspect that is positive over the first aspect price and negative for the second aspect service, respectively. Prevailing solutions of ABSA treated ATE (Liu et al., 2015; Li and Lam, 2017; Angelidis and Lapata, 2018; Liao et al., 2019; Luo et al., 2019b; Ma et al., 2019), OTE (Wang et al., 2017; Wang and Pan, 2019) and ASC (Wang et al., 2016b; Chen et al., 2017; He et al., 2018; Li et al., 2018b; Du et al., 2019; Xu et al., 2021) as separate tasks and were individually studied for decades. These separate tasks need to be integrated into a pipeline for practical use (Hu et al., 2019; Phan and Ogunbona, 2020). The key problem with pipeline approaches is that errors can accumulate and that the pipeline model fails to fully exploit the interactive relations among different sub-tasks (He et al., 2019). Some recent efforts have been proposed to remedy these issues by using joint learning to enhance the interactions among sub-tasks (Wan"
2021.findings-acl.238,P18-2094,0,0.609844,"ssed to other classification sub-tasks and assist in their features extractions. Then every sub-task predicts ˆ A, Y ˆO the corresponding sequence labels, i.e., Y ˆ S , by its decode layer. The model also adopts and Y an information feedback mechanism that concatenates representations of all sub-tasks to fine-tune the shared representations. In the following, we first describe the MIN model in more detail and then illustrate the learning process. 3.4 3.4.1 Features Extraction for Extractive Sub-tasks Multi-layer CNN For the extractive sub-tasks, we use a multi-layer CNN structure proposed by (Xu et al., 2018) to learn private features of each task separately. Specifically, there are many 1D-convolution filters in each CNN layer, and each filter has a fixed kernel size of k = 2c + 1. As a result, each filter performs convolution operation on a window of k word representations, and compute the representation for the i-th word along with 2c nearby words in its context. We can extract private features HA of ATE and HO of OTE by the above multi-layer CNN algorithm, HA = MC(H), HA ∈ Rdp ×n , 3.3 Shared Representation Generation HO = MC(H), HO ∈ Rdp ×n , For a sequence of tokens {x1 , x2 , ..., xn }, we"
2021.findings-acl.238,D19-1464,0,0.139555,"e usually formulated as a sequence tagging problem, and various neutral networks with attention mechanisms were proposed to solve the task. For ASC, (Liu and Zhang, 2017; Cheng et al., 2017; He et al., 2018; Tang et al., 2019; Liang et al., 2019; Lei et al., 2019) attempted to exploit contextual and positional proximity of aspect terms for prediction by attentional neural networks. And (Tian et al., 2020) proposed to learn a unified sentiment representation for different sentiment analysis tasks. Recently, capsule network (Du et al., 2019; Chen and Qian, 2019), and graph convolution networks (Zhang et al., 2019) were also utilized in ASC and achieved SOTA performance. These separate learning approaches may have disadvantages in practical applications as they need to be pipelined and the interactions between different sub-tasks are totally neglected. Joint learning strives to combine sub-tasks of ABSA into a unified learning process. For example, some studies proposed to handle ATE and ASC in a pipeline or an integrated model. The pipeline models (Hu et al., 2019; Phan and Ogunbona, 2020) are extract-then-classify processes and were proposed to solve the two tasks successively. However, they can still"
2021.findings-acl.238,2020.acl-main.296,0,0.431553,"or an integrated model. The pipeline models (Hu et al., 2019; Phan and Ogunbona, 2020) are extract-then-classify processes and were proposed to solve the two tasks successively. However, they can still derive error accumulations. For integrated models, (Wang et al., 2018; Li et al., 2019) solved ATE and ASC by collapsed tagging that is a unified tagging scheme to link the two tasks. (Luo et al., 2019a) considered the relationship between the two tasks and attempted to investigate useful information from one task to another. Some works (Wang et al., 2017; Dai and Song, 2019; Chen et al., 2020; Zhao et al., 2020) integrated ATE and OTE in the same framework to illustrate these two tasks can benefit from each other. Then emerging methods (He et al., 2019; Chen and Qian, 2020; Peng et al., 2020) proposed to inject OTE as an auxiliary task to further improve the performance of ABSA. However, the number of sub-tasks and interactions among them in existing integrated methods are fixed, which can be restricted when sub-tasks vary in practice. 3 3.1 The Multiplex Interaction Network Task Definition All the sub-tasks related to ABSA are categorized into extractive and classification sub-tasks, respectively, i"
2021.findings-emnlp.115,2020.acl-main.582,0,0.0167977,"terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a joint learning framework. terms from a given sentence. Then either the ini- Wang et al. (2018); Li et al. (2019b) used a unified tial aspect terms or opinion terms are deemed as tagging schema to solve ATE and ASC simultanea query to extract corresponding opinion terms ously. Wang et al. (2017); Dai and Song (2019); or aspect terms as answers. The roles of query Luo et al. (2019a); Chen et al. (2020); Zhao et al. and answer can be flipped to perform a multi-hop (2020) integrated ATE and ASC in the same framequestion-answering process. In this manner, we can work to make these two tasks benefit from each progressively obtain the aspect or opinion terms we other. Some emerging methods (He et al., 2019; need without manually designing queries. Mean- Chen and Qian, 2020; Peng et al., 2020; Xu et al., while, the aspect terms could be potentially as- 2020; Yu et al., 2021) added OTE as an auxiliary sociated with relevant opinion terms as the mul- task and connect aspects with respective opinion"
2021.findings-emnlp.115,2020.acl-main.340,0,0.321644,"on for Computational Linguistics: EMNLP 2021, pages 1331–1342 November 7–11, 2021. ©2021 Association for Computational Linguistics Tang et al., 2020; Hou et al., 2021b) was proposed to link aspect terms with interrelated opinion terms more directly. They can account for long-range word dependencies and refrain from identifying contextual words unrelated to aspect terms. Despite their effectiveness, these methods will be infeasible if the given aspects are absent. As a result, some researchers proposed to incorporate all sub-tasks in a framework of unified ABSA. These methods (He et al., 2019; Chen and Qian, 2020) formulated sub-tasks of ABSA as sequence labeling tasks. By multifarious interaction mechanisms performed on sentence representations of different sub-tasks, they made the aspect terms come into contact with opinion terms. Furthermore, recent researches (Peng et al., 2020; Mao et al., 2021) put forward to extract (aspect, opinion, sentiment) triples from sentences without given aspect terms. They strive to clarify each aspect-opinion pair for sentiment prediction and needed additional labels of triples compared to the previous unified ABSA. In this paper, we examine the unified ABSA from a pe"
2021.findings-emnlp.115,P19-1520,0,0.016524,"ously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a joint learning framework. terms from a given sentence. Then either the ini- Wang et al. (2018); Li et al. (2019b) used a unified tial aspect terms or opinion terms are deemed as tagging schema to solve ATE and ASC simultanea query to extract corresponding opinion terms ously. Wang et al. (2017); Dai and Song (2019); or aspect terms as answers. The roles of query Luo et al. (2019a); Chen et al. (2020); Zhao et al. and answer can be flipped to perform a multi-hop (2020) integrated ATE and ASC in the same framequestion-answering process. In this manner, we can work to make these two tasks benefit from each progressively obtain the aspect or opinion terms we other. Some emerging methods (He et al., 2019; need without manually designing queries. Mean- Chen and Qian, 2020; Peng et al., 2020; Xu et al., while, the aspect terms could be potentially as- 2020; Yu et al., 2021) added OTE as an auxiliary sociated w"
2021.findings-emnlp.115,N19-1423,0,0.0215415,"sentiment classification. The overall architecture is shown in Figure 3 and the algorithm is elaborated in appendix of the supplementary materials. Given a sequence of tokens X = {x1 , x2 , ..., xn }, where n denotes the length of sentence, Aspect Terms Extraction (ATE) aims to find aspect terms ˆ A = {ˆ in X and assign a label Y y1A , yˆ2A , ..., yˆnA } to it. Opinion Terms Extraction (OTE) aims to find all opinion terms in X and assign a label of ˆ O = {ˆ 3.3 Input Representations Y y1O , yˆ2O , ..., yˆnO } to it. Aspect Sentiment Classification (ASC) aims to predict a sequence We use BERT (Devlin et al., 2019) to obtain inS S S S ˆ of sentiment label Y = {ˆ y1 , yˆ2 , ..., yˆn }. Specifi- put representation following (Li et al., 2019b) cally, yˆiA , yˆiO ∈ {B, I, O} denote the beginning of, and (Chen and Qian, 2020). For a sequence inside of, and out of aspect and opinion terms, re- of tokens X(0) = {x1 , x2 , ..., xn }, we map the spectively. yˆiS ∈ {pos, neg, neu} denotes positive, word sequence with pre-trained BERT model to negative, neutral sentiment polarities, respectively. generate a sequence of units vectors H(0) = 1333 ( ) ( ) A1(T) A2(T) ... ( ) ( ) Aq(T) ( ) Question Answering (1) A1(1)"
2021.findings-emnlp.115,D19-1551,0,0.0196011,"swer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020"
2021.findings-emnlp.115,D15-1076,0,0.0312872,"ied B Answer O O O O but the chicken was fine B O FFNN BERT [CLS] falafel chicken [SEP] The falafel was Query rather over cooked and dried Context fine [SEP] Figure 2: An example to examine the unified ABSA from a perspective of MRC. 2.2 Solving NLP Tasks by MRC Machine reading comprehension is a prevalent and elastic framework, which aims to extract answers from context according to query. Many tasks in natural language processing can be framed as comprehension reading. McCann et al. (2018) introduced a natural language decathlon and transformed ten tasks into reading comprehension problems. He et al. (2015) used question-answering pairs to represent the predicate-argument structure in the semantic role labeling annotations. Levy et al. (2017) showed that relation extraction can be reduced to answer simple reading comprehension questions. Li et al. (2020b) designed a unified machine reading comprehension framework to solve the task of nested named entity recognition. Li et al. (2019a) cast the entity-relation extraction as a multi-turn question answering problem. Wu et al. (2020) used a mention with its surrounding words as a query to extract its coreference words as answers. All the above method"
2021.findings-emnlp.115,P19-1048,0,0.236256,"of the Association for Computational Linguistics: EMNLP 2021, pages 1331–1342 November 7–11, 2021. ©2021 Association for Computational Linguistics Tang et al., 2020; Hou et al., 2021b) was proposed to link aspect terms with interrelated opinion terms more directly. They can account for long-range word dependencies and refrain from identifying contextual words unrelated to aspect terms. Despite their effectiveness, these methods will be infeasible if the given aspects are absent. As a result, some researchers proposed to incorporate all sub-tasks in a framework of unified ABSA. These methods (He et al., 2019; Chen and Qian, 2020) formulated sub-tasks of ABSA as sequence labeling tasks. By multifarious interaction mechanisms performed on sentence representations of different sub-tasks, they made the aspect terms come into contact with opinion terms. Furthermore, recent researches (Peng et al., 2020; Mao et al., 2021) put forward to extract (aspect, opinion, sentiment) triples from sentences without given aspect terms. They strive to clarify each aspect-opinion pair for sentiment prediction and needed additional labels of triples compared to the previous unified ABSA. In this paper, we examine the"
2021.findings-emnlp.115,2021.textgraphs-1.8,0,0.137834,"et al., 2019) or gating mechanisms (Zhang namely, aspect terms extraction (ATE), opinion et al., 2016; Xue and Li, 2018) to collect aspectterms extraction (OTE), and aspect sentiment clas- related information (e.g., opinion terms) from consification (ASC). ATE and OTE extract aspect and text. Recently, Graph Neural Network over dif∗ Corresponding author ferent dependency trees (Huang and Carley, 2019; 1331 1 Introduction Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1331–1342 November 7–11, 2021. ©2021 Association for Computational Linguistics Tang et al., 2020; Hou et al., 2021b) was proposed to link aspect terms with interrelated opinion terms more directly. They can account for long-range word dependencies and refrain from identifying contextual words unrelated to aspect terms. Despite their effectiveness, these methods will be infeasible if the given aspects are absent. As a result, some researchers proposed to incorporate all sub-tasks in a framework of unified ABSA. These methods (He et al., 2019; Chen and Qian, 2020) formulated sub-tasks of ABSA as sequence labeling tasks. By multifarious interaction mechanisms performed on sentence representations of differen"
2021.findings-emnlp.115,2021.naacl-main.229,0,0.021467,"Missing"
2021.findings-emnlp.115,P19-1051,0,0.0138372,"9); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a joint learning framework. terms from a given sentence. Then either the ini- Wang et al. (2018); Li et al. (2019b) used a unified tial aspect terms or opinion terms are deemed as tagging schema to solve ATE and ASC simultanea query to extract corresponding opinion terms ously. Wang et al. (2017); Dai and Song (2019); or"
2021.findings-emnlp.115,D19-1549,0,0.0200921,"erall sen- given aspect terms. Among them, a series of methtiment polarity in a given sentence (Liu, 2012). It ods designed attention mechanisms (He et al., 2018; generally consists of three fundamental sub-tasks, Tang et al., 2019) or gating mechanisms (Zhang namely, aspect terms extraction (ATE), opinion et al., 2016; Xue and Li, 2018) to collect aspectterms extraction (OTE), and aspect sentiment clas- related information (e.g., opinion terms) from consification (ASC). ATE and OTE extract aspect and text. Recently, Graph Neural Network over dif∗ Corresponding author ferent dependency trees (Huang and Carley, 2019; 1331 1 Introduction Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1331–1342 November 7–11, 2021. ©2021 Association for Computational Linguistics Tang et al., 2020; Hou et al., 2021b) was proposed to link aspect terms with interrelated opinion terms more directly. They can account for long-range word dependencies and refrain from identifying contextual words unrelated to aspect terms. Despite their effectiveness, these methods will be infeasible if the given aspects are absent. As a result, some researchers proposed to incorporate all sub-tasks in a framework of"
2021.findings-emnlp.115,2020.emnlp-main.568,0,0.0335958,"allenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the"
2021.findings-emnlp.115,D19-1654,1,0.79212,"ochs using Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and batch O size 8. The task coefficients {λA t , λt , α, β, γ} are set to {1, 1, 1, 1, 1}. The code is implemented in 4.5 Auxiliary Experiments PyTorch 1.9.0 and launched on Ubuntu server with a NVidia Tesla V100(32GB). To demonstrate the ability of the proposed model Following the protocols in He et al. (2019), we to analyze the sentiment in complex sentences, we use four metrics, i.e., AE-F1, OE-F1, AS-F1, and run an auxiliary experiment on a more challenging Overall-F1, representing macro F1 scores for ATE, MAMS (Jiang et al., 2019) dataset. Each sentence 1336 Model M1 M2 M3 M4 M5 M6 M7 CMLA+TNet CMLA+TCap DECNN+TNet DECNN+TCap MNN E2E-TBSA DOER AE-F1 81.91 81.91 82.79 82.79 83.05 83.92 84.63 M8 M9 M10 M11 SPAN IMN RACL RF-MRC 86.71 84.06 86.38 88.22 Restaurant14 OE-F1 AS-F1 83.84 69.69 83.84 71.32 – 70.45 – 71.77 84.55 68.45 84.97 68.38 – 64.50 – 85.10 87.18 86.62 71.75 75.67 81.61 81.28 Overall-F1 64.49 65.68 65.80 66.84 63.87 66.60 68.55 AE-F1 77.49 77.49 79.38 79.38 76.94 77.34 80.21 Laptop14 OE-F1 AS-F1 76.06 68.30 76.06 69.49 – 68.69 – 69.61 77.77 65.98 76.62 68.24 – 60.18 73.68 70.72 75.42 76.87 82.34 77.55 81.79"
2021.findings-emnlp.115,K17-1034,0,0.0120942,"ed Context fine [SEP] Figure 2: An example to examine the unified ABSA from a perspective of MRC. 2.2 Solving NLP Tasks by MRC Machine reading comprehension is a prevalent and elastic framework, which aims to extract answers from context according to query. Many tasks in natural language processing can be framed as comprehension reading. McCann et al. (2018) introduced a natural language decathlon and transformed ten tasks into reading comprehension problems. He et al. (2015) used question-answering pairs to represent the predicate-argument structure in the semantic role labeling annotations. Levy et al. (2017) showed that relation extraction can be reduced to answer simple reading comprehension questions. Li et al. (2020b) designed a unified machine reading comprehension framework to solve the task of nested named entity recognition. Li et al. (2019a) cast the entity-relation extraction as a multi-turn question answering problem. Wu et al. (2020) used a mention with its surrounding words as a query to extract its coreference words as answers. All the above methods have demonstrated that machine reading comprehension is an effective framework to solve natural language processing tasks. 3 3.1 Model T"
2021.findings-emnlp.115,2020.acl-main.631,0,0.0662425,"e context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between asp"
2021.findings-emnlp.115,2020.acl-main.519,1,0.870587,"e context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between asp"
2021.findings-emnlp.115,P19-1129,1,0.845788,"Missing"
2021.findings-emnlp.115,P18-1087,0,0.0328462,"Missing"
2021.findings-emnlp.115,D17-1310,0,0.0249578,"hine Reading Comprehension (MRC). The MRC framework operates on the context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over depend"
2021.findings-emnlp.115,P19-1462,0,0.0203129,"e context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan"
2021.findings-emnlp.115,P19-1056,0,0.235018,"nd answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and releva"
2021.findings-emnlp.115,D16-1264,0,0.0485137,"epresentations of different sub-tasks, they made the aspect terms come into contact with opinion terms. Furthermore, recent researches (Peng et al., 2020; Mao et al., 2021) put forward to extract (aspect, opinion, sentiment) triples from sentences without given aspect terms. They strive to clarify each aspect-opinion pair for sentiment prediction and needed additional labels of triples compared to the previous unified ABSA. In this paper, we examine the unified ABSA from a perspective of Machine Reading Comprehension (MRC). The MRC framework operates on the context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different"
2021.findings-emnlp.115,D19-1569,0,0.0195453,"g, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a jo"
2021.findings-emnlp.115,P19-1344,0,0.0128718,"ork operates on the context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic rela"
2021.findings-emnlp.115,2020.acl-main.293,0,0.0264852,"2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a joint learning framework. terms from a given sentence. Then either the ini- Wang et al. (2018); Li et al. (2019b) used a unified tial aspect terms or opinion terms are deemed as tagging schema to solve ATE and ASC simultanea query to extract corresponding opinion terms ously. Wang et al. (2017); Dai and Song (2019); or aspect terms as answers."
2021.findings-emnlp.115,S15-2082,0,0.0366297,"er O O O O but the chicken was fine B O FFNN BERT [CLS] falafel chicken [SEP] The falafel was Query rather over cooked and dried Context fine [SEP] Figure 2: An example to examine the unified ABSA from a perspective of MRC. 2.2 Solving NLP Tasks by MRC Machine reading comprehension is a prevalent and elastic framework, which aims to extract answers from context according to query. Many tasks in natural language processing can be framed as comprehension reading. McCann et al. (2018) introduced a natural language decathlon and transformed ten tasks into reading comprehension problems. He et al. (2015) used question-answering pairs to represent the predicate-argument structure in the semantic role labeling annotations. Levy et al. (2017) showed that relation extraction can be reduced to answer simple reading comprehension questions. Li et al. (2020b) designed a unified machine reading comprehension framework to solve the task of nested named entity recognition. Li et al. (2019a) cast the entity-relation extraction as a multi-turn question answering problem. Wu et al. (2020) used a mention with its surrounding words as a query to extract its coreference words as answers. All the above method"
2021.findings-emnlp.115,P19-1053,0,0.0446708,"Missing"
2021.findings-emnlp.115,2020.acl-main.374,0,0.0210471,"benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect t"
2021.findings-emnlp.115,2020.acl-main.295,0,0.0116423,"one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neural network over dependency trees to capture long-range syntactic relations between aspect terms and relevant opinion terms. Joint learning methods strive to solve multiple sub-tasks simultaneously. Hu et al. (2019); Phan and Ogunbona (2020) used pipeline models to extract aspect terms then predict the sentiment polarities, which are vulnerable due to error accumulation. To tackle this issue, some studies proposed First, we extract the initial aspect and opinion to solve all sub-tasks in a joint learning framework. terms from a given sentence. Then eit"
2021.findings-emnlp.115,D16-1059,0,0.0287307,"a perspective of Machine Reading Comprehension (MRC). The MRC framework operates on the context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neura"
2021.findings-emnlp.115,D16-1058,0,0.0409373,"a perspective of Machine Reading Comprehension (MRC). The MRC framework operates on the context, query, and answer triples (Rajpurkar et al., 2016, 2018), in which the constructed natural language query is asked to the context, and the answer is extracted from the context. used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework. 2 2.1 Related Work Aspect-based Sentiment Analysis Existing methods for ABSA consist of separate learning and joint learning, respectively. Methods for separate learning only focus on one of the sub-tasks of ABSA. To name some, Wang et al. (2016a); Li and Lam (2017); Angelidis and Lapata (2018); Ma et al. (2019); Li et al. (2020a); Luo et al. (2019b) came up with different un/supervised methods to solve aspect extraction. Tang et al. (2019); Liang et al. (2019); Du et al. (2019); Chen and Qian (2019); Tian et al. (2020); Huang et al. (2020); Xu et al. (2021) designed different neural networks with attention mechanisms to exploit contextual and positional proximity related to aspect terms for sentiment prediction. Sun et al. (2019); Zhang et al. (2019); Hou et al. (2021a); Wang et al. (2020); Tang et al. (2020) established graph neura"
2021.findings-emnlp.115,2020.acl-main.622,1,0.760185,"n et al. (2018) introduced a natural language decathlon and transformed ten tasks into reading comprehension problems. He et al. (2015) used question-answering pairs to represent the predicate-argument structure in the semantic role labeling annotations. Levy et al. (2017) showed that relation extraction can be reduced to answer simple reading comprehension questions. Li et al. (2020b) designed a unified machine reading comprehension framework to solve the task of nested named entity recognition. Li et al. (2019a) cast the entity-relation extraction as a multi-turn question answering problem. Wu et al. (2020) used a mention with its surrounding words as a query to extract its coreference words as answers. All the above methods have demonstrated that machine reading comprehension is an effective framework to solve natural language processing tasks. 3 3.1 Model The formulation of unified ABSA Sentiment labels of tokens that are not aspects are set to “NULL”. 3.2 Examine ABSA from MRC perspective Recall that the Machine Reading Comprehension (MRC) aims to determine the answer to a given query from context. The query encodes significant prior information and the answer can be extracted by detecting it"
2021.findings-emnlp.115,2020.acl-main.296,0,0.0743045,"Missing"
2021.findings-emnlp.115,P18-2094,0,0.0233033,"76.05 Overall-F1 55.94 56.30 57.39 57.71 53.80 55.88 56.71 AE-F1 67.73 67.73 68.52 68.52 70.24 69.40 67.47 Restaurant15 OE-F1 AS-F1 70.56 62.27 70.56 63.32 – 62.41 – 63.60 69.38 57.90 71.43 58.81 – 36.76 61.25 61.73 63.40 65.31 74.63 69.90 73.99 75.57 – 73.29 76.00 78.60 50.28 70.10 74.91 75.79 Overall-F1 55.00 55.47 55.69 56.22 56.57 57.38 50.31 62.29 60.22 66.05 67.86 Table 2: Comparison results. The best scores are in bold face and the second best ones are underlined. The scores for models from M1 to M10 are taken from Chen and Qian (2020). Models from M1 to M7 are based double embeddings (Xu et al., 2018), while M8 to M11 used BERTlarge as a backbone. ‘-’ denotes the method does not have the metric OE-F1. Model AE-F1 AS-F1 Overall-F1 SPAN 73.90 82.51 61.51 IMN 73.03 84.29 61.68 RACL 75.14 83.63 63.03 RF-MRC 76.00 84.71 64.53 Table 3: Auxiliary results in MAMS. Model Restaurant14 Laptop14 Restaurant15 w/o A2O 74.01 63.62 67.77 w/o O2A 75.21 64.22 67.38 Full Model 76.87 65.31 67.86 Table 4: Ablation Test. “w/o” denotes without. in this dataset consists of at least two unique aspects with different polarities. Because the opinion labels are not annotated in MAMS, we did not compute the loss LO an"
2021.findings-emnlp.115,2020.emnlp-main.183,0,0.0600679,"Missing"
2021.findings-emnlp.115,P18-1234,0,0.0208305,"g the relations between aspect terms and Aspect-based Sentiment Analysis (ABSA) aims at their potential corresponding opinion terms. Early detecting opinions towards different targets (also methods only focused on ASC task and relied on known as aspects) instead of inferring overall sen- given aspect terms. Among them, a series of methtiment polarity in a given sentence (Liu, 2012). It ods designed attention mechanisms (He et al., 2018; generally consists of three fundamental sub-tasks, Tang et al., 2019) or gating mechanisms (Zhang namely, aspect terms extraction (ATE), opinion et al., 2016; Xue and Li, 2018) to collect aspectterms extraction (OTE), and aspect sentiment clas- related information (e.g., opinion terms) from consification (ASC). ATE and OTE extract aspect and text. Recently, Graph Neural Network over dif∗ Corresponding author ferent dependency trees (Huang and Carley, 2019; 1331 1 Introduction Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1331–1342 November 7–11, 2021. ©2021 Association for Computational Linguistics Tang et al., 2020; Hou et al., 2021b) was proposed to link aspect terms with interrelated opinion terms more directly. They can account for"
2021.findings-emnlp.115,2021.findings-acl.238,1,0.711079,"terms ously. Wang et al. (2017); Dai and Song (2019); or aspect terms as answers. The roles of query Luo et al. (2019a); Chen et al. (2020); Zhao et al. and answer can be flipped to perform a multi-hop (2020) integrated ATE and ASC in the same framequestion-answering process. In this manner, we can work to make these two tasks benefit from each progressively obtain the aspect or opinion terms we other. Some emerging methods (He et al., 2019; need without manually designing queries. Mean- Chen and Qian, 2020; Peng et al., 2020; Xu et al., while, the aspect terms could be potentially as- 2020; Yu et al., 2021) added OTE as an auxiliary sociated with relevant opinion terms as the mul- task and connect aspects with respective opinion tiple question-answering proceeds. Furthermore, terms to derive easier sentiment prediction. In addiwe propose a matching module to match all the tion, recent studies defined a task of (aspect, opinextracted aspects and relevant opinion terms in ion, sentiment) triples extraction and resolve it in a pairs simultaneously instead of extracting only one two-stage framework (Peng et al., 2020) or a uniaspect-opinion pair at one time, considering a com- fied framework (Mao et"
2021.findings-emnlp.115,D19-1464,0,0.0238407,"Missing"
C12-1098,C08-2006,0,0.17358,"about the same topic. Recently, there have been many attempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty dete"
C12-1098,P11-1050,0,0.0162171,"cy (repeating less the same information). The score is an integer between 1 (very poor) and 5 (very good). We randomly select 28 topics from TAC 2011 data and assign each topic to three different assessors7. In Table 3, the left four columns report the average scores of each criterion for the three systems. The experimental results indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum (based on paired t-test with p-value &lt; 0.01). Simultaneously, a fairly standard approach for manual evaluation is conducted through pairwise comparison (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011). According to the rating scores, each pair of summaries is judged which one is better under each criterion. If two summaries have the same score, they are judged a tie (of the equal quality). We record the times of ‘winning’ (having a higher score) and tie for each system. In Table 3, the right six columns show the evaluation results in frequencies respectively for h-uHDPSum vs. Peer 43, and h-uHDPSum vs. 2LevLDASum. The experimental results also indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum. We also observe that the winning times of h-uHDPSum under the nove"
C12-1098,E12-1022,0,0.74095,"ome new may appear over time, causing the number of aspects and aspect structures to change at different epochs. 1 Aspect in this article is usually called cluster in evolutionary clustering. 1604 Under the framework of extractive summarization, it is important to acquire the relationship between sentences and aspects for sentence selection. However, in most existing HDP models, the sentence level is disregarded and we cannot directly get the aspect distribution of sentences. Inspired by the progress made in Latent Dirichlet Allocation (LDA) models (Chemudugunta et al., 2007; Li et al., 2010; Delort and Alfonseca, 2012), we newly add the sentence level between the word level and document level in the h-uHDP model. Since neighboring sentences in one document usually talk about one same aspect, we assume that the aspect assignment of each sentence is not conditionally independently. With such assumption, the aspect of each sentence is determined by the aspect distribution of both the document and its neighboring sentences. Our huHDP model is capable of mapping multiple levels of information into the latent aspect space. The rest of this paper is organized as follows. Section 2 discusses the related work on upd"
C12-1098,N09-1041,0,0.139189,"summarization aims to produce an update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the as"
C12-1098,P10-1066,0,0.12074,"ization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approaches are mainly based on the computation of lexical similarities between sentences and fail to consider higher level information to avoid semantic redundancy in update summarization. To solve these two problems, we borrow the techniques of evolutionary clustering which focuses on detecting the dynamics of a given topic. Normally, one topic is described from various specific aspects 1 , accompanied with the background information running the whole topic (Chemudugunta et al., 2007; Li et al., 2010). For example, the topic “Quebec independence” may involve the specific aspects including “leader in independence movement”, “referendum”, “related efforts in independence movement” and so on, while “Quebec” and “independence” are seen as the general background information. The evolving dynamics of a topic is mainly embodied in the birth, splitting, merging and death of the specific aspects (Ren et al., 2008). Then, the commonality and diversity between history documents and update documents can be easily summarized from the aspect level and update summarization is not limited to lexical redun"
C12-1098,C08-1062,0,0.370449,"tempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approac"
C12-1098,N03-1020,0,0.315773,"Missing"
C12-1098,W11-0507,0,0.0204407,"n update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the aspect distribution of SH an"
C12-1098,C10-1111,0,0.0419546,"Missing"
C12-1098,W04-3252,0,\N,Missing
C12-1168,P12-1007,0,0.0222664,"ed relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the su"
C12-1168,D09-1036,0,0.2519,"to each typical examp le in the set Ai . Each examp le in the union A0 =∪Bi (1≤i≤n) is labelled as R0 . Then the set of ordered pairs &lt;Ai , Ri > (0≤i≤n) can be used to train an implicit relat ion classifier for labelling Ri (1 ≤ i ≤ n). Both clustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inqui"
C12-1168,P02-1047,0,0.541136,"Missing"
C12-1168,C08-2022,0,0.703215,"Missing"
C12-1168,P09-1077,0,0.564315,"relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the supervised framework (Pitler, Louis and Nenkova, 2009; Lin, Kan and Ng, 2009; Wang, Su and Tan, 2010). L in, Kan and Ng (2009) conducted a thorough performance analysis for four classes of features including contextual relations, constituent parse features, dependency parse features and cross -argument lexical pairs, while Pit ler et al. (2009) applied several linguistically informed features, such as word polarity, verb classes, and word pairs. Wang, Su and Tan (2010) adopted the tree kernel approach to mine more structure informat ion and got better results. These efforts of feature selection have achieved better performance though not that satisfying. The quality of training data are partly responsible for the difficulty of improving the performance of implicit relation recognition . To better recognize the imp licit discourse relations, we propose to review the annotated discourse corpora available at hand, identify and choose t"
C12-1168,prasad-etal-2008-penn,0,0.784226,"gue that an effective train ing se t is co mposed of typical examples, wh ich have distinct characteristics to signify their discourse relations. These typical examples, however, can be either the natively implicit relations or the created imp licit relations with connectives removed fro m the exp licit relat ions. Using the typical examp les as training data, 2758 an implicit relation classifier with higher discrimination power can be built according to the linguistic features in the two arguments. We provide three Comparison relat ion examp les fro m the Penn Discourse TreeBank (PDTB) v2.0 (Prasad et al., 2008) wh ich is widely used in the research of relation recognition as follows to illustrate what the possible typical examples are like. (1) Arg 1: 44 North Koreans oppose the plan, Arg 2: (while) South Koreans, Japanese and Taiwanese accept it or are neutral. (2) Arg 1: In such situations, you cannot write rules in advance. Arg 2: you can only make sure the President takes the responsibility. (3) Arg 1: Columbia Savings is a major holder of so-called junk bonds. Arg 2: New federal leg islation requires that all thrifts divest themselves of such speculative securities over a period of years. Here,"
C12-1168,N06-2034,0,0.0224094,"Missing"
C12-1168,N03-1030,0,0.0878457,"ion. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al."
C12-1168,P10-1073,0,0.0778405,"Missing"
C12-1168,H05-1044,0,0.00261514,"lustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inquirer lexicon (Stone et al., 1966) divides each word into fine-g rained semantic categories described by the inquirer tags. Fro m all the categories, we select 21 pairs of complementary categories, such as: Rise versus Fall, or Pleasure versus Pain,"
C12-1168,W06-1317,0,0.26788,"Missing"
C12-1168,P95-1026,0,0.113132,"Missing"
C12-1168,W10-4326,0,0.0401837,"one 2759 presented by Marcu and Echihabi (2002) who applied massive amounts of unannotated explicit relations and lexical features to train the Naïve Bayes classifier for both exp licit and implicit discourse relation recognition. Following the same idea, Saito, Yamamoto and Sekine (2006) conducted the experiments with the co mb ination of cross -argument wo rd pairs and phrasal patterns as features on Japanese sentences. Blair-Go ldensohn (2007) further extended the work of Marcu and Echihabi (2002) by involving syntactic filtering and topic segmentation. Another interesting work is that of Zhou et al. (2010), which predicted discourse connectives between arguments via a language model. Then the generated connectives plus other linguistic features were combined in a supervised framework to determine the implicit discourse relation. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that u"
C12-2068,C02-1130,0,0.112702,"Missing"
C12-2068,C08-1034,0,0.0166596,"ks whether the context of entity mention Wi contains the word in the relevant classspecific word set. If context words surrounding Wi hit the word in the class-specific feature set of Cj, the binary feature corresponding to Cj is set to 1. 697 3 3.1 Experiments Experimental Settings We test our approach on UKWAC3( M. Baroni et al., 2009), a 2 billion word English corpora constructed from the Web limiting the crawl to the .uk domain which has been PoS-tagged and lemmatized. The input person instances for each class are the same as used by Giuliano (2009) based on the People Ontology defined by Giuliano and Gliozzo (2008). The ontology extracted from WordNet is arranged in a multi-level taxonomy with 21 fine-grained classes, containing 1,657 distinct person instances. The taxonomy has a maximum depth of 4. We extract all entity mentions together with their contexts in the entire corpus. All the contexts in which NEs occur are randomly partitioned into two equally sized subsets. One is used for training and the other for testing, and vice versa. Like other hierarchical classification tasks, the hypernym classes contain all instances of their hyponym classes when constructing the datasets. For example, Mozart is"
C12-2068,W09-1125,0,0.0795897,"amed entity categories defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context inf"
C12-2068,W10-2415,0,0.0688523,"s defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context information “NBA Champions”,"
C12-2068,P11-1053,0,0.0465885,"Missing"
C12-2068,J92-4003,0,0.444087,"e this cluster-based features into our model. Combining these motivations, we present a method exploiting Multi-features for fine-grained classification of NEs in this paper. The only input data for our algorithm is a few manually annotated entities for each class. In addition to adopting the context word features and the word sense disambiguation features proposed by prior work, this paper puts forward three new features: the cluster-based features, the entity-related features and the class-specific features. 1. Cluster-based features are generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. 2. Entity-related features are context features introduced by other related entities. 3. Class-specific features are words extracted for each class. Each word is given a classspecific score denoting its ability to indicate the relevant class. 694 Our work presented here concentrates on the subcategorization of person names, since the previous researches have indicated that the classification of person names which relies on much more contextual information are often more challenging. The person instances are already identified as entities, and only being classifi"
C12-2068,N04-1043,0,0.0179531,"n a window for each entity mention. Only three individual word tokens and their PoS tags before and after the occurrence of the mention will be added into the feature set. In this paper, a context word and its PoS tag are tied together as an ensemble feature. For an entity mention Wi, its context words will be represented as: fcii33 ( wi 3& posi 3) ( wi 3& posi 3) . 2.2 Cluster-based Features Bag-of-words model cannot deal with synonyms. To address this flaw, some work took advantage of the cluster-based features. The preliminary idea of using word clusters as features was presented by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. Ang Sun et al. (2011) use the Brown algorithm to generate the word clusters as additional features which are applying to improve the performance of the relation extraction system. They use the English portion of the TDT5 corpora as their unlabeled data for inducing word clusters. The result of this word clusters is a binary tree. A particular word can be assigned a binary string by following the path from the root to"
C12-2068,W02-2002,0,0.0969582,"Missing"
D13-1199,C12-1047,0,0.017771,"Missing"
D13-1199,P13-2039,1,0.67587,"Missing"
D13-1199,P10-1066,0,0.0164922,"Missing"
D13-1199,P11-1032,1,0.846027,"models to generalize better across reviews of different offerings. We evaluate our approach on the task of identifying (ranking) manipulated hotels. In particular, in the absence of gold standard offering-level labels, we introduce a novel evaluation procedure for this task, in which we rank numerous versions of each hotel, where each hotel version contains a different number of injected, known deceptive reviews. Thus, we expect hotel versions with larger proportions of deceptive reviews to be ranked higher than those with smaller proportions. For labeled training data, we use the Ott et al. (2011) dataset of 800 positive (5-star) reviews of 20 Chicago hotels (400 deceptive and 400 truthful). For evaluation, we construct a new FOUR - CITIES dataset, containing 40 deceptive and 40 truthful reviews for each of eight hotels in four different cities (640 reviews total), following the procedure outlined in Ott et al. (2011). We find that our manifold ranking approach outperforms several state-of-theart learning baselines on this task, including transductive Support Vector Regression. We additionally apply our approach to a large-scale collection of real-world reviews from TripAdvisor and exp"
D13-1199,D09-1026,0,0.00798871,". Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects” of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., user-specific, location-specific, timespecific) (Ramage et al., 2009). Manifold Ranking Algorithm. The manifoldranking method (Zhou et al, 2003a; Zhou et al, 2003b) is a mutual reinforcement ranking approach initially proposed to rank data points along their underlying manifold structure. It has been widely used in many different ranking applications, such as summarization (Wan et al, 2007; Wan and Yang, 2007). 3 Dataset In this paper, we train all of our models using the CHICAGO dataset of Ott et al (2011), which contains 20 deceptive and 20 truthful reviews from each of 20 Chicago hotels (800 reviews total). This dataset is 2 Approaches for identifying indivi"
D13-1199,W03-0502,0,0.0248288,"Missing"
D14-1053,W06-1655,0,0.0153919,"vernment”) are retained (Example 3). Some words (i.e. U.S. imperialism) can be both target and expression, and there can be multiple targets (Example 2) within one sentence. We use a semi-Markov Conditional Random Fields (semi-CRFs) (Sarawagi and Cohen, 2004; Okanohara et al., 2006) algorithm for target and expression extraction. Semi-CRF are CRFs that relax the Markovian assumptions and allow for sequence labeling at the segment level. It has been demonstrated more powerful that CRFs in multiple sequence labeling applications including NER (Okanohara et al., 2006), Chinese word segmentation (Andrew, 2006) and opinion expression identification (Yang and Cardie, 2012). Our approach is an extension of Yang and Cardie (2012)’s system9 . Features we adopted included: • word, part of speech tag, word length. • left and right context words within a window of 2 and the correspondent POS tags. • NER feature. • subjectivity lexicon features from dictionary10 . The lexicon consists of a set of Chinese words that can act as strong or weak cues to subjectivity. • segment-level syntactic features defined in (Yang and Cardie, 2012). Most existing NER systems can barely recognize entities such as [ Vietnamese"
D14-1053,O07-6006,0,0.0647467,"Missing"
D14-1053,P06-1059,0,0.0232479,"related work is the approach introduced by O’Connor et al. (O’Connor et al., 2013) that extracts international relations from political contexts. 3 While the majority of subjective sentences omit the opinion holder, as in Examples 1 and 2, there are still a few circumstances where opinion holders (e.g., “we”, “Chinese people”, “Chinese government”) are retained (Example 3). Some words (i.e. U.S. imperialism) can be both target and expression, and there can be multiple targets (Example 2) within one sentence. We use a semi-Markov Conditional Random Fields (semi-CRFs) (Sarawagi and Cohen, 2004; Okanohara et al., 2006) algorithm for target and expression extraction. Semi-CRF are CRFs that relax the Markovian assumptions and allow for sequence labeling at the segment level. It has been demonstrated more powerful that CRFs in multiple sequence labeling applications including NER (Okanohara et al., 2006), Chinese word segmentation (Andrew, 2006) and opinion expression identification (Yang and Cardie, 2012). Our approach is an extension of Yang and Cardie (2012)’s system9 . Features we adopted included: • word, part of speech tag, word length. • left and right context words within a window of 2 and the correspo"
D14-1053,H05-1045,0,0.0304063,"tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-superv"
D14-1053,P13-1108,0,0.067508,"Missing"
D14-1053,W06-1651,0,0.0348598,"traction. In one direction, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations"
D14-1053,W02-1011,0,0.0160184,"in this specific task: First, the heavy use of linguistic phenomenon in the People’s Daily including rhetoric, metaphor, proverb, or even nicknames, makes existing approaches less effective for sentiment inference as identifying these expressions is a hard NLP problem in nature. Second, as we are more interested in the degree of sentiment rather than binary classification (i.e., positive versus negative) towards an entity (e.g. country or individual) in the news article, straightforward algorithms to apply would be documentlevel sentiment analysis approaches such as vector machine/regression (Pang et al., 2002) or supervised LDA (Blei and McAuliffe, 2010). A single news article, usually contains different attitudes towards multiple countries or individuals simultaneously (say praising “friends” and criticizing “enemies”), as shown in the following example from the People’s Daily of Mar. 17th, 1966: We propose a semi-supervised bootstrapping algorithm for analyzing China’s foreign relations from the People’s Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, ti"
D14-1053,J11-1002,0,0.0203972,"ted time period, China held a pretty negative attitude towards the USA based on clues such as common negative expressions (e.g., “evil” or “reactionary”), we can easily induce that “a tiger made of paper”, is a negative word. Based on aforementioned two assumptions, we formulate our approach as a semi-supervised model, which simultaneously bootstrap sentiment target lists, extracts subjective vocabularies and 3 Leader of South Vietnam Ruling political party of Vietnam. 5 One of Founders of Democratic Republic of Vietnam (North Vietnam) and Vietnam Workers’ party. 4 468 propagation algorithms (Qiu et al., 2011; Qiu et al., 2009; Zhang et al., 2010; Duyu et al., 2013). Concretely, Qiu et al. (2011) proposed a rulebased semi-supervised framework called double propagation for jointly extracting opinion words and targets. Compared to existing bootstrapping approaches, our framework is more general one with less restrictions6 . In addition, our approach harness global information (e.g. document-level, time-level) to guide the bootstrapping algorithm. Another related work is the approach introduced by O’Connor et al. (O’Connor et al., 2013) that extracts international relations from political contexts. 3"
D14-1053,W10-2910,0,0.0218248,"employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain pe"
D14-1053,C08-1103,0,0.0179182,"t al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a sing"
D14-1053,C04-1200,1,0.426499,"Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain period of time, sentiments towards an entity ar"
D14-1053,W06-0301,1,0.7609,"rection, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approac"
D14-1053,D12-1122,0,0.126061,"ning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain period of time, sentiments"
D14-1053,P14-1031,0,0.0316619,"or metaphor recognition. 2. In Analytical Political Science, the quantitative evaluation of diplomatic relations is usually a manual task (Robinson and Shambaugh, 1995). We are hopeful that our algorithm can enable automated political analysis and facilitate political scientists’ and historians’ work. 2 Related Works Significant research efforts have been invested into sentiment analysis and opinion extraction. In one direction, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holder"
D14-1053,W03-1730,0,0.0110686,"m 1950 to 2010, across a 60-year time span. 15 Negative effect of strict sentence selection can be partly compensated by the consideration of time-level information 471 antagonism (m=1) tension (m=2) disharmony (m=3) neutrality (m=4) goodness (m=5) friendship (m=6) brotherhood (m=7) P 残暴(extremely cruel), 敌人(enemy) 愤慨(indignation), 侵犯(offend) 失望(disappointed), 遗憾(regret) 关切, 关注(concern) 发展的(developmental), 尊重(respect) 友谊(friendship), 朋友(friend) 伟大(firmly), 兄弟(brother) Table 1: Illustration of subjective list M News articles are first segmented using ICTCLAS Chinese segmentation word system16 (Zhang et al., 2003). Articles with fewer than 200 Chinese words are discarded. News articles are clustered by the presence of a country’s name more than 2 times based on a country name list from Wikipedia17 . Articles mentioning more than 5 different countries are discarded since they usually talk about international conferences. Note that one article can appear in different collections (example in Section 1 will appear in both Vietnam and the U.S. collection). Compound sentences are segmented into clauses based on dependency parse tree. Then those containing more than 50 characters or less than 4 characters are"
D14-1053,C10-2167,0,0.0491108,"Missing"
D14-1053,W11-0705,0,\N,Missing
D14-1214,P11-2000,0,0.166133,"Missing"
D14-1214,P11-1040,0,0.0160923,"sonal topic needs to be adequately discussed by the user and their followers in order to be detected16 . Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structu"
D14-1214,D13-1114,0,0.00534075,"roughly constant, but recall increases as more life events and C ONGRATULA TIONS and C ONDOLENCES are discovered. 8 Related Work Our work is related to three lines of NLP researches. (1) user-level information extraction on social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitl"
D14-1214,P07-1030,0,0.0127633,"disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major li"
D14-1214,D13-1192,0,0.0182098,"social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoid"
D14-1214,P12-1056,0,0.222329,"important life events on which algorithms can rely for extraction or classification. Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter Challenge 2: Noisiness of Twitter Data: The user-generated text found in social media websites such as Twitter is extremely noisy. The language used to describe life events is highly varied and ambiguous and social media users frequently discuss public news and mund"
D14-1214,P11-1055,0,0.00961363,"of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and s"
D14-1214,D14-1108,0,0.00709329,"e Stanford PragBank10 , 8 Most tweets in the bootstrapping output are positive. The majority of results returned by Twitter Search are negative examples. 10 http://compprag.christopherpotts.net/ factbank.html 2002 9 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11 . • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from (Kong et al., 2014). As the tweet parser we use only supports one-to-one dependency path identification but no dependency properties, Dependency is a binary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and"
D14-1214,P10-1150,1,0.666806,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,N10-1087,1,0.304045,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,P08-1119,1,0.214259,"potting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a rel"
D14-1214,P14-1016,1,0.167411,"ic event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of"
D14-1214,D11-1024,0,0.00393037,"by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics"
D14-1214,P09-1113,0,0.0027458,"on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying o"
D14-1214,D08-1027,0,0.0347829,"Missing"
D14-1214,N10-1012,0,0.00569359,"2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D ="
D14-1214,N13-1039,0,0.0248148,"Missing"
D14-1214,D11-1135,0,0.0127003,"lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not sto"
D14-1214,N10-1020,1,0.423031,"nd C ONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2 Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twi"
D14-1214,D11-1141,1,0.538137,"Missing"
D14-1214,D11-1091,0,0.0235403,"Missing"
D14-1214,W09-1703,0,\N,Missing
D14-1218,P11-2022,0,0.482017,"equest from the first author. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when"
D14-1218,P08-2011,0,0.828832,"le. 1 Code available at stanford.edu/˜jiweil/ or by request from the first author. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which as"
D14-1218,N07-1055,0,0.0447232,"pplied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5 For more details on backpropagation through RNNs, see Socher et al. (2010). 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption has been verified in Lin et"
D14-1218,W07-2321,0,0.0776975,"ependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in"
D14-1218,D12-1106,0,0.274934,"k survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when deciding on ordering. More importantly, the f"
D14-1218,J86-3001,0,0.370411,"f which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve"
D14-1218,J95-2003,0,0.34486,"es and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document ve"
D14-1218,P13-1010,0,0.763213,"or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporat"
D14-1218,P88-1012,0,0.172648,"1 and Eduard Hovy3 Science Department, Stanford University, Stanford, CA 94305, USA Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA jiweil@stanford.edu ehovy@andrew.cmu.edu Abstract Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to gover"
D14-1218,P88-1020,1,0.68675,"t adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argume"
D14-1218,W13-3214,0,0.0117049,"ly, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework. Specifically, we consider a WINDOW approach for sentences, as shown in Figure 1, where positive examples are windows of sentences selected from original articles generated by humans, and negatives examples are genera"
D14-1218,P91-1008,0,0.771789,"Stanford, CA 94305, USA Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA jiweil@stanford.edu ehovy@andrew.cmu.edu Abstract Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defi"
D14-1218,P11-1100,0,0.0697077,"Missing"
D14-1218,P00-1052,0,0.0375599,"he rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for examp"
D14-1218,P89-1025,0,0.708875,"xplanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such a"
D14-1218,P14-1028,0,0.00637534,"with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments Θ = [WRecurrent , Wsen , Usen ] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5 For more details on backpropagation through RNNs, see Socher et al. (2010). 2043 2011) that all use"
D14-1218,J04-3003,0,0.022481,"Missing"
D14-1218,P05-1065,0,0.017419,"Missing"
D14-1218,P12-2018,0,0.00599174,"clause or a coherent text to consider when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representation"
D14-1218,P05-1044,0,0.00959549,"ate children recursively in a bottom-up fashion until reaching the where Wsen is a H × (L × K) dimensional matrix root of the tree. Concretely, for a given parent p and bsen is a H × 1 dimensional bias vector. in the tree and its two children c1 (associated with 4 vector representation hc1 ) and c2 (associated with instead of a binary classification (correct/incorrect), another commonly used approach is the contrastive approach vector representation hc2 ), standard recursive netthat minimizes the score function max(0, 1 − s + sc ) (Colworks calculates hp for p as follows: lobert et al., 2011; Smith and Eisner, 2005). s denotes the hp = f (WRecursive · [hc1 , hc2 ] + bRecursive ) (3) where [hc1 , hc2 ] denotes the concatenating vector for children vector representation hc1 and hc2 . score of a true (coherent) window and sc the score of a corrupt (containing incoherence) one) in an attempt to make the score of true windows larger and corrupt windows smaller. We tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task. 2042 Figure 3: An example of coherence model based on a window of sentences (clique). The output la"
D14-1218,P13-2032,0,0.018192,"been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King −man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence"
D14-1218,D13-1141,0,0.00588231,"corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King −man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence of words s = {w1 , w2 , ..., wns }, where ns denotes the number of words withi"
D14-1218,D12-1110,0,0.00876346,"urrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem (Bengio et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3 . Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3 For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Repre"
D14-1218,D13-1170,0,0.0254596,"r when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning frame"
D14-1218,J99-3001,0,0.0253628,"ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input"
D14-1218,N04-1015,0,\N,Missing
D14-1218,J08-1001,0,\N,Missing
D14-1220,W05-0613,0,0.0430951,"to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs. . . . Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernaul"
D14-1220,P09-1075,0,0.120896,"discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler"
D14-1220,P12-1007,0,0.547761,"e parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context"
D14-1220,P14-1048,0,0.252924,"core in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach HILDA Joty et al. Feng and Hirst Ji and Eisenstein Unified (with feature) Ours (no feature) Ours (with feature) human Span 75.3 82.5 85.7 82.1 82.0 82.4 84.0 88.7 Nuclearity 60.0 68.4 71.0 71.1 70.0 69.2 70.8 77.7 relies on two linear-chain CRFs to obtain a sequence of discourse constituents. Relation 46.8 55.7 58.2 61.6 57.1 56.8 58.6 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NU CLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by the multi-class classifier. 6.1 hp = f (Wsen · [hei , hej ] + bsen ) Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning"
D14-1220,P07-1062,0,0.664439,"possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous"
D14-1220,D10-1039,0,0.0555268,"etely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it pl"
D14-1220,P13-2032,0,0.0187173,"children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with disco"
D14-1220,P14-1002,0,0.622711,"a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requ"
D14-1220,D12-1083,0,0.697012,"arly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level"
D14-1220,P13-1048,0,0.855845,"ntify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it"
D14-1220,D09-1036,0,0.0597413,"Missing"
D14-1220,W10-4327,0,0.181757,"n a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the se"
D14-1220,J00-3005,0,0.874306,"communicative (rhetorical) function. The functions are reflected in text as signals of the author’s intentions, and take various forms (including expressions such as “therefore”, “for example”, “the answer is”, and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an entire text , each one associated with some discourse relation. In order to build a text’s hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs. . . . Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea f"
D14-1220,P14-1028,0,0.00774173,"× n × n dynamic programming table P r, the cell P r[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds between the two spans within i to j. P r[r, i, j] is computed as follows: P r[r, i, j] =maxr1 ,r2 ,k P r[r1 , i, k] · P r[r2 , k, j] • POS at the beginning and end of the EDUs. ×P (tbinary (e[i,k] , e[k,j] ) = 1) • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let gτi denote the subgradient at time step t for parameter θi obtained from backpropagation, the parameter update at time step t is given by: α p gτi gτi2 t=0 θτ = θτ −1 − Pτ (11) where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr }, W , Gbinary , Gmulti , Ubinary , Umulti are initialized by randomly drawing from the uniform distribution [−, ], where  is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0"
D14-1220,prasad-etal-2008-penn,0,0.114236,"Missing"
D14-1220,D12-1110,0,0.0322764,"this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the R"
D14-1220,D13-1170,0,0.022025,"ions. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad"
D14-1220,N03-1030,0,0.412895,"each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how"
D14-1220,H05-1033,0,0.0225916,"s, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less stra"
D14-1220,N09-1064,0,0.284224,"Missing"
D14-1220,miltsakaki-etal-2004-penn,0,\N,Missing
D14-1220,C04-1048,0,\N,Missing
D14-1220,W01-1605,0,\N,Missing
D14-1220,Q13-1026,0,\N,Missing
D14-1220,D13-1031,0,\N,Missing
D15-1200,D14-1110,0,0.589369,"Missing"
D15-1200,S14-2001,0,0.0144059,"12). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) 0.748 Standard(100) 0.770 Standard(300) 0.798 Greedy (50) 0.760 (+0.12) Global+G (100) 0.782 (+0.12) Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related). In our setting, the similarity between two sentences is measured based on sentence-level embeddings. Let s1 and s2 denote two sentences and es1 and es2 denote corresponding embeddings. es1 and es2 are achieved through recurrent or recursive models (as illustrated in Appendix section). Again,"
D15-1200,W09-2415,0,0.0878349,"Missing"
D15-1200,P12-1092,0,0.656101,"bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while ‘multi-se"
D15-1200,D14-1113,0,0.655089,"for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations."
D15-1200,W02-1011,0,0.019858,"–21 for validation and sections 22–24 for testing. Similar to NER, we trained 5layer neural models which take the concatenation of neighboring embeddings as inputs. We adopt a similar training and parameter tuning strategy as for POS tagging. Standard (50) 0.925 Standard (100) 0.940 Standard (300) 0.954 Greedy (50) 0.934 (+0.09) Global+G (100) 0.946 (+0.06) Expectation (50) 0.938 (+0.13) Global+E (100) 0.952 (+0.12) Table 4: Accuracy for Different Models on Part of Speech Tagging. P-value 0.033 for 50d and 0.031 for 100d. Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure. Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section). The obtained embedding is then fed into a sigmoid classifier. Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models. For train"
D15-1200,D14-1162,0,0.0815287,"increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Hu"
D15-1200,N10-1013,0,0.83385,"(2006)). Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vec"
D15-1200,D12-1110,0,0.026977,"Sentiment Treebank.). P-value 0.250 for 50d and 0.401 for 100d. Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2 ” classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details. We follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) 0.748 Standard(100) 0.770 Standard(300) 0.798 Greedy (50) 0.760 (+0.12) Global+G (100) 0.782 (+0.12) Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Mar"
D15-1200,D13-1170,0,0.0130186,"dels. For training, we adopt AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set. Due to space limitations, we omit details of recurrent models and training. Standard (50) 0.750 Standard (100) 0.768 Standard (300) 0.774 Greedy (50) 0.752(+0.02) Global+G (100) 0.765(-0.03) Expectation (50) 0.750(+0.00) Global+E (100) 0.763(-0.05) Table 5: Accuracy for Different Models on Sentiment Analysis (Pang et al.’s dataset). P-value 0.442 for 50d and 0.375 for 100d. Sentiment Analysis–Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset. Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree c"
D15-1200,P15-1150,0,0.00866231,"state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d). 6.1 The Tasks Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a mult"
D15-1278,P14-2009,0,0.0279958,"based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direc"
D15-1278,D13-1137,0,0.036062,"bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the par"
D15-1278,W09-2415,0,0.0297319,"Missing"
D15-1278,D14-1070,0,0.0537703,"Missing"
D15-1278,D13-1176,0,0.00928056,"ly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase"
D15-1278,D14-1218,1,0.0712307,"parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). H"
D15-1278,D14-1220,1,0.925709,"3): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially. • Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations. In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances). Tree Sequence P-value Bi-Sequence P-value • LST"
D15-1278,N13-1090,0,0.0122332,"ks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robin"
D15-1278,W02-1011,0,0.0304553,"alchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive. • Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether parsing is useful for finding similarities between questio"
D15-1278,D13-1170,0,0.526172,"uentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For"
D15-1278,D11-1014,0,0.164886,"pacted into one component, and the error propagation is thus given by: error→ second-clause → first-clause → was→plot→the→as→simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4 . Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible explanation is the distance 4 https://code.google.com/p/word2vec/ of the supervision signal from the local compositional structure. The Pang et al. dataset has an average sentence length of 22.5 words,"
D15-1278,D12-1110,0,0.765115,"ing recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but the"
D15-1278,W01-1605,0,\N,Missing
D15-1278,P15-1150,0,\N,Missing
D15-1278,P15-1002,1,\N,Missing
D15-1278,D14-1162,0,\N,Missing
D16-1127,D11-1054,1,\N,Missing
D16-1127,W00-0306,0,\N,Missing
D16-1127,P02-1040,0,\N,Missing
D16-1127,P10-1083,0,\N,Missing
D16-1127,P15-1152,0,\N,Missing
D16-1127,P16-1094,1,\N,Missing
D16-1127,D16-1230,0,\N,Missing
D16-1127,P11-1028,0,\N,Missing
D16-1127,P16-1153,1,\N,Missing
D17-1019,P04-1051,0,0.0280782,"Missing"
D17-1019,J08-1001,0,0.935501,"model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed description of this commonly used dataset and training/testing are found in the Appendix. We report the performance of the following baselines widely used in the coherence literature. (1) Entity Grid Model: The grid model presented in Barzilay and Lapata (2008). Results are directly taken from Barzilay and Lapata’s (2008) paper. We also consider variations of entity grid models, such as Louis and Nenkova (2012) which models the cluster transition probability and the Graph Based Approach which uses a graph to represent the entity transitions needed for local coherence computation ("
D17-1019,N04-1015,0,0.0306036,"Missing"
D17-1019,J05-3002,0,0.0368956,"entences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder"
D17-1019,E12-1032,0,0.686483,"Missing"
D17-1019,P13-1010,0,0.170712,"dels obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text generated by the model. 198 Our work marks an initial step in building endto-end systems to evaluate open-domain discourse coherence, and more importantly, generating coherent texts given discourse contexts. 1 Adding coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011) and entity graphs (Guinaudeau and Strube, 2013). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 The Discriminative Model The discriminative model treats cliques (sets of sentences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples. The discriminative model can be viewed as an extended version of Li and Hovy’s (2014) model but is practical at large scale2 . We thus make th"
D17-1019,P88-1020,0,0.699611,"s that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder models with latent variabl"
D17-1019,N16-1037,0,0.0494803,"Missing"
D17-1019,P03-1069,0,0.0819021,"0.108 0.101 adver-2 0.120 0.104 0.090 0.078 0.068 adver-3 0.054 0.043 0.039 0.030 0.024 Table 4: Adversarial Success for different models. 4.2.2 Paragraph Reconstruction The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large open-domain version. We therefore believe it is time for the community to move to a more difficult task for measuring coherence. We suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003). More formally, given a set of permuted sentences s1 , s2 , ..., sN (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s. Because the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don’t know the surrounding context. Hence, we only use the generative model. The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partia"
D17-1019,J06-4002,0,0.157342,"Missing"
D17-1019,P91-1008,0,0.6263,"Missing"
D17-1019,D14-1218,1,0.901993,"S EQ model, the only difference being that the current token to predict not only depends on the LSTM output ht , but also zn . Given the sampled zn , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix). 202 Acci 0.930 0.755 0.770 0.864 0.904 0.822 0.842 0.742 0.846 0.705 0.660 Earthq 0.992 0.930 0.931 0.976 0.872 0.938 0.911 0.953 0.635 0.682 0.667 Aver 0.956 0.842 0.851 0.920 0.888 0.880 0.876 0.847 0.740 0.688 0.664 Table 1: Results from different coherence models. Results for the Recursive model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed desc"
D17-1019,P15-1107,1,0.740173,"Missing"
D17-1019,P11-1100,0,0.126817,"Missing"
D17-1019,D12-1106,0,0.0954984,"the current token to predict not only depends on the LSTM output ht , but also zn . Given the sampled zn , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix). 202 Acci 0.930 0.755 0.770 0.864 0.904 0.822 0.842 0.742 0.846 0.705 0.660 Earthq 0.992 0.930 0.931 0.976 0.872 0.938 0.911 0.953 0.635 0.682 0.667 Aver 0.956 0.842 0.851 0.920 0.888 0.880 0.876 0.847 0.740 0.688 0.664 Table 1: Results from different coherence models. Results for the Recursive model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed description of this commonly used dataset and trainin"
D17-1019,D15-1166,0,0.0247322,"Kiros et al., 2015), which build an encoder-decoder model by predicting tokens in neighboring sentences. As shown in Figure 1a, given two consecutive sentences [si , si+1 ], one can measure the coherence by the likelihood of generating si+1 given its preceding sentence si (denoted by uni). This likelihood is scaled by the number of words in si+1 (denoted by Ni+1 ) to avoid favoring short sequences. L(si , si+1 ) = 1 log p(si+1 |si ) Ni+1 (1) The probability can be directly computed using a pretrained S EQ 2S EQ model (Sutskever et al., 2014) or an attention-based model (Bahdanau et al., 2015; Luong et al., 2015). In a coherent context, a machine should not only be able to guess the next utterance given the preceding ones, but also the preceding one given the following ones. This gives rise to the coherence model (denoted by bi) that measures the bidirectional dependency between the two consecutive sentences: L(si , si+1 ) = 1 log pB (si |si+1 ) Ni 1 + log pF (si+1 |si ) Ni+1 (2) We separately train two models: a forward model pF (si+1 |si ) that predicts the next sentence based on the previous one and a backward model pB (si |si+1 ) that predicts the previous sentence given the next sentence. pB (si"
D17-1019,J91-1002,0,0.597882,"a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMMLDA topic model into the generative model and (2) an end-to-end model that introduces a Markovstructured neural latent variable, inspired by recent work on trainin"
D17-1019,P11-1153,0,0.0213961,"proposed generative models for discourse coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sample"
D17-1019,D12-1009,0,0.0165449,"e coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the"
D17-1019,D14-1162,0,0.0976568,"Missing"
D17-1019,N10-1020,0,0.0213572,"decoder about the Figure 1: Overview of the proposed generative models for discourse coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from"
D17-1019,D11-1116,0,0.0523017,"Missing"
D17-1019,P11-2022,0,\N,Missing
D17-1019,P08-2011,0,\N,Missing
D17-1230,N16-1014,1,0.34709,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,P16-1094,1,0.386843,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,K16-1002,0,0.642309,"mpirical Methods in Natural Language Processing, pages 2157–2169 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics human-generated one. The output from the discriminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues. The idea of a Turing test—employing an evaluator to distinguish machine-generated texts from human-generated ones—can be applied not only to training but also testing, where it goes by the name of adversarial evaluation. Adversarial evaluation was first employed in Bowman et al. (2016) to evaluate sentence generation quality, and preliminarily studied for dialogue generation by Kannan and Vinyals (2016). In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable. Experimental results demonstrate that our approach produces more interactive, interesting, and non-repetitive responses than standard S EQ 2S EQ models trained using the MLE objective function. 2 Related Work Dialogue generation Response generation for dialogue can be viewed as a source-to-target transduction problem. Ritter et al. (2011) f"
D17-1230,D16-1137,0,0.00721876,"sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al. (2017) apply the idea to domain adaptation tasks. Our work is distantly related to recent work that formalizes sequence generation as an action-taking problem in reinforcement learning. Ranzato et al. (2016) train RNN decoders in a S EQ 2S EQ model using policy gradient to obtain competitive machine translation results. Bahdanau et al. (2017) take this a step further by training an actor-critic RL model for machine translation. Also related is recent work (Shen et al., 2016; Wiseman and Rush, 2016) to address the issues of exposure bias and loss-evaluation mismatch in neural translation. 3 Adversarial Training for Dialogue Generation In this section, we describe in detail the components of the proposed adversarial reinforcement 2158 learning model. The problem can be framed as follows: given a dialogue history x consisting of a sequence of dialogue utterances,1 the model needs to generate a response y = {y1 , y2 , ..., yT }. We view the process of sentence generation as a sequence of actions that are taken according to a policy defined by an encoder-decoder recurrent neural network. 3.1"
D17-1230,P15-1152,0,0.321537,"Missing"
D18-1431,H05-1042,0,0.0508253,"to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulation allows us to systematically combine information from three different models to produce better responses in terms of topic and semantic relevance. Conceptually, the likelihood term governs the grammatical structure of the response while the topic and semantic constraints drive content selection (Nenkova and Passonneau, 2004; Barzilay and Lapata, 2005). 4 Decoding with Distributional Constraints In Section 3, we defined two constraints (one topic constraint and one semantic) for use in the decoding objective. Incorporating these constraints during decoding requires that they factorize in a way that is compatible with left-to-right beam search over words in the response. The standard approach to computing posterior distributions in topic models requires a probabilistic inference procedure over the entire source and target. Furthermore, computing semantic representations can involve the use of complex neural architectures. Both of these proce"
D18-1431,D12-1091,0,0.0140613,"information to the conversation? We asked the evaluators to respond on a 5-point scale to the questions above (Strongly Agree, Agree, Unsure, Disagree, Strongly Disagree). These were later collapsed to 3 categories (Agree, Unsure, Disagree). The results for plausibility and content richness of our model in addition to the MMI and TA-Seq2Seq baselines and human responses are presented in Table 5. We observe that MMI200 and TA-10 models Statistical Significance of Results To verify the statistical significance of our findings, we conducted a pairwise bootstrap test (Efron and Tibshirani, 1994; Berg-Kirkpatrick et al., 2012) comparing the difference between percentage of Agree annotations (Yes column in the Table 5). We computed p-values for each pair of models: MMI200 vs DC-MMI200 and TA vs DC-MMI200. For plausibility, we did not find a significant difference in either comparison (pvalue ≈ 0.25) while for content richness, both differences were found to be significant (p-value &lt;10−4 ). To summarize: our model significantly beats both baselines in terms of content richness while the difference in plausibility was not found to be statistically significant. 3976 7.2.2 Pairwise Evaluation of Interestingness To furth"
D18-1431,N18-1016,0,0.0657807,"Missing"
D18-1431,D18-1241,0,0.0303352,"Missing"
D18-1431,W11-0609,0,0.0366974,"data by assuming each line corresponds to a full speaker turn. Although this assumption is often violated, prior work has successfully trained 3973 and evaluated neural conversation models using this corpus. In our experiments we used a preprocessed version of this dataset distributed by Li et. al. (2016a).6 The dataset contains large number of two turn dialogues out of which we sampled 23M to use as our training set and 10k as a validation set. Due to the noisy nature of the OpenSubtitles conversations we do not use them for evaluation. Instead, we leverage the Cornell Movie Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) which is much smaller but contains accurate speaker annotations. We extracted all two turn conversations (source target pair) from this corpus and removed those with less than three and more than 25 words. After this, we divided the remaining conversations into three buckets based on source length. The numbers can be found in Table 2. From each bucket we randomly sampled ≈333 dialogues for a total of 1000 dialogues in our test set. We evaluate all models on this test set. Since automatic metrics do not correlate with human judgment, we manually tuned the hyperparameters (α and β) on a small d"
D18-1431,D13-1111,0,0.0744224,"Missing"
D18-1431,P17-4012,0,0.0513069,"I-bidi reranking with a beam size of 200 (DC-MMI200). We test all configurations on the 1000 conversations test set described in Section 5 and compare them on automatic metrics and also in a crowdsourced human evaluation. We do not consider TA-200 (TA-Seq2Seq, Beam=200), DC-200 and MMI-10 for human evaluation as they appear to perform worse than other model variants in automatic metrics and also on our set of development sentences. Sample responses for all the remaining models are presented in Table 3. 7.1 http://nlp.stanford.edu/data/OpenSubData.tar 7 OpenNMT is used for training our models (Klein et al., 2017). Results and Analysis Automatic Metrics Following Li et. al. (2016a), we report distinct-1 and distinct-2, which measure the diversity of re3974 Source Target (ground truth) in there , sir . MMI here ’s your jacket ! uh , thanks ... i don ’t want it ! what ’s so damn funny ? been to any good &lt;unk&gt;lately ? what are you laughing at ? what kind of suit is this ? what ’s the matter with you ? well , what exactly does our platoon do ? serve &lt;unk&gt;? process paperwork ? left us here to rot . that ’s what they ’ve done . heroes of the newspapers ! that ’s it . you ’re not setting foot off this ship un"
D18-1431,D17-1259,0,0.0405372,"Missing"
D18-1431,N16-1014,1,0.743863,"ents, without relying on hand-written rules or manual annotation. Such response generation models could be combined with traditional dialogue systems to enable more natural and adaptive conversation, in addition to new applications such as predictive response suggestion (Kannan et al., 2016), however many challenges remain. A major drawback of neural conversation generation is that it tends to produce too many “safe” or generic responses, for example: “I don’t know” or “What are you talking about ?”. This is a pervasive problem that has been independently reported by multiple research groups (Li et al., 2016a; Serban et al., 2016; Li et al., 2016c).1 The effect is due to the use of conditional likelihood as a decoding objective – maximizing conditional likelihood is a suitable choice for text-to-text generation tasks such as machine translation, where the source and target are semantically equivalent, however, in conversation there are many acceptable ways to respond. Simply choosing most predictable reply often leads to very dull conversation. Figure 1 illustrates the problem with conditional likelihood using an example. After encoding the source message using a bidirectional LSTM with attention"
D18-1431,D16-1127,1,0.753379,"ents, without relying on hand-written rules or manual annotation. Such response generation models could be combined with traditional dialogue systems to enable more natural and adaptive conversation, in addition to new applications such as predictive response suggestion (Kannan et al., 2016), however many challenges remain. A major drawback of neural conversation generation is that it tends to produce too many “safe” or generic responses, for example: “I don’t know” or “What are you talking about ?”. This is a pervasive problem that has been independently reported by multiple research groups (Li et al., 2016a; Serban et al., 2016; Li et al., 2016c).1 The effect is due to the use of conditional likelihood as a decoding objective – maximizing conditional likelihood is a suitable choice for text-to-text generation tasks such as machine translation, where the source and target are semantically equivalent, however, in conversation there are many acceptable ways to respond. Simply choosing most predictable reply often leads to very dull conversation. Figure 1 illustrates the problem with conditional likelihood using an example. After encoding the source message using a bidirectional LSTM with attention"
D18-1431,D17-1230,1,0.868792,"l. (2018) develop models which converse while assuming a persona defined by a short description of attributes. Wang et. al. (2017) suggested decoding methods that influence the style and topic of the generated response. Bosselutet al. (2018) develop discourse-aware rewards with reinforcement learning (RL) to generate long and coherent texts. Li et. al. (2016c) applied deep reinforcement learning to dialogue generation to maximize long-term reward of the conversation, as opposed to directly maximizing likelihood of the response. This line of work was further extended with adversarial learning (Li et al., 2017) that rewards generated conversations that are indistinguishable from real conversations in the data. Lewis et. al. (2017) applied reinforcement learning with dialogue rollouts to generate replies that maximize expected reward, while learning to generate responses from a crowdsourced dataset of negotiation dialogues. Choi et. al. (2018) used crowd-workers to gather a corpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. Niu and Bansal (2018) designed a number of weakly-supervised models that generate polite, neutral or rude responses. Their fusion"
D18-1431,D16-1230,0,0.209707,"Missing"
D18-1431,P17-1103,0,0.0466107,"d model is Polite-RL which assigns a reward based on a politeness classifier. Gimpel et. al. (2013) explored methods for increasing the diversity of N-best lists in machine translation by in3977 troducing a pairwise dissimilarity function. Similar ideas have been explored in the context of neural generation models. (Vijayakumar et al., 2016; Li and Jurafsky, 2016; Li et al., 2016b) Following previous work we evaluated our approach using a combination of automatic metrics and human judgments. Some recent work has explored the possibility of adversarial evaluation of neural conversation models (Lowe et al., 2017; Li et al., 2017). 9 Conclusions We presented an approach to generate more interesting responses in neural conversation models by incorporating side information in the form of distributional constraints. When using maximum likelihood decoding objectives, neural conversation models tend to generate safe responses, such as “I don’t know” for most inputs. Our proposed approach provides a flexible method of incorporating a broad range of distributional constraints into the decoding objective. We proposed and empirically evaluated two constraints that factorize over words, and therefore naturally"
D18-1431,P08-1099,0,0.0303836,"017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior regularization introduces similar distributional constraints on expectations computed over unlabeled data using a model’s parameters. These are typically added to the learning objective for semi-supervised scenarios where available labeled data is limited. In contrast, our approach introduces distributional constraints into the decoding objective as a way to combine neural conversation models trained on large quantities of conversational data with separately trained models of topics and semantic similarity that can drive content selection. T"
D18-1431,N04-1019,0,0.136084,"unction that maps an utterance to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulation allows us to systematically combine information from three different models to produce better responses in terms of topic and semantic relevance. Conceptually, the likelihood term governs the grammatical structure of the response while the topic and semantic constraints drive content selection (Nenkova and Passonneau, 2004; Barzilay and Lapata, 2005). 4 Decoding with Distributional Constraints In Section 3, we defined two constraints (one topic constraint and one semantic) for use in the decoding objective. Incorporating these constraints during decoding requires that they factorize in a way that is compatible with left-to-right beam search over words in the response. The standard approach to computing posterior distributions in topic models requires a probabilistic inference procedure over the entire source and target. Furthermore, computing semantic representations can involve the use of complex neural archit"
D18-1431,Q18-1027,0,0.0130513,"irectly maximizing likelihood of the response. This line of work was further extended with adversarial learning (Li et al., 2017) that rewards generated conversations that are indistinguishable from real conversations in the data. Lewis et. al. (2017) applied reinforcement learning with dialogue rollouts to generate replies that maximize expected reward, while learning to generate responses from a crowdsourced dataset of negotiation dialogues. Choi et. al. (2018) used crowd-workers to gather a corpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. Niu and Bansal (2018) designed a number of weakly-supervised models that generate polite, neutral or rude responses. Their fusion model combines a language model trained on polite utterances with the decoder. In the second method they prepend the utterance with a politeness label and scale its embedding to vary politeness. The third model is Polite-RL which assigns a reward based on a politeness classifier. Gimpel et. al. (2013) explored methods for increasing the diversity of N-best lists in machine translation by in3977 troducing a pairwise dissimilarity function. Similar ideas have been explored in the context"
D18-1431,N10-1020,1,0.744556,"DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior regularization introduces similar distributional constraints on expectations computed over unlabeled data using a model’s parameters. These are typica"
D18-1431,D11-1054,1,0.821319,"uccessfully inject content words into candidate hypotheses and that MMI is able to effectively choose plausible candidates. In the case of DC-10 and DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior r"
D18-1431,P15-1152,0,0.0778861,"Missing"
D18-1431,D17-1235,0,0.0611142,"d P (T |Y ), where T is a random variable defined over k topics. Then we can modify the decoding objective from Eq 1: Y https://www.ranks.nl/stopwords The top 10 topic words were taken from each of the 50 topics inferred by an HMM-LDA model (after removing stop words). (1) log P (wi |w1 , . . . wi−1 , X)} Yˆ T = arg max{ log P (Y |X)+ As a starting point for our approach we leverage the Seq2Seq model (Sutskever et al., 2014; Bahdanau et al., 2014) which has been used as a basis for a broad range of recent work on neural conversation (Kannan et al., 2016; Li et al., 2016a; Serban et al., 2016; Shao et al., 2017). This model consists of two parts, an encoder and a decoder both of which are typically stacked LSTM layers. The encoder reads the input sequence and creates 3 = arg max{log P (Y |X)} Y Neural Conversation Generation 2 Distributional Topic and Semantic Similarity Constraints (2) α × ∆(P (T |X), P (T |Y ))} Here, ∆ is a similarity function between the two probability distributions and α is a tunable hyperparameter to adjust impact of this constraint. Much recent work has investigated how to encode the semantic meaning of a sentence into a fixed high dimensional embedding space (Kiros et al., 2"
D18-1431,N15-1020,1,0.872914,"Missing"
D18-1431,D17-1228,0,0.032418,"Missing"
D18-1431,D15-1199,0,0.08673,"Missing"
D18-1431,P17-1190,0,0.0234424,"s model consists of two parts, an encoder and a decoder both of which are typically stacked LSTM layers. The encoder reads the input sequence and creates 3 = arg max{log P (Y |X)} Y Neural Conversation Generation 2 Distributional Topic and Semantic Similarity Constraints (2) α × ∆(P (T |X), P (T |Y ))} Here, ∆ is a similarity function between the two probability distributions and α is a tunable hyperparameter to adjust impact of this constraint. Much recent work has investigated how to encode the semantic meaning of a sentence into a fixed high dimensional embedding space (Kiros et al., 2015; Wieting and Gimpel, 2017). Given such an embedding representation of X and Y , one can find the semantic similarity between the two and similar to Eq 2 we can add a semantic similarity constraint to the likelihood objective as 3971 follows: Yˆ Emb = arg max{ log(P (Y |X))+ Y (3) β × ∇(Emb(X), Emb(Y ))} where, Emb() is a function that maps an utterance to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulatio"
D18-1431,W13-4065,0,0.0112187,"in Table 6. We observe that with a beam size of 10 our model is able to generate content rich responses, but suffers in terms of plausibility. The values in the table suggests the decoding constraints defined in this work successfully inject content words into candidate hypotheses and that MMI is able to effectively choose plausible candidates. In the case of DC-10 and DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a)"
D18-1431,P18-1205,0,0.121134,"Missing"
N16-1014,P12-3007,0,0.0207071,"ilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h wh"
N16-1014,P15-2073,1,0.256939,"ecutive message-response pairs spoken by different characters. We randomly selected two subsets as development and test datasets, each containing 2k pairs, with source and target length restricted to the range of [6,18]. 5.2 7 IMSDB (http://www.imsdb.com/) is a relatively small database of around 0.4 million sentences and thus not suitable for open domain dialogue training. 115 Model S EQ 2S EQ MMI-antiLM Evaluation For parameter tuning and final evaluation, we used B LEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference B LEU. As the IMSDB data is too limited to support extraction of multiple references, only single reference B LEU was used in training and evaluating the OSDb models. We did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard S EQ 2S EQ model in order to diversify the outputs. We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated respon"
N16-1014,P14-1066,1,0.170591,"systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h where ek denotes the vector for an individual text unit (for example, a word or sentence) at time step k while hk denotes the vector computed by LSTM model at time k by combining ek and hk−1 . ck is the cell state vector at time k, and σ denotes the sigmoid function. Then, the vector representation hk for each time step 2 Augmenting our technique"
N16-1014,D13-1111,0,0.0100372,"Missing"
N16-1014,P07-2045,0,0.0627244,"et We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset. The baseline is the S EQ 2S EQ model with its standard likelihood objective and a beam size of 200. We compare this baseline against greedy-search S EQ 2S EQ (Vinyals and Le, 2015), which can help achieve higher diversity by increasing search errors.8 Machine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1, 2, 3, 4]-gram matches between context and response and between message and context (conte"
N16-1014,P15-1002,0,0.00622466,"200. The top examples are the responses with the highest average probability loglikelihoods in the N-best list. Lower-ranked, less-generic responses were manually chosen. speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs. Below, we present practical strategies for neural generation models that use MMI as an objective function. We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs. al., 2015; Luong et al., 2015) has inspired attempts to extend these neural techniques to response generation. Sordoni et al. (2015) improved upon Ritter et al. (2011) by rescoring the output of a phrasal SMT-based conversation system with a S EQ 2S EQ model that incorporates prior context. (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015) apply direct end-to-end S EQ 2S EQ models These S EQ 2S EQ models are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that can implicitly capture compositionality and long-span dependencies. (Wen et al., 2015) attempt to le"
N16-1014,P03-1021,0,0.0536546,"sponses (T ) interchanged. 4.5 4.5.1 Decoding MMI-antiLM As described in Section 4.3.1, decoding using log p(T |S) − λU (T ) can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decod114 ing. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target T as follows: Score(T ) = p(T |S) − λU (T ) + γNt (15) where Nt denotes the length of the target and γ denotes associated weight. We optimize γ and λ using MERT (Och, 2003) on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size B = 200. We set a maximum length of 20 for generated candidates. At each time step of decoding, we are presented with B × B candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top B unfinished hypotheses and move to next time step. We therefore maintain beam size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best"
N16-1014,W00-0306,0,0.0299638,"oes not require identifying lexical overlap to foster diversity.2 On a somewhat different task, Mao et al. (2015, Section 6) utilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time"
N16-1014,P02-1040,0,0.120743,"Missing"
N16-1014,D11-1054,0,0.770418,"tantive gains in B LEU scores on two conversational datasets and in human evaluations. 1 Introduction Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response"
N16-1014,P15-1152,0,0.523144,"h interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models tend to ge"
N16-1014,N15-1020,1,0.849031,"of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, howe"
N16-1014,D15-1199,0,0.506634,"Missing"
N16-1082,E14-1049,0,0.00880291,"tasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consi"
N16-1082,P15-1144,0,0.130602,"2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represe"
N16-1082,N15-1004,0,0.0189816,", June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable i"
N16-1082,P14-1002,0,0.00988943,"this work. We describe datasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet vis"
N16-1082,C12-1118,0,0.0188047,"ire the work we present in this paper, there are fundamental differences between vision and NLP. In NLP words function as basic units, and hence (word) vectors rather than single pixels are the basic units. Sequences of words (e.g., phrases and sentences) are also presented in a more structured way than arrangements of pixels. In parallel to our research, independent researches (Karpathy et al., 2015) have been conducted to explore similar direction from an error-analysis point of view, by analyzing predictions and errors from a recurrent neural models. Other distantly relevant works include: Murphy et al. (2012; Fyshe et al. (2015) used an manual task to quantify the interpretability of semantic dimensions by presetting human users with a list of words and ask them to choose the one that does not belong to the list. Faruqui et al. (2015). Similar strategy is adopted in (Faruqui et al., 2015) by extracting top-ranked words in each vector dimension. 3 Datasets and Neural Models We explored two datasets on which neural models are trained, one of which is of relatively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used"
N16-1082,D13-1170,0,0.0729884,"atively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used for neural model evaluations. The dataset contains gold-standard sentiment labels for every parse tree constituent, from sentences to phrases to individual words, for 215,154 phrases in 11,855 sentences. The task is to perform both finegrained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive vs negative) classification at both the phrase and sentence level. For more details about the dataset, please refer to Socher et al. (2013). While many studies on this dataset use recursive parse-tree models, in this work we employ only standard sequence models (RNNs and LSTMs) since these are the most widely used current neural models, and sequential visualization is more straightforward. We therefore first transform each parse tree node to a sequence of tokens. The sequence is first mapped to a phrase/sentence representation and fed into a softmax classifier. Phrase/sentence representations are built with the following three models: Standard Recurrent Sequence with TANH activation functions, LSTMs and Bidirectional LSTMs. For d"
N16-1082,P15-1002,0,\N,Missing
N16-1082,N16-1014,1,\N,Missing
N19-5001,P18-1044,0,0.0297444,"2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solvi"
N19-5001,C18-1055,0,0.0668385,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,P18-2006,0,0.13551,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,N19-1337,1,0.835986,"ks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo Tree Search. Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI (Li et al., 2017). We will summarize the lessons lea"
N19-5001,D17-1215,0,0.0493758,"for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is"
N19-5001,D18-1026,0,0.0654804,"Missing"
N19-5001,P18-1225,0,0.0266302,"rning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP p"
N19-5001,P18-1046,1,0.924824,"Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adver"
N19-5001,P17-1119,0,0.0214364,"s of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learni"
N19-5001,P18-1079,1,0.824017,"adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo"
N19-5001,D17-1187,0,0.141197,"018a,b; Shi et al., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pro"
N19-5001,D18-1077,0,0.122559,"al focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, w"
N19-5001,D18-1428,0,0.138691,"ial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data clean"
N19-5001,D18-1131,0,0.153172,"oduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP"
N19-5001,N18-1122,0,0.0638405,"Missing"
N19-5001,D18-1125,0,0.0412213,"Missing"
N19-5001,N18-1089,0,0.0232576,", 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the"
N19-5001,C18-1315,0,0.127664,"ation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adversarial learning. We further Tutorial Description Adversar"
N19-5001,C18-1103,0,0.030798,"et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tuto"
N19-5001,D18-1009,0,0.140633,"negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle"
N19-5001,D18-1031,0,0.154184,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1316,0,0.0180701,"ation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP pr"
N19-5001,C18-1099,0,0.133892,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,C18-1037,0,0.0290058,"on natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP"
N19-5001,P18-1083,1,0.909559,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1451,0,0.0867524,"ces in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al.,"
N19-5001,N18-2091,0,0.0348511,"., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to p"
P13-2039,P11-1032,1,0.75723,"alities of reviewers could predict spammers, without using any textual features. Li et al. (2011) carefully explored review-related features based on content and sentiment, training a semi-supervised classifier for opinion spam detection. However, the disadvantages of standard supervised learning methods are obvious. First, they do not generally provide readers with a clear probabilistic preIntroduction Consumers rely increasingly on user-generated online reviews to make purchase decisions. Positive opinions can result in significant financial gains. This gives rise to deceptive opinion spam (Ott et al., 2011; Jindal et al., 2008), fake reviews written to sound authentic and deliberately mislead readers. Previous research has shown that humans have difficulty distinguishing fake from truthful reviews, operating for the most part at chance (Ott et al., 2011). Consider, for example, the following two hotel reviews. One is truthful and the other is deceptive1 : 1. My husband and I stayed for two nights at the Hilton Chicago. We were very pleased with the accommodations and enjoyed the service every minute of it! The bedrooms are immaculate, and the linens are very soft. We also appreciated the free w"
P13-2039,D09-1026,0,0.19765,"Missing"
P13-2039,P10-1066,0,0.0510505,"Missing"
P13-2099,D11-1040,0,0.363441,"Missing"
P13-2099,N03-1020,0,\N,Missing
P14-1016,P11-1055,0,0.0115094,"consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Sec"
P14-1016,P11-1040,0,0.0339493,"emainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented in Section 4. We present experimental results in Section 5 and conclude in Section 6. 166 Figure 2: Example of fetching tweets containing entity USC mention from Miranda Cosgrove (an American actress and singer-songwriter)’s twitter stream. Figure 1: Illustration of Goolge Plus “knowledge base”. fields such as protein relation extraction (Craven et al., 1999; Ravikumar et al., 2012), event extraction from Twitter (Benson et al., 2011), sentiment analysis (Go et al., 2009) and Wikipedia infobox generation (Wu and Weld, 2007). Education/Job We first used the Google Plus API5 (shown in Figure 1) to obtain a seed set of users whose profiles contain both their education/job status and a link to their twitter account.6 Then, we fetched tweets containing the mention of the education/job entity from each correspondent user’s twitter stream using Twitter’s search API7 (shown in Figure 2) and used them to construct positive bags of tweets expressing the associated attribute, namely E DUCATION(Useri , Entityj ), or E MPLOYER(Useri ,"
P14-1016,P09-1113,0,0.688937,"annotations, which are expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The"
P14-1016,D13-1114,0,0.0432055,"N , HOMETOWN , LIVING LOCA TION , FAMILY MEMBERS and so on, where training data can be obtained by matching ground truth retrieved from multiple types of online social media such as Facebook, Google Plus, or LinkedIn. Our contributions are as follows: Related Work While user profile inference from social media has received considerable attention (Al Zamal et al., 2012; Rao and Yarowsky, 2010; Rao et al., 2010; Rao et al., 2011), most previous work has treated this as a classification task where the goal is to predict unary predicates describing attributes of the user. Examples include gender (Ciot et al., 2013; Liu and Ruths, 2013; Liu et al., 2012), age (Rao et al., 2010), or political polarity (Pennacchiotti and Popescu, 2011; Conover et al., 2011). A significant challenge that has limited previous efforts in this area is the lack of available training data. For example, researchers obtain training data by employing workers from Amazon Mechanical Turk to manually identify users’ gender from profile pictures (Ciot et al., 2013). This approach is appropriate for attributes such as gender with a small numbers of possible values (e.g., male or female), for which the values can be directly identified."
P14-1016,N13-1039,0,0.0107717,"s using Equ.6. k zi,e = argmax Ψ(z 0 , Xi , Fik ) z0 AFFINITY Job 14.5 Table 3: Affinity values for Education and Job. 5.1 Preprocessing and Experiment Setup Each tweet posting is tokenized using Twitter NLP tool introduced by Noah’s Ark14 with # and @ separated following tokens. We assume that attribute values should be either name entities or terms following @ and #. Name entities are extracted using Ritter et al.’s NER system (2011). Consecutive tokens with the same named entity tag are chunked (Mintz et al., 2009). Part-ofspeech tags are assigned based on Owoputi et al’s tweet POS system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Att"
P14-1016,D10-1048,0,0.0124034,"system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Attributes are initialized using only text features, maximizing Ψtext (e, Xi ), and ignoring network information. Then for each user we iteratively reestimate their profile given both their text features and network features (computed based on the current predictions made for their friends) which provide additional evidence. In this way, highly confident predictions will be made strictly from text in the first round, then the network can either support or contradict low confidence predictions as more decisions are made. This process continues until no changes are made at which point the algo"
P14-1016,D11-1141,1,0.441201,"Missing"
P14-1016,Q13-1030,1,0.223646,"re expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our data"
P14-1016,D12-1042,0,0.00990537,"distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of"
P14-1016,P12-1076,0,0.011214,"verages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented i"
P14-1147,I13-1039,0,0.0487601,"on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with similar contexts. Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them. Ott et al. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group"
P14-1147,P12-2034,0,0.368025,"Missing"
P14-1147,P13-2039,1,0.838567,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,D13-1199,1,0.866691,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,P11-1032,1,0.703381,"used on developing supervised learningbased algorithms to help users identify deceptive opinion spam, which are highly dependent upon high-quality gold-standard labeled data (Jindal and Liu, 2008; Jindal et al., 2010; Lim et al., 2010; Wang et al., 2011; Wu et al., 2010). Studies in the literature rely on a couple of approaches for obtaining labeled data, which usually fall into two categories. The first relies on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the ge"
P14-1147,N13-1053,1,0.453732,"Missing"
P14-1147,D13-1113,0,0.00665599,"res from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group of reviewers who work collaboratively to write fake reviews. Qian and Liu (2013) identified multiple user IDs that are generated by the same author, as these authors are more likely to generate deceptive reviews. In the psychological literature, researchers have looked into possible linguistic cues to deception (Newman et al., 2003), such as decreased spatial detail, which is consistent with theories of reality monitoring (Johnson and Raye, 1981), increased negative emotion terms (Newman et al., 2003), or the writing style difference between informative (truthful) and imaginative (deceptive) writings in (Rayson et al., 2001). The former typically consists of more nouns, a"
P15-1107,J08-1001,0,0.439993,"nguage generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alt"
P15-1107,N04-1015,0,0.0207602,"er BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.3 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved. We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched"
P15-1107,P11-1050,0,0.0188877,"Missing"
P15-1107,P08-2011,0,0.00764121,"ariety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji a"
P15-1107,P12-1007,0,0.00679266,"coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for th"
P15-1107,P14-1002,0,0.0129671,"2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to for"
P15-1107,P91-1008,0,0.143169,"e ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the commu"
P15-1107,D14-1218,1,0.360495,"r representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014"
P15-1107,D14-1220,1,0.434012,"inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that t"
P15-1107,N03-1020,0,0.305314,"ed our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU"
P15-1107,P11-1100,0,0.00981517,"Missing"
P15-1107,W04-1013,0,0.0284614,"s. We trained our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest comm"
P15-1107,P15-1002,1,0.430297,"ped features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summarizatio"
P15-1107,J00-3005,0,0.0201056,"in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within"
P15-1107,P02-1040,0,0.127401,"epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU Purely measuring recall will ina"
P15-1107,J05-2005,0,0.00782263,"zilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3 Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Review Output-Review washington was unanim"
P15-1107,D11-1040,0,0.0223866,"(precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3 Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Review Output-Review washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types . washington established many forms in government still used today , such as the cabinet system and inaugural address"
P15-1107,C04-1048,0,\N,Missing
P16-1094,P12-3007,0,0.0527842,"s the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et"
P16-1094,P15-2073,1,0.826361,"Missing"
P16-1094,P14-1066,1,0.260835,", 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conv"
P16-1094,N16-1014,1,0.261728,"? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or game characters.1 For present purposes, we"
P16-1094,P15-1002,0,0.023784,"2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history. Recent work by Li et al. ("
P16-1094,P03-1021,0,0.0474555,"rate generic and commonplace responses such as I don’t know, we follow Li et al. (2016) by reranking the generated N-best list using 997 a scoring function that linearly combines a length penalty and the log likelihood of the source given the target: log p(R|M, v) + λ log p(M |R) + γ|R| (11) where p(R|M, v) denotes the probability of the generated response given the message M and the respondent’s speaker ID. |R |denotes the length of the target and γ denotes the associated penalty weight. We optimize γ and λ on N-best lists of response candidates generated from the development set using MERT (Och, 2003) by optimizing B LEU. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. We trained standard S EQ 2S EQ models for p(M |R) with no speaker information considered. 5 Datasets 5.1 • Learning rate is set to 1.0. • Parameters are initialized by sampling from the uniform distribution [−0.1, 0.1]. • Gradients are clipped to avoid gradient explosion with a threshold of 5. • Vocabulary size is limited to 50,000. • Dropout rate is set to 0.2. Twitter Persona Dataset Data Collection Training data for the Speaker Model was extracted from the Twitter FireHose for"
P16-1094,W00-0306,0,0.39345,"nnotators. 2 Related Work This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural languag"
P16-1094,P02-1040,0,0.111268,"Missing"
P16-1094,D11-1054,0,0.655954,"ou? Where were you born? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized ava"
P16-1094,walker-etal-2012-annotated,0,0.0254272,"to reduce the proportion of generic responses typical of S EQ 2S EQ systems. Yao et al. (2015) employ an intention network to maintain the relevance of responses. Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in an open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the S EQ 2S EQ models of Vinyals and Le (2015) and Li et al. (2016), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder. 995 3 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xnX }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as it , ft and ot . We distinguish e and h where et denotes the vector for an individual text"
P16-1094,D15-1199,0,0.168657,"Missing"
P16-1094,P15-1152,0,0.72735,"Missing"
P16-1094,N15-1020,1,0.919869,"rn? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or ga"
P18-5007,D17-1106,0,0.0203824,", Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b),"
P18-5007,N18-1113,1,0.878952,"Missing"
P18-5007,D17-1060,1,0.831141,"2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b), discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforcement learning. The intended length of the tutorial is 3 hours, including a coffee break. Many Natural Language Processing (NLP) tasks (including generation, language grounding, reasoning, information extraction, coreferenc"
P18-5007,D15-1001,0,0.0312245,"forcement Learning for NLP William Yang Wang UC Santa Barbara william@cs.ucsb.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we d"
P18-5007,P18-1053,1,0.885681,"Missing"
P18-5007,D16-1261,0,0.0216518,"om Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study"
P18-5007,P18-1104,1,0.822012,"b.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong"
P18-5007,D17-1103,0,0.0282946,"etworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning ("
P18-5007,P18-1199,1,0.823639,"deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical de"
P19-1129,P81-1022,0,0.366668,"Missing"
P19-1129,C16-1239,0,0.144405,"Missing"
P19-1129,P17-1147,0,0.0324288,"tion to jointly train the two models under the framework work of Minimum Risk Training. Takanobu et al. (2018) used hierarchical reinforcement learning to extract entities and relations in a hierarchical manner. 2.2 Machine Reading Comprehension Main-stream MRC models (Seo et al., 2016; Wang and Jiang, 2016; Xiong et al., 2017; Wang et al., 2016b) extract text spans in passages given queries. Text span extraction can be simplified to two multiclass classification tasks, i.e., predicting the starting and the ending positions of the answer. Similar strategy can be extended to multi-passage MRC (Joshi et al., 2017; Dunn et al., 2017) where the answer needs to be selected from multiple passages. Multi-passage MRC tasks can be easily simplified to single-passage MRC tasks by concatenating passages (Shen et al., 2017; Wang et al., 2017b). Wang et al. (2017a) first rank the passages and then run single-passage MRC on the selected passage. Tan et al. (2017) train the passage ranking model jointly with the reading comprehension model. Pretraining methods like BERT (Devlin et al., 2018) or Elmo (Peters et al., 2018) have proved to be extremely helpful in MRC tasks. There has been a tendency of casting non-QA"
P19-1129,W10-2924,0,0.0348829,"): an entity extraction model first identifies entities of interest and a relation extraction model then constructs relations between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). In later studies, Li and Ji (2014) extract entity mentions and relations using structured perceptron with efficient beam-search, which is significantly more efficient and less Time-consuming than constraintbased approaches. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search orders in decoding and global features. Neural network models have bee"
P19-1129,D09-1013,0,0.134196,"e base from RESUME requires four or five turns of QA. We also show that this multi-turn QA setting could easilty integrate reinforcement learning (just as in multi-turn dialog systems) to gain additional performance boost. The rest of this paper is organized as follows: Section 2 details related work. We describe the dataset and setting in Section 3, the proposed model in Section 4, and experimental results in Section 5. We conclude this paper in Section 6. 2 2.1 Related Work Extracting Entities and Relations Many earlier entity-relation extraction systems are pipelined (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016): an entity extraction model first identifies entities of interest and a relation extraction model then constructs relations between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth"
P19-1129,D14-1200,0,0.816712,"is issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). In later studies, Li and Ji (2014) extract entity mentions and relations using structured perceptron with efficient beam-search, which is significantly more efficient and less Time-consuming than constraintbased approaches. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search orders in decoding and global features. Neural network models have been widely used in the literature as well. Miwa and Bansal (2016) introduced an end-to-end approach that extract entities and their relations using neural network models with shared parameters, i.e., extracting entities using a neural tagging model and extracting relations using a neural multiclass classification model based on tr"
P19-1129,E06-2009,0,0.0205486,"; • Q: which Company / companies did Musk work for? A: SpaceX, Tesla, SolarCity, Neuralink and The Boring Company; • Q: when did Musk join SpaceX? A: 2002; • Q: what was Musk’s Position in SpaceX? A: CEO. Treating the entity-relation extraction task as a multi-turn QA task has the following key advantages: (1) the multi-turn QA setting provides an elegant way to capture the hierarchical dependency of tags. As the multi-turn QA proceeds, we progressively obtain the entities we need for the next turn. This is closely akin to the multi-turn slot filling dialogue system (Williams and Young, 2005; Lemon et al., 2006); (2) the question query encodes important prior information for the relation 2 e.g., in text A B C D, (A, C) is a pair and (B, D) is a pair. class we want to identify. This informativeness can potentially solve the issues that existing relation extraction models fail to solve, such as distantlyseparated entity pairs, relation span overlap, etc; (3) the QA framework provides a natural way to simultaneously extract entities and relations: most MRC models support outputting special N ONE tokens, indicating that there is no answer to the question. Throught this, the original two tasks, entity ext"
P19-1129,K17-1034,0,0.216913,"Missing"
P19-1129,P14-1038,0,0.62044,"s between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). In later studies, Li and Ji (2014) extract entity mentions and relations using structured perceptron with efficient beam-search, which is significantly more efficient and less Time-consuming than constraintbased approaches. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search orders in decoding and global features. Neural network models have been widely used in the literature as well. Miwa and Bansal (2016) introduced an end-to-end approach that extract entitie"
P19-1129,P16-1200,0,0.0158483,"turns of QA. We also show that this multi-turn QA setting could easilty integrate reinforcement learning (just as in multi-turn dialog systems) to gain additional performance boost. The rest of this paper is organized as follows: Section 2 details related work. We describe the dataset and setting in Section 3, the proposed model in Section 4, and experimental results in Section 5. We conclude this paper in Section 6. 2 2.1 Related Work Extracting Entities and Relations Many earlier entity-relation extraction systems are pipelined (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016): an entity extraction model first identifies entities of interest and a relation extraction model then constructs relations between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (K"
P19-1129,P16-1101,0,0.0387994,"is the tokenized question and C is the context. The representation of each context token is obtained using multi-layer transformers. Traditional MRC models (Wang and Jiang, 2016; Xiong et al., 2017) predict the starting and ending indices by applying two softmax layers to the context tokens. This softmax-based span extraction strategy only fits for single-answer extraction tasks, but not for our task, since one sentence/passage in our setting might contain multiple answers. To tackle this issue, we formalize the task as a query-based tagging problem (Lafferty et al., 2001; Huang et al., 2015; Ma and Hovy, 2016). Specially, we predict a BMEO (beginning, inside, ending and outside) label for each token in the context given the query. The representation of each word is fed to a softmax layer to output a BMEO label. One can think that we are transforming two Nclass classification tasks of predicting the starting and the ending indices (where N denotes the length of sentence) to N 5-class classification tasks4 . Training and Test At the training time, we jointly train the objectives for the two stages: L = (1 − λ)L(head-entity) + λL(tail-entity, rel) (1) λ ∈ [0, 1] is the parameter controling the trade-o"
P19-1129,P16-1105,0,0.363749,"August 2, 2019. 2019 Association for Computational Linguistics and the algorithm. At the formalization level, the REL (e1 , e2 ) triplet structure is not enough to fully express the data structure behind the text. Take the Musk case as an example, there is a hierarchical dependency between the tags: the extraction of Time depends on Position since a Person can hold multiple Positions in a Company during different Time periods. The extraction of Position also depends on Company since a Person can work for multiple companies. At the algorithm level, for most existing relation extraction models (Miwa and Bansal, 2016; Wang et al., 2016a; Ye et al., 2016), the input to the model is a raw sentence with two marked mentions, and the output is whether a relation holds between the two mentions. As pointed out in Wang et al. (2016a); Zeng et al. (2018), it is hard for neural models to capture all the lexical, semantic and syntactic cues in this formalization, especially when (1) entities are far away; (2) one entity is involved in multiple triplets; or (3) relation spans have overlaps2 . In the paper, we propose a new paradigm to handle the task of entity-relation extraction. We formalize the task as a multi-tur"
P19-1129,P15-2130,0,0.0735955,"Missing"
P19-1129,N18-1202,0,0.03372,"g and the ending positions of the answer. Similar strategy can be extended to multi-passage MRC (Joshi et al., 2017; Dunn et al., 2017) where the answer needs to be selected from multiple passages. Multi-passage MRC tasks can be easily simplified to single-passage MRC tasks by concatenating passages (Shen et al., 2017; Wang et al., 2017b). Wang et al. (2017a) first rank the passages and then run single-passage MRC on the selected passage. Tan et al. (2017) train the passage ranking model jointly with the reading comprehension model. Pretraining methods like BERT (Devlin et al., 2018) or Elmo (Peters et al., 2018) have proved to be extremely helpful in MRC tasks. There has been a tendency of casting non-QA NLP tasks as QA tasks (McCann et al., 2018). Our work is highly inspired by Levy et al. (2017). Levy et al. (2017) and McCann et al. (2018) focus on identifying the relation between two pre-defined entities and the authors formalize the task of relation extraction as a single-turn QA task. In the current paper we study a more complicated scenario, where hierarchical tag dependency needs to be modeled and single-turn QA approach no longer suffices. We show that our multi-turn QA method is able to solv"
P19-1129,D16-1264,0,0.0691955,"= Extract Answer(rel question, s) 16: if e 6= N ONE 17: ent list = ent list + e 18: endif 19: end for 20: end for 21: if len(ent list)=len([rel, rel temp]) 22: M = M + ent list 23: endif 24: end for 25: return M Algorithm 1: Transforming the entity-relation extraction task to a multi-turn QA task. ber of words in C, we need to predict the answer span. For the QA framework, we use BERT (Devlin et al., 2018) as a backbone. BERT performs bidirectional language model pretraining on largescale datasets using transformers (Vaswani et al., 2017) and achieves SOTA results on MRC datasets like SQUAD (Rajpurkar et al., 2016). To align with the BERT framework, the question Q and the context C are combined by concatenating the list [CLS, Q, SEP, C, SEP], where CLS and SEP are special tokens, Q is the tokenized question and C is the context. The representation of each context token is obtained using multi-layer transformers. Traditional MRC models (Wang and Jiang, 2016; Xiong et al., 2017) predict the starting and ending indices by applying two softmax layers to the context tokens. This softmax-based span extraction strategy only fits for single-answer extraction tasks, but not for our task, since one sentence/passa"
P19-1129,W04-2401,0,0.188518,"ur work is highly inspired by Levy et al. (2017). Levy et al. (2017) and McCann et al. (2018) focus on identifying the relation between two pre-defined entities and the authors formalize the task of relation extraction as a single-turn QA task. In the current paper we study a more complicated scenario, where hierarchical tag dependency needs to be modeled and single-turn QA approach no longer suffices. We show that our multi-turn QA method is able to solve this challenge and obtain new state-of-the-art results. 3 Datasets and Tasks 3.1 ACE04, ACE05 and CoNLL04 We use ACE04, ACE05 and CoNLL04 (Roth and Yih, 2004), the widely used entity-relation extraction benchmarks for evaluation. ACE04 defines 7 entity types, including Person (PER), Organization (ORG), Geographical Entities (GPE), Location (loc), Facility (FAC), Weapon (WEA) and Vehicle (VEH). For each pair of entities, it defines 7 relation categories, including Physical (PHYS), Person-Social (PER - SOC), EmploymentOrganization (EMP - ORG), Agent-Artifact (ART), PER/ORG Affiliation (OTHER - AFF), GPE- Affiliation (GPE - AFF) and Discourse (DISC). ACE05 was built upon ACE04. It kept the PER - SOC, ART and GPE - AFF categories from ACE04 but split P"
P19-1129,D18-1249,0,0.405342,"g a neural tagging model and extracting relations using a neural multiclass classification model based on tree LSTMs (Tai et al., 2015). Wang et al. (2016a) extract relations using multi-level attention CNNs. Zeng et al. (2018) proposed a new framework that uses sequence-to-sequence models to generate entityrelation triples, naturally combining entity detection and relation detection. Another way to bind the entity and the relation extraction models is to use reinforcement learning or Minimum Risk Training, in which the training signals are given based on the joint decision by the two models. Sun et al. (2018) optimized a global loss function to jointly train the two models under the framework work of Minimum Risk Training. Takanobu et al. (2018) used hierarchical reinforcement learning to extract entities and relations in a hierarchical manner. 2.2 Machine Reading Comprehension Main-stream MRC models (Seo et al., 2016; Wang and Jiang, 2016; Xiong et al., 2017; Wang et al., 2016b) extract text spans in passages given queries. Text span extraction can be simplified to two multiclass classification tasks, i.e., predicting the starting and the ending positions of the answer. Similar strategy can be ex"
P19-1129,P15-1150,0,0.0340114,"al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search orders in decoding and global features. Neural network models have been widely used in the literature as well. Miwa and Bansal (2016) introduced an end-to-end approach that extract entities and their relations using neural network models with shared parameters, i.e., extracting entities using a neural tagging model and extracting relations using a neural multiclass classification model based on tree LSTMs (Tai et al., 2015). Wang et al. (2016a) extract relations using multi-level attention CNNs. Zeng et al. (2018) proposed a new framework that uses sequence-to-sequence models to generate entityrelation triples, naturally combining entity detection and relation detection. Another way to bind the entity and the relation extraction models is to use reinforcement learning or Minimum Risk Training, in which the training signals are given based on the joint decision by the two models. Sun et al. (2018) optimized a global loss function to jointly train the two models under the framework work of Minimum Risk Training. T"
P19-1129,P16-1123,0,0.107695,"Missing"
P19-1129,P17-1018,0,0.0392584,"Reading Comprehension Main-stream MRC models (Seo et al., 2016; Wang and Jiang, 2016; Xiong et al., 2017; Wang et al., 2016b) extract text spans in passages given queries. Text span extraction can be simplified to two multiclass classification tasks, i.e., predicting the starting and the ending positions of the answer. Similar strategy can be extended to multi-passage MRC (Joshi et al., 2017; Dunn et al., 2017) where the answer needs to be selected from multiple passages. Multi-passage MRC tasks can be easily simplified to single-passage MRC tasks by concatenating passages (Shen et al., 2017; Wang et al., 2017b). Wang et al. (2017a) first rank the passages and then run single-passage MRC on the selected passage. Tan et al. (2017) train the passage ranking model jointly with the reading comprehension model. Pretraining methods like BERT (Devlin et al., 2018) or Elmo (Peters et al., 2018) have proved to be extremely helpful in MRC tasks. There has been a tendency of casting non-QA NLP tasks as QA tasks (McCann et al., 2018). Our work is highly inspired by Levy et al. (2017). Levy et al. (2017) and McCann et al. (2018) focus on identifying the relation between two pre-defined entities and the authors"
P19-1129,P13-1161,0,0.0523646,"l., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016): an entity extraction model first identifies entities of interest and a relation extraction model then constructs relations between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). In later studies, Li and Ji (2014) extract entity mentions and relations using structured perceptron with efficient beam-search, which is significantly more efficient and less Time-consuming than constraintbased approaches. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search ord"
P19-1129,C10-2160,0,0.172443,"and a relation extraction model then constructs relations between the extracted entities. Although pipelined systems has the flexibility of integrating different data sources and learning algorithms, they suffer significantly from error propagation. To tackle this issue, joint learning models have been proposed. Earlier joint learning approaches connect the two models through various dependen1341 cies, including constraints solved by integer linear programming (Yang and Cardie, 2013; Roth and Yih, 2007), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). In later studies, Li and Ji (2014) extract entity mentions and relations using structured perceptron with efficient beam-search, which is significantly more efficient and less Time-consuming than constraintbased approaches. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) proposed the tablefilling approach, which provides an opportunity to incorporating more sophisticated features and algorithms into the model, such as search orders in decoding and global features. Neural network models have been widely used in the literature as well. Miwa and Bansal (201"
P19-1129,P18-1047,0,0.262968,"an example, there is a hierarchical dependency between the tags: the extraction of Time depends on Position since a Person can hold multiple Positions in a Company during different Time periods. The extraction of Position also depends on Company since a Person can work for multiple companies. At the algorithm level, for most existing relation extraction models (Miwa and Bansal, 2016; Wang et al., 2016a; Ye et al., 2016), the input to the model is a raw sentence with two marked mentions, and the output is whether a relation holds between the two mentions. As pointed out in Wang et al. (2016a); Zeng et al. (2018), it is hard for neural models to capture all the lexical, semantic and syntactic cues in this formalization, especially when (1) entities are far away; (2) one entity is involved in multiple triplets; or (3) relation spans have overlaps2 . In the paper, we propose a new paradigm to handle the task of entity-relation extraction. We formalize the task as a multi-turn question answering task: each entity type and relation type is characterized by a question answering template, and entities and relations are extracted by answering template questions. Answers are text spans, extracted using the no"
P19-1129,D17-1182,0,0.116703,"Missing"
P19-1129,D14-1082,0,\N,Missing
P19-1129,P17-1085,0,\N,Missing
P19-1129,P17-1166,0,\N,Missing
P19-1129,D18-1307,0,\N,Missing
P19-1129,P11-1056,0,\N,Missing
P19-1314,D15-1075,0,0.0274703,"e UNK word at decoding time, We also implemented the BPE subword model (Sennrich et al., 2016b,a) on the Chinese target side. The BPE model achieves a performance of 41.44 for the Seq2Seq+attn setting and 44.35 for bag-ofwords, significantly outperforming the word-based model, but still underperforming the char-based model by about 0.8-0.9 in BLEU. We conclude that for Chinese, generating characters has the advantage over generating words in deep learning decoding. 3.3 Sentence Matching/Paraphrase There are two Chinese datasets similar to the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015): BQ and LCQMC, in which we need to assign a label to a pair of sentences depending on whether they share similar meanings. For the BQ dataset (Chen et al., 2018), it contains 120,000 Chinese sentence pairs, and each pair is associated with a label indicating whether the two sentences are of equivalent semantic meanings. The dataset is deliberately constructed so that sentences in some pairs may have significant word overlap but complete different meanings, while others are the other way around. For LCQMC (Liu et al., 2018), it aims at identifying whether two sentences have the same intention."
P19-1314,P16-1039,0,0.0251769,"f a word(End), inside a word (Middel) or a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+char) hybrid (char only) Related Work Experimental Results In this section, we evaluate the effect of word segmentation in deep learning-based Chinese NLP in four tasks, language modeling, machine translation, text classification and sentence matching/paraphrase. To enforce app"
P19-1314,P17-2096,0,0.0336771,"nd CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+char) hybrid (char only) Related Work Experimental Results In this section, we evaluate the effect of word segmentation in deep learning-based Chinese NLP in four tasks, language modeling, machine translation, text classification and sentence matching/paraphrase. To enforce apples-to-apples comparison, for both the word-based model and the char-based model, we use grid search to tune all dimen"
P19-1314,W06-0129,0,0.0479816,"and advances to the end of the matched word in the string. Different models are proposed based on different segmentation criteria (Huang and Zhao, 2007). With the rise of statistical machine learning methods, the task of CWS is formalized as a tagging task, i.e., assigning a BEMS label to each character of a string that indicates whether the character is the start of a word(Begin), the end of a word(End), inside a word (Middel) or a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF feat"
P19-1314,C10-3004,0,0.065143,"Missing"
P19-1314,P17-1177,0,0.389312,"a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+char) hybrid (char only) Related Work Experimental Results In this section, we evaluate the effect of word segmentation in deep learning-based Chinese NLP in four tasks, language modeling, machine translation, text classification and sentence matching/paraphrase. To enforce apples-to-apples comparison, for both the w"
P19-1314,D18-1536,0,0.295848,"f 41.44 for the Seq2Seq+attn setting and 44.35 for bag-ofwords, significantly outperforming the word-based model, but still underperforming the char-based model by about 0.8-0.9 in BLEU. We conclude that for Chinese, generating characters has the advantage over generating words in deep learning decoding. 3.3 Sentence Matching/Paraphrase There are two Chinese datasets similar to the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015): BQ and LCQMC, in which we need to assign a label to a pair of sentences depending on whether they share similar meanings. For the BQ dataset (Chen et al., 2018), it contains 120,000 Chinese sentence pairs, and each pair is associated with a label indicating whether the two sentences are of equivalent semantic meanings. The dataset is deliberately constructed so that sentences in some pairs may have significant word overlap but complete different meanings, while others are the other way around. For LCQMC (Liu et al., 2018), it aims at identifying whether two sentences have the same intention. This task is similar to but not exactly the same as the paraphrase detection task in BQ: two sentences can have different meanings but share the same intention."
P19-1314,P08-1102,0,0.115146,"Missing"
P19-1314,P07-1033,0,0.0326444,"Missing"
P19-1314,D14-1181,0,0.00784629,"ia (Huang and Zhao, 2007). With the rise of statistical machine learning methods, the task of CWS is formalized as a tagging task, i.e., assigning a BEMS label to each character of a string that indicates whether the character is the start of a word(Begin), the end of a word(End), inside a word (Middel) or a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+cha"
P19-1314,P17-1064,0,0.0178289,"issue and the overfitting issue of the word-based model. In conclusion, for the language modeling task on CTB, word segmentation does not provide any additional performance boost, and including word embeddings worsen the result. 3.2 Machine Translation In our experiments on machine translation, we use the standard Ch-En setting. The training set consists of 1.25M sentence pairs extracted from the LDC corpora.4 The validation set is from NIST 2002 and the models are evaluated on NIST 2003, 2004, 2005, 2006 and 2008. We followed exactly the common setup in Ma et al. (2018); Chen et al. (2017a); Li et al. (2017); Zhang et al. (2018), which use top 30,000 English words and 27,500 Chinese words. For the char-based model, vocab size is set to 4,500. We report results in both the Ch-En and the En-Ch settings. Regarding the implementation, we compare char-based models with word-based models under the standard framework of SEQ 2 SEQ +attention (Sutskever et al., 2014; Luong et al., 2015). The current state-of-the-art model is from Ma et al. (2018), which uses both the sentences (seq2seq) and the bag-of-words as targets in the training stage. We simply change the word-level encoding in Ma et al. (2018) to c"
P19-1314,P15-1168,0,0.0142753,"word(Begin), the end of a word(End), inside a word (Middel) or a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+char) hybrid (char only) Related Work Experimental Results In this section, we evaluate the effect of word segmentation in deep learning-based Chinese NLP in four tasks, language modeling, machine translation, text classification and sentence matching/par"
P19-1314,D15-1141,0,0.0305351,"Missing"
P19-1314,C18-1166,0,0.198287,"Missing"
P19-1314,P17-1110,0,0.047703,"Missing"
P19-1314,D15-1166,0,0.0359453,"pairs extracted from the LDC corpora.4 The validation set is from NIST 2002 and the models are evaluated on NIST 2003, 2004, 2005, 2006 and 2008. We followed exactly the common setup in Ma et al. (2018); Chen et al. (2017a); Li et al. (2017); Zhang et al. (2018), which use top 30,000 English words and 27,500 Chinese words. For the char-based model, vocab size is set to 4,500. We report results in both the Ch-En and the En-Ch settings. Regarding the implementation, we compare char-based models with word-based models under the standard framework of SEQ 2 SEQ +attention (Sutskever et al., 2014; Luong et al., 2015). The current state-of-the-art model is from Ma et al. (2018), which uses both the sentences (seq2seq) and the bag-of-words as targets in the training stage. We simply change the word-level encoding in Ma et al. (2018) to char-level encoding. For En-Ch translation, we use the same dataset to train and test both models. As in Ma et al. (2018), the dimensionality for word vectors and char vectors is set to 512. 4 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Results for Ch-En are shown in Table 4. As can be seen, for the vanilla SEQ 2 SEQ +attenti"
P19-1314,P18-2053,0,0.0889047,"ffer from the data sparsity issue, OOV issue and the overfitting issue of the word-based model. In conclusion, for the language modeling task on CTB, word segmentation does not provide any additional performance boost, and including word embeddings worsen the result. 3.2 Machine Translation In our experiments on machine translation, we use the standard Ch-En setting. The training set consists of 1.25M sentence pairs extracted from the LDC corpora.4 The validation set is from NIST 2002 and the models are evaluated on NIST 2003, 2004, 2005, 2006 and 2008. We followed exactly the common setup in Ma et al. (2018); Chen et al. (2017a); Li et al. (2017); Zhang et al. (2018), which use top 30,000 English words and 27,500 Chinese words. For the char-based model, vocab size is set to 4,500. We report results in both the Ch-En and the En-Ch settings. Regarding the implementation, we compare char-based models with word-based models under the standard framework of SEQ 2 SEQ +attention (Sutskever et al., 2014; Luong et al., 2015). The current state-of-the-art model is from Ma et al. (2018), which uses both the sentences (seq2seq) and the bag-of-words as targets in the training stage. We simply change the word-"
P19-1314,P14-2034,0,0.0666152,"Missing"
P19-1314,C04-1081,0,0.48049,"k the necessity of word segmentation in deep learning-based Chinese Natural Language Processing. 1 1 Introduction There is a key difference between English (or more broadly, languages that use some form of the Latin alphabet) and Chinese (or other languages that do not have obvious word delimiters such as Korean and Japanese) : words in English can be easily recognized since the space token is a good approximation of a word divider, whereas no word divider is present between words in written Chinese sentences. This gives rise to the task of Chinese Word Segmentation (CWS) (Zhang et al., 2003; Peng et al., 2004; Huang and Zhao, 2007; Zhao et al., 2006; Zheng et al., 2013; Zhou et al., 2017; Yang et al., 2017, 2018). In the context of deep learning, the segmented words are usually treated as the basic units for operations (we call these models the word-based models for the rest of this paper). Each segmented word is associated with a fixed-length vector representation, which will be processed by deep learning models in the same way as how English words are processed. Word-based models come with a few fundamental disadvantages, as will be discussed below. Firstly, word data sparsity inevitably leads t"
P19-1314,W16-2323,0,0.0227485,"nted in Table 5. As can be seen, the char-based model outperforms the word-based model by a huge margin (+3.13), and this margin is greater than the improvement in the Ch-En translation task. This is because in Ch-En translation, the difference between word-based and char-based models is only present in the source encoding stage, whereas in En-Ch translation it is present in both the source encoding and the target decoding stage. Another major reason that contributes to the inferior performance of the wordbased model is the UNK word at decoding time, We also implemented the BPE subword model (Sennrich et al., 2016b,a) on the Chinese target side. The BPE model achieves a performance of 41.44 for the Seq2Seq+attn setting and 44.35 for bag-ofwords, significantly outperforming the word-based model, but still underperforming the char-based model by about 0.8-0.9 in BLEU. We conclude that for Chinese, generating characters has the advantage over generating words in deep learning decoding. 3.3 Sentence Matching/Paraphrase There are two Chinese datasets similar to the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015): BQ and LCQMC, in which we need to assign a label to a pair of sentences"
P19-1314,P16-1162,0,0.0762647,"nted in Table 5. As can be seen, the char-based model outperforms the word-based model by a huge margin (+3.13), and this margin is greater than the improvement in the Ch-En translation task. This is because in Ch-En translation, the difference between word-based and char-based models is only present in the source encoding stage, whereas in En-Ch translation it is present in both the source encoding and the target decoding stage. Another major reason that contributes to the inferior performance of the wordbased model is the UNK word at decoding time, We also implemented the BPE subword model (Sennrich et al., 2016b,a) on the Chinese target side. The BPE model achieves a performance of 41.44 for the Seq2Seq+attn setting and 44.35 for bag-ofwords, significantly outperforming the word-based model, but still underperforming the char-based model by about 0.8-0.9 in BLEU. We conclude that for Chinese, generating characters has the advantage over generating words in deep learning decoding. 3.3 Sentence Matching/Paraphrase There are two Chinese datasets similar to the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015): BQ and LCQMC, in which we need to assign a label to a pair of sentences"
P19-1314,D16-1100,0,0.234956,"st as word spaced text. In this paper, we ask the fundamental question of whether word segmentation is necessary for deep learning-based Chinese natural language processing. We first benchmark word-based models against char-based models (those do not involve Chinese word segmentation). We run apples-toapples comparison between these two types of models on four NLP tasks: language modeling, document classification, machine translation and sentence matching. We observe that char-based models consistently outperform word-based model. We also compare char-based models with wordchar hybrid models (Yin et al., 2016; Dong et al., 2016; Yu et al., 2017), and observe that char-based models perform better or at least as good as the hybrid model, indicating that char-based models already encode sufficient semantic information. It is also crucial to understand the inadequacy of word-based models. To this end, we perform comprehensive analyses on the behavior of wordbased models and char-based models. We identify the major factor contributing to the disadvantage of word-based models, i.e., data sparsity, which in turn leads to overfitting, prevelance of OOV words, and weak domain transfer ability. Instead of m"
P19-1314,D17-1027,0,0.101364,", we ask the fundamental question of whether word segmentation is necessary for deep learning-based Chinese natural language processing. We first benchmark word-based models against char-based models (those do not involve Chinese word segmentation). We run apples-toapples comparison between these two types of models on four NLP tasks: language modeling, document classification, machine translation and sentence matching. We observe that char-based models consistently outperform word-based model. We also compare char-based models with wordchar hybrid models (Yin et al., 2016; Dong et al., 2016; Yu et al., 2017), and observe that char-based models perform better or at least as good as the hybrid model, indicating that char-based models already encode sufficient semantic information. It is also crucial to understand the inadequacy of word-based models. To this end, we perform comprehensive analyses on the behavior of wordbased models and char-based models. We identify the major factor contributing to the disadvantage of word-based models, i.e., data sparsity, which in turn leads to overfitting, prevelance of OOV words, and weak domain transfer ability. Instead of making a conclusive (and arrogant) arg"
P19-1314,W03-1719,0,0.158598,"wordbased models and char-based models. We identify the major factor contributing to the disadvantage of word-based models, i.e., data sparsity, which in turn leads to overfitting, prevelance of OOV words, and weak domain transfer ability. Instead of making a conclusive (and arrogant) argument that Chinese word segmentation is not necessary, we hope this paper could foster more discussions and explorations on the necessity of the long-existing task of CWS in the community, alongside with its underlying mechanisms. 3243 2 Since the First International Chinese Word Segmentation Bakeoff in 2003 (Sproat and Emerson, 2003) , a lot of effort has been made on Chinese word segmentation. Most of the models in the early years are based on a dictionary, which is pre-defined and thus independent of the Chinese text to be segmented. The simplest but remarkably robust model is the maximum matching model (Jurafsky and Martin, 2014). The simplest version of it is the left-to-right maximum matching model (maxmatch). Starting with the beginning of a string, maxmatch chooses the longest word in the dictionary that matches the current position, and advances to the end of the matched word in the string. Different models are pr"
P19-1314,W03-1730,0,0.480295,"Missing"
P19-1314,W04-1118,0,0.370411,"ns on whether CWS is necessary and how much improvement it can bring about. In information retrieval(IR), Foo and Li (2004) discussed CWS’s effect on IR systems and revealed that segmentation approach has an effect on IR effectiveness as long as the SAME segmentation method is used for query and document, and that CWS does not always work better than models without segmentation. In cases where CWS does lead to better performance, the gap between word-based models and char-based models can be closed if bigrams of characters are used in charbased models. In the phrase-based machine translation, Xu et al. (2004) reported that CWS only showed non-significant improvements over models without word segmentation. Zhao et al. (2013) found that segmentation itself does not guarantee better MT performance and it is not key to MT improvement. For text classification, Liu et al. (2007) compared a na¨ıve character bigram model with word-based models, and concluded that CWS is not necessary for text classification. Outside the literature of computational linguistics, there have been discussions in the field of cognitive science. Based on eye movement data, Tsai and McConkie (2003) found that fixations of Chinese"
P19-1314,P17-1078,0,0.213252,"1 Introduction There is a key difference between English (or more broadly, languages that use some form of the Latin alphabet) and Chinese (or other languages that do not have obvious word delimiters such as Korean and Japanese) : words in English can be easily recognized since the space token is a good approximation of a word divider, whereas no word divider is present between words in written Chinese sentences. This gives rise to the task of Chinese Word Segmentation (CWS) (Zhang et al., 2003; Peng et al., 2004; Huang and Zhao, 2007; Zhao et al., 2006; Zheng et al., 2013; Zhou et al., 2017; Yang et al., 2017, 2018). In the context of deep learning, the segmented words are usually treated as the basic units for operations (we call these models the word-based models for the rest of this paper). Each segmented word is associated with a fixed-length vector representation, which will be processed by deep learning models in the same way as how English words are processed. Word-based models come with a few fundamental disadvantages, as will be discussed below. Firstly, word data sparsity inevitably leads to overfitting and the ubiquity of OOV words limits the model’s learning capacity. Particularly, Zip"
P19-1314,P16-1040,0,0.0316618,"). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006; Carpenter, 2006). . Neural CWS Models such as RNNs, LSTMs (Hochreiter and Schmidhuber, 1997) and CNNs (Krizhevsky et al., 2012; Kim, 2014) not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years (Chen et al., 2015b,a; Cai and Zhao, 2016; Yao and Huang, 2016; Chen et al., 2017b; Zhang et al., 2016; Chen et al., 2017c; Yang et al., 2017; Cai et al., 2017; Zhang et al., 2017). Neural representations can be used either as a set of CRF features or as input to the decision layer. 3 model word char word char hybrid (word+char) hybrid (word+char) hybrid (word+char) hybrid (char only) Related Work Experimental Results In this section, we evaluate the effect of word segmentation in deep learning-based Chinese NLP in four tasks, language modeling, machine translation, text classification and sentence matching/paraphrase. To enforce apples-to-apples comparison, for both the word-based model and t"
P19-1314,W06-0127,0,0.318003,"eep learning-based Chinese Natural Language Processing. 1 1 Introduction There is a key difference between English (or more broadly, languages that use some form of the Latin alphabet) and Chinese (or other languages that do not have obvious word delimiters such as Korean and Japanese) : words in English can be easily recognized since the space token is a good approximation of a word divider, whereas no word divider is present between words in written Chinese sentences. This gives rise to the task of Chinese Word Segmentation (CWS) (Zhang et al., 2003; Peng et al., 2004; Huang and Zhao, 2007; Zhao et al., 2006; Zheng et al., 2013; Zhou et al., 2017; Yang et al., 2017, 2018). In the context of deep learning, the segmented words are usually treated as the basic units for operations (we call these models the word-based models for the rest of this paper). Each segmented word is associated with a fixed-length vector representation, which will be processed by deep learning models in the same way as how English words are processed. Word-based models come with a few fundamental disadvantages, as will be discussed below. Firstly, word data sparsity inevitably leads to overfitting and the ubiquity of OOV wor"
P19-1314,D13-1061,0,0.11877,"Missing"
P19-1314,D17-1079,0,0.0625883,"Missing"
Q13-1008,P10-1084,0,0.607486,"s that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA model to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both useful text features and the latent semantic structures from by LDA topic model. One related work is from Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for sentence evaluation. Then the scores are used for tuning the weights of various features that helpful for summary generation. Their work made a good step of combining topic model with feature based supervised learning. However, what their approach confuses us is that whether a topic model only based on word frequency is good enough to generate an appropriate sentence score for regression. Actually, how to incorporate features into LDA topic model has"
Q13-1008,P06-1039,0,0.0725469,"Missing"
Q13-1008,N06-1059,0,0.311964,"Missing"
Q13-1008,W10-4327,0,0.0500763,"ncy. E-step initialize φ0sk := 1/K for all i and s. initialize γmi := αmi + N )m/K for all i. initialize ηkt = 0 for all k and t. while not convergence for m = 1 : M t+1 according to Eqn.(6) update γm for s = 1 : Nm for k = 1 : K update φt+1 sk according to Eqn.(7) normalize the sum of φt+1 sk to 1. Minimize L(η) according to Eqn.(11)-(15). M-step: update β according to Eqn.(8)  K T X X    Ysy exp[Ψ(γms i ) − Ψ( γ ) + ηkt Ysy ]  m k s    t=1 k=1 Y ∝ × βkw if k = x     w∈s    4.3 0 if k 6= x (15) 4.2 Feature Space Lots of features have been proven to be useful for summarization (Louis et al., 2010). Here we discuss several types of features which are adopted in S-sLDA model. The feature values are either binary or normalized to the interval [0,1]. The following features are used in S-sLDA: Cosine Similarity with query: Cosine similarity is based on the tf-idf value of terms. 2 This is reasonable because the influence of γ and β have been embodied in φ during each iteration. 93 Figure 5: Learning process of η in S-sLDA Sentence Selection Strategy Next we explain our sentence selection strategy. According to our intuition that the desired summary should have a small KL divergence with que"
Q13-1008,W11-0507,0,0.0145846,"blem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA mode"
Q13-1008,W02-0401,0,0.425884,"-supervised) approaches, supervised approaches, and Bayesian approaches. Unsupervised (semi-supervised) approaches such as Lexrank (Erkan and Radex, 2004), manifold (Wan et al., 2007) treat summarization as a graphbased ranking problem. The relatedness between the query and each sentence is achieved by imposing querys influence on each sentence along with the propagation of graph. Most supervised approaches regard summarization task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on 90 query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and con"
Q13-1008,H05-1115,0,0.0759342,"Missing"
Q13-1008,D09-1026,0,0.151958,"Missing"
Q13-1008,W03-0502,0,0.102642,"Missing"
Q13-1008,N09-1041,0,\N,Missing
