2021.metanlp-1.3,Zero-Shot Compositional Concept Learning,2021,-1,-1,3,0,5275,guangyue xu,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,0,"In this paper, we study the problem of recognizing compositional attribute-object concepts within the zero-shot learning (ZSL) framework. We propose an episode-based cross-attention (EpiCA) network which combines merits of cross-attention mechanism and episode-based training strategy to recognize novel compositional concepts. Firstly, EpiCA bases on cross-attention to correlate conceptvisual information and utilizes the gated pooling layer to build contextualized representations for both images and concepts. The updated representations are used for a more indepth multi-modal relevance calculation for concept recognition. Secondly, a two-phase episode training strategy, especially the ransductive phase, is adopted to utilize unlabeled test examples to alleviate the low-resource learning problem. Experiments on two widelyused zero-shot compositional learning (ZSCL) benchmarks have demonstrated the effectiveness of the model compared with recent approaches on both conventional and generalized ZSCL settings."
2021.findings-emnlp.272,Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers,2021,-1,-1,2,0,7086,shane storks,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"As large-scale, pre-trained language models achieve human-level and superhuman accuracy on existing language understanding tasks, statistical bias in benchmark data and probing studies have recently called into question their true capabilities. For a more informative evaluation than accuracy on text classification tasks can offer, we propose evaluating systems through a novel measure of prediction coherence. We apply our framework to two existing language understanding benchmarks with different properties to demonstrate its versatility. Our experimental results show that this evaluation framework, although simple in ideas and implementation, is a quick, effective, and versatile measure to provide insight into the coherence of machines{'} predictions."
2021.findings-emnlp.422,Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding,2021,-1,-1,4,0,7086,shane storks,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machines{'} true ability in language understanding and reasoning. In this paper, we highlight the importance of evaluating the underlying reasoning process in addition to end performance. Toward this goal, we introduce Tiered Reasoning for Intuitive Physics (TRIP), a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machines{'} reasoning process. Our empirical results show that while large LMs can achieve high end performance, they struggle to support their predictions with valid supporting evidence. The TRIP dataset and our baseline results will motivate verifiable evaluation of commonsense reasoning and facilitate future research toward developing better language understanding and reasoning models."
2021.findings-acl.368,Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring,2021,-1,-1,2,0,7479,yichi zhang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.85,{M}ind{C}raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks,2021,-1,-1,3,0,8802,cristianpaul bara,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners{'} beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks."
2020.emnlp-main.703,Experience Grounds Language,2020,171,7,6,0,8387,yonatan bisk,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication."
P18-1086,What Action Causes This? Towards Naive Physical Action-Effect Prediction,2018,0,2,3,1,7478,qiaozi gao,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples."
D18-1283,Commonsense Justification for Action Explanation,2018,0,2,4,1,29126,shaohua yang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents."
P17-1150,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,2017,33,11,2,1,32648,lanbo she,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics. The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the models acquired from interactive learning result in a 48{\%} to 145{\%} performance gain when applied in new situations."
P16-1011,Incremental Acquisition of Verb Hypothesis Space towards Physical World Interaction,2016,22,9,2,1,32648,lanbo she,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
P16-1171,Physical Causality of Action Verbs in Grounded Language Understanding,2016,34,18,4,1,7478,qiaozi gao,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1019,Grounded Semantic Role Labeling,2016,37,15,6,1,29126,shaohua yang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1155,Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration,2016,35,17,7,1,34671,changsong liu,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W14-4313,Back to the Blocks World: Learning New Actions through Situated Human-Robot Dialogue,2014,21,39,5,1,32648,lanbo she,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"This paper describes an approach for a robotic arm to learn new actions through dialogue in a simplified blocks world. In particular, we have developed a threetier action knowledge representation that on one hand, supports the connection between symbolic representations of language and continuous sensorimotor representations of the robot; and on the other hand, supports the application of existing planning algorithms to address novel situations. Our empirical studies have shown that, based on this representation the robot was able to learn and execute basic actions in the blocks world. When a human is engaged in a dialogue to teach the robot new actions, step-by-step instructions lead to better learning performance compared to one-shot instructions."
P14-2003,Probabilistic Labeling for Efficient Referential Grounding based on Collaborative Discourse,2014,14,12,4,1,34671,changsong liu,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"When humans and artificial agents (e.g. robots) have mismatched perceptions of the shared environment, referential communication between them becomes difficult. To mediate perceptual differences, this paper presents a new approach using probabilistic labeling for referential grounding. This approach aims to integrate different types of evidence from the collaborative referential discourse into a unified scheme. Its probabilistic labeling procedure can generate multiple grounding hypotheses to facilitate follow-up dialogue. Our empirical results have shown the probabilistic labeling approach significantly outperforms a previous graphmatching approach for referential grounding."
W13-4010,Modeling Collaborative Referring for Situated Referential Grounding,2013,18,18,4,1,34671,changsong liu,Proceedings of the {SIGDIAL} 2013 Conference,0,"In situated dialogue, because humans and agents have mismatched capabilities of perceiving the shared physical world, referential grounding becomes difficult. Humans and agents will need to make extra efforts by collaborating with each other to mediate a shared perceptual basis and to come to a mutual understanding of intended referents in the environment. In this paper, we have extended our previous graph-matching based approach to explicitly incorporate collaborative referring behaviors into the referential grounding algorithm. In addition, hypergraph-based representations have been used to account for group descriptions that are likely to occur in spatial communications. Our empirical results have shown that incorporating the most prevalent pattern of collaboration with our hypergraph-based approach significantly improves reference resolution in situated dialogue by an absolute gain of over 18%."
D13-1038,Towards Situated Dialogue: Revisiting Referring Expression Generation,2013,46,13,4,1,32349,rui fang,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue."
W12-1621,Towards Mediating Shared Perceptual Basis in Situated Dialogue,2012,34,33,3,1,34671,changsong liu,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction."
N12-1089,Autonomous Self-Assessment of Autocorrections: Exploring Text Message Dialogues,2012,18,10,2,1,12160,tyler baldwin,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Text input aids such as automatic correction systems play an increasingly important role in facilitating fast text entry and efficient communication between text message users. Although these tools are beneficial when they work correctly, they can cause significant communication problems when they fail. To improve its autocorrection performance, it is important for the system to have the capability to assess its own performance and learn from its mistakes. To address this, this paper presents a novel task of self-assessment of autocorrection performance based on interactions between text message users. As part of this investigation, we collected a dataset of autocorrection mistakes from true text message users and experimented with a rich set of features in our self-assessment task. Our experimental results indicate that there are salient cues from the text message discourse that allow systems to assess their own behaviors with high precision."
J12-4003,Semantic Role Labeling of Implicit Arguments for Nominal Predicates,2012,117,48,2,1,37464,matthew gerber,Computational Linguistics,0,"Nominal predicates often carry implicit arguments. Recent work on semantic role labeling has focused on identifying arguments within the local context of a predicate; implicit arguments, however, have not been systematically examined. To address this limitation, we have manually annotated a corpus of implicit arguments for ten predicates from NomBank. Through analysis of this corpus, we find that implicit arguments add 71% to the argument structures that are present in NomBank. Using the corpus, we train a discriminative model that is able to identify implicit arguments with an F1 score of 50%, significantly outperforming an informed baseline model. This article describes our investigation, explores a wide variety of features important for the task, and discusses future directions for work on implicit argument identification."
W11-0909,A Joint Model of Implicit Arguments for Nominal Predicates,2011,11,2,2,1,37464,matthew gerber,Proceedings of the {ACL} 2011 Workshop on Relational Models of Semantics,0,"Many prior studies have investigated the recovery of semantic arguments for nominal predicates. The models in many of these studies have assumed that arguments are independent of each other. This assumption simplifies the computational modeling of semantic arguments, but it ignores the joint nature of natural language. This paper presents a preliminary investigation into the joint modeling of implicit arguments for nominal predicates. The joint model uses propositional knowledge extracted from millions of Internet webpages to help guide prediction."
I11-1169,Beyond Normalization: Pragmatics of Word Form in Text Messages,2011,18,4,2,1,12160,tyler baldwin,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Non-standard spellings in text messages often convey extra pragmatic information not found in the standard word form. However, text message normalization systems that transform non-standard text message spellings to standard form tend to ignore this information. To address this problem, this paper examines the types of extra pragmatic information that are conveyed by non-standard word forms. Empirical analysis of our data shows that 40% of non-standard word forms contain emotional information not found in the standard form, and 38% contain additional emphasis. This extra information can be important to downstream applications such as text-to-speech synthesis. We further investigated the automatic detection of non-standard forms that display additional information. Our empirical results show that character level features can provide important cues for such detection."
W10-4357,Hand Gestures in Disambiguating Types of You Expressions in Multiparty Meetings,2010,13,0,2,1,12160,tyler baldwin,Proceedings of the {SIGDIAL} 2010 Conference,0,"The second person pronoun you serves different functions in English. Each of these different types often corresponds to a different term when translated into another language. Correctly identifying different types of you can be beneficial to machine translation systems. To address this issue, we investigate disambiguation of different types of you occurrences in multiparty meetings with a new focus on the role of hand gesture. Our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic use of you (e.g., refer to people in general) and the referential use of you (e.g., refer to a specific person or a group of people). Incorporation of gesture can also compensate for limitations in automated language processing (e.g., reliable recognition of dialogue acts) and achieve comparable results."
P10-1160,Beyond {N}om{B}ank: A Study of Implicit Arguments for Nominal Predicates,2010,21,81,2,1,37464,matthew gerber,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task."
D10-1046,Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue,2010,23,27,2,0,46394,zahar prasov,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance."
D10-1074,Towards Conversation Entailment: An Empirical Investigation,2010,22,7,2,1,7045,chen zhang,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"While a significant amount of research has been devoted to textual entailment, automated entailment from conversational scripts has received less attention. To address this limitation, this paper investigates the problem of conversation entailment: automated inference of hypotheses from conversation scripts. We examine two levels of semantic representations: a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations."
W09-3928,The Role of Interactivity in Human-Machine Conversation for Automatic Word Acquisition,2009,21,10,2,1,46822,shaolin qu,Proceedings of the {SIGDIAL} 2009 Conference,0,"Motivated by the psycholinguistic finding that human eye gaze is tightly linked to speech production, previous work has applied naturally occurring eye gaze for automatic vocabulary acquisition. However, unlike in the typical settings for psycholinguistic studies, eye gaze can serve different functions in human-machine conversation. Some gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance."
W09-3930,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,20,5,2,1,7045,chen zhang,Proceedings of the {SIGDIAL} 2009 Conference,0,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."
N09-1017,The Role of Implicit Argumentation in Nominal {SRL},2009,16,19,2,1,37464,matthew gerber,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Nominals frequently surface without overtly expressed arguments. In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL."
D08-1026,Incorporating Temporal and Semantic Information with Eye Gaze for Automatic Word Acquisition in Multimodal Conversational Systems,2008,14,12,2,1,46822,shaolin qu,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"One major bottleneck in conversational systems is their incapability in interpreting unexpected user language inputs such as out-of-vocabulary words. To overcome this problem, conversational systems must be able to learn new words automatically during human machine conversation. Motivated by psycholinguistic findings on eye gaze and human language processing, we are developing techniques to incorporate human eye gaze for automatic word acquisition in multimodal conversational systems. This paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition. Our experiment results indicate that eye gaze provides a potential channel for automatically acquiring new words. The use of extra temporal and domain knowledge can significantly improve acquisition performance."
P07-1047,Automated Vocabulary Acquisition and Interpretation in Multimodal Conversational Systems,2007,12,13,2,0,875,yi liu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Motivated by psycholinguistic findings that eye gaze is tightly linked to human language production, we developed an unsupervised approach based on translation models to automatically learn the mappings between words and objects on a graphic display during human machine conversation. The experimental results indicate that user eye gaze can provide useful information to establish such mappings, which have important implications in automatically acquiring and interpreting user vocabularies for conversational systems."
N07-1036,An Exploration of Eye Gaze in Spoken Language Processing for Multimodal Conversational Interfaces,2007,21,25,2,1,46822,shaolin qu,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Motivated by psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding for multimodal conversational systems. Our assumption is that, during human machine conversation, a userxe2x80x99s eye gaze on the graphical display indicates salient entities on which the userxe2x80x99s attention is focused. The specific domain information about the salient entities is likely to be the content of communication and therefore can be used to constrain speech hypotheses and help language understanding. Based on this assumption, this paper describes an exploratory study that incorporates eye gaze in salience modeling for spoken language processing. Our empirical results show that eye gaze has a potential in improving automated language processing. Eye gaze is subconscious and involuntary during human machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing."
P06-2008,Towards Conversational {QA}: Automatic Identification of Problematic Situations and User Intent,2006,13,3,1,1,5276,joyce chai,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"To enable conversational QA, it is important to examine key issues addressed in conversational systems in the context of question answering. In conversational systems, understanding user intent is critical to the success of interaction. Recent studies have also shown that the capability to automatically identify problematic situations during interaction can significantly improve the system performance. Therefore, this paper investigates the new implications of user intent and problematic situations in the context of question answering. Our studies indicate that, in basic interactive QA, there are different types of user intent that are tied to different kinds of system performance (e.g., problematic/error free situations). Once users are motivated to find specific information related to their information goals, the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent."
H05-1028,A Salience Driven Approach to Robust Input Interpretation in Multimodal Conversational Systems,2005,35,10,1,1,5276,joyce chai,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"To improve the robustness in multimodal input interpretation, this paper presents a new salience driven approach. This approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on a graphical display can signal a part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication. This salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding, thus improving overall input interpretation. Our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation."
W04-2504,Discourse Structure for Context Question Answering,2004,-1,-1,1,1,5276,joyce chai,Proceedings of the Workshop on Pragmatics of Question Answering at {HLT}-{NAACL} 2004,0,None
P04-1001,Optimization in Multimodal Interpretation,2004,25,22,1,1,5276,joyce chai,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities, coherence of conversation, and the domain semantics. There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures."
N04-4011,Performance Evaluation and Error Analysis for Multimodal Reference Resolution in a Conversation System,2004,18,5,1,1,5276,joyce chai,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"Multimodal reference resolution is a process that automatically identifies what users refer to during multimodal human-machine conversation. Given the substantial work on multimodal reference resolution; it is important to evaluate the current state of the art, understand the limitations, and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. This paper analyzes the main error sources during real-time human-machine interaction and presents key strategies for designing robust multimodal reference resolution algorithms."
W03-0701,Combining Semantic and Temporal Constraints for Multimodal Integration in Conversation Systems,2003,7,4,1,1,5276,joyce chai,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Research Directions in Dialogue Processing,0,"In a multimodal conversation, user referring patterns could be complex, involving multiple referring expressions from speech utterances and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints to achieve a high performance."
C02-1035,Semantics-based Representation for Multimodal Interpretation in Conversational Systems,2002,11,5,1,1,5276,joyce chai,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"To support context-based multimodal interpretation in conversational systems, we have developed a semantics-based representation to capture salient information from user inputs and the overall conversation. In particular, we present three unique characteristics: fine-grained semantic models, flexible composition of feature structures, and consistent representation at multiple levels. This representation allows our system to use rich contexts to resolve ambiguities, infer unspecified information, and improve multimodal alignment. As a result, our system is able to enhance understanding of multimodal inputs including those abbreviated, imprecise, or complex ones."
H01-1012,A Conversational Interface for Online Shopping,2001,7,7,1,1,5276,joyce chai,Proceedings of the First International Conference on Human Language Technology Research,0,"We present a deployed, conversational dialog system that assists users in finding computers based on their usage patterns and constraints on specifications. We discuss findings from a market survey and two user studies. We compared our system to a directed dialog system and a menu driven navigation system. We found that the conversational interface reduced the average number of clicks by 63% and the average interaction time by 33% over a menu driven search system. The focus of our continuing work includes developing a dynamic, adaptive dialog management strategy, robustly handling user input and improving the user interface."
H01-1013,Conversational Sales Assistant for Online Shopping,2001,6,3,2,0,53932,margo budzikowska,Proceedings of the First International Conference on Human Language Technology Research,0,"Websites of businesses should accommodate both customer needs and business requirements. Traditional menu-driven navigation and key word search do not allow users to describe their intentions precisely. We have developed a conversational interface to online shopping that provides convenient, personalized access to information using natural language dialog. User studies show significantly reduced length of interactions in terms of time and number of clicks in finding products. The core dialog engine is easily adaptable to other domains."
W00-1011,Dynamic User Level and Utility Measurement for Adaptive Dialog in a Help-Desk System,2000,8,7,2,0,53286,preetam maloor,1st {SIG}dial Workshop on Discourse and Dialogue,0,"The learning and self-adaptive capability in dialog systems has become increasingly important with the advances in a wide range of applications. For any application, particularly the one dealing with a technical domain, the system should pay attention to not only the user experience level and dialog goals, but more importantly, the mechanism to adapt the system behavior to the evolving state of the user. This paper describes a methodology that first identifies the user experience level and utility metrics of the goal and sub-goals, then automatically adjusts those parameters based on discourse history and thus directs adaptive dialog management."
chai-2000-evaluation,Evaluation of a Generic Lexical Semantic Resource in Information Extraction,2000,14,0,1,1,5276,joyce chai,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We have created an information extraction system that allows users to train the system on a domain of interest. The system helps to maximize the effect of user training by applying WordNet to rule generation and validation. The results show that, with careful control, WordNet is helpful in generating useful rules to cover more instances and hence improve the overall performance. This is particularly true when the training set is small, where F-measure is increased from 65% to 72%. However, the impact of WordNet diminishes as the size of training data increases. This paper describes our experience in applying WordNet to this system and gives an evaluation of such"
W97-1001,A Trainable Message Understanding System,1997,8,6,2,0,53560,amit bagga,{C}o{NLL}97: Computational Natural Language Learning,0,None
W97-0809,The Use of Lexical Semantics in Information Extraction,1997,-1,-1,1,1,5276,joyce chai,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
W97-0110,Corpus Based Statistical Generalization Tree in Rule Optimization,1997,8,5,1,1,5276,joyce chai,Fifth Workshop on Very Large Corpora,0,None
A97-2004,Duke{'}s Trainable Information and Meaning Extraction System (Duke {TIMES}),1997,3,0,2,0,53560,amit bagga,Fifth Conference on Applied Natural Language Processing: Descriptions of System Demonstrations and Videos,0,None
