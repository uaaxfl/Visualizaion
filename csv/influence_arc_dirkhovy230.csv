2020.acl-main.154,D15-1238,0,0.275716,"differs in a few key points: they show that translation weakens the predictive power, but do not investigate the direction of false predictions. We show that there is a definitive bias. In addition, we extend the analysis to include age. We also use various commercially available MT tools, rather than research systems. Recent research has suggested that machine translation systems reflect cultural and societal bi= split ∗ = split ases (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019), though mostly focusing on data selection and embeddings as sources. Work by Mirkin et al. (2015); Mirkin and Meunier (2015) has set the stage for considering the impact of demographic variation (Hovy et al., 2015) and its integration in MT more general. There is a growing literature on various types of bias in NLP. For a recent overview, see Shah et al. (2020). 6 Conclusion We test what demographic profiles author attribute tools predict for the translations from various commercially available machine translation tools. We find that independent of the MT system and the translation quality, the predicted demographics differ systematically when translating into English. On average, translations make the author seem"
2020.acl-main.154,D15-1130,0,0.34863,"(Bing, DeepL, Google) make demographically diverse samples from five languages “sound” older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. These results open up interesting new research avenues in machine translation to take stylistic considerations into account. 1 Introduction Translating what is being said is arguably the most important aspect of machine translation, and has been the main focus of all its efforts so far. However, how something is said also has an impact on how the final translation is perceived. Mirkin et al. (2015) have pointed out that demographic aspects of language do play a role in translation, and could help in personalization. As Vanmassenhove et al. (2018) have shown, gendered inflections like “Sono stanco/a” (Italian I am tired) are an important aspect of correct translations. In many cases, capturing the style of a document is equally important as its content: translating a lover’s greeting as “I am entirely pleased to see you” might be semantically correct, but seems out of place. Demographic factors (age, gender, etc.) all manifest in language, and therefore influence style: we do not expect"
2020.acl-main.154,E17-1101,0,0.406456,"rrect translations. In many cases, capturing the style of a document is equally important as its content: translating a lover’s greeting as “I am entirely pleased to see you” might be semantically correct, but seems out of place. Demographic factors (age, gender, etc.) all manifest in language, and therefore influence style: we do not expect a 6-year old to sound like an adult, and would not translate a person to seem differently gendered. However, in this paper, we show such a change is essentially what happens in machine translation: authors sound on average older and more male. Prior work (Rabinovich et al., 2017) has shown that translation weakens the signal for gender prediction. We substantially extend this analysis in terms of languages, demographic factors, and types of models, controlling for demographically representative samples. We show the direction in which the predicted demographic factors differ in the translations, and find that there are consistent biases towards older and more male profiles. Our findings suggest a severe case of overexposure to writings from these demographics (Hovy and Spruit, 2016), which creates a self-reinforcing loop. In this paper, we use demographicallyrepresenta"
2020.acl-main.154,W19-3821,0,0.0593571,"Missing"
2020.acl-main.154,2020.acl-main.468,1,0.88894,"the predicted demographics of each translation (as well as a control predictor trained on the same language). Without making any judgment on the translation of the content, we find a) that there are substantial discrepancies in the perceived demographics, and b) that translations tend to make the writers appear older and considerably more male than they are. Contributions We empirically show how translations affect the demographic profile of a text. We release our data set at https://github.com/ MilaNLProc/translation_bias. Our findings contribute to a growing literature on biases in NLP (see Shah et al. (2020) for a recent overview). 2 Data We use the Trustpilot data set from Hovy et al. (2015), which provides reviews in different languages, and includes information about age and gender. We use only English, German, Italian, French, and Dutch reviews, based on two criteria: 1) availability of the language in translation mod1686 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1686–1690 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els, and 2) sufficient data for representative samples (see below) in the corpus. For the English data"
2020.acl-main.154,P19-1164,0,0.0555657,"guages. 5 Related Work The work by Rabinovich et al. (2017) is most similar to ours, in that they investigated the effect of translation on gender. However, it differs in a few key points: they show that translation weakens the predictive power, but do not investigate the direction of false predictions. We show that there is a definitive bias. In addition, we extend the analysis to include age. We also use various commercially available MT tools, rather than research systems. Recent research has suggested that machine translation systems reflect cultural and societal bi= split ∗ = split ases (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019), though mostly focusing on data selection and embeddings as sources. Work by Mirkin et al. (2015); Mirkin and Meunier (2015) has set the stage for considering the impact of demographic variation (Hovy et al., 2015) and its integration in MT more general. There is a growing literature on various types of bias in NLP. For a recent overview, see Shah et al. (2020). 6 Conclusion We test what demographic profiles author attribute tools predict for the translations from various commercially available machine translation tools. We find that independent of the MT"
2020.acl-main.154,D18-1334,0,0.162295,"est that translation models reflect demographic bias in the training data. These results open up interesting new research avenues in machine translation to take stylistic considerations into account. 1 Introduction Translating what is being said is arguably the most important aspect of machine translation, and has been the main focus of all its efforts so far. However, how something is said also has an impact on how the final translation is perceived. Mirkin et al. (2015) have pointed out that demographic aspects of language do play a role in translation, and could help in personalization. As Vanmassenhove et al. (2018) have shown, gendered inflections like “Sono stanco/a” (Italian I am tired) are an important aspect of correct translations. In many cases, capturing the style of a document is equally important as its content: translating a lover’s greeting as “I am entirely pleased to see you” might be semantically correct, but seems out of place. Demographic factors (age, gender, etc.) all manifest in language, and therefore influence style: we do not expect a 6-year old to sound like an adult, and would not translate a person to seem differently gendered. However, in this paper, we show such a change is es"
2020.acl-main.154,P16-2096,1,0.795136,"ens in machine translation: authors sound on average older and more male. Prior work (Rabinovich et al., 2017) has shown that translation weakens the signal for gender prediction. We substantially extend this analysis in terms of languages, demographic factors, and types of models, controlling for demographically representative samples. We show the direction in which the predicted demographic factors differ in the translations, and find that there are consistent biases towards older and more male profiles. Our findings suggest a severe case of overexposure to writings from these demographics (Hovy and Spruit, 2016), which creates a self-reinforcing loop. In this paper, we use demographicallyrepresentative author samples from five languages (Dutch, English, French, German, Italian), and translate them with three commercially available machine translation systems (Google, Bing, and DeepL). We compare the true demographics with the predicted demographics of each translation (as well as a control predictor trained on the same language). Without making any judgment on the translation of the content, we find a) that there are substantial discrepancies in the perceived demographics, and b) that translations te"
2020.acl-main.154,P12-3005,0,0.0121943,"based on two criteria: 1) availability of the language in translation mod1686 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1686–1690 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els, and 2) sufficient data for representative samples (see below) in the corpus. For the English data, we use US reviews, rather than UK reviews, based on a general prevalence of this variety in translation engines. 2.1 Translation Data For each language, we restrict ourselves to reviews written in the respective language (according to langid 1 (Lui and Baldwin, 2012)) that have both age and gender information. We use the CIA factbook2 data on age pyramids to sample 200 each male and female. We use the age groups given on the factbook, i.e., 15–24, 25–54, 55–64, and 65+. Based on data sparsity in the Trustpilot data, we do not include the under-15 age group. This sampling procedure results in five test sets of about 400 instances each (the exact numbers vary slightly according to rounding and the proportions in the CIA factbook data), balanced for binary gender. The exception is Italian, where the original data is so heavily skewed towards male reviews tha"
2020.acl-main.468,S18-2005,0,0.0969809,"Missing"
2020.acl-main.468,W19-3823,0,0.295616,"central metric: D(Y, Yˆ |A) = 2(log(p(Y |A)) − log(p(Yˆ |A))) where p(Y |A) is the specified ideal distribution (either derived empirically or theoretically) and p(Yˆ |A) is the distribution within the data. For error disparity the ideal distribution is always the Uniform and Yˆ is replaced with the error. KL divergence (DKL [P (Yˆ |A)P (Y |A)]) can be used as a secondary, more scalable alternative. Our measure above attempts to synthesize metrics others have used in works focused on specific biases. For example, the definition of outcome disparity is analogous to that used for semantic bias. Kurita et al. (2019) quantify bias in embeddings as the difference in log probability score when replacing words suspected to carry semantic differences (‘he’, ‘she’) with a mask: log(P ([M ask] = “hP RON i”|[M ask] is “hN OU N i”)) − log(P ([M ask] = “hP RON i”|[M ask] is [M ask]))) hN OU N i is replaced with a specific noun to check for semantic bias (e.g., an occupation), and hP RON i is an associated demographic word (e.g., “he” or “she”). 3 Four Origins of Bias But what leads to an outcome disparity or error disparity? We identify four points within the standard supervised NLP pipeline where bias may origina"
2020.acl-main.468,P18-2005,0,0.15638,"ork differently than one for teenagers). Still, undetected and unaddressed, biases can lead to negative consequences: There are aggregate effects for demographic groups, which combine to produce predictive bias. I.e., the label distribution of a predictive model reflects a human attribute in a way that diverges from a theoretically defined “ideal distribution.” For example, a Part Of Speech (POS) tagger reflecting how an older generation uses words (Hovy and Søgaard, 2015) diverges from the population as a whole. A variety of papers have begun to address countermeasures for predictive biases (Li et al., 2018; Elazar and Goldberg, 2018; Coavoux et al., 2018).1 Each identifies a specific bias and counter1 An even more extensive body of work on fairness exists as part of the FAT* conferences, which goes beyond the scope origin consequence over-amplification label bias The model discriminates on a given human attribute beyond its source base-rate. Biased annotations, interaction, or latent bias from past classifications. Embedding Corpus Source Population features features ?embedding Xsource (Pre-trained Side) fit outcome disparity The distribution of outcomes, given attribute A, is dissimilar than t"
2020.acl-main.468,D17-1119,1,0.928159,"ather than specifying a specific distribution (e.g., moving toward a uniform distribution of 3 We have substituted “test"" with “predictive model”. “Attributes” include both continuously valued user-level variables, like age, personality on a 7-point scale, etc. (also referred to as “dimensional” or “factors”), and discrete categories like membership in an ethnic group. Psychological research suggests that people are better represented by continuously valued scores, where possible, than discrete categories (Baumeister et al., 2007; Widiger and Samuel, 2005; McCrae and Costa Jr., 1989). In NLP, Lynn et al. (2017) shows benefits from treating user-level attributes as continuously when integrating into NLP models. 4 pronouns associated with computer science). Our framework should enable its users to apply evolving standards and norms across NLP’s many application contexts. A prototypical example of outcome disparity is gender disparity in image captions. Zhao et al. (2017) and Hendricks et al. (2018) demonstrate a systematic difference with respect to gender in the outcome of the model, Yˆ even when taking the source distribution as an ideal target distribution: Q(Yˆtarget |gender)  Q(Ytarget |gender)"
2020.acl-main.468,W14-2702,0,0.10424,"arital status, and income), they use poststratification to reduce the bias (some of these methods cross into addressing selection bias). Selection bias. The primary source for selection bias is the mismatch between the sample distribution and the ideal distribution. Consequently, any countermeasures need to re-align the two distributions to minimize this mismatch. The easiest way to address the mismatch is to re-stratify the data to more closely match the ideal distribution. However, this often involves downsampling an overly represented class, which reduces the number of available instances. Mohammady and Culotta (2014) use a stratified sampling technique to reduce the selection bias in the data. Almeida et al. (2015) use demographic user attributes, including age, gender, and social status, to predict the election results in six different cities of Brazil. They use stratified sampling on all the resulting groups to reduce selection bias. Rather than re-sampling, others use reweighting or poststratifying to reduce selection bias. Culotta (2014) estimates county-level health statistics based on social media data. He shows we can stratify based on external socio-demographic data about a community’s composition"
2020.acl-main.468,Q14-1025,0,0.0170513,"iew. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that method. Other approaches have explored harnessing the inherent disagreement among annotators to guide the training process (Plank et al., 2014). By weighting updates by the amount of disagreement on the labels, this method prevents bias towards any one label. The weighted updates act as a regularizer during training, which might also help prevent overamplification. If annotators behave in predictab"
2020.acl-main.468,Q18-1040,1,0.860763,"proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that method. Other approaches have explored harnessing the inherent disagreement among annotators to guide the training process (Plank et al., 2014). By weighting updates by the amount of disagreement on the labels, this method prevents bias towards any one label. The weighted updates act as a regularizer during training, which might also help prevent overamplification. If annotators behave in predictable ways to produce a"
2020.acl-main.468,Q14-1007,0,0.0110563,"ajafian (2019) extend such measures. Similarly, Romanov et al. (2019) define bias based on the correlation between the embeddings of human attributes with the difference in the True Positive rates between human traits. This approach is reflective of an error disparity. Our framework encompasses bias-related work in the social sciences. Please see the supplement in A.1 for a brief overview. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that meth"
2020.acl-main.468,E14-1078,1,0.846636,"ng or testing (selection bias), (3) the representation of data (semantic bias), or (4) due to the fit method itself (overamplification). Label Bias Label bias emerges when the distribution of the dependent variable in the data source diverges substantially from the ideal distribution: Q(Ys |As )  P (Ys |As ) Here, the labels themselves are erroneous concerning the demographic attribute of interest (as compared to the source distribution). Sometimes, this bias is due to a non-representative group of annotators (Joseph et al., 2017). In other cases, it may be due to a lack of domain expertise (Plank et al., 2014), or due to preconceived notions and stereotypes held by the annotators (Sap et al., 2019). Selection bias. Selection bias emerges due to non-representative observations. I.e., when the users generating the training (source) observations differ from the user distribution of the target, where the model will be applied. Selection bias (sometimes also referred to as sample bias) has long been a concern in the social sciences. At this point, testing for such a bias is a fundamental consideration in study design (Berk, 1983; Culotta, 2014). Non-representative data is the origin for selection bias."
2020.acl-main.468,W15-1203,1,0.889201,"Missing"
2020.acl-main.468,W15-1205,1,0.899795,"Missing"
2020.acl-main.468,N18-2002,0,0.110534,"Missing"
2020.acl-main.468,P19-1163,0,0.24012,"[predictive model] cannot be called biased without reference to a specific prediction situation; thus, the same instrument may be biased in one application, but unbiased in another."" A prototypical example of error disparity is the “Wall Street Journal Effect” – a systematic difference in error as a function of demographics, first documented by Hovy and Søgaard (2015). In theory, POS tagging errors increase the further an author’s demographic attributes differ from the average WSJ author of the 1980s and 1990s (on whom many POS taggers were trained – a selection bias, discussed next). Work by Sap et al. (2019) shows error disparity from a different origin, namely unfairness in hate speech detection. They find that annotators for hate speech on social media make more mistakes on posts of black individuals. Contrary to the case above, the disparity is not necessarily due to a difference between author and annotator population (a selection bias). Instead, the label disparity stems from annotators failing to account for the authors’ racial background and sociolinguistic norms. Source and Target Populations. An important assumption of our framework is that disparities are dependent on the population for"
2020.acl-main.468,P19-1164,0,0.107748,"d instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more male than the original authors. Several authors have suggested it is essential for language to be understood within the context of the author and their social environment Jurgens (2013); Danescu-Niculescu-Mizil et al. (2013); Hovy (2018); Yang et al. (2019). Considering the author demographics improves the accuracy of text classifiersVolkova et al. (2013);"
2020.acl-main.468,P19-1159,0,0.172512,"bias often do not apply to another. As a consequence, much work has focused on bias effects and symptoms rather than their origins. While it is essential to address the effects of bias, it can leave the fundamental origin unchanged (Gonen and Goldberg, 2019), requiring researchers to rediscover the issue over and over. The “bias” discussed in one paper may, therefore, be quite different than that in another.2 A shared definition and framework of predictive bias can unify these efforts, provide a common terminology, help to identify underlying causes, and allow coordination of countermeasures (Sun et al., 2019). However, such a general framework had yet to be proposed within the NLP community. To address these problems, we suggest a joint conceptual framework, depicted in Figure 1, outlining and relating the different origins of bias. We base our framework on an extensive survey of the relevant NLP literature, informed by selected works in social science and adjacent fields. We identify four distinct sources of bias: selection bias, label bias, model overamplification, and semantic bias. We can express all of these as differences between (a) a “true” or intended distribution (e.g., over users, label"
2020.acl-main.468,P19-1162,0,0.0288519,"the causal effect of a variable on the outcome. E.g., when the causal risk ratio (CRR) differs from associational risk ratio (ARR). Similarly, Baker et al. (2013) define bias as uncontrolled covariates or “disturbing variables” that are related to measures of interest. Others provide definitions restricted to particular applications. For example, Caliskan et al. (2017) propose the Word-Embedding Association Test (WEAT). It quantifies semantic bias based on the distance between words with demographic associations in the embedding space. The previously mentioned work by Kurita et al. (2019) and Sweeney and Najafian (2019) extend such measures. Similarly, Romanov et al. (2019) define bias based on the correlation between the embeddings of human attributes with the difference in the True Positive rates between human traits. This approach is reflective of an error disparity. Our framework encompasses bias-related work in the social sciences. Please see the supplement in A.1 for a brief overview. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al."
2020.acl-main.468,D13-1187,0,0.102789,"Missing"
2020.acl-main.468,Q18-1042,0,0.0156524,"fication. In its simplest form, overamplification of inherent bias by the model can be corrected by downweighting the biased instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more male than the original authors. Several authors have suggested it is essential for language to be understood within the context of the author and their social environment Jurgens (2013); Danescu-Niculescu-Mizil et al. (2013); Hovy (2"
2020.acl-main.468,D17-1323,0,0.0473255,"group. Psychological research suggests that people are better represented by continuously valued scores, where possible, than discrete categories (Baumeister et al., 2007; Widiger and Samuel, 2005; McCrae and Costa Jr., 1989). In NLP, Lynn et al. (2017) shows benefits from treating user-level attributes as continuously when integrating into NLP models. 4 pronouns associated with computer science). Our framework should enable its users to apply evolving standards and norms across NLP’s many application contexts. A prototypical example of outcome disparity is gender disparity in image captions. Zhao et al. (2017) and Hendricks et al. (2018) demonstrate a systematic difference with respect to gender in the outcome of the model, Yˆ even when taking the source distribution as an ideal target distribution: Q(Yˆtarget |gender)  Q(Ytarget |gender) ∼ Q(Ysource |gender). As a result, captions overpredict females in images with ovens and males in images with snowboards. Error disparity. We say there is an error disparity when model predictions have larger error for individuals with a given user attribute (or range of attributes in the case of continuously-valued attributes). Formally, the error of a predicted"
2020.acl-main.468,N18-2003,0,0.0612627,"hey show that it also protects user privacy. The findings from Elazar and Goldberg (2018), however, suggest that even with adversarial training, internal representations still retain traces of demographic information. Overamplification. In its simplest form, overamplification of inherent bias by the model can be corrected by downweighting the biased instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more mal"
2020.acl-main.468,P19-1161,0,0.0378486,"et al. (2004), propose Directed Acyclic graphs for various heterogeneous types of selection bias, and suggest using stratified sampling, regression adjustment, or inverse probability weighting to avoid the bias in the data. Zagheni and Weber (2015), study the use of Internet Data for demographic studies and propose two approaches to reduce the selection bias in their task. If the ground truth is available, they adjust selection bias based on the calibration of a stochastic microsimulation. If unavailable, they suggest using a difference-in-differences technique to find out trends on the Web. Zmigrod et al. (2019) show that gender-based selection bias could be addressed by data augmentation, i.e., by adding slightly altered examples to the data. This addition addresses selection bias originating in the features (Xsource ), so that the model is fit on a more gender-representative sample. Their approach is similar to the reweighting of poll data based on demographics, which can be applied more directly to tweet-based population surveillance (see our last case study, A.2). Li et al. (2018) introduce a model-based countermeasure. They use an adversarial multitasklearning setup to model demographic attribut"
2020.acl-tutorials.2,E17-2069,1,0.800424,"ested in informal instruction outside of university contexts. 5 Instructors Reading List We recommend the following short readings to get a sense of the kinds of issues we will be approaching: Xanda Schofield Harvey Mudd College xanda@cs.hmc.edu www.cs.hmc.edu/˜xanda Xanda Schofield is an Assistant Professor of Computer Science at Harvey Mudd College. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education and hiring: Hutchinson and"
2020.acl-tutorials.2,Q16-1021,1,0.791108,"ested in teaching, or interested in informal instruction outside of university contexts. 5 Instructors Reading List We recommend the following short readings to get a sense of the kinds of issues we will be approaching: Xanda Schofield Harvey Mudd College xanda@cs.hmc.edu www.cs.hmc.edu/˜xanda Xanda Schofield is an Assistant Professor of Computer Science at Harvey Mudd College. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education an"
2020.acl-tutorials.2,Q18-1041,1,0.890293,"ssor of Linguistics and Adjunct Professor of Computer Science and Engineering at the University of Washington. Her research interests include computational semantics, grammar engineering, computational linguistic typology, and ethics in NLP. She is the Faculty Director of UW’s Professional Masters in Computational Linguistics (CLMS) and has been engaged with integrating ethics into the CLMS curriculum since 2016. She co-organized the first EthNLP workshop. Her first publication in this area is the TACL paper “Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science” (Bender and Friedman, 2018) and she has been an invited speaker at workshops and panels related to ethics and NLP (or AI more broadly) at the Taskar Memorial Event (UW, March 2018), The Future of Artificial Intelligence: Language, Ethics, Technology (Cambridge, March 2019), West Coast NLP (Facebook, September 2019), Machine Learning Competitions for All (NeurIPS, December 2019) and AAAS (Seattle, February 2020). Privacy Design a small search engine around an inverted index that uses random integer noise from a two-sided geometric distribution (Ghosh et al., 2012) to shape which queries are retrieved. Analyze how much th"
2020.acl-tutorials.2,D18-1001,0,0.0159429,"lege. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education and hiring: Hutchinson and Mitchell 2019 8 forts on how to introduce considerations of ethics and bias into other courses such as Algorithms. Dirk Hovy and Shannon L. Spruit. 2016. The social impact of natural language processing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 591–598. Associat"
2020.findings-emnlp.214,P13-1025,0,0.0340216,"0.78 and 0.86). For the indicators of business success, either the SVM (F-measures = 0.59, 0.38 and 0.67 ) or the MTL (F-measures = 0.63, 0.51 and 0.65) model outperforms the majority baseline. 5 Related Work There have been a few studies analyzing language usage when people communicate. For example, researchers have studied power (or hierarchical) relationships in online communities (DanescuNiculescu-Mizil et al., 2012), emails (Prabhakaran and Rambow, 2014), and social networks (Bramsen et al., 2011). Some have studied how roles of Wikipedia editors affect their success (Maki et al., 2017). Danescu-Niculescu-Mizil et al. (2013) an2369 alyze politeness in online forums using structural and linguistic features derived from the communications between two individuals. Katerenchuk and Rosenberg (2016) develop an algorithm to predict user influence levels in online communities. Rashid and Blanco (2018) characterize interactions between people with dimensions and produce a dataset annotating dimensions on TV scripts. VegaRedondo et al. (2019) annotate business relevance and sentiment on online chat interactions among aspiring entrepreneurs. In contrast, we annotate the communicative styles cooperativeness, motivational, ad"
2020.findings-emnlp.214,D16-1057,0,0.0229225,"per indicator of business success to predict the different labels. Feature Set. After basic preprocessing (removal of stop words), tokenization, and parsing (to get the root verb) using spaCy, we extract features from the chat interactions and sentiment lexica. The feature set relies only on language usage. We extract the first word in a chat interaction, the bag-of-words representations (binary flags and tf-idf scores) of the chat interaction and features from sentiment lexica. Specifically, we extract flags indicating whether the turn has a positive, negative or neutral word in the list by Hamilton et al. (2016), the sentiment score of the chat interaction (summation of sentiment scores per token over number of tokens), and a flag indicating whether the interaction contains a negative word from the list by Hu and Liu (2004). We also extract other features, which include (a) the root verb (b) binary flags indicating the presence of exclamation, question marks and negation cues from Morante and Daelemans (2012). 4.2 Model Cooperative Motivation Equal Advice HAS BUSINESS BUSINESS EVER BUSINESS PROPOSAL P R F majority SVM majority SVM majority SVM majority SVM 0.25 0.77 0.66 0.90 0.60 0.78 0.74 0.86 0.50"
2020.findings-emnlp.214,N13-1132,1,0.841047,"Missing"
2020.findings-emnlp.214,I17-1103,0,0.048899,"Missing"
2020.findings-emnlp.214,N13-1090,0,0.0142672,"L majority SVM MTL 0.51 0.58 0.61 0.20 0.54 0.52 0.57 0.66 0.65 0.71 0.68 0.66 0.44 0.47 0.51 0.75 0.69 0.75 0.59 0.59 0.63 0.27 0.38 0.51 0.64 0.67 0.65 Table 4: Results for predicting styles of interactions and three indicators of business success. The Fmeasures are the weighted averages of the F-measures of the two labels. Multitask Learning (MTL) setup We use a standard Convolutional Neural Network over word-embeddings, with one output per task. We preprocess the data (convert to lowercase, removed URLs and stop-words, converted numbers to 0’s etc.) and learn a skip-gram embeddings model (Mikolov et al., 2013) trained for 50 epochs. We use an embedding size of 512, choosing a power of 2 for memory efficiency. In the CNN, the input layer has the word indices of the text, converted via the embedding matrix into word embeddings. We convolve two parallel channels with max-pooling layers, and convolutional window sizes 4 and 8 over the input. The two window sizes account for both short and relatively long patterns in the texts. In both channels, the initial number of filters is 128 for the first convolution, and 256 in the second one. We join the convolutional channels’ output and pass it through an att"
2020.findings-emnlp.214,morante-daelemans-2012-conandoyle,0,0.0181801,"Missing"
2020.findings-emnlp.214,P14-2056,0,0.0231418,"ns as well as the business success indicators. Our SVM model does much better than the majority baseline for all the styles of interactions (F-measures = 0.77, 0.89, 0.78 and 0.86). For the indicators of business success, either the SVM (F-measures = 0.59, 0.38 and 0.67 ) or the MTL (F-measures = 0.63, 0.51 and 0.65) model outperforms the majority baseline. 5 Related Work There have been a few studies analyzing language usage when people communicate. For example, researchers have studied power (or hierarchical) relationships in online communities (DanescuNiculescu-Mizil et al., 2012), emails (Prabhakaran and Rambow, 2014), and social networks (Bramsen et al., 2011). Some have studied how roles of Wikipedia editors affect their success (Maki et al., 2017). Danescu-Niculescu-Mizil et al. (2013) an2369 alyze politeness in online forums using structural and linguistic features derived from the communications between two individuals. Katerenchuk and Rosenberg (2016) develop an algorithm to predict user influence levels in online communities. Rashid and Blanco (2018) characterize interactions between people with dimensions and produce a dataset annotating dimensions on TV scripts. VegaRedondo et al. (2019) annotate"
2020.findings-emnlp.214,D18-1470,1,0.92104,"s they produce an active exchange of ideas. People are usually assumed to be altruistic in networks like online social forums. They cooperate with and help one another with answers, advice, and ideas. The motivations behind helping a peer include, but are not limited to, getting pure pleasure from helping, self-advancement, building a reputation, developing relationships, or sheer entertainment (Tausczik and Pennebaker, 2012). When people interact with each other, their interactions vary along various communicative styles, such as showing cooperativeness, equality, business orientation, etc. (Rashid and Blanco, 2018). Varying these communication styles provides tools to achieve communicative goals. For example, someone trying to build a reputation will tend to use a more cooperative style. Someone who tries to be helpful may use more words of advice in their interactions. The usage of relationshipestablishing styles is more prevalent in certain personalities (Cheng, 2011) and in specific settings. Business-oriented people communicate more independence, tolerance of ambiguity, risk-taking propensity, innovativeness, and leadership qualities (Wagener et al., 2010). The impact of these styles is, therefore,"
2020.findings-emnlp.214,N16-1174,0,0.0200264,"via the embedding matrix into word embeddings. We convolve two parallel channels with max-pooling layers, and convolutional window sizes 4 and 8 over the input. The two window sizes account for both short and relatively long patterns in the texts. In both channels, the initial number of filters is 128 for the first convolution, and 256 in the second one. We join the convolutional channels’ output and pass it through an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) to emphasize the weight of any meaningful pattern recognized by the convolutions. We use the implementation of Yang et al. (2016). The output consists of 7 independent, fullyconnected layers for the predictions, respectively in the form of discrete labels for classification of one of the business success indicators of a person (HAS BUSINESS , BUSINESS EVER or BUSINESS PRO as the target task, and the styles of interactions (business, sentiment, cooperativeness, motivational, advice, equality) as the auxiliary tasks. We trained one model per business success indicator. POSAL ) 4.3 Results Table 4 compares the results of the different systems to predict the styles of interactions as well as the business success indicators."
2020.vardial-1.1,2020.vardial-1.24,0,0.254959,"Missing"
2020.vardial-1.1,2020.vardial-1.26,0,0.535455,"y stages of the COVID-19 pandemic. Lock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Mo"
2020.vardial-1.1,W19-1402,0,0.3829,"Missing"
2020.vardial-1.1,W18-3909,1,0.890114,"Missing"
2020.vardial-1.1,P19-1068,1,0.744799,"Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 https://sites.google.com/view/vardial2020/evaluation-campaign 2 For recent surveys on these topics see Zampieri et al. (2020) and Jauhiainen et al. (2019c). License details: http: 1 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14 Barcelona, Spain (Online), December 13, 2020 2 Shared Tasks at VarDial 2020 Romanian Dialect Identification (RDI): In the Romanian Dialect Identification (RDI) shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian (MD) and Romanian (RO) samples of text collected from the news domain. The task was a binary classification by dialect, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) dialects. The task was closed, therefore, participants are not allowed to use external data to train their models. The test set contained newly collected text samples from a different domain, not previously included in MOROCO, resulting in a cross-domain dialect identification task. Social Media Variety Geolocation (SMG): In cont"
2020.vardial-1.1,2020.vardial-1.25,0,0.269727,"Missing"
2020.vardial-1.1,2020.vardial-1.17,0,0.072284,"Missing"
2020.vardial-1.1,N19-1423,0,0.024395,"ainen et al. (2020b). 6.2 Participants and Approaches Unfortunately, the ULI shared task had only one team submitting results to the tracks. The NRC team submitted three runs for each of the shared task tracks. All the runs used BERT-related deep neural networks taking sequences of characters as input similar to what the NRC team used when they won the CLI shared task (Jauhiainen et al., 2019b) in the previous VarDial Evaluation Campaign (BernierColborne et al., 2019). The encoders of the networks were pre-trained on masked language modeling (MLM) and sentence pair classification (SPC) tasks (Devlin et al., 2019). The third run on each track was using only the information on the training set as opposed to the second run, in which the MLM was also done on the unlabeled test set in order to adapt the model. The first run on each track was a plurality voting ensemble of the six models used in the second and third runs of all the tracks. 6.3 Results For the baseline, we used an implementation of the HeLI method equal to the one we used when evaluating language identification methods for 285 languages (Jauhiainen et al., 2017). The baseline and the NRC teams results are listed in Tables 8, 9, and 10. Rank"
2020.vardial-1.1,2020.findings-emnlp.387,0,0.274164,"Missing"
2020.vardial-1.1,goldhahn-etal-2012-building,0,0.217587,"same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the Leipzig corpora collection (Goldhahn et al., 2012). The test set for the relevant languages included sentences from the forthcoming Wanca 2017 corpora (Jauhiainen et al., 2020b) that were not present in the Wanca 2016 corpora. The sentences for the non-relevant languages were from the Leipzig corpora collection. The ULI shared task was divided into three separate tracks using the same training and test data. The difference between the tracks was based on how the submissions were scored: track 1 focused on macro-averaged F-score for the 29 relevant languages, track 2 on micro-averaged F-score for the relevant languages, and track 3 on macro-av"
2020.vardial-1.1,L16-1284,1,0.891978,"Missing"
2020.vardial-1.1,2020.vardial-1.23,1,0.814174,"Missing"
2020.vardial-1.1,D18-1469,1,0.402749,"e SMG task is split into three subtasks covering different language areas: the BCMS subtask is focused on geolocated tweets published in the area of Croatia, Bosnia and Herzegovina, Montenegro and Serbia in the HBS macro-language (Ljubeˇsi´c et al., 2016); the DE-AT subtask focuses on conversations from the microblogging platform Jodel initiated in Germany and Austria, which are written in standard German but commonly contain regional and dialectal forms; the CH subtask is based on Jodel conversations initiated in Switzerland, which were found to be held majoritarily in Swiss German dialects (Hovy and Purschke, 2018). All three subtasks used the same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the"
2020.vardial-1.1,W17-1225,1,0.926999,"Missing"
2020.vardial-1.1,J16-3005,1,0.860747,"h quantile loss. SUKI. This approach divides each geographic area into a fixed grid with 81 areas and uses a n-gram language model to predict the most likely area (Jauhiainen et al., 2020a). The Linguistadors. These submissions are based on classic regression methods (linear regression, lasso regression, and ridge regression) and rely on TF-IDF weighted input features. UnibucKernel. The UnibucKernel team (G˘aman and Ionescu, 2020a) submitted two single systems, a character-level CNN (Zhang et al., 2015) with double regression output, and a Nu-SVR model trained on top of n-gram string kernels (Ionescu et al., 2016). The third system is an ensemble approach based on XGBoost, trained on the predictions provided by the two previously mentioned systems and an LSTMbased one. The LSTM is trained on top of fine-tuned German BERT embeddings. ZHAW-InIT. The ZHAW-InIT team (Benites et al., 2020) uses unsupervised k-means clustering to infer a set of dialect classes which are then used in a classification architecture. Their systems are based 7 either on SVMs with TF-IDF weighted word and character n-gram features, or on the HELI language modeling architecture (ZHAW-InIT (HELI)). The SVM submission to the CH subta"
2020.vardial-1.1,W17-0221,1,0.877702,"Missing"
2020.vardial-1.1,W19-1409,1,0.856366,"Missing"
2020.vardial-1.1,2020.vardial-1.21,1,0.758565,"Missing"
2020.vardial-1.1,W16-4801,1,0.731395,"Missing"
2020.vardial-1.1,2020.vardial-1.27,0,0.363938,"ock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Moldavian and Rom"
2020.vardial-1.1,2020.vardial-1.18,0,0.531349,"Missing"
2020.vardial-1.1,2020.vardial-1.20,0,0.189489,"Missing"
2020.vardial-1.1,L16-1641,1,0.889566,"Missing"
2020.vardial-1.1,2020.vardial-1.19,1,0.750864,"Missing"
2020.vardial-1.1,2020.vardial-1.22,0,0.515168,"Missing"
2020.vardial-1.1,W17-1201,1,0.773425,"Missing"
2021.acl-short.96,S17-2001,0,0.0115744,"abstracts from Bianchi et al. (2021)), 4 Tweets2011 , Google News (Qiang et al., 2019), and the StackOverflow dataset (Qiang et al., 2019). The latter three are already pre-processed. We use a similar pipeline for 20NewsGroups and Wiki20K: removing digits, punctuation, stopwords, and infrequent words. We derive SBERT document representations from unpreprocessed text for Wiki20k For more details see (Srivastava and Sutton, 2017). https://github.com/UKPLab/ sentence-transformers 3 2 4 760 http://qwone.com/˜jason/20Newsgroups/ https://trec.nist.gov/data/tweets/ Model Avg τ Avg α 6 and the STSb (Cer et al., 2017) dataset. Avg ρ 3.2 Results for the Wiki20K Dataset: Ours PLDA MLDA NVDM ETM LDA 0.1823 0.1397 0.1443 -0.2938 0.0740 -0.0481 0.1980 0.1799 0.2110 0.0797 0.1948 0.1333 We evaluate each model on three different metrics: two for topic coherence (normalized pointwise mutual information and a word-embedding based measure) and one metric to quantify the diversity of the topic solutions. 0.9950 0.9901 0.9843 0.9604 0.8632 0.9931 Normalized Pointwise Mutual Information (τ ) (Lau et al., 2014) measures how related the top-10 words of a topic are to each other, considering the words’ empirical frequency"
2021.acl-short.96,E14-1056,0,0.539983,"ate that future improvements in language models will translate into better topic models. 1 Introduction One of the crucial issues with topic models is the quality of the topics they discover. Coherent topics are easier to interpret and are considered more meaningful. E.g., a topic represented by the words “apple, pear, lemon, banana, kiwi” would be considered a meaningful topic on FRUIT and is more coherent than one defined by “apple, knife, lemon, banana, spoon.” Coherence can be measured in numerous ways, from human evaluation via intrusion tests (Chang et al., 2009) to approximated scores (Lau et al., 2014; R¨oder et al., 2015). However, most topic models still use Bag-ofWords (BoW) document representations as input. These representations, though, disregard the syntactic and semantic relationships among the words in a document, the two main linguistic avenues to coherent text. I.e., BoW models represent the input in an inherently incoherent manner. Meanwhile, pre-trained language models are becoming ubiquitous in Natural Language Processing (NLP), precisely for their ability to capDirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it ture and maintain sententi"
2021.acl-short.96,2021.naacl-main.243,0,0.037777,"the same embedding space. Srivastava and Sutton (2017) propose a neural variational framework that explicitly approximates the Dirichlet prior using a Gaussian distribution. Our approach builds on this work but includes a crucial component, i.e., the representations from a pre-trained transformer that can benefit from both general language knowledge and corpusdependent information. Similarly, Bianchi et al. (2021) replace the BOW document representation with pre-trained contextualized representations to tackle a problem of cross-lingual zero-shot topic modeling. This approach was extended by Mueller and Dredze (2021) that also considered fine-tuning the representations. A very recent approach (Hoyle et al., 2020) which follows a similar direction uses knowledge distillation (Hinton et al., 2015) to combine neural topic models and pre-trained transformers. 6 Ethical Statement In this research work, we used datasets from the recent literature, and we do not use or infer any sensible information. The risk of possible abuse of our models is low. Acknowledgments We thank our colleague Debora Nozza and Wray Buntine for providing insightful comments on a previous version of this paper. Federico Bianchi and Dirk"
2021.acl-short.96,Q15-1022,0,0.014294,"us to extract pre-trained word and sentence representations. Their use as input has advanced state-of-the-art performance across many tasks. Consequently, BERT representations are used in a diverse set of NLP applications (Rogers et al., 2020; Nozza et al., 2020). Various extensions of topic models incorporate several types of information (Xun et al., 2017; Zhao et al., 2017; Terragni et al., 2020a), use word relationships derived from external knowledge bases (Chen et al., 2013; Yang et al., 2015; Terragni et al., 2020b), or pre-trained word embeddings (Das et al., 2015; Dieng et al., 2020; Nguyen et al., 2015; Zhao et al., 2017). Even for neural topic models, there exists work on incorporating external knowledge, e.g., via word embeddings (Gupta et al., 2019, 2020; Dieng et al., 2020). In this paper, we show that adding contextual information to neural topic models provides a significant increase in topic coherence. This effect is even more remarkable given that we cannot embed long documents due to the sentence length limit in recent pre-trained language models architectures. Concretely, we extend Neural ProdLDA (Product-of-Experts LDA) (Srivastava and Sutton, 2017), a state-of-the-art topic mode"
2021.acl-short.96,N18-1101,0,0.0111705,"words of two topics. It allows disjointedness between the lists of topics (i.e., two topics can have different words in them) and uses weighted ranking. I.e., two lists that share some of the same words, albeit at different rankings, are penalized less than two lists that share the same words at the highest ranks. ρ is 0 for identical topics and 1 for completely different topics. and 20NewsGroups. For the others, we use the 5 pre-processed text; See Table 1 for dataset statistics. The sentence encoding model used is the pretrained RoBERTa model fine-tuned on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), 5 This can be sub-optimal, but many datasets in the literature are already pre-processed. 761 3.3 Models Our main objective is to show that contextual information increases coherence. To show this, we compare our approach to ProdLDA (Srivastava 7 and Sutton, 2017, the model we extend) , and the 6 7 stsb-roberta-large We use the implementation of Carrow (2018). following models: (ii) Neural Variational Document Model (NVDM) (Miao et al., 2016); (iii) the very recent ETM (Dieng et al., 2020), MetaLDA (MLDA) (Zhao et al., 2017) and (iv) LDA (Blei et al., 2003). 3.4 Ours MLDA Ours MLDA 50 75 ♣ 0"
2021.acl-short.96,D19-1410,0,0.0200128,"ttps://github.com/MilaNLProc/ contextualized-topic-models. 2 sentation. Figure 1 briefly sketches the architecture of our model. The hidden layer size could be tuned, but an extensive evaluation of different architectures is out of the scope of this paper. Neural Topic Models with Language Model Pre-training We introduce a Combined Topic Model (CombinedTM) to investigate the incorporation of contextualized representations in topic models. Our model is built around two main components: (i) the neural topic model ProdLDA (Srivastava and Sutton, 2017) and (ii) the SBERT embedded representations (Reimers and Gurevych, 2019). Let us notice that our method is indeed agnostic about the choice of the topic model and the pre-trained representations, as long as the topic model extends an autoencoder and the pre-trained representations embed the documents. ProdLDA is a neural topic modeling approach based on the Variational AutoEncoder (VAE). The neural variational framework trains a neural inference network to directly map the BoW document representation into a continuous latent representation. Then, a decoder network reconstructs the BoW by generating its words from the latent doc1 ument representation . The framewor"
2021.acl-short.96,2020.tacl-1.54,0,0.0188559,"e models are becoming ubiquitous in Natural Language Processing (NLP), precisely for their ability to capDirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it ture and maintain sentential coherence. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), the most prominent architecture in this category, allows us to extract pre-trained word and sentence representations. Their use as input has advanced state-of-the-art performance across many tasks. Consequently, BERT representations are used in a diverse set of NLP applications (Rogers et al., 2020; Nozza et al., 2020). Various extensions of topic models incorporate several types of information (Xun et al., 2017; Zhao et al., 2017; Terragni et al., 2020a), use word relationships derived from external knowledge bases (Chen et al., 2013; Yang et al., 2015; Terragni et al., 2020b), or pre-trained word embeddings (Das et al., 2015; Dieng et al., 2020; Nguyen et al., 2015; Zhao et al., 2017). Even for neural topic models, there exists work on incorporating external knowledge, e.g., via word embeddings (Gupta et al., 2019, 2020; Dieng et al., 2020). In this paper, we show that adding contextu"
2021.acl-short.96,D15-1037,0,0.027958,"entations from Transformers (BERT) (Devlin et al., 2019), the most prominent architecture in this category, allows us to extract pre-trained word and sentence representations. Their use as input has advanced state-of-the-art performance across many tasks. Consequently, BERT representations are used in a diverse set of NLP applications (Rogers et al., 2020; Nozza et al., 2020). Various extensions of topic models incorporate several types of information (Xun et al., 2017; Zhao et al., 2017; Terragni et al., 2020a), use word relationships derived from external knowledge bases (Chen et al., 2013; Yang et al., 2015; Terragni et al., 2020b), or pre-trained word embeddings (Das et al., 2015; Dieng et al., 2020; Nguyen et al., 2015; Zhao et al., 2017). Even for neural topic models, there exists work on incorporating external knowledge, e.g., via word embeddings (Gupta et al., 2019, 2020; Dieng et al., 2020). In this paper, we show that adding contextual information to neural topic models provides a significant increase in topic coherence. This effect is even more remarkable given that we cannot embed long documents due to the sentence length limit in recent pre-trained language models architectures. Concre"
2021.acl-short.96,2021.eacl-demos.31,1,0.892675,"NVDM ETM LDA 0.1008 0.0612 0.0122 -0.5105 -0.3613 -0.3227 0.1493 0.1327 0.1272 0.0797 0.1166 0.1025 0.9901 0.9847 0.9956 0.9751 0.4335 0.8169 Results for the 20NewsGroups Dataset: Ours PLDA MLDA NVDM ETM LDA 0.1025 0.0632 0.1300 -0.1720 0.0766 0.0173 0.1715 0.1554 0.2210 0.0839 0.2539 0.1627 Metrics 0.9917 0.9931 0.9808 0.9805 0.8642 0.9897 Table 2: Averaged results over 5 numbers of topics. Best results are marked in bold. Inversed Rank-Biased Overlap (ρ) evaluates how diverse the topics generated by a single model are. We define ρ as the reciprocal of the standard RBO (Webber et al., 2010; Terragni et al., 2021b). RBO compares the 10-top words of two topics. It allows disjointedness between the lists of topics (i.e., two topics can have different words in them) and uses weighted ranking. I.e., two lists that share some of the same words, albeit at different rankings, are penalized less than two lists that share the same words at the highest ranks. ρ is 0 for identical topics and 1 for completely different topics. and 20NewsGroups. For the others, we use the 5 pre-processed text; See Table 1 for dataset statistics. The sentence encoding model used is the pretrained RoBERTa model fine-tuned on SNLI (B"
2021.acl-short.96,2020.insights-1.5,1,0.78636,"Milan, Italy dirk.hovy@unibocconi.it ture and maintain sentential coherence. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), the most prominent architecture in this category, allows us to extract pre-trained word and sentence representations. Their use as input has advanced state-of-the-art performance across many tasks. Consequently, BERT representations are used in a diverse set of NLP applications (Rogers et al., 2020; Nozza et al., 2020). Various extensions of topic models incorporate several types of information (Xun et al., 2017; Zhao et al., 2017; Terragni et al., 2020a), use word relationships derived from external knowledge bases (Chen et al., 2013; Yang et al., 2015; Terragni et al., 2020b), or pre-trained word embeddings (Das et al., 2015; Dieng et al., 2020; Nguyen et al., 2015; Zhao et al., 2017). Even for neural topic models, there exists work on incorporating external knowledge, e.g., via word embeddings (Gupta et al., 2019, 2020; Dieng et al., 2020). In this paper, we show that adding contextual information to neural topic models provides a significant increase in topic coherence. This effect is even more remarkable given that we cannot embed long"
2021.bppf-1.3,N13-1062,0,0.0223759,"it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the as"
2021.bppf-1.3,2021.naacl-main.204,1,0.779829,"s disagree. There is no consensus yet on this form of evaluation, but a few proposals have been used already. In fact, a way of performing soft evaluation exists which is a natural extension of current practice in NLP. This is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to a full distribution of labels, instead of a ‘one-hot’ approach. This can be done using, for example, cross-entropy, although other options also exist. This approach was adopted in, inter alia, (Peterson et al., 2019; Uma et al., 2020; Fornaciari et al., 2021). Peterson et al. (2019) tested this approach on image classification tasks, generating the soft label by transforming the item annotation distribution using standard normalization. Uma et al. (2020) employed this form of soft metric evaluation for NLP , also comparing different ways to obtain a soft label from the raw data. They use soft metrics to compare the classifiers’ distribution to the human-derived label distributions, complementing traditional hard evaluation measures. Basile (2020) suggested a more extreme evaluation framework, where a model is required to produce different outputs"
2021.bppf-1.3,N18-1171,0,0.0210479,"unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already do exist. Removing the disagreement might lead to better evaluation scores, but it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality."
2021.bppf-1.3,P11-2008,0,0.163137,"Missing"
2021.bppf-1.3,W04-1013,0,0.0362296,"nd, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what"
2021.bppf-1.3,P02-1040,0,0.11206,"he annotation outcome and, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreem"
2021.bppf-1.3,S13-2025,0,0.0191364,"based on the assumption that a gold standard exists has long been accepted for end-to-end tasks, particularly those involving an aspect of natural language generation, such as conversational agents, machine translation, surface realisation, image captioning, or summarization. Metrics such as BLEU for machine translation/generation, ROUGE for summarization, or NDCG for ranking Web searches all support more than one gold standard reference. Shared tasks in this areas (particularly on paraphrasing), have also considered the role of disagreement in their evaluation metrics (Butnariu et al., 2009; Hendrickx et al., 2013). Variability in the annotation is a feature of 2 Disagreement in NLP In this section, we outline three possible sources of disagreement. Afterward, we describe how disagreement has been studied in objective and arguably more subjective tasks in NLP. 2.1 Sources of Disagreement Annotation implies an interaction between the human judge, the instance which has to be evaluated, and the moment/context in which the process takes place. For each instance, the annotation outcome 16 depends on these three elements, assuming the task is properly defined, designed, and carried out, e.g., in terms of qua"
2021.bppf-1.3,W13-2323,0,0.0253399,"arking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if p"
2021.bppf-1.3,N13-1132,1,0.771702,"Workshop on Benchmarking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should reco"
2021.bppf-1.3,Q18-1040,1,0.826112,", pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extre"
2021.bppf-1.3,Q19-1043,0,0.0234722,"have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what can be concluded from a natural language statement (Pavlick and Kwiatkowski, 2019). Stimulus Characteristics. Instance characteristics have paramount importance for the annotation as well. Language meaning is often equivocal and carries ambiguities of several kinds: lexical, syntactical, semantic, and others. Humour, for example, often relies on lexical or syntactic ambiguity (Raskin, 1985; Poesio, 2020). Other genres using deliberate ambiguity as a rhetorical device include poetry (Su, 1994) or political discourse (Winkler, 2015). For some instances, more than one label is correct, and the relative annotation task would be better framed as multi-label multi-class, rather t"
2021.bppf-1.3,D15-1035,0,0.0223806,"though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only eval"
2021.bppf-1.3,P14-2083,1,0.879095,"issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) ad"
2021.bppf-1.3,W05-0311,1,0.768942,"the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the assumption that a gold standard exists has long been acc"
2021.bppf-1.3,P19-1572,0,0.0275709,"arning from disagreements in NLP and CV using datasets containing information about disagreements for interpreting language and classifying images. Five well-known datasets for very different NLP and CV tasks were identified, all characterized by a multiplicity of labels for each instance, by having a size sufficient to train state-of-the-art models, and by evincing different characteristics in terms of the crowd annotators and data collection procedure. These include: a dataset of Twitter posts annotated with POS tags collected by Gimpel et al. (2011), a datasets for humour identification by Simpson et al. (2019), and two CV datasets on object identification namely the LabelMe (Russell et al., 2008) and CIFAR -10 datasets (Peterson et al., 2019). Both hard evaluation metrics (F1) and soft evaluation metrics (cross-entropy, as discussed in Section 3) were used for evaluation (Uma et al., 2021). The results showed that in nearly all cases, models that account for noise and disagreement have the best (lowest) cross-entropy scores. These results are consistent with the findings of Uma et al. (2020) and Peterson et al. (2019). Evaluation in Light of Disagreement While the research mentioned in the previous"
2021.bppf-1.3,2021.semeval-1.41,1,0.865681,"discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only evaluating on “easy” (as in, highly agreed upon) instances. Based on the evidence about the prevalence of disagreement in NLP judgments, we argue against this approach. First, it leads to information loss in the attempt to reducing noise in the data. Second, it is unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already d"
2021.bppf-1.3,W19-8643,0,0.0631318,"Missing"
2021.eacl-main.143,D09-1092,0,0.056722,"i um poeta, dramaturgo, artista pl´astico [...] book, french, novel, written book, french, novel, written painting, art, painter, works Table 4: Examples of zero-shot cross-lingual topic classification in various languages with ZeroShotTM. 4 Related Work While not in a zero-shot fashion, several researchers have studied multilingual and crosslingual topic modeling (Ma and Nasukawa, 2017; Guti´errez et al., 2016; Hao and Paul, 2018; Heyman et al., 2016; Liu et al., 2015; Krstovski et al., 2016). The first model proposed to process multilingual corpora with LDA is the Polylingual Topic Model by Mimno et al. (2009). It uses LDA to extract language-consistent topics from parallel multilingual corpora, assuming that translations share the same topic distributions. Models that transfer knowledge on the document level have many variants, including (Hao and Paul, 2018; Heyman et al., 2016; Liu et al., 2015; Krstovski et al., 2016). However, existing models require to be trained on multilingual corpora and are always language-dependent: they cannot predict the main topics of a document in an unseen language. Other models use multilingual dictionaries (Boyd-Graber and Blei, 2009; Jagarlamudi and Daum´e, 2010),"
2021.eacl-main.143,D19-1410,0,0.131855,"al framework trains an inference network, i.e., a neural network that directly maps the BoW representation of a document onto a continuous latent representation. A decoder network then reconstructs the BoW by generating its words from the latent document representation. This latent representation is sampled from a Gaussian distribution parameterized by µ and σ 2 that are part of the variational inference framework (Kingma and Welling, 2014) — see (Srivastava and Sutton, 2017) for more details. We replace the input BoW in Neural-ProdLDA with pre-trained multilingual representations from SBERT (Reimers and Gurevych, 2019), a recent and effective model for contextualized representations. In Figure 1, we sketch the architecture of our contextualized neural topic model. The final reconstructed BoW layer is still a component of our model: the BoW representation is necessary for the model’s training to obtain the topic indicators (i.e., the most likely words representing a topic), but it becomes useless during testing. Figure 1: High-level schema of the architecture for the proposed contextualized neural topic model. Our proposed model, Zero-Shot Topic Model (ZeroShotTM), is trained with input document representati"
2021.eacl-main.143,2020.tacl-1.54,0,0.0301933,"Missing"
2021.eacl-main.143,N16-1053,0,0.0161166,"ght, graphic designer [...] Fu l’ispiratore e uno dei fondatori della rivista ”Dau al Set”[...] Joan Brossa i Cuervo [...] foi um poeta, dramaturgo, artista pl´astico [...] book, french, novel, written book, french, novel, written painting, art, painter, works Table 4: Examples of zero-shot cross-lingual topic classification in various languages with ZeroShotTM. 4 Related Work While not in a zero-shot fashion, several researchers have studied multilingual and crosslingual topic modeling (Ma and Nasukawa, 2017; Guti´errez et al., 2016; Hao and Paul, 2018; Heyman et al., 2016; Liu et al., 2015; Krstovski et al., 2016). The first model proposed to process multilingual corpora with LDA is the Polylingual Topic Model by Mimno et al. (2009). It uses LDA to extract language-consistent topics from parallel multilingual corpora, assuming that translations share the same topic distributions. Models that transfer knowledge on the document level have many variants, including (Hao and Paul, 2018; Heyman et al., 2016; Liu et al., 2015; Krstovski et al., 2016). However, existing models require to be trained on multilingual corpora and are always language-dependent: they cannot predict the main topics of a document in a"
2021.eacl-main.143,2020.insights-1.5,1,0.385838,"gual setup proves ideal for transfer learning: provided that the gist of topics is the same across languages, we can learn this gist on texts in one language and then apply it to others. This setup is zero-shot learning: we train a model on one language and test it on several other languages to which the model had no access during training. To this end, we need to leverage external information to support the topic modeling task. Indeed, topic models have often gained significant advantages from introducing external knowledge, e.g., document relationships (Yang et al., 2015; Wang et al., 2020; Terragni et al., 2020a,b) and word embeddings (Nozza et al., 2016; Li et al., 2016; Zhao et al., 2017; Dieng et al., 2020). Recently, pre-trained contextualized embeddings, e.g., BERT (Devlin et al., 2019) embeddings, have enabled exciting new results in several NLP tasks (Rogers et al., 2020; Nozza et al., 2020). More importantly, there do exist contextualized embeddings that are also multilingual. This paper introduces a novel neural topic modeling architecture in which we replace the input BoW document representations with multilingual contextualized embeddings. Neural topic models take in input the document Bo"
2021.eacl-main.232,W12-0405,0,0.0220821,"implications on personal, economic, legal, and political levels. There has been a growing interest in automatic deception detection from academia and industry in recent years (see section 9). One of the main research lines tries to increase the collection of deception cues in terms of number and variety. For example, several successful studies show how to exploit multi-modal signals, jointly analyzing verbal, video, and audio data (P´erez-Rosas et al., 2015). For the same reason, several early studies tried to identify deception cues through manual feature annotation, like irony or ambiguity (Fitzpatrick and Bachenko, 2012). While these approaches offer a broad and interpretable description of the phenomenon, their main limitation lies in data collection and preprocessing difficulty. Surprisingly, so far, little attention has been paid to expanding the targets’ linguistic context, which is the easiest source of additional cues and data. Even in dialogues, which by definition are exchanges between different speakers/writers, the main focus is typically on the target text. None consider the preceding statements, be they issued by the same speaker of an interlocutor. We hypothesize that linguistic context can be us"
2021.eacl-main.232,fornaciari-poesio-2012-decour,1,0.749023,"2021. ©2021 Association for Computational Linguistics these methods on data collected from real, highstakes conditions for the subjects and not from a laboratory or game environment. Contributions The contributions of this paper are as follows: • We evaluate ways to incorporate contextual information for detecting deception on reallife data. • We significantly outperform the previous stateof-the-art results. • We show that language models are useful for the task, but they need the support of methods dedicated to detect deception’s stylometric features. 2 Dataset We use the D E C OUR dataset (Fornaciari and Poesio, 2012), which includes courtroom data transcripts of 35 hearings for criminal proceedings held in Italian courts. This provides a unique source of real deception data. The corpus is in Italian. It consists of dialogues between an interviewee and some interviewers (such as the judge, the prosecutor, the lawyer). Each dialogue contains a sequence of utterances of the different speakers. These utterances are called turns. By definition, adjacent turns come from different speakers. Each turn contains one or more utterances. Each utterance by the interviewee is labeled as True, False or Uncertain. The ut"
2021.eacl-main.232,W14-1601,1,0.712961,"Missing"
2021.eacl-main.232,N18-1176,0,0.0393176,"Missing"
2021.eacl-main.232,P11-1032,0,0.0484129,"Missing"
2021.eacl-main.232,2020.acl-main.353,0,0.0187805,"emember I pushed him away IG*100 21.858 10.831 09.257 08.674 07.789 07.627 06.843 06.677 06.674 06.674 Table 3: Information Gain (rescaled by 100 to avoid tiny values) of tri-grams indicative of truth (left) and deception (right) Figure 2: Output of the SOC algorithm. The red terms predict deception, the blue ones predict truthfulness. we speculate that BERT’s contextual knowledge works as a regularizer, which provides the Transformer with previously weighted inputs, according to the sentences’ meaning. Our results concerning BERT’s usefulness with context are different from those obtained by Peskov et al. (2020), who work on Diplomacy board-game deception data. Their study associated BERT to LSTM-based contextual models, and they did not find a BERT contribution in their model’s performance. They tried to fine-tune it, and they hypothesized that the lack of performance improvement was motivated by the “relatively small size” of the training data. This hypothesis could be correct, but our outcome allows us to formulate another hypothesis. Their data set concerns an online game, where the range of topics in the dialogues is presumably restricted and specific. This limitation would not allow BERT’s broa"
2021.eacl-main.232,2020.acl-main.468,1,0.713708,"Missing"
2021.findings-acl.301,K19-1024,0,0.0167513,"wever, we are specifically interested in the identification of election pledges. This is similar to the task studied by Subramanian et al. (2019a). They focus on eleven Australian federal election cycles and distinguish rhetorical (broad) from detailed (narrow) pledges. The annotation of the Swedish texts considers this distinction, while the annotated Indian texts of our corpus do not (Section 2). Subramanian et al. (2019a) use a bidirectional Gated Recurrent Unit (biGRU) to carry out the prediction over ordinal classes. From a methodological point of view, our approach is related to that of Abercrombie et al. (2019), which also uses BERT. They work on motions tabled in the UK Parliament and find that BERT effectively detects specific categories of proposals in the politicians’ speeches. Concerning the MTL methods, our study is analogous to that of Subramanian et al. (2019b). They consider texts from the 2016 Australian election and propose a new annotation scheme for different speech acts. They also perform the classification task using biGRU networks with ELMo embeddings (Peters et al., 2018), relying on a MTL framework in which the auxiliary task is the party prediction: this is also one of our experim"
2021.findings-acl.301,N18-1202,0,0.0321616,"he prediction over ordinal classes. From a methodological point of view, our approach is related to that of Abercrombie et al. (2019), which also uses BERT. They work on motions tabled in the UK Parliament and find that BERT effectively detects specific categories of proposals in the politicians’ speeches. Concerning the MTL methods, our study is analogous to that of Subramanian et al. (2019b). They consider texts from the 2016 Australian election and propose a new annotation scheme for different speech acts. They also perform the classification task using biGRU networks with ELMo embeddings (Peters et al., 2018), relying on a MTL framework in which the auxiliary task is the party prediction: this is also one of our experimental conditions. 8 Conclusion We propose deep neural models that combine pretrained language models and trainable attention mechanisms to identify election pledges in party manifestos. We find that these models outperform a non-neural baseline. Even in zero-shot cross-lingual conditions (with some contribution by the MTL methods), the performance of the multilingual models indicates that we could identify pledges in low-resource languages. Finally, we gained some insight into elect"
2021.findings-acl.340,2020.emnlp-main.703,0,0.0235533,"Missing"
2021.findings-acl.340,2020.tacl-1.3,0,0.0176072,"apers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods. If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward. 1 Early Adoption When BERT (Devlin et al., 2019) was introduced in 2019, it revolutionized Natural Language Processing (NLP), showing impressive capabilities on many tasks in various languages (Nozza et al., 2020). However, papers soon highlighted its limits (Ettinger, 2020; Bender and Koller, 2020) and identified issues with bias (Nozza et al., 2021), i.e., that contextual models show different performances for different genders and ethnic groups (Zhang et al., 2020). While Rogers et al. (2020a)’s “BERTology” paper outlines what we know about BERT, much remains unexplored. We do not really know what BERT “understands”, but, as Bender and Koller (2020) pointed out, we frequently overestimate its understanding capabilities. E.g., Ettinger (2020) shows that BERT has very low sensitivity to the concept of negation shows. This is not to say that we should stop using"
2021.findings-acl.340,2020.acl-tutorials.4,0,0.0992229,"Missing"
2021.findings-acl.340,N19-1423,0,0.0222429,"of particular concern: 1) the early adoption of methods without sufficient understanding or analysis; 2) the preference for computational methods regardless of risks associated with their limitations; 3) the resulting bias in the papers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods. If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward. 1 Early Adoption When BERT (Devlin et al., 2019) was introduced in 2019, it revolutionized Natural Language Processing (NLP), showing impressive capabilities on many tasks in various languages (Nozza et al., 2020). However, papers soon highlighted its limits (Ettinger, 2020; Bender and Koller, 2020) and identified issues with bias (Nozza et al., 2021), i.e., that contextual models show different performances for different genders and ethnic groups (Zhang et al., 2020). While Rogers et al. (2020a)’s “BERTology” paper outlines what we know about BERT, much remains unexplored. We do not really know what BERT “understands”, but, as Bender and K"
2021.findings-acl.340,P13-1166,0,0.0831846,"Missing"
2021.findings-acl.340,2020.emnlp-main.567,0,0.0301508,"ltiple algorithms (Terragni et al., 2021): this process can help in reducing methodological errors in the evaluation. We can cite HuggingFace2 as a notable example of a repository with good systematicity that has allowed many researchers to replicate most of the recent and popular research in the NLP field. Similarly, Sentence BERT (Reimers and Gurevych, 2019) - a method for sentence embeddings based on BERT - was released with a well organized and easy-to-use source code3 that made it possible to use the models as a baseline or as the foundation of many other research works (Li et al., 2020; Ke et al., 2020; Zemlyanskiy et al., 2021; Bianchi et al., 2021c,b, inter alia), suggesting that this kind of release can be of great benefit to the community. 3 Publication Bias Most researchers want to publish in A+ venues and Q1 journals as academic positions use publication output as a criterion for promotions. While quality should play into this assessment, it would be wrong to assume university committees are familiar enough with all venues and subfields in a discipline to make an accurate assessment of the relative value of each contribution. So the number of papers and citation count often trump othe"
2021.findings-acl.340,2020.acl-main.261,0,0.0762755,"Missing"
2021.findings-acl.340,W17-1603,0,0.053886,"Missing"
2021.findings-acl.340,R19-1089,0,0.0273952,"Missing"
2021.findings-acl.340,2021.naacl-main.191,1,0.452253,"Missing"
2021.findings-acl.340,D19-1410,0,0.0275995,"d code, writing documented code that is at least easy to use (with the use of high-level interfaces and convenient wrappers) can be required practice. Moreover, this can be of help in the context of systematic evaluation initiatives like code base that integrate multiple algorithms (Terragni et al., 2021): this process can help in reducing methodological errors in the evaluation. We can cite HuggingFace2 as a notable example of a repository with good systematicity that has allowed many researchers to replicate most of the recent and popular research in the NLP field. Similarly, Sentence BERT (Reimers and Gurevych, 2019) - a method for sentence embeddings based on BERT - was released with a well organized and easy-to-use source code3 that made it possible to use the models as a baseline or as the foundation of many other research works (Li et al., 2020; Ke et al., 2020; Zemlyanskiy et al., 2021; Bianchi et al., 2021c,b, inter alia), suggesting that this kind of release can be of great benefit to the community. 3 Publication Bias Most researchers want to publish in A+ venues and Q1 journals as academic positions use publication output as a criterion for promotions. While quality should play into this assessmen"
2021.findings-acl.340,P19-1355,0,0.119683,"ave access to the same resources would be unreasonable (however desirable). Industry players need to maintain a competitive edge, and we can hardly hope to address the inequality between national academic systems and their funding. But this reality creates a situation where reproducibility becomes impossible. If team A presents results from a model that only team A can train, then the rest of us need to take those results at face value. Whether we believe them or not becomes irrelevant. In addition to this problem, consider the environmental concerns generated by the training of large models (Strubell et al., 2019), and bigger does not necessarily equate better for reproducibility. Lack of reproducibility, though, is a danger to scientific reliability. The fallout from the reproducibility crisis in social psychology (see Section 4) has tainted the reputation of the entire field. But it has also led to remarkable innovations. Studies now need to be pre-registered to prevent “fishing expeditions” to find the most “interesting” result. International research teams have organized trials to replicate famous experiments—often disproving canonical theories, but also re-opening avenues of research that had been"
2021.findings-acl.340,2021.eacl-demos.31,0,0.0360339,"iding a data statement, we believe that a code statement similar to that offered by Mitchell et al. (2019) should be provided along with a paper. Code is part of the scientific contribution similar to the experiments we run to prove a hypothesis. While there can be different opinions 3896 on how to write good or bad code, writing documented code that is at least easy to use (with the use of high-level interfaces and convenient wrappers) can be required practice. Moreover, this can be of help in the context of systematic evaluation initiatives like code base that integrate multiple algorithms (Terragni et al., 2021): this process can help in reducing methodological errors in the evaluation. We can cite HuggingFace2 as a notable example of a repository with good systematicity that has allowed many researchers to replicate most of the recent and popular research in the NLP field. Similarly, Sentence BERT (Reimers and Gurevych, 2019) - a method for sentence embeddings based on BERT - was released with a well organized and easy-to-use source code3 that made it possible to use the models as a baseline or as the foundation of many other research works (Li et al., 2020; Ke et al., 2020; Zemlyanskiy et al., 2021"
2021.findings-acl.340,2021.eacl-main.217,0,0.0125941,"(Terragni et al., 2021): this process can help in reducing methodological errors in the evaluation. We can cite HuggingFace2 as a notable example of a repository with good systematicity that has allowed many researchers to replicate most of the recent and popular research in the NLP field. Similarly, Sentence BERT (Reimers and Gurevych, 2019) - a method for sentence embeddings based on BERT - was released with a well organized and easy-to-use source code3 that made it possible to use the models as a baseline or as the foundation of many other research works (Li et al., 2020; Ke et al., 2020; Zemlyanskiy et al., 2021; Bianchi et al., 2021c,b, inter alia), suggesting that this kind of release can be of great benefit to the community. 3 Publication Bias Most researchers want to publish in A+ venues and Q1 journals as academic positions use publication output as a criterion for promotions. While quality should play into this assessment, it would be wrong to assume university committees are familiar enough with all venues and subfields in a discipline to make an accurate assessment of the relative value of each contribution. So the number of papers and citation count often trump other considerations, especial"
2021.findings-acl.340,2020.tacl-1.54,0,0.123683,"bsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward. 1 Early Adoption When BERT (Devlin et al., 2019) was introduced in 2019, it revolutionized Natural Language Processing (NLP), showing impressive capabilities on many tasks in various languages (Nozza et al., 2020). However, papers soon highlighted its limits (Ettinger, 2020; Bender and Koller, 2020) and identified issues with bias (Nozza et al., 2021), i.e., that contextual models show different performances for different genders and ethnic groups (Zhang et al., 2020). While Rogers et al. (2020a)’s “BERTology” paper outlines what we know about BERT, much remains unexplored. We do not really know what BERT “understands”, but, as Bender and Koller (2020) pointed out, we frequently overestimate its understanding capabilities. E.g., Ettinger (2020) shows that BERT has very low sensitivity to the concept of negation shows. This is not to say that we should stop using BERT; indeed, it is hard to imagine the field today without it. But it does illustrate the gap between adoption and understanding (GAU) of a new technology. As the field grows, this situation is likely to play out more frequ"
2021.naacl-main.191,2020.acl-main.463,0,0.0652462,"to spot any biases we might have missed. Templates were generated by native speakers of the respective languages from European Countries, all in the age group 25-30. The data we share is not sensitive to personal information, as it does not contain information about individuals. Our data does not contain hurtful messages that can be used in hurtful ways. Our experimental results suggest a need to discuss the ethical aspect of these models. BERT and GPT-2 have shown astonishing capabilities and pushed the envelope of natural language understanding - not without some doubts (Bisk et al., 2020; Bender and Koller, 2020). However, our results, together with those of (Sheng et al., 2019; Acknowledgements Kurita et al., 2019; Zhou et al., 2019), should make us reflect on the dual use of these models, i.e., how This project has partially received funding from they are used outside our research community. the European Research Council (ERC) under the Can BERT or GPT-2 harm someone if used in European Union’s Horizon 2020 research and inproduction, by proliferating and amplifying harm- novation program (grant agreement No. 949944, ful stereotypes? These models are now often in- INTEGRATOR). The authors are members"
2021.naacl-main.191,2020.emnlp-main.703,0,0.127171,"Missing"
2021.naacl-main.191,W19-3804,0,0.0350853,"ble 5: HONEST scores for the language models. et al., 2018; Garimella et al., 2019). This interest is also reflected in the organization of dedicated workshops (ws-, 2019, 2017). More generally, language models generating taboo words and insults is the result of NLP systems not incorporating social norms (Hovy and Yang, 2021). The pioneering work of (Bolukbasi et al., 2016) demonstrated that word embeddings (even when trained on formal corpora) exhibit gender stereotypes to a disturbing extent. On top of that, several studies have been proposed to measure and mitigate bias in word embeddings (Chaloner and Maldonado, 2019; Zhou et al., 2019; Nissim et al., 2020) and more recently on pre-trained contextualized embeddings models (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Field and Tsvetkov, 2019; Sheng et al., 2019; Nangia et al., 2020; Vig et al., 2020). However, most studies focus on English. Despite a plethora of available language-specific models (Nozza et al., 2020), there currently exist few studies on biases in other languages. This is a severe limitation, as English findings do not automatically extend to other languages, especially if those exhibit morphological gender agreement. Only Mc"
2021.naacl-main.191,N19-1423,0,0.0176371,"m in word embeddings. Zhao et al. 1 Dirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it Note: this paper contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, these models can also generate or complete sentences in a cloze-test style. This capability opens new avenues for text generation, but also includes the risk of producing hurtful and stereotyped sentences. We are the first to investigate the generation of explicitly hu"
2021.naacl-main.191,2020.tacl-1.3,0,0.0518094,"ark data set of manually-created sentence templates to measure the generation of hurtful sentence completions in six languages; 2) we use this dataset to assess gendered stereotype bias in the generated results; 3) we propose a measure, HONEST, to understand which language model generates more hurtful sentences; 4) we release code and data for reproducibility at https://github.com/MilaNLProc/ honest. 2 Method Template-closed forms are an effective way of evaluating language models. Petroni et al. (2019) use cloze-based forms to evaluate the amount of relational knowledge included in BERT, and Ettinger (2020) use them as a set of psycholinguistic diagnostic tools. Cloze-based forms have a long history in psycholinguistics to understand human sentence processing (Ettinger, 2020). Here, we use a similar methodology to test hurtful language in different language models. For example, our templates look as follows: Lang IT EN FR PT ES RO Term Template donna woman femme mulher mujer femeia La donna sogna di essere una ___ The woman dreams of being a ___ La femme rêve d’être une ___ A mulher sonha ser ___ La mujer sueña con ser una ___ Femeia viseaz˘a s˘a fie ___ Table 2: Example of the template “dreams"
2021.naacl-main.191,P19-1243,0,0.0137278,"language models generating taboo words and insults is the result of NLP systems not incorporating social norms (Hovy and Yang, 2021). The pioneering work of (Bolukbasi et al., 2016) demonstrated that word embeddings (even when trained on formal corpora) exhibit gender stereotypes to a disturbing extent. On top of that, several studies have been proposed to measure and mitigate bias in word embeddings (Chaloner and Maldonado, 2019; Zhou et al., 2019; Nissim et al., 2020) and more recently on pre-trained contextualized embeddings models (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Field and Tsvetkov, 2019; Sheng et al., 2019; Nangia et al., 2020; Vig et al., 2020). However, most studies focus on English. Despite a plethora of available language-specific models (Nozza et al., 2020), there currently exist few studies on biases in other languages. This is a severe limitation, as English findings do not automatically extend to other languages, especially if those exhibit morphological gender agreement. Only McCurdy and Serbetçi (2017); Zhou et al. (2019) examine the bias in word embeddings of 4 Related Work gender-inflected languages, demonstrating the need The analysis of bias in Natural Language"
2021.naacl-main.191,P19-1339,1,0.843501,"CamemBERT-base (CCnet) BETO BERTimbau BERTimbau-large RomanianBERT BERT-base BERT-large RoBERTa-base RoBERTa-large DistilBERT-base GPT-2 (IT) GPT-2 (FR) GPT-2 (PT) GPT-2 (EN) 5.24 5.48 7.14 9.05 4.76 18.57 16.90 7.62 13.33 17.86 4.29 4.05 3.57 4.76 1.19 3.33 2.38 2.62 1.90 12.86 19.76 9.52 17.14 8.19 7.19 11.57 10.67 3.29 9.62 8.62 4.90 8.62 9.48 5.95 6.00 5.52 3.90 2.67 3.43 5.38 2.33 3.81 11.76 19.67 10.71 12.81 7.14 5.14 8.68 9.12 2.43 7.07 6.42 4.19 5.43 6.83 6.88 5.04 4.08 4.61 3.55 4.30 5.74 3.05 3.96 12.56 17.81 10.29 13.00 Table 5: HONEST scores for the language models. et al., 2018; Garimella et al., 2019). This interest is also reflected in the organization of dedicated workshops (ws-, 2019, 2017). More generally, language models generating taboo words and insults is the result of NLP systems not incorporating social norms (Hovy and Yang, 2021). The pioneering work of (Bolukbasi et al., 2016) demonstrated that word embeddings (even when trained on formal corpora) exhibit gender stereotypes to a disturbing extent. On top of that, several studies have been proposed to measure and mitigate bias in word embeddings (Chaloner and Maldonado, 2019; Zhou et al., 2019; Nissim et al., 2020) and more rece"
2021.naacl-main.191,W19-3621,0,0.0291029,"nts, or recruiting tools. Increasingly, these applications include text generation. Unfortunately, these methods are likely to reproduce and reinforce a wide range of existing stereotypes in real-world systems. It is therefore important to quantify and understand these biases. Both to avoid the psychological burden of different vulnerable groups, and to advocate for equal treatment and opportunities. Recent research has focused on uncovering and measuring bias in input representations, models, and other aspects (Shah et al., 2020). For example, Bolukbasi et al. (2016); Caliskan et al. (2017); Gonen and Goldberg (2019) demonstrated the presence of implicit sexism in word embeddings. Zhao et al. 1 Dirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it Note: this paper contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encode"
2021.naacl-main.191,P16-2096,1,0.814088,"za et al., 2020), there currently exist few studies on biases in other languages. This is a severe limitation, as English findings do not automatically extend to other languages, especially if those exhibit morphological gender agreement. Only McCurdy and Serbetçi (2017); Zhou et al. (2019) examine the bias in word embeddings of 4 Related Work gender-inflected languages, demonstrating the need The analysis of bias in Natural Language Process- for an adequate framework different from the ones ing has gained a lot of attention in recent years proposed for English. To the best of our knowledge, (Hovy and Spruit, 2016; Shah et al., 2020), specifi- we are the first to investigate stereotype bias in varcally on gender bias (Zhao et al., 2018; Rudinger ious language model completions beyond English. 2401 5 Conclusion We present the first analysis of stereotyped sentence completions generated by contextual models in gender-inflected languages. We introduce the HONEST score to quantify the amount of hurtful completions in a language model. We release a novel benchmark data set of manually created templates, validated by native speakers in five gender-inflected languages, i.e., Italian, French, Portuguese, Roman"
2021.naacl-main.191,2021.naacl-main.49,1,0.761762,"76 1.19 3.33 2.38 2.62 1.90 12.86 19.76 9.52 17.14 8.19 7.19 11.57 10.67 3.29 9.62 8.62 4.90 8.62 9.48 5.95 6.00 5.52 3.90 2.67 3.43 5.38 2.33 3.81 11.76 19.67 10.71 12.81 7.14 5.14 8.68 9.12 2.43 7.07 6.42 4.19 5.43 6.83 6.88 5.04 4.08 4.61 3.55 4.30 5.74 3.05 3.96 12.56 17.81 10.29 13.00 Table 5: HONEST scores for the language models. et al., 2018; Garimella et al., 2019). This interest is also reflected in the organization of dedicated workshops (ws-, 2019, 2017). More generally, language models generating taboo words and insults is the result of NLP systems not incorporating social norms (Hovy and Yang, 2021). The pioneering work of (Bolukbasi et al., 2016) demonstrated that word embeddings (even when trained on formal corpora) exhibit gender stereotypes to a disturbing extent. On top of that, several studies have been proposed to measure and mitigate bias in word embeddings (Chaloner and Maldonado, 2019; Zhou et al., 2019; Nissim et al., 2020) and more recently on pre-trained contextualized embeddings models (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Field and Tsvetkov, 2019; Sheng et al., 2019; Nangia et al., 2020; Vig et al., 2020). However, most studies focus on English. Despit"
2021.naacl-main.191,W17-2902,0,0.0838346,"Missing"
2021.naacl-main.191,2020.acl-main.483,0,0.116565,"Missing"
2021.naacl-main.191,S18-2005,0,0.0385227,"ortunities. Recent research has focused on uncovering and measuring bias in input representations, models, and other aspects (Shah et al., 2020). For example, Bolukbasi et al. (2016); Caliskan et al. (2017); Gonen and Goldberg (2019) demonstrated the presence of implicit sexism in word embeddings. Zhao et al. 1 Dirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it Note: this paper contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, the"
2021.naacl-main.191,W19-3823,0,0.382267,"ni.it Note: this paper contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, these models can also generate or complete sentences in a cloze-test style. This capability opens new avenues for text generation, but also includes the risk of producing hurtful and stereotyped sentences. We are the first to investigate the generation of explicitly hurtful stereotypes in language models for English and five gender-inflected languages (Italian, French, Portuguese, Rom"
2021.naacl-main.191,2020.lrec-1.302,0,0.0341184,"Missing"
2021.naacl-main.191,2021.ccl-1.108,0,0.0381281,"Missing"
2021.naacl-main.191,N19-1063,0,0.134869,"r contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, these models can also generate or complete sentences in a cloze-test style. This capability opens new avenues for text generation, but also includes the risk of producing hurtful and stereotyped sentences. We are the first to investigate the generation of explicitly hurtful stereotypes in language models for English and five gender-inflected languages (Italian, French, Portuguese, Romanian, and Span2 I"
2021.naacl-main.191,2020.acl-main.702,0,0.0763009,"re of the consequences the naïve use of these models can have. Democratizing without educating can damage those people who fight the most to be recognized as equal members of our society, if our models continue to spread old hurtful stereotypes. Finally, we want to explicitly address the limitation of our approach with respect to the binary nature of our gender analysis. The lack of representation for non-binary people and the gender assumption of the identity terms is a major limitation in our work. It is due to data and language constraints, not a value judgment. We want to add our voice to Mohammad (2020) in the hope of future work to disaggregate information for different genders. Data Statement We follow Bender and Friedman (2018) on providing a Data Statement for our templates to provide a better picture of the possibilities and limitations of the data, and to allow future researchers to spot any biases we might have missed. Templates were generated by native speakers of the respective languages from European Countries, all in the age group 25-30. The data we share is not sensitive to personal information, as it does not contain information about individuals. Our data does not contain hurtf"
2021.naacl-main.191,2020.emnlp-main.154,0,0.451982,"anguage in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, these models can also generate or complete sentences in a cloze-test style. This capability opens new avenues for text generation, but also includes the risk of producing hurtful and stereotyped sentences. We are the first to investigate the generation of explicitly hurtful stereotypes in language models for English and five gender-inflected languages (Italian, French, Portuguese, Romanian, and Span2 In this paper, we use the general term language models to refe"
2021.naacl-main.191,2020.cl-2.7,0,0.0323876,"Missing"
2021.naacl-main.191,D19-1250,0,0.0181899,"pute how likely each language model is to produce hurtful completions. Contributions 1) We release a novel benchmark data set of manually-created sentence templates to measure the generation of hurtful sentence completions in six languages; 2) we use this dataset to assess gendered stereotype bias in the generated results; 3) we propose a measure, HONEST, to understand which language model generates more hurtful sentences; 4) we release code and data for reproducibility at https://github.com/MilaNLProc/ honest. 2 Method Template-closed forms are an effective way of evaluating language models. Petroni et al. (2019) use cloze-based forms to evaluate the amount of relational knowledge included in BERT, and Ettinger (2020) use them as a set of psycholinguistic diagnostic tools. Cloze-based forms have a long history in psycholinguistics to understand human sentence processing (Ettinger, 2020). Here, we use a similar methodology to test hurtful language in different language models. For example, our templates look as follows: Lang IT EN FR PT ES RO Term Template donna woman femme mulher mujer femeia La donna sogna di essere una ___ The woman dreams of being a ___ La femme rêve d’être une ___ A mulher sonha s"
2021.naacl-main.191,N18-2002,0,0.0454972,"Missing"
2021.naacl-main.191,2020.acl-main.468,1,0.881536,"se (or are subjected to) every day, e.g., internet search engines, virtual assistants, or recruiting tools. Increasingly, these applications include text generation. Unfortunately, these methods are likely to reproduce and reinforce a wide range of existing stereotypes in real-world systems. It is therefore important to quantify and understand these biases. Both to avoid the psychological burden of different vulnerable groups, and to advocate for equal treatment and opportunities. Recent research has focused on uncovering and measuring bias in input representations, models, and other aspects (Shah et al., 2020). For example, Bolukbasi et al. (2016); Caliskan et al. (2017); Gonen and Goldberg (2019) demonstrated the presence of implicit sexism in word embeddings. Zhao et al. 1 Dirk Hovy Bocconi University Via Sarfatti 25, 20136 Milan, Italy dirk.hovy@unibocconi.it Note: this paper contains explicit statements of hurtful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise i"
2021.naacl-main.191,D19-1339,0,0.543371,"tful and offensive language in various languages, which may be upsetting to readers. (2017) demonstrated that models exaggerate found biases, and Kiritchenko and Mohammad (2018) showed that a simple change of pronouns or first names could significantly alter the sentiment of an otherwise identical sentence. Recently, contextualized language models, lead by Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have become the standard in NLP leaderboards.2 Several studies (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Sheng et al., 2019; Nangia et al., 2020) have analyzed their implicit biases related to word use and associations based on word similarity. However, apart from associations, these models can also generate or complete sentences in a cloze-test style. This capability opens new avenues for text generation, but also includes the risk of producing hurtful and stereotyped sentences. We are the first to investigate the generation of explicitly hurtful stereotypes in language models for English and five gender-inflected languages (Italian, French, Portuguese, Romanian, and Span2 In this paper, we use the general term l"
2021.naacl-main.191,N19-1064,0,0.0295629,"Missing"
2021.naacl-main.191,D17-1323,0,0.0917853,"Missing"
2021.naacl-main.191,N18-2003,0,0.0167473,"gs do not automatically extend to other languages, especially if those exhibit morphological gender agreement. Only McCurdy and Serbetçi (2017); Zhou et al. (2019) examine the bias in word embeddings of 4 Related Work gender-inflected languages, demonstrating the need The analysis of bias in Natural Language Process- for an adequate framework different from the ones ing has gained a lot of attention in recent years proposed for English. To the best of our knowledge, (Hovy and Spruit, 2016; Shah et al., 2020), specifi- we are the first to investigate stereotype bias in varcally on gender bias (Zhao et al., 2018; Rudinger ious language model completions beyond English. 2401 5 Conclusion We present the first analysis of stereotyped sentence completions generated by contextual models in gender-inflected languages. We introduce the HONEST score to quantify the amount of hurtful completions in a language model. We release a novel benchmark data set of manually created templates, validated by native speakers in five gender-inflected languages, i.e., Italian, French, Portuguese, Romanian, and Spanish. Our results show that BERT and GPT-2, nowadays ubiquitous in research and industrial NLP applications, dem"
2021.naacl-main.191,D19-1531,0,0.13246,"anguage models. et al., 2018; Garimella et al., 2019). This interest is also reflected in the organization of dedicated workshops (ws-, 2019, 2017). More generally, language models generating taboo words and insults is the result of NLP systems not incorporating social norms (Hovy and Yang, 2021). The pioneering work of (Bolukbasi et al., 2016) demonstrated that word embeddings (even when trained on formal corpora) exhibit gender stereotypes to a disturbing extent. On top of that, several studies have been proposed to measure and mitigate bias in word embeddings (Chaloner and Maldonado, 2019; Zhou et al., 2019; Nissim et al., 2020) and more recently on pre-trained contextualized embeddings models (Kurita et al., 2019; May et al., 2019; Zhao et al., 2019; Field and Tsvetkov, 2019; Sheng et al., 2019; Nangia et al., 2020; Vig et al., 2020). However, most studies focus on English. Despite a plethora of available language-specific models (Nozza et al., 2020), there currently exist few studies on biases in other languages. This is a severe limitation, as English findings do not automatically extend to other languages, especially if those exhibit morphological gender agreement. Only McCurdy and Serbetçi"
2021.naacl-main.204,P14-2064,0,0.0628865,"Missing"
2021.naacl-main.204,D12-1091,0,0.0822475,"Missing"
2021.naacl-main.204,P13-1004,0,0.0147721,"; Han functions is not significant, we find that the inverse et al., 2018b,a) treat disagreement as a corruption version of KL gives the best results in all the experof a theoretical gold standard. Since the robustness imental conditions but one. This finding supports of machine learning models is affected by the data our idea of emphasizing the coders’ disagreement annotation quality, reducing noisy labels generally during training. We conjecture that predicting the improves the models’ performance. The closest to soft labels acts as a regularizer, reducing overfitour work are the studies of Cohn and Specia (2013) ting. That effect is especially likely for ambiguous and Rodrigues and Pereira (2018), who both use instances, where annotators’ label distributions difMTL. In contrast to our approach, though, each fer especially strongly from one-hot encoded gold of their tasks represents an annotator. We instead labels. propose to learn from both the gold labels and the Acknowledgements distribution over multiple annotators, which we treat as soft label distributions in a single auxil- DH and TF are members of the Data and Marketing iary task. Compared to treating each annotator as a Insights Unit at the B"
2021.naacl-main.204,N13-1132,1,0.807851,"ed labels. The 1 Introduction main impediment to the direct use of soft labels as targets, though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gu"
2021.naacl-main.204,P14-2062,1,0.739372,"ging and morphological stemming. We use the respective data sets from Plank et al. (2014) and Jamison and Gurevych (2015) (where data sets are sufficiently large to train a neural model). In both cases, we use data sets where both one-hot (gold) and probabilistic (soft) labels (i.e., distributions over labels annotations) are available. The code for all models in this paper will be available on github.com/fornaciari. 3.1 POS tagging Data set For this task, we use the data set released by Gimpel et al. (2010) with the crowdThis measures the divergence from P to Q and sourced labels provided by Hovy et al. (2014). The encourages a narrow Q distribution because the same data set was used by Jamison and Gurevych model will try to allocate mass to Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessar"
2021.naacl-main.204,2021.naacl-main.49,1,0.714309,"ularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on POS-tagging, we evalu"
2021.naacl-main.204,D15-1035,0,0.537521,"y et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxiliary task of predictstance (Aroyo and Welty, 2015). Se"
2021.naacl-main.204,Q18-1040,1,0.944915,"though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagree"
2021.naacl-main.204,Q19-1043,0,0.110921,"Missing"
2021.naacl-main.204,D14-1162,0,0.0838039,"Missing"
2021.naacl-main.204,petrov-etal-2012-universal,0,0.119656,"Missing"
2021.naacl-main.204,W14-1601,1,0.870017,"Missing"
2021.naacl-main.204,E14-1078,1,0.897989,"tations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxil"
2021.naacl-main.204,P16-2067,1,0.774899,"Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessary to test our models. Model We use a tagging model that takes two kinds of input representations, at the character and the word level (Plank et al., 2016). At the character level, we use character embeddings trained on the same data set; at the word level, we use Glove embeddings (Pennington et al., 2014). We feed the word representation into a ‘context bi-RNN’, selecting the hidden state of the RNN at the target word’s position in the sentence. The character representation is then fed into a ‘sequence bi-RNN’, whose output is its final state. The two outputs are concatenated and passed to an attention mechanism, as proposed by Vaswani et al. (2017). In the STL models, the attention mechanisms’ output is passed to a last attention mechanism and"
2021.naacl-main.204,W05-0311,1,0.842083,"Missing"
2021.naacl-main.204,P19-1163,0,0.0225307,"search area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on"
2021.naacl-main.204,2020.acl-main.468,1,0.71779,"y belongs to the research area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In"
2021.naacl-main.49,Q18-1041,0,0.0407513,"Missing"
2021.naacl-main.49,2020.acl-main.463,0,0.47612,"in et al., 2019) and GPT-3 (Brown et al., 2020) ever, it came at the cost of ignoring essential asseemingly picked up enough language behavior to pects of human decision making, which oversimproduce natural-looking sentences that show pragplified an inherently complex matter in a way that 588 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 588–602 June 6–11, 2021. ©2021 Association for Computational Linguistics matic constraints and interact in dialogues. However, recent work has pointed out (Bender and Koller, 2020; Bisk et al., 2020) that language is more than just words strung together: it has a social function and relates to non-linguistic context. Nonetheless, current NLP systems still largely ignore the social aspect of language. Instead, they only pay attention to what is said, not to who says it, in what context, and for which goals. We go further to argue that the simplifying focus on information content has effectively limited NLP to a narrow range of information-based applications. Consequently, NLP systems struggle with applications related to pragmatics and interaction, or when “what is said"
2021.naacl-main.49,E17-1015,1,0.931774,"er to argue that the simplifying focus on information content has effectively limited NLP to a narrow range of information-based applications. Consequently, NLP systems struggle with applications related to pragmatics and interaction, or when “what is said is not what is meant,” e.g., sarcasm, irony, deception, and any other situation that requires a “social” interpretation (Abercrombie and Hovy, 2016). This approach is especially crucial for any system related to pragmatics, such as dialogue systems, machine translation (Mirkin and Meunier, 2015), text-to-speech, and mental healthcare tools (Benton et al., 2017). Examples include conversational agents’ inconsistent personality in conducting dialogues with humans (Cercas Curry et al., 2020), the failure of machine translation systems in generating culturally appropriate and polite outputs (Jones and Irvine, 2013; Matusov, 2019; Vanmassenhove et al., 2019), or the general struggles of current systems with social intelligence (Cercas Curry and Rieser, 2018). Ultimately, the goal of NLP is to process language at a human level. However, NLP’s current approach—ignoring social factors—prevents us from reaching human-level competence and performance because"
2021.naacl-main.49,2020.emnlp-main.703,0,0.0917471,"Missing"
2021.naacl-main.49,2020.acl-main.485,0,0.058152,"Missing"
2021.naacl-main.49,D16-1120,0,0.0538966,"Missing"
2021.naacl-main.49,P16-2096,1,0.905665,"ings call for work to look at the ideology, beliefs, and culture behind language content to mitigate biases and social stereotypes beyond data-level manifestations. The fact that embeddings reflect these stereotypes, cultural beliefs, and ideologies make them also an ideal diagnostic tool for social science scholars (Garg et al., 2018; Kozlowski et al., 2018). However, it also creates fundamental biases that cannot easily be mitigated (Gonen and Goldberg, 2019), which poses severe problems for their use in predictive models. Adding cultural awareness can also help counteract the overexposure (Hovy and Spruit, 2016) to the English language (Joshi et al., 2020)6 and Anglo-Western culture. 2.7 Communicative Goal Finally, communicative goals cover what people want to achieve with their language use, e.g., information, decision making, social chitchat, negotiation, etc. SFL represents this factor as multiple metafunctions of language. Two metafunctions are of particular relevance here: the interpersonal metafunction, whereby language enables us to enact social relationships, to cooperate, form bonds, negotiate, ask for things, and instruct; and the ideational metafunction, whereby language enables us to talk"
2021.naacl-main.49,N16-1180,0,0.0447128,"Missing"
2021.naacl-main.49,N15-1185,0,0.0687439,"Missing"
2021.naacl-main.49,P19-1041,0,0.0178472,"d to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can be controlled independently of content(Prabhumoye et al., 2018; John et al., 2019). Some of the early work on NLP (Hovy, 1987) explicitly considered communicative goals in sentence generation, albeit modeled explicitly. More recently, Sap et al. (2020) modeled speaker intent to resolve conversational implicature. and around the world, how we will equip NLP models with a grounding in social factors becomes extremely important, especially these two dimensions. Detailed modeling of these social factors is essential if NLP systems are to have any impact. It can also help avoid hegemonic approaches from assuming all conversations follow Western norms, culture, and ideology. Real"
2021.naacl-main.49,N15-1016,0,0.0142915,"c, and 2) the focus of NLP models has always been on learning applications based on text alone (amplified by the seeming ability of neural approaches to do so, see Collobert et al. (2011)). Some recent papers have commented on the artificial limitation of relying solely on text (Bender and Koller, 2020; Bisk et al., 2020), demonstrating how even large pretrained language models are essentially just mimicking people’s language use, instead of actual use. Several works have shown, though, how incorporating non-textual information can improve performance, specifically in conjunction with images (Lazaridou et al., 2015; Caglayan et al., 2019). These approaches help various tasks, from concept learning to machine translation, and improve inherently multimodal applications such as scene descriptions and image labeling. However, even including more linguistic context (i.e., text beyond the current sentence) can drastically improve performance of text classification (Yang et al., 2016) and the detection of irony (Wallace et al., 2014) and sarcasm (Abercrombie and Hovy, 2016).2 2.5 Social Norm Social norms refer to acceptable group conduct, shared understandings, or informal rules, representing speakers’ and rec"
2021.naacl-main.49,W13-2713,0,0.0311791,"said is not what is meant,” e.g., sarcasm, irony, deception, and any other situation that requires a “social” interpretation (Abercrombie and Hovy, 2016). This approach is especially crucial for any system related to pragmatics, such as dialogue systems, machine translation (Mirkin and Meunier, 2015), text-to-speech, and mental healthcare tools (Benton et al., 2017). Examples include conversational agents’ inconsistent personality in conducting dialogues with humans (Cercas Curry et al., 2020), the failure of machine translation systems in generating culturally appropriate and polite outputs (Jones and Irvine, 2013; Matusov, 2019; Vanmassenhove et al., 2019), or the general struggles of current systems with social intelligence (Cercas Curry and Rieser, 2018). Ultimately, the goal of NLP is to process language at a human level. However, NLP’s current approach—ignoring social factors—prevents us from reaching human-level competence and performance because language is more than just information content. Unless we start paying attention to the social factors of language, we are artificially limiting NLP’s potential as a field and the applications we can develop, including the performance of the applications"
2021.naacl-main.49,P16-1094,0,0.0326294,"ar-old American male after translation (Hovy et al., 2020). This effect is a big issue for any text generation, where the lack of speaker personality can create incongruous responses in conversational agents. Despite conversational agents’ recent successes (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016), their lack of a consistent personality is still one of the common issues in using data-driven approaches. The main reason is that these models are often trained over conversations by different people, averaging and thereby virtually ignoring individual speakers’ personalities (Li et al., 2016; Wei et al., 2017; Zhang et al., 2018; Wu et al., 2021). There have not been many attempts to make NLP systems more robust to language variation across speakers (Yang and Eisenstein, 2017), though attempts at creating personalized language technologies exist in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), machine translation (Mirkin and Meunier, 2015), and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Vo"
2021.naacl-main.49,N16-1130,1,0.890267,"Missing"
2021.naacl-main.49,2020.acl-main.560,0,0.0252698,"efs, and culture behind language content to mitigate biases and social stereotypes beyond data-level manifestations. The fact that embeddings reflect these stereotypes, cultural beliefs, and ideologies make them also an ideal diagnostic tool for social science scholars (Garg et al., 2018; Kozlowski et al., 2018). However, it also creates fundamental biases that cannot easily be mitigated (Gonen and Goldberg, 2019), which poses severe problems for their use in predictive models. Adding cultural awareness can also help counteract the overexposure (Hovy and Spruit, 2016) to the English language (Joshi et al., 2020)6 and Anglo-Western culture. 2.7 Communicative Goal Finally, communicative goals cover what people want to achieve with their language use, e.g., information, decision making, social chitchat, negotiation, etc. SFL represents this factor as multiple metafunctions of language. Two metafunctions are of particular relevance here: the interpersonal metafunction, whereby language enables us to enact social relationships, to cooperate, form bonds, negotiate, ask for things, and instruct; and the ideational metafunction, whereby language enables us to talk about inner and outer experiences, people an"
2021.naacl-main.49,jwalapuram-2017-evaluating,0,0.0149553,"parent, the conversational partner can use the resulting inconsistency to construct an alternative meaning. E.g., inferring that “Take your time, I love waiting for you” violates the maxim of quality and is probably not true lets us assume sarcasm. Gricean maxims and their selective violations can explain why “what is said is not what is meant.” This inference process is called conversational implicature, and can help explain why NLP applications struggle with tasks such as sarcasm detection or entailment. Some previous works have consequently used them to evaluate the quality of NLP systems (Jwalapuram, 2017; Qwaider et al., 2017). Building upon these two frameworks, we lay out a set of seven social factors that NLP systems need to be aware of to overcome current limitations (see Figure 2). We cover SPEAKER characteristics (Section 2.1), RECEIVER characteristics (Section 2.2), SOCIAL RELATIONS (Section 2.3), CONTEXT (Section 2.4), SOCIAL NORMS (Section 2.5), CULTURE AND IDEOLOGY Figure 2: Taxonomy of social factors (Section 2.6), and COMMUNICATIVE GOALS (Section 2.7). We first outline each factor and its relation to SFL and the cooperation principle and then discuss the associated limitations for"
2021.naacl-main.49,P18-2005,0,0.0204148,"speakers (Yang and Eisenstein, 2017), though attempts at creating personalized language technologies exist in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), machine translation (Mirkin and Meunier, 2015), and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Volkova et al., 2013), through conditional embeddings (Hovy, 2015; Lynn et al., 2017), or via neural models for multi-task learning (Benton et al., 2017; Li et al., 2018). By accounting for a speaker’s specific demographic attributes, models achieve better performance in a variety of tasks, such as sentiment analysis, user attributes, part-of-speech tagging, and response generation (Wu et al., 2021). Rashkin et al. (2016) showed the value of modelling speaker perspective to discover opinions or biases in the way things are expressed. Hovy (2016) showed that demographically-conditioned generated text also is more convincing. is talking. However, when it comes to broadcasting or highly public spaces, receivers are often “imagined” by the speaker (Litt, 2012) and"
2021.naacl-main.49,D17-1119,0,0.0222818,"ave not been many attempts to make NLP systems more robust to language variation across speakers (Yang and Eisenstein, 2017), though attempts at creating personalized language technologies exist in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), machine translation (Mirkin and Meunier, 2015), and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Volkova et al., 2013), through conditional embeddings (Hovy, 2015; Lynn et al., 2017), or via neural models for multi-task learning (Benton et al., 2017; Li et al., 2018). By accounting for a speaker’s specific demographic attributes, models achieve better performance in a variety of tasks, such as sentiment analysis, user attributes, part-of-speech tagging, and response generation (Wu et al., 2021). Rashkin et al. (2016) showed the value of modelling speaker perspective to discover opinions or biases in the way things are expressed. Hovy (2016) showed that demographically-conditioned generated text also is more convincing. is talking. However, when it comes to broadcasting or"
2021.naacl-main.49,2020.acl-main.169,0,0.0136468,"necessitates more finegrained politeness and formality levels than in Western cultures. The terms of address also vary in terms of social and age differences, i.e., inferior members address superior ones with a relationship term instead of using personal names (see also Section 2.3). In many Asian cultures, family terms like “uncle” or “big sister” are used as honorifics. While it is common amongst native speakers of North American English to use “please” in requests even to close friends, such an act would be considered awkward, if not rude, in Arabicspeaking cultures (Kádár and Mills, 2011; Madaan et al., 2020). Cultural norms can impose a hierarchy on Gricean maxims. For example, whether it is better to give made-up directions (which violates the maxim of relevance) instead of not saying anything (adhering to the maxim of quality) if you do not know the right answer. Context and social and cultural norms can combine in unexpected ways, such as in the case of Korean Airline co-pilots not correcting pilot mistakes (a social and cultural taboo in ordinary contexts), which resulted in a series of accidents. Differing perceptions of the context, respect for seniority and age, and a hierarchical communic"
2021.naacl-main.49,W19-7302,0,0.0168776,"t,” e.g., sarcasm, irony, deception, and any other situation that requires a “social” interpretation (Abercrombie and Hovy, 2016). This approach is especially crucial for any system related to pragmatics, such as dialogue systems, machine translation (Mirkin and Meunier, 2015), text-to-speech, and mental healthcare tools (Benton et al., 2017). Examples include conversational agents’ inconsistent personality in conducting dialogues with humans (Cercas Curry et al., 2020), the failure of machine translation systems in generating culturally appropriate and polite outputs (Jones and Irvine, 2013; Matusov, 2019; Vanmassenhove et al., 2019), or the general struggles of current systems with social intelligence (Cercas Curry and Rieser, 2018). Ultimately, the goal of NLP is to process language at a human level. However, NLP’s current approach—ignoring social factors—prevents us from reaching human-level competence and performance because language is more than just information content. Unless we start paying attention to the social factors of language, we are artificially limiting NLP’s potential as a field and the applications we can develop, including the performance of the applications that exist tod"
2021.naacl-main.49,D15-1238,0,0.131076,"d, not to who says it, in what context, and for which goals. We go further to argue that the simplifying focus on information content has effectively limited NLP to a narrow range of information-based applications. Consequently, NLP systems struggle with applications related to pragmatics and interaction, or when “what is said is not what is meant,” e.g., sarcasm, irony, deception, and any other situation that requires a “social” interpretation (Abercrombie and Hovy, 2016). This approach is especially crucial for any system related to pragmatics, such as dialogue systems, machine translation (Mirkin and Meunier, 2015), text-to-speech, and mental healthcare tools (Benton et al., 2017). Examples include conversational agents’ inconsistent personality in conducting dialogues with humans (Cercas Curry et al., 2020), the failure of machine translation systems in generating culturally appropriate and polite outputs (Jones and Irvine, 2013; Matusov, 2019; Vanmassenhove et al., 2019), or the general struggles of current systems with social intelligence (Cercas Curry and Rieser, 2018). Ultimately, the goal of NLP is to process language at a human level. However, NLP’s current approach—ignoring social factors—preven"
2021.naacl-main.49,J16-3007,0,0.0601436,"Missing"
2021.naacl-main.49,2021.naacl-main.191,1,0.839724,"Missing"
2021.naacl-main.49,2020.emnlp-main.428,0,0.0431348,"thegradient.pub/the-benderru le-on-naming-the-languages-we-study-an d-why-it-matters/ 594 stance, text that aims to convince others often uses various persuasion strategies (Yang et al., 2019a; Chen and Yang, 2021), argumentation techniques (Stab and Gurevych, 2014), rhetorical structures (Rapp, 2011), and the exchange of social support (Wang and Jurgens, 2018; Yang et al., 2019b). Messages trying to entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can be controlled independently of content(Prabhumoye et al., 2018; John et al., 2019). Some of the early work on NLP (Hovy, 1987) explicitly considered communicative goals in sentence generation, albeit modeled explicitly. More recently, Sap et al. (2020) modele"
2021.naacl-main.49,N12-1057,0,0.0705938,"Missing"
2021.naacl-main.49,P18-1080,0,0.0127246,"o entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can be controlled independently of content(Prabhumoye et al., 2018; John et al., 2019). Some of the early work on NLP (Hovy, 1987) explicitly considered communicative goals in sentence generation, albeit modeled explicitly. More recently, Sap et al. (2020) modeled speaker intent to resolve conversational implicature. and around the world, how we will equip NLP models with a grounding in social factors becomes extremely important, especially these two dimensions. Detailed modeling of these social factors is essential if NLP systems are to have any impact. It can also help avoid hegemonic approaches from assuming all conversations follow Western norms, culture"
2021.naacl-main.49,S17-2043,0,0.0344768,"Missing"
2021.naacl-main.49,P18-1187,0,0.024525,"Missing"
2021.naacl-main.49,D17-1244,0,0.0272402,"Missing"
2021.naacl-main.49,2020.findings-emnlp.214,1,0.828232,"Missing"
2021.naacl-main.49,P16-1030,0,0.0186618,", and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Volkova et al., 2013), through conditional embeddings (Hovy, 2015; Lynn et al., 2017), or via neural models for multi-task learning (Benton et al., 2017; Li et al., 2018). By accounting for a speaker’s specific demographic attributes, models achieve better performance in a variety of tasks, such as sentiment analysis, user attributes, part-of-speech tagging, and response generation (Wu et al., 2021). Rashkin et al. (2016) showed the value of modelling speaker perspective to discover opinions or biases in the way things are expressed. Hovy (2016) showed that demographically-conditioned generated text also is more convincing. is talking. However, when it comes to broadcasting or highly public spaces, receivers are often “imagined” by the speaker (Litt, 2012) and are potentially numerous and invisible. This imagined audience is a speaker’s mental conceptualization of the people with whom he or she is communicating. This conceptualization of receiver characteristics influences the conversation: a speaker who calls"
2021.naacl-main.49,D11-1054,0,0.127004,"Missing"
2021.naacl-main.49,P19-1163,0,0.441364,"ic models currently fail to consider receiver characteristics. For instance, when writing to the president of a company vs. messaging your best friend, the politeness levels and register differ substantially, but current large, pretrained models cannot deal with this difference effectively (for an exception, see Fu et al. (2020)). What is more, they can generate messages that are actively hurtful to receivers (Nozza et al., 2021). In other cases like hateful-content detection (Warner and Hirschberg, 2012), a message might be toxic to outsiders but perceived as appropriate among close friends (Sap et al., 2019a). This self-reference or joking use of slurs by a group of intimates might introduce significant noise to the automatic recognition of hate speech, causing existing classifiers to fail in 2.2 Receiver many instances. Detecting such hateful or toxic speech online might require classifiers to take into Audiences that receive text from a speaker are account both content and receivers, as well as a made up of receivers, depending on the situation broader context. Receiver differences markedly and medium. The number of receivers can vary add to the complexity and difficulty in machine substantial"
2021.naacl-main.49,2020.acl-main.486,0,0.0328395,"(Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can be controlled independently of content(Prabhumoye et al., 2018; John et al., 2019). Some of the early work on NLP (Hovy, 1987) explicitly considered communicative goals in sentence generation, albeit modeled explicitly. More recently, Sap et al. (2020) modeled speaker intent to resolve conversational implicature. and around the world, how we will equip NLP models with a grounding in social factors becomes extremely important, especially these two dimensions. Detailed modeling of these social factors is essential if NLP systems are to have any impact. It can also help avoid hegemonic approaches from assuming all conversations follow Western norms, culture, and ideology. Real-world interaction involves more than the exchange of information or decision making via language; it involves a wide range of aspects related to social factors and inter"
2021.naacl-main.49,D14-1006,0,0.0115222,"lay out the reasons we need them to join. However, to make it more likely that they agree, we might choose to exaggerate the expected payoff and to leave out some of the difficulties involved, which violates the maxims of quality and quantity, respectively. Applications Communicative goals shape how speakers arrange their words and styles. For in6 https://thegradient.pub/the-benderru le-on-naming-the-languages-we-study-an d-why-it-matters/ 594 stance, text that aims to convince others often uses various persuasion strategies (Yang et al., 2019a; Chen and Yang, 2021), argumentation techniques (Stab and Gurevych, 2014), rhetorical structures (Rapp, 2011), and the exchange of social support (Wang and Jurgens, 2018; Yang et al., 2019b). Messages trying to entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles"
2021.naacl-main.49,D19-1454,0,0.0650577,"Missing"
2021.naacl-main.49,P19-1164,0,0.0224983,"t of communication, rather than relying on background conversations from different communicators in different contexts. Models have mostly learned to relate words to other words. For instance, current machine translation models are trained on huge corpora of text. However, nuances in language often make it difficult to provide an accurate and di592 rect translation from one social context to another. Studies show that current popular industrial MT systems and recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages (Stanovsky et al., 2019; Vanmassenhove et al., 2019; Hovy et al., 2020). There is hilarious content caused by translation fails (see #translationfail on Twitter), especially when it comes to the social context or cultural-specific nuances of language. Current text generation models also usually fail to account for social context, generating text that lacks nuance. This factor is one of the most difficult ones to overcome, because 1) social context is almost always extralinguistic, and 2) the focus of NLP models has always been on learning applications based on text alone (amplified by the seeming ability of neural a"
2021.naacl-main.49,2020.acl-main.468,1,0.791046,"ll conversations follow Western norms, culture, and ideology. Real-world interaction involves more than the exchange of information or decision making via language; it involves a wide range of aspects related to social factors and interpersonal relations, reflected in rich modalities such as voice or facial expression. Though this work’s focus is on the language side, we argue that the introduced taxonomy can be beneficial in broader scenarios for next-level multi-modal models. Data, Ethics, and Privacy Our work here is related to some of the recent work on bias in NLP (Hovy and Spruit, 2016; Shah et al., 2020). On the one hand, the cooperative principle can be seen as a possible positive bias: a pre-existing expectation of how we interact, the violation of which signals an alternative approach. So far, models do not integrate this positive bias. On the other hand, work 3 Outlook and Challenges on speaker and receiver characteristics is affected by the models’ predictive biases: exaggerating Social Factors in Different NLP Tasks When or overestimating one particular group’s attributes and how, though, should we consider these various can skew the results, for example, in the case of social factors f"
2021.naacl-main.49,D19-1339,0,0.0593822,"ons Culture and ideology are probably the most complicated language constructs. Despite their substantial influence on communication interpretation and language understanding, most NLP models, like text generation or translation, have not included politeness or other similar subtle cultural signatures. A growing body of research has paid attention to the biases and cul5 https://www.cnbc.com/id/100869966 tural stereotypes encoded and amplified by current NLP models, e.g., inappropriate occupation predictions by large pretrained language models like “the black woman who worked as a babysitter” (Sheng et al., 2019). These findings call for work to look at the ideology, beliefs, and culture behind language content to mitigate biases and social stereotypes beyond data-level manifestations. The fact that embeddings reflect these stereotypes, cultural beliefs, and ideologies make them also an ideal diagnostic tool for social science scholars (Garg et al., 2018; Kozlowski et al., 2018). However, it also creates fundamental biases that cannot easily be mitigated (Gonen and Goldberg, 2019), which poses severe problems for their use in predictive models. Adding cultural awareness can also help counteract the ov"
2021.naacl-main.49,W19-6622,0,0.127332,"sm, irony, deception, and any other situation that requires a “social” interpretation (Abercrombie and Hovy, 2016). This approach is especially crucial for any system related to pragmatics, such as dialogue systems, machine translation (Mirkin and Meunier, 2015), text-to-speech, and mental healthcare tools (Benton et al., 2017). Examples include conversational agents’ inconsistent personality in conducting dialogues with humans (Cercas Curry et al., 2020), the failure of machine translation systems in generating culturally appropriate and polite outputs (Jones and Irvine, 2013; Matusov, 2019; Vanmassenhove et al., 2019), or the general struggles of current systems with social intelligence (Cercas Curry and Rieser, 2018). Ultimately, the goal of NLP is to process language at a human level. However, NLP’s current approach—ignoring social factors—prevents us from reaching human-level competence and performance because language is more than just information content. Unless we start paying attention to the social factors of language, we are artificially limiting NLP’s potential as a field and the applications we can develop, including the performance of the applications that exist today. We want to be clear that"
2021.naacl-main.49,D13-1187,0,0.0469367,"Missing"
2021.naacl-main.49,P14-2084,0,0.0244547,"language use, instead of actual use. Several works have shown, though, how incorporating non-textual information can improve performance, specifically in conjunction with images (Lazaridou et al., 2015; Caglayan et al., 2019). These approaches help various tasks, from concept learning to machine translation, and improve inherently multimodal applications such as scene descriptions and image labeling. However, even including more linguistic context (i.e., text beyond the current sentence) can drastically improve performance of text classification (Yang et al., 2016) and the detection of irony (Wallace et al., 2014) and sarcasm (Abercrombie and Hovy, 2016).2 2.5 Social Norm Social norms refer to acceptable group conduct, shared understandings, or informal rules, representing speakers’ and receivers’ basic knowledge of what others do and what others think they should and should not do (Fehr and Fischbacher, 2004), such as dining etiquette, community norms on Reddit (Chandrasekharan et al., 2018), or hierarchical greetings. Norms are therefore closely related to the factors of relation (Section 2.3) and context (Section 2.4). For instance, greet2 Note that the latter two show that human speakers depend on"
2021.naacl-main.49,D18-1004,0,0.0147831,"t choose to exaggerate the expected payoff and to leave out some of the difficulties involved, which violates the maxims of quality and quantity, respectively. Applications Communicative goals shape how speakers arrange their words and styles. For in6 https://thegradient.pub/the-benderru le-on-naming-the-languages-we-study-an d-why-it-matters/ 594 stance, text that aims to convince others often uses various persuasion strategies (Yang et al., 2019a; Chen and Yang, 2021), argumentation techniques (Stab and Gurevych, 2014), rhetorical structures (Rapp, 2011), and the exchange of social support (Wang and Jurgens, 2018; Yang et al., 2019b). Messages trying to entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can"
2021.naacl-main.49,W12-2103,0,0.0371021,"by saying things that are not true: you really do want another piece of cake). Applications Spellchecking and stylistic models currently fail to consider receiver characteristics. For instance, when writing to the president of a company vs. messaging your best friend, the politeness levels and register differ substantially, but current large, pretrained models cannot deal with this difference effectively (for an exception, see Fu et al. (2020)). What is more, they can generate messages that are actively hurtful to receivers (Nozza et al., 2021). In other cases like hateful-content detection (Warner and Hirschberg, 2012), a message might be toxic to outsiders but perceived as appropriate among close friends (Sap et al., 2019a). This self-reference or joking use of slurs by a group of intimates might introduce significant noise to the automatic recognition of hate speech, causing existing classifiers to fail in 2.2 Receiver many instances. Detecting such hateful or toxic speech online might require classifiers to take into Audiences that receive text from a speaker are account both content and receivers, as well as a made up of receivers, depending on the situation broader context. Receiver differences markedl"
2021.naacl-main.49,1983.tc-1.13,0,0.704547,"Missing"
2021.naacl-main.49,N19-1364,1,0.818548,"a project, we might adhere to the maxims of relevance and concisely lay out the reasons we need them to join. However, to make it more likely that they agree, we might choose to exaggerate the expected payoff and to leave out some of the difficulties involved, which violates the maxims of quality and quantity, respectively. Applications Communicative goals shape how speakers arrange their words and styles. For in6 https://thegradient.pub/the-benderru le-on-naming-the-languages-we-study-an d-why-it-matters/ 594 stance, text that aims to convince others often uses various persuasion strategies (Yang et al., 2019a; Chen and Yang, 2021), argumentation techniques (Stab and Gurevych, 2014), rhetorical structures (Rapp, 2011), and the exchange of social support (Wang and Jurgens, 2018; Yang et al., 2019b). Messages trying to entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to b"
2021.naacl-main.49,D15-1284,1,0.697397,"antity, respectively. Applications Communicative goals shape how speakers arrange their words and styles. For in6 https://thegradient.pub/the-benderru le-on-naming-the-languages-we-study-an d-why-it-matters/ 594 stance, text that aims to convince others often uses various persuasion strategies (Yang et al., 2019a; Chen and Yang, 2021), argumentation techniques (Stab and Gurevych, 2014), rhetorical structures (Rapp, 2011), and the exchange of social support (Wang and Jurgens, 2018; Yang et al., 2019b). Messages trying to entertain audiences need to be structured in ways that can trigger humor (Yang et al., 2015). People might use informal language or text with a high level of intimacy to indicate close relations (Pei and Jurgens, 2020) or reduce social distance between speakers and receivers (Bernstein, 1960; Keshavarz, 2001). Therefore, it is essential for NLP systems like text generation models to be aware of communicative goals in order to arrange word choice, and styles to form a grammatically responsible and coherent text. Ongoing research has shown that style can be controlled independently of content(Prabhumoye et al., 2018; John et al., 2019). Some of the early work on NLP (Hovy, 1987) explic"
2021.naacl-main.49,Q17-1021,0,0.0265273,"nses in conversational agents. Despite conversational agents’ recent successes (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016), their lack of a consistent personality is still one of the common issues in using data-driven approaches. The main reason is that these models are often trained over conversations by different people, averaging and thereby virtually ignoring individual speakers’ personalities (Li et al., 2016; Wei et al., 2017; Zhang et al., 2018; Wu et al., 2021). There have not been many attempts to make NLP systems more robust to language variation across speakers (Yang and Eisenstein, 2017), though attempts at creating personalized language technologies exist in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), machine translation (Mirkin and Meunier, 2015), and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Volkova et al., 2013), through conditional embeddings (Hovy, 2015; Lynn et al., 2017), or via neural models for multi-task learning (Benton et al., 2017; Li et al., 2018). By accounting for a"
2021.naacl-main.49,N16-1174,1,0.290556,"models are essentially just mimicking people’s language use, instead of actual use. Several works have shown, though, how incorporating non-textual information can improve performance, specifically in conjunction with images (Lazaridou et al., 2015; Caglayan et al., 2019). These approaches help various tasks, from concept learning to machine translation, and improve inherently multimodal applications such as scene descriptions and image labeling. However, even including more linguistic context (i.e., text beyond the current sentence) can drastically improve performance of text classification (Yang et al., 2016) and the detection of irony (Wallace et al., 2014) and sarcasm (Abercrombie and Hovy, 2016).2 2.5 Social Norm Social norms refer to acceptable group conduct, shared understandings, or informal rules, representing speakers’ and receivers’ basic knowledge of what others do and what others think they should and should not do (Fehr and Fischbacher, 2004), such as dining etiquette, community norms on Reddit (Chandrasekharan et al., 2018), or hierarchical greetings. Norms are therefore closely related to the factors of relation (Section 2.3) and context (Section 2.4). For instance, greet2 Note that"
2021.naacl-main.49,P18-1205,0,0.0234529,"ion (Hovy et al., 2020). This effect is a big issue for any text generation, where the lack of speaker personality can create incongruous responses in conversational agents. Despite conversational agents’ recent successes (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016), their lack of a consistent personality is still one of the common issues in using data-driven approaches. The main reason is that these models are often trained over conversations by different people, averaging and thereby virtually ignoring individual speakers’ personalities (Li et al., 2016; Wei et al., 2017; Zhang et al., 2018; Wu et al., 2021). There have not been many attempts to make NLP systems more robust to language variation across speakers (Yang and Eisenstein, 2017), though attempts at creating personalized language technologies exist in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), machine translation (Mirkin and Meunier, 2015), and language modeling (Federico, 1996). Meanwhile, various approaches have shown the positive impact of incorporating speaker characteristics into NLP applications, either as explicit features (Volkova et al., 2013), through condition"
2021.wassa-1.29,P17-1067,0,0.028193,"We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to singletask learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks – at least for the purpose of prediction. 1 Introduction Different researchers have been exploring emotion prediction from text (Abdul-Mageed and Ungar, 2017; Nozza et al., 2017). The WASSA-2021 shared task (Tafreshi et al., 2021) tackles the prediction of empathy (Track 1 of the challenge) and emotion (Track 2 of the challenge) in text. We, the MilaNLP lab, participated in Track 2 of the challenge. Nozza et al. (2020) show that Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results"
2021.wassa-1.29,E17-1015,1,0.790305,"Missing"
2021.wassa-1.29,2021.wassa-1.8,1,0.757219,"loring emotion prediction from text (Abdul-Mageed and Ungar, 2017; Nozza et al., 2017). The WASSA-2021 shared task (Tafreshi et al., 2021) tackles the prediction of empathy (Track 1 of the challenge) and emotion (Track 2 of the challenge) in text. We, the MilaNLP lab, participated in Track 2 of the challenge. Nozza et al. (2020) show that Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results in the emotion prediction task. This system paper describes our approach to the emotion prediction shared task. Based on 2 Data In this paper, we focused on emotion prediction (Track 2) of reactions to English news stories. The data set is an extended version of the one presented in Buechel et al. (2018). Each instance corresponds to an empathic reaction to news stories extracted by popular online news platforms. A set of 1860 training documents annotated with seven emotions was given (see Table 2 for the data set size). With each text document, an"
2021.wassa-1.29,D18-1507,0,0.019298,"onal Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results in the emotion prediction task. This system paper describes our approach to the emotion prediction shared task. Based on 2 Data In this paper, we focused on emotion prediction (Track 2) of reactions to English news stories. The data set is an extended version of the one presented in Buechel et al. (2018). Each instance corresponds to an empathic reaction to news stories extracted by popular online news platforms. A set of 1860 training documents annotated with seven emotions was given (see Table 2 for the data set size). With each text document, an empathy score that ranges from 1 to seven has been associated; in Table 1 we show some examples of text with the emotion and the empathy that come from the data set. 269 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 269–273 April 19, 2021. ©2021 Association for Computational"
2021.wassa-1.29,N19-1423,0,0.173578,"titive in terms of the competition, our results suggest that emotion and empathy are not related tasks – at least for the purpose of prediction. 1 Introduction Different researchers have been exploring emotion prediction from text (Abdul-Mageed and Ungar, 2017; Nozza et al., 2017). The WASSA-2021 shared task (Tafreshi et al., 2021) tackles the prediction of empathy (Track 1 of the challenge) and emotion (Track 2 of the challenge) in text. We, the MilaNLP lab, participated in Track 2 of the challenge. Nozza et al. (2020) show that Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results in the emotion prediction task. This system paper describes our approach to the emotion prediction shared task. Based on 2 Data In this paper, we focused on emotion prediction (Track 2) of reactions to English news stories. The data set is an extended version of the one presented in Buechel et al. (2018). Each instance corresponds to an empathic reaction to"
2021.wassa-1.29,D19-5528,1,0.885835,"Missing"
2021.wassa-1.29,P15-1073,1,0.893615,"Missing"
2021.wassa-1.29,2021.wassa-1.7,1,0.755576,"researchers have been exploring emotion prediction from text (Abdul-Mageed and Ungar, 2017; Nozza et al., 2017). The WASSA-2021 shared task (Tafreshi et al., 2021) tackles the prediction of empathy (Track 1 of the challenge) and emotion (Track 2 of the challenge) in text. We, the MilaNLP lab, participated in Track 2 of the challenge. Nozza et al. (2020) show that Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results in the emotion prediction task. This system paper describes our approach to the emotion prediction shared task. Based on 2 Data In this paper, we focused on emotion prediction (Track 2) of reactions to English news stories. The data set is an extended version of the one presented in Buechel et al. (2018). Each instance corresponds to an empathic reaction to news stories extracted by popular online news platforms. A set of 1860 training documents annotated with seven emotions was given (see Table 2 for the data set size). With"
2021.wassa-1.29,P18-2005,0,0.028499,"Missing"
2021.wassa-1.29,D17-1119,0,0.0484532,"Missing"
2021.wassa-1.29,E17-1005,0,0.0233739,"rmance on the main task, by exploring complementary information in the tasks, and by acting as a regularizer (a model that has to be able to predict more than one task is less prone to overfitting to any one of them). However, we unexpectedly find evidence for the opposite: using empathy as an auxiliary task in multi-task learning in this setting does not work as expected. In fact, adding empathy prediction hurts performance compared to a single-task model. This finding adds to the literature that auxiliary tasks in MTL setups need to be related to the main task to help performance (Mart´ınez Alonso and Plank, 2017). It also indicates that empathy is not directly a contributing factor to emotions, i.e., that there is no strong correlation between the two tasks. The paper describes the MilaNLP team’s submission (Bocconi University, Milan) in the WASSA 2021 Shared Task on Empathy Detection and Emotion Classification. We focus on Track 2 – Emotion Classification – which consists of predicting the emotion of reactions to English news stories at the essay-level. We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the d"
2021.wassa-1.29,E17-1026,1,0.836116,"d on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to singletask learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks – at least for the purpose of prediction. 1 Introduction Different researchers have been exploring emotion prediction from text (Abdul-Mageed and Ungar, 2017; Nozza et al., 2017). The WASSA-2021 shared task (Tafreshi et al., 2021) tackles the prediction of empathy (Track 1 of the challenge) and emotion (Track 2 of the challenge) in text. We, the MilaNLP lab, participated in Track 2 of the challenge. Nozza et al. (2020) show that Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) can provide accurate results for many different languages in different tasks. Indeed, we contributed to this year’s WASSA workshop with two papers (Lamprinidis et al., 2021; Bianchi et al., 2021) that show that BERT can obtain good results in the emotion predi"
2021.wassa-1.29,2020.findings-emnlp.214,1,0.806262,"Missing"
2021.wassa-1.29,W14-1601,1,0.828794,"Missing"
2021.wassa-1.29,2021.wassa-1.10,0,0.0587507,"Missing"
2021.wassa-1.29,D13-1187,0,0.0862329,"Missing"
2021.wassa-1.7,N19-1423,0,0.0204798,"LR models we use the same tokenization as in 3.2, from the pre-trained multilingual BERT model. Figure 1: Heatmap of relative emotion distribution per language. 2.1 Test Datasets Training Datasets We create three versions of the dataset for training purposes: • Small: this version includes the five languages with sufficient training data for the least frequent emotion, fear: namely eng, spa, por, cmn, tgl. This dataset is balanced by language, so that there are 2,947 posts for each language. 3.2 Multilingual BERT To optimize performance, we use the multilingual BERT (mBERT) model.7 We follow Devlin et al. (2019) in optimizing the model using a machine with an Intel i9-9940X CPU, 32GB RAM and a NVIDIA Quadro RTX 6000 GPU. The loss L is the mean over the individual losses l ∈ L for each emotion: • Large: this version includes 29,364 posts for each of the three most frequent languages: eng, spa, por. Note that each of these is a superset of the corresponding language in the Small training set. 6 https://scikit-learn.org https://github.com/google-research/ bert • Huge: this version contains 283,853 posts from the single most frequent language, eng. 7 64 Language Anger Anticipation Fear Joy Sadness Sum cm"
2021.wassa-1.8,D18-1005,0,0.0258327,"d and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results on FEEL-IT, we evaluate recent neural models trained on our corpus for emotion recognition Introduction Emotions shape our lives"
2021.wassa-1.8,2020.acl-main.370,0,0.0350402,"e most frequent emotion, because of their focus on music or movies. https://github.com/MilaNLProc/feel-it 77 that FEEL-IT can also be used to perform sentiment classification with competitive results. 3.1 Model UmBERTo-FT UmBERTo-PT W2V TF-IDF MFC Emotion Classification We first experiment with emotion recognition in the FEEL-IT dataset. Contextualized representations, such as BERT (Devlin et al., 2019) have obtained a lot of attention due to the great results (Rogers et al., 2021; Nozza et al., 2020) on multiple languages and on different tasks (Scarlini et al., 2020; Mass and Roitman, 2020; Du et al., 2020; Pasini et al., 2020; Peinelt et al., 2020; Bianchi et al., 2021; Nozza et al., 2021, inter alia). In this paper, we use the Italian BERT model UmBERTo trained on Commoncrawl ITA.4 As the first experimental condition, we fine-tune the UmBERTo model for the task of emotion classification with the considered training data (UmBERTo-FT). As additional experimental frameworks, we use three different approaches to represent tweets: (i) We collect pre-trained UmBERTo representations using average pooling of the last layer (UmBERToPT); (ii) we use an Italian word2vec model (W2V)5 and create the repre"
2021.wassa-1.8,S18-1001,0,0.127112,"an, Italy dirk.hovy@unibocconi.it anger fear joy sadness Total 912 103 728 294 2037 Table 1: FEEL-IT corpus statistics. Despite the huge interest of the Natural Language Processing community, the majority of benchmark datasets have been proposed for English (Calefato et al., 2017; Abdul-Mageed and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of ou"
2021.wassa-1.8,W17-0237,0,0.0228344,"Missing"
2021.wassa-1.8,E17-1026,1,0.729764,"khtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results on FEEL-IT, we evaluate recent neural models trained on our corpus for emotion recognition Introduction Emotions shape our lives and the way we commun"
2021.wassa-1.8,2020.findings-emnlp.402,0,0.0422204,"recent neural models trained on our corpus for emotion recognition Introduction Emotions shape our lives and the way we communicate. We can be happy, sad, or angry, and we can let others know of our emotional state through language. Thus, efficiently detecting emotion in text is essential for analyzing people’s position towards a topic. Product and service companies frequently use emotion and sentiment data to inform advertising campaigns and measure customer satisfaction (Ahmad et al., 2020). Emotions have a central role in a political campaigns, and political discourse in particular (Huguet Cabot et al., 2020). Emotion and sentiment recognition can also aid in the critical decision-making process of crisis management or emergency scenarios (Stowe et al., 2016; Desai et al., 2020). ∗ Both authors contributed equally to this research and are ordered alphabetically. 1 We focus on these emotions because they appear most frequently in text. 76 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 76–83 April 19, 2021. ©2021 Association for Computational Linguistics Example Emotion Pagliacci ammaestrati dal Grillo parlante di Pinocchio Th"
2021.wassa-1.8,D18-1147,0,0.0188915,"to et al., 2017; Abdul-Mageed and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results on FEEL-IT, we evaluate recent neural models trained on our corpus for emotion recognition Introduction Emoti"
2021.wassa-1.8,pak-paroubek-2010-twitter,0,0.0941761,"unity, the majority of benchmark datasets have been proposed for English (Calefato et al., 2017; Abdul-Mageed and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results on FEEL-IT, we evaluate r"
2021.wassa-1.8,2021.wassa-1.7,1,0.418446,"enchmark datasets have been proposed for English (Calefato et al., 2017; Abdul-Mageed and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results on FEEL-IT, we evaluate recent neural models trained"
2021.wassa-1.8,2020.acl-main.369,0,0.0318296,"motion, because of their focus on music or movies. https://github.com/MilaNLProc/feel-it 77 that FEEL-IT can also be used to perform sentiment classification with competitive results. 3.1 Model UmBERTo-FT UmBERTo-PT W2V TF-IDF MFC Emotion Classification We first experiment with emotion recognition in the FEEL-IT dataset. Contextualized representations, such as BERT (Devlin et al., 2019) have obtained a lot of attention due to the great results (Rogers et al., 2021; Nozza et al., 2020) on multiple languages and on different tasks (Scarlini et al., 2020; Mass and Roitman, 2020; Du et al., 2020; Pasini et al., 2020; Peinelt et al., 2020; Bianchi et al., 2021; Nozza et al., 2021, inter alia). In this paper, we use the Italian BERT model UmBERTo trained on Commoncrawl ITA.4 As the first experimental condition, we fine-tune the UmBERTo model for the task of emotion classification with the considered training data (UmBERTo-FT). As additional experimental frameworks, we use three different approaches to represent tweets: (i) We collect pre-trained UmBERTo representations using average pooling of the last layer (UmBERToPT); (ii) we use an Italian word2vec model (W2V)5 and create the representation of the twee"
2021.wassa-1.8,2020.acl-main.630,0,0.0251288,"eir focus on music or movies. https://github.com/MilaNLProc/feel-it 77 that FEEL-IT can also be used to perform sentiment classification with competitive results. 3.1 Model UmBERTo-FT UmBERTo-PT W2V TF-IDF MFC Emotion Classification We first experiment with emotion recognition in the FEEL-IT dataset. Contextualized representations, such as BERT (Devlin et al., 2019) have obtained a lot of attention due to the great results (Rogers et al., 2021; Nozza et al., 2020) on multiple languages and on different tasks (Scarlini et al., 2020; Mass and Roitman, 2020; Du et al., 2020; Pasini et al., 2020; Peinelt et al., 2020; Bianchi et al., 2021; Nozza et al., 2021, inter alia). In this paper, we use the Italian BERT model UmBERTo trained on Commoncrawl ITA.4 As the first experimental condition, we fine-tune the UmBERTo model for the task of emotion classification with the considered training data (UmBERTo-FT). As additional experimental frameworks, we use three different approaches to represent tweets: (i) We collect pre-trained UmBERTo representations using average pooling of the last layer (UmBERToPT); (ii) we use an Italian word2vec model (W2V)5 and create the representation of the tweet as the average of th"
2021.wassa-1.8,2020.emnlp-main.343,0,0.0342197,"ther datasets, joy is the most frequent emotion, because of their focus on music or movies. https://github.com/MilaNLProc/feel-it 77 that FEEL-IT can also be used to perform sentiment classification with competitive results. 3.1 Model UmBERTo-FT UmBERTo-PT W2V TF-IDF MFC Emotion Classification We first experiment with emotion recognition in the FEEL-IT dataset. Contextualized representations, such as BERT (Devlin et al., 2019) have obtained a lot of attention due to the great results (Rogers et al., 2021; Nozza et al., 2020) on multiple languages and on different tasks (Scarlini et al., 2020; Mass and Roitman, 2020; Du et al., 2020; Pasini et al., 2020; Peinelt et al., 2020; Bianchi et al., 2021; Nozza et al., 2021, inter alia). In this paper, we use the Italian BERT model UmBERTo trained on Commoncrawl ITA.4 As the first experimental condition, we fine-tune the UmBERTo model for the task of emotion classification with the considered training data (UmBERTo-FT). As additional experimental frameworks, we use three different approaches to represent tweets: (i) We collect pre-trained UmBERTo representations using average pooling of the last layer (UmBERToPT); (ii) we use an Italian word2vec model (W2V)5 and"
2021.wassa-1.8,S12-1033,0,0.211615,"Processing community, the majority of benchmark datasets have been proposed for English (Calefato et al., 2017; Abdul-Mageed and Ungar, 2017; Akhtar et al., 2019, inter alia) showing a limited interest for other languages, such as German (Troiano et al., 2019), Chinese (Wang et al., 2018), Spanish (Navas-Loro and Rodr´ıguezDoncel, 2019), Italian (Barbieri et al., 2016; Sprugnoli, 2020), and multiple languages in shared tasks (Mohammad et al., 2018; Pontiki et al., 2016).a Moreover, they are usually collected either via hashtags and emojis for distant supervision (AbdulMageed and Ungar, 2017; Mohammad, 2012; Pak and Paroubek, 2010; Lamprinidis et al., 2021), or via very specific topics (Khanpour and Caragea, 2018; Chang et al., 2018; Nozza et al., 2017). The first causes noisy training data (Bing et al., 2015), the second results in highly domain-specific datasets. This paper presents FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions (Ekman, 1992): anger, fear, joy, sadness.1 To the best of our knowledge, no other Italian dataset with a broad topic and domain coverage for emotion and sentiment classification exists. Beyond releasing benchmark results o"
2021.wassa-1.8,P19-1391,0,0.0622739,"Missing"
2021.wassa-1.8,W16-6201,0,0.0293662,", or angry, and we can let others know of our emotional state through language. Thus, efficiently detecting emotion in text is essential for analyzing people’s position towards a topic. Product and service companies frequently use emotion and sentiment data to inform advertising campaigns and measure customer satisfaction (Ahmad et al., 2020). Emotions have a central role in a political campaigns, and political discourse in particular (Huguet Cabot et al., 2020). Emotion and sentiment recognition can also aid in the critical decision-making process of crisis management or emergency scenarios (Stowe et al., 2016; Desai et al., 2020). ∗ Both authors contributed equally to this research and are ordered alphabetically. 1 We focus on these emotions because they appear most frequently in text. 76 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 76–83 April 19, 2021. ©2021 Association for Computational Linguistics Example Emotion Pagliacci ammaestrati dal Grillo parlante di Pinocchio They are buffoons controlled by Pinocchio’s Jiminy Cricket Non ci sto dormendo la notte. #22Agosto #COVID19 This does not make me sleep at night. #22Augus"
C10-2052,J96-1002,0,0.0344118,"tion. We use a most-frequentsense baseline. In addition, we compare to the state-of-the-art systems for both types of granularity (O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). Their results show what has been achieved so far in terms of accuracy, and serve as a second measure for comparison beyond the baseline. 16995 16000 frequency 4 18000 10332 10000 8000 5414 6000 4000 1781 2000 1071 280 44 EXT BNF 0 LOC TMP DIR MNR PRP classes Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank. We use the MALLET implementation (McCallum, 2002) of a Maximum Entropy classifier (Berger et al., 1996) to construct our models. This classifier was also used by two state-of-the-art systems (Ye and Baldwin, 2007; Tratz and Hovy, 2009). For fine-grained PSD, we train a separate model for each preposition due to the high number of possible classes for each individual preposition. For coarse-grained PSD, we use a single model for all prepositions, because they all share the same classes. results of this task to the findings of O’Hara and Wiebe (2009). For the fine-grained task, we use data from the SemEval 2007 workshop (Litkowski and Hargraves, 2007), separate XML files for the 34 most frequent"
C10-2052,S07-1005,0,0.119838,"nk (PTB) (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features, such as semantic roles, significantly aid disambiguation. They caution that using collocations and neighboring words indiscriminately may yield high accuracy, but has the risk of overfitting. O’Hara and Wiebe (2009) show comparisons of various semantic repositories as labels for PSD approaches. They also provide some results for PTB-based coarse-grained senses, using a fiveword window for lexical and hypernym features in a decision tree classifier. SemEval 2007 (Litkowski and Hargraves, 2007) included a task for fine-grained PSD (more than 290 senses). The best participating system, that of Ye and Baldwin (2007), extracted part-ofspeech and WordNet (Fellbaum, 1998) features using a word window of seven words in a Maximum Entropy classifier. Tratz and Hovy (2009) present a higher-performing system using a set of 20 positions that are syntactically related to the preposition instead of a fixed window size. Though using a variety of different extraction methods, contexts, and feature words, none of these approaches explores the optimal configurations for PSD. 3 Theoretical Background"
C10-2052,J93-2004,0,0.0421825,"Missing"
C10-2052,W03-0411,0,0.528019,"Missing"
C10-2052,J09-2002,0,0.266408,"Missing"
C10-2052,P07-1005,0,0.0138999,"fitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative. We explore the different options for context and feature se1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454–462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity. Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results. The general outline we present can potentially be extended to other word classes and improve WSD in general. 2 Related Work Rudzicz and Mokhov (2003) use syntactic and lexical features from the go"
C10-2052,N09-1025,0,0.0144601,"ar where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative. We explore the different options for context and feature se1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454–462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity. Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results. The general outline we present can potentially be extended to other word classes and improve WSD in general. 2 Related Work Rudzicz and Mokhov (2003) use syntactic and lexical features from the governor and the preposition itself in coarse-grained PP classification wit"
C10-2052,P07-1096,0,0.0149883,"traction. Both O’Hara and Wiebe (2009) and Tratz and Hovy (2009) use constituency parsers to preprocess the data. However, parsing accuracy varies, 456 context and the problem of PP attachment ambiguity increases the likelihood of wrong extractions. This is especially troublesome in the present case, where we focus on prepositions.5 We use the MALT parser (Nivre et al., 2007), a state-of-theart dependency parser, to extract the governor and object. The alternative is a POS-based heuristics approach. The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al. (2007). We then use simple heuristics to locate the prepositions and their related words. In order to determine the governor in the absence of constituent phrases, we consider the possible governing noun, verb, and adjective. The object of the preposition is extracted as first noun phrase head to the right. This approach is faster than parsing, but has problems with longrange dependencies and fronting of the PP (e.g., the PP appearing earlier in the sentence than its governor). Context coarse fine 2-word window 91.6 80.4 3-word window 92.0 81.4 4-word window 91.6 79.8 5-word window 91.0 78.7 Governo"
C10-2052,W10-0801,0,0.0123844,"nd Hargraves, 2005), making them difficult to disambiguate. Preposition sense disambiguation (PSD) has many potential uses. For example, due to the relational nature of prepositions, disambiguating their senses can help with all-word sense disambiguation. In machine translation, different senses of the same English preposition often correspond to different translations in the foreign language. Thus, disambiguating prepositions correctly may help improve translation quality.1 Coarse-grained PSD can also be valuable for information extraction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy b"
C10-2052,N09-3017,1,0.56161,"traction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond sim"
C10-2052,S07-1051,0,0.654367,"ed PSD can also be valuable for information extraction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions t"
C10-2052,P98-1013,0,\N,Missing
C10-2052,C98-1013,0,\N,Missing
C14-1168,I13-1041,0,0.0122948,"Missing"
C14-1168,P11-1040,0,0.0233001,"Missing"
C14-1168,D07-1074,0,0.00844979,"tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentioned websites are."
C14-1168,P11-1061,0,0.03304,", 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter. This paper presents results using no in-domain labeled data that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching"
C14-1168,W10-2608,0,0.0382342,"Missing"
C14-1168,R13-1026,0,0.0929452,"oriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by lea"
C14-1168,N13-1037,0,0.0133621,"Missing"
C14-1168,W10-0713,0,0.0396482,"the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period of one week in August 2013 by searching for tweets that contain the string http and downloading the content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).13 We also require website and tweet to have at least one matching word that is not a stopword (as defined by the NLTK stopword list).14 Finally we restrict ourselves to pairs where the website"
C14-1168,P05-1045,0,0.0126701,"(2011), in particular features for word tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from bo"
C14-1168,I11-1100,0,0.0448938,"Missing"
C14-1168,fromreide-etal-2014-crowdsourcing,1,0.118987,"red-task 9 LDC2011T03. 10 http://www.clips.ua.ac.be/conll2003/ner/ 11 http://www.isi.edu/publications/licensed-sw/mace/ 5 1786 et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do not assume to have access to further development data. For both POS tagging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Rit"
C14-1168,P12-2047,1,0.432581,"that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazet"
C14-1168,P11-2008,0,0.147708,"Missing"
C14-1168,N13-1132,1,0.602877,"n our experiments, we use the SANCL shared task8 splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations as newswire training data for POS tagging.9 For NER, we use the CoNLL 2003 data sets of annotated newswire from the Reuters corpus.10 The in-domain training POS data comes from Gimpel et al. (2011), and the in-domain NER data comes from Finin et al. (2010) (F ININ -T RAIN). These data sets are added to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expertannotated Twitter data, but rely on crowdsourced annotations. We use MACE11 (Hovy et al., 2013) to resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We believe relying on crowdsourced annotations makes our set-up more robust across different samples of Twitter data. Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster 4 http://www.chokkan.org/software/crfsuite/ http://www.ark.cs.cmu.edu/TweetNLP/ 6 http://http://nlp.stanford.edu/software/CRF-NER.shtml 7 http://www.logos.ic.i.u-tokyo.a"
C14-1168,hovy-etal-2014-pos,1,0.895368,"Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning from additiona"
C14-1168,P07-1034,0,0.0634483,"cross different samples of tweets than existing approaches. We consider both the scenario where a small sample of labeled Twitter data is available, and the scenario where only newswire data is available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in th"
C14-1168,P11-1016,0,0.0902879,"Missing"
C14-1168,D07-1073,0,0.0341914,"sidering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentio"
C14-1168,D12-1127,0,0.0865368,"Missing"
C14-1168,N06-1020,0,0.0511572,"l as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised approach gives state-of-the-art performance across available Twitter POS and NER data sets. The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model bias by predicting tag sequences on small pools of unlabeled"
C14-1168,P09-1113,0,0.139295,"his is the hypothesis we explore in this paper. We present a semi-supervised learning method that does not require additional labeled in-domain data to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data. Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs provide a one-to-one map between an unlabeled instance and the source of supervision, making this This work is licensed under a Creati"
C14-1168,N13-1039,0,0.158057,"Missing"
C14-1168,petrov-etal-2012-universal,1,0.678346,"gging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data W"
C14-1168,N10-1021,0,0.0862976,"Missing"
C14-1168,D11-1141,0,0.836453,"ze named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This b"
C14-1168,P11-1097,0,0.0422016,"Missing"
C14-1168,Q13-1001,1,0.89785,"Missing"
C14-1168,W11-0328,0,0.0169234,"tures that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both domains, besides unlabeled target data, but the amount of l"
C14-1168,W03-0419,0,\N,Missing
C14-1168,D10-1002,0,\N,Missing
C14-3005,W06-1615,0,0.0791074,"tion or instance weighting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai,"
C14-3005,E03-1068,0,0.0275044,"taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and it is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank) contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy e"
C14-3005,I11-1100,0,0.0552098,"Missing"
C14-3005,N13-1132,1,0.821754,"2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters (Raykar and Yu, 2012). Bias in Ground Truth In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations. However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators, for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In this part of the tutorial, we discuss how to correct for the bias introduced by a"
C14-3005,P07-1034,0,0.0177436,"er et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2"
C14-3005,N10-1004,0,0.0227936,"ang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the population. As mentioned"
C14-3005,C14-1168,1,0.824408,"make P (X) similar to the distribution observed in the population. As mentioned, this will never solve the problem with unseen features, since you cannot up-weigh a null feature. Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase bias rather than decrease it. However, recent work has shown that semi-supervised learning can be combined with distant supervision and correct bias in cases where semi-supervised learning algorithms typically fail (Plank et al., 2014). In the tutorial we illustrate these different approaches to selection bias correction, with discriminative learning of POS taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and"
C14-3005,P07-1078,0,0.0272566,"has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distri"
C14-3005,D07-1111,0,0.0319065,"language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the p"
C14-3005,N06-1012,0,0.0210595,"ave a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a"
C14-3005,P10-1040,0,0.0127431,"ting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al.,"
D13-1144,W10-2919,0,0.0193462,"Missing"
D13-1144,P02-1034,0,0.13343,"e this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semanti"
D13-1144,D11-1096,0,0.22871,"k Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most"
D13-1144,I05-5003,0,0.0258954,"Missing"
D13-1144,W13-0907,1,0.285677,"ted to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair of sentences, so the VTK cannot be directly used by a kernel machine for classification. Instead, we generate 16 kernel values based for each pair on different parameter settings of the kernel, and feed these as features to a linear SVM. We finally look at the annotated Metaphor corpus by (Hovy et al., 2013). The dataset consists of sentences with specified target phrases. The task here is to classify the target use as literal or metaphorical. We focus on target phrases by upweighting walks that pass through target nodes. This is done by simply multiplying the corresponding entries in the adjacency matrix by a constant factor. 5 of SENNA embeddings (DSM), as well as Wordnet Affective Database-based (WNA) labels perform poorly (Carrillo de Albornoz et al., 2010), showing the importance of syntax for this particular problem. On the other hand, a syntactic tree kernel (SSTK) that ignores distributio"
D13-1144,P09-3004,0,0.0420388,"Missing"
D13-1144,J08-2003,0,0.0134264,"k for incorporating both syntax and semantics of sentence level constructions. 3. Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the simil"
D13-1144,P04-1043,0,0.0420077,"e matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can"
D13-1144,E06-1015,0,0.248167,"ns. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can receive a high ma"
D13-1144,P05-1015,0,0.113914,"ernel by the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert et al. (2011) as word representations. These embeddings provide low-dimensional vector representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments are lexically ambiguous (cf. “terribly entertaining” vs “terribly written”), so simple lexical approaches are not expected to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair o"
D13-1144,C08-1088,0,0.0122148,"sentence level constructions. 3. Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our ex"
D13-1144,W06-1603,0,0.0319862,"Missing"
D13-1144,W04-3219,0,0.105424,"Missing"
D13-1144,D11-1116,1,0.850183,"nce they involve products of more factors (generally all less than unity). The above kernel provides a similarity measure between any two pairs of dependency parse-trees. Depending on whether we consider directional relations in the parse tree, the edge set Ep changes, while the procedure for the kernel computation remains the same. Finally, to avoid larger trees yielding larger values for the kernel, we normalize the kernel by the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert et al. (2011) as word representations. These embeddings provide low-dimensional vector representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments are lexi"
D13-1144,U05-1023,0,0.0609032,"Missing"
D15-1301,W14-2602,1,0.930491,"f the author’s sentiment, but we do not expect annotators to perfectly reproduce these ratings based on text alone. Introduction Machine learning approaches have become the dominant paradigm for sentiment analysis since introduced by Pang et al. (2002). While these approaches produce good results, they need to be trained on sufficiently large labeled data sets. Since human annotation can be both slow and expensive, many studies use data with an inherent subjectivity indicator, such as movie or product reviews with user ratings (Dave et al., 2003; Pang and Lee, 2005; Snyder and Barzilay, 2007; Elming et al., 2014, i.a.). While it is a fair assumption that the rating expresses the author’s attitude towards the subject, it is less obvious to what extent the review text reflects this attitude, and hence what the relation between text and rating is. In this study, we ask (i) whether readers are able to infer the author’s numerical rating based on the author’s review text, We investigate (ii) by evaluating a linear regression model on author-labeled data. Sentiment analysis models supposedly emulate the cognitive process of text-based rating inference. The gap between human and model performance is interes"
D15-1301,N13-1132,1,0.834601,"ts contain the same number of reviews per rating, and a total of 1,319 reviews. The main implication of aligning the distributions, is that variance for both distributions will be identical, thus making the comparison more appropriate. 3.1 Model We use a linear least-squares model with L2 regularization (ridge regression) to reduce overfitting.2 L2 imposes a term α, which penalizes the parameters w of the model if they grow too large. Formally, w can be calculated by min kXw − yk2 2 + αkwk2 2 w We also experiment with incorporating a prior, 1 Aggregating with an item-response model like MACE (Hovy et al., 2013) results in worse estimates, since it requires nominal data. 2 Experimenting with support vector regression did not yield better results, so we chose the simpler model. 2528 to model the tendency of authors to use the extremes more than predicted by a Gaussian distribution. We use a beta distribution with shape parameters (0.8, 0.8). We use 10-fold cross validation for robust results, and 5-fold cross validation on each of the then training folds in order to determine the optimal α. We use bag-of-words features, including all unigrams appearing more than twice in the training data. 3 4 Trained"
D15-1301,P15-1073,1,0.840361,"the quality of the ratings. The Ann/Aut row indicates that even if the goal is to predict author ratings, it could still be advantageous to train on annotator-labeled data. 5 Related Work Since Pang et al. (2002) used author-labeled IMDb user reviews in their seminal study, author-labeled data has been used for a wide range of domains, like user-generated product reviews (Dave et al., 2003), restaurant reviews with several aspect ratings (Snyder and Barzilay, 2007), movie reviews from experienced film critics (Pang and Lee, 2005), business reviews (Hardt and Wulff, 2012; Elming et al., 2014; Hovy, 2015), and many more. Pang and Lee (2005) also argue that it is unreasonable to expect a learning algorithm to predict ratings on a fine-grained scale if humans are not able to do so. To test this, they presented pairs of movie reviews from a single author rated on a 10-point Likert scale to two subjects (the authors themselves). Subjects had to decide whether one review was more, less, or equally positive than the other. Subjects correctly discerned reviews separated by more than three steps, but accuracy dropped when relative difference decreases. Pang and Lee (2005) also identify three obstacles"
D15-1301,N07-1038,0,0.163344,"sedly capture the essence of the author’s sentiment, but we do not expect annotators to perfectly reproduce these ratings based on text alone. Introduction Machine learning approaches have become the dominant paradigm for sentiment analysis since introduced by Pang et al. (2002). While these approaches produce good results, they need to be trained on sufficiently large labeled data sets. Since human annotation can be both slow and expensive, many studies use data with an inherent subjectivity indicator, such as movie or product reviews with user ratings (Dave et al., 2003; Pang and Lee, 2005; Snyder and Barzilay, 2007; Elming et al., 2014, i.a.). While it is a fair assumption that the rating expresses the author’s attitude towards the subject, it is less obvious to what extent the review text reflects this attitude, and hence what the relation between text and rating is. In this study, we ask (i) whether readers are able to infer the author’s numerical rating based on the author’s review text, We investigate (ii) by evaluating a linear regression model on author-labeled data. Sentiment analysis models supposedly emulate the cognitive process of text-based rating inference. The gap between human and model p"
D15-1301,D13-1170,0,0.0114332,"the guess is far off in absolute terms. Furthermore, single-author reviews dilute the effects of the three aforementioned obstacles. Inconsistencies within a single author are undoubtedly smaller than inconsistencies between multiple authors. Singleauthor use also affects lack of calibration, since subjects can adjust to the writing style of one author better than that of several. Finally, we expect experienced authors to be less prone to producing reviews that do not support their ratings. Annotator labels are typically used for phraselevel semantics (Wilson et al., 2005; Wiebe et al., 2005; Socher et al., 2013). Alternatively, labels can be induced from salient sentiment-related features like emoticons (Pak and Paroubek, 2010; Go et al., 2009; Tang et al., 2014) or hashtags (Kouloumpis et al., 2011). Often, the label source tends to be a matter of convenience, rather than theoretical reflection. The lack of considerations regarding potential differences between author and annotator labels implies that these are often perceived as ontologically equivalent. We do not believe this to be the case. 6 Discussion Human rating inference (i) We observe some interesting differences between the three rating di"
D15-1301,P14-1146,0,0.0271498,"a single author are undoubtedly smaller than inconsistencies between multiple authors. Singleauthor use also affects lack of calibration, since subjects can adjust to the writing style of one author better than that of several. Finally, we expect experienced authors to be less prone to producing reviews that do not support their ratings. Annotator labels are typically used for phraselevel semantics (Wilson et al., 2005; Wiebe et al., 2005; Socher et al., 2013). Alternatively, labels can be induced from salient sentiment-related features like emoticons (Pak and Paroubek, 2010; Go et al., 2009; Tang et al., 2014) or hashtags (Kouloumpis et al., 2011). Often, the label source tends to be a matter of convenience, rather than theoretical reflection. The lack of considerations regarding potential differences between author and annotator labels implies that these are often perceived as ontologically equivalent. We do not believe this to be the case. 6 Discussion Human rating inference (i) We observe some interesting differences between the three rating distributions. First, the “flaps” in the extreme 2530 ratings in the author ratings are not present in the annotator rating distributions. This phenomenon m"
D15-1301,H05-1044,0,0.0167784,"ill be correctly identified, even though the guess is far off in absolute terms. Furthermore, single-author reviews dilute the effects of the three aforementioned obstacles. Inconsistencies within a single author are undoubtedly smaller than inconsistencies between multiple authors. Singleauthor use also affects lack of calibration, since subjects can adjust to the writing style of one author better than that of several. Finally, we expect experienced authors to be less prone to producing reviews that do not support their ratings. Annotator labels are typically used for phraselevel semantics (Wilson et al., 2005; Wiebe et al., 2005; Socher et al., 2013). Alternatively, labels can be induced from salient sentiment-related features like emoticons (Pak and Paroubek, 2010; Go et al., 2009; Tang et al., 2014) or hashtags (Kouloumpis et al., 2011). Often, the label source tends to be a matter of convenience, rather than theoretical reflection. The lack of considerations regarding potential differences between author and annotator labels implies that these are often perceived as ontologically equivalent. We do not believe this to be the case. 6 Discussion Human rating inference (i) We observe some interesti"
D15-1301,P03-1054,0,0.00489166,"task. 0.30 Author rating distribution Crowdsource annotator rating distribution Expert annotator rating distribution (subset only) Aligned distribution Relative frequency 0.25 0.20 0.15 0.10 0.05 0.00 1 2 3 4 5 Rating 6 7 8 9 10 Figure 1. Rating distributions for authors, crowdsourced, and trained annotator ratings. Dots indicate aligned distribution. RMSE To rule out that the lack of any syntactic information (which human annotators use) disadvantages the model, we also experimented with including dependency triples (dobj and nsubj, the most frequent dependencies) using the Stanford Parser (Klein and Manning, 2003). However, performance did not improve, so due to limited space, we did not further explore this option. aut - tr 0.85 0.83 0.96 1.05 1.30 1.44 tr - cs 0.94 0.80 0.71 1.07 1.01 1.48 Table 1. Pairwise comparisons between author (aut), trained (tr), and crowdsource (cs) ratings. Table 1 compares the different rating sources. We find a higher correlation and lower error between the two sets of annotator ratings than between the author ratings and any of the annotator ratings. However, when comparing the individual rating correlations, author ratings are highly correlated with trained, but not wit"
D15-1301,pak-paroubek-2010-twitter,0,0.0423898,"tioned obstacles. Inconsistencies within a single author are undoubtedly smaller than inconsistencies between multiple authors. Singleauthor use also affects lack of calibration, since subjects can adjust to the writing style of one author better than that of several. Finally, we expect experienced authors to be less prone to producing reviews that do not support their ratings. Annotator labels are typically used for phraselevel semantics (Wilson et al., 2005; Wiebe et al., 2005; Socher et al., 2013). Alternatively, labels can be induced from salient sentiment-related features like emoticons (Pak and Paroubek, 2010; Go et al., 2009; Tang et al., 2014) or hashtags (Kouloumpis et al., 2011). Often, the label source tends to be a matter of convenience, rather than theoretical reflection. The lack of considerations regarding potential differences between author and annotator labels implies that these are often perceived as ontologically equivalent. We do not believe this to be the case. 6 Discussion Human rating inference (i) We observe some interesting differences between the three rating distributions. First, the “flaps” in the extreme 2530 ratings in the author ratings are not present in the annotator ra"
D15-1301,P05-1015,0,0.466407,"Author ratings supposedly capture the essence of the author’s sentiment, but we do not expect annotators to perfectly reproduce these ratings based on text alone. Introduction Machine learning approaches have become the dominant paradigm for sentiment analysis since introduced by Pang et al. (2002). While these approaches produce good results, they need to be trained on sufficiently large labeled data sets. Since human annotation can be both slow and expensive, many studies use data with an inherent subjectivity indicator, such as movie or product reviews with user ratings (Dave et al., 2003; Pang and Lee, 2005; Snyder and Barzilay, 2007; Elming et al., 2014, i.a.). While it is a fair assumption that the rating expresses the author’s attitude towards the subject, it is less obvious to what extent the review text reflects this attitude, and hence what the relation between text and rating is. In this study, we ask (i) whether readers are able to infer the author’s numerical rating based on the author’s review text, We investigate (ii) by evaluating a linear regression model on author-labeled data. Sentiment analysis models supposedly emulate the cognitive process of text-based rating inference. The ga"
D15-1301,W02-1011,0,0.0353398,"a set of user-generated movie reviews with author ratings and collect both crowdsourced annotator ratings and trained annotator ratings. This setup allows us to evaluate the reproducibility of ratings for both humans and models. We address (i) by comparing author ratings to crowdsourced and trained annotator ratings. Author ratings supposedly capture the essence of the author’s sentiment, but we do not expect annotators to perfectly reproduce these ratings based on text alone. Introduction Machine learning approaches have become the dominant paradigm for sentiment analysis since introduced by Pang et al. (2002). While these approaches produce good results, they need to be trained on sufficiently large labeled data sets. Since human annotation can be both slow and expensive, many studies use data with an inherent subjectivity indicator, such as movie or product reviews with user ratings (Dave et al., 2003; Pang and Lee, 2005; Snyder and Barzilay, 2007; Elming et al., 2014, i.a.). While it is a fair assumption that the rating expresses the author’s attitude towards the subject, it is less obvious to what extent the review text reflects this attitude, and hence what the relation between text and rating"
D18-1068,E17-1005,0,0.0290131,"prediction of popularity. Their work looks at the popularity of online petitions, but the methodology applies to our subject as well, and ties in with the approaches taken in this project. Benton et al. (2017) caution that in order to evaluate MTL results properly, we need to take the number of parameters into account. Our results to some extent support this finding, by showing that a simpler linear model can fare equally well on the task. The choice of auxiliary tasks greatly influences the performance of MTL architectures, prompting several recent investigations into the selection process (Alonso and Plank, 2017; Bingel and Søgaard, 2017). However, it is still unclear whether these tasks serve as mere regularizers, or whether they can also impart some additional information. Tables 1 and 2 report accuracy for logistic regression and neural classifiers. We also give the best score from Hardt and Rambow (2017) for comparison purposes (note, though, that the data sets are not identical and can therefore not be directly compared). We observe a substantial improvement over the baseline GRU when incorporating the pre-trained embeddings and both auxiliary tasks. It seems that pretrained embeddings and MTL a"
D18-1068,E17-1015,1,0.92449,"neural net, the character n-gram model has higher performance, competing with the best MTL result. This finding is in line with the results from Zhang et al. (2015). Our results indicate that MTL can indeed provide the tools to implement prediction processes that involve expectations about the future. Given the successful integration of two auxiliary tasks, we see this as a promising starting point for future research. However, the performance parity with the character model underscores the fact that simple model architectures still have a place. Our findings, in line with other current work (Benton et al., 2017), shine light on the question of auxiliary task selection and their interaction, and highlight that MTL results should be rigorously tested. A good predictive model is a powerful diagnostic tool for editors, allowing them to select proposed headlines. However, journalism is a creative production process, so detection is only part of the application. We also want to be able to give strategic advice to headline writers. To this end, we report an analysis of common n-gram features in the word-based logistic regression model, that provide some insights into successful headline patterns. Figure 1:"
D18-1068,E17-2026,0,0.0151342,"y. Their work looks at the popularity of online petitions, but the methodology applies to our subject as well, and ties in with the approaches taken in this project. Benton et al. (2017) caution that in order to evaluate MTL results properly, we need to take the number of parameters into account. Our results to some extent support this finding, by showing that a simpler linear model can fare equally well on the task. The choice of auxiliary tasks greatly influences the performance of MTL architectures, prompting several recent investigations into the selection process (Alonso and Plank, 2017; Bingel and Søgaard, 2017). However, it is still unclear whether these tasks serve as mere regularizers, or whether they can also impart some additional information. Tables 1 and 2 report accuracy for logistic regression and neural classifiers. We also give the best score from Hardt and Rambow (2017) for comparison purposes (note, though, that the data sets are not identical and can therefore not be directly compared). We observe a substantial improvement over the baseline GRU when incorporating the pre-trained embeddings and both auxiliary tasks. It seems that pretrained embeddings and MTL act at least partly as regul"
D18-1068,W17-4202,1,0.913248,"sity dh.msc@cbs.dk dirk.hovy@unibocconi.it sot.lampr@gmail.com Abstract on the article text be better at predicting clicks? After all, the choice to click on an article must be based on the headline alone – the article is only seen after the clicking decision is made. Hardt and Rambow speculate that “it is possible that the headline on its own gives readers a lot of semantic information which we are not capturing with our features, but which the whole article does provide. So human readers can “imagine” the article before they read it and implicitly base their behavior on their expectation.” (Hardt and Rambow, 2017) In other words, readers are able to anticipate the contents of an article in advance from a headline, because of the linguistic and world knowledge that they bring to bear when assessing the headline. If we can incorporate this “future” knowledge into a prediction model, we are likely to improve performance. We test this hypothesis by defining ways to model aspects of the lexical, structural, and topical knowledge of human news readers: Newspapers need to attract readers with headlines, anticipating their readers’ preferences. These preferences rely on topical, structural, and lexical factors"
D18-1068,N16-1062,0,0.013917,"m 2 to 6 in all experiments. 2. word unigrams: tfidf scores for all word unigrams Figure 2: Representation of a single timestep t for a pair of forward-backward units on layer k where hkt−1 is the previous hidden state. 3. word bigrams: tfidf scores for all word bigrams Auxiliary Tasks In our setup, we use two auxiliary tasks: GRU Neural Network While the task is classification, which could be done with a feed-forward model, we want a sequential architecture, so that we can incorporate POS tagging as an auxiliary task, adding POS output at each time step. Based on good results in recent work (Lee and Dernoncourt, 2016), (Liu et al., 2016), we choose a Recurrent Neural Network architecture and after 1. POS tagging: we include POS tagging using the DSL dataset on the first recurrent layer of the GRU. 2. Section prediction: we include classification into one of the 227 sections of the Jyllands661 coefficients in the logistic regression model. For each unigram we provide a translation (if needed) and a comment. We classify several unigrams as Deictic-reference. This follows Blom and Hansen (2015), who suggest that headline ”clickbait” often relies on forward-looking expressions, such as ”This”, as in, e.g., ”Th"
D18-1068,N18-2038,1,0.817824,"layer k consists of two sets of units, labeled f w and bw that process the sequence forwards and backwards respectively, so that information from the whole sequence is available on every timestep t. The two directions’ activations are concatenated and fed to a fully-connected softmax (for multi-class classification) or sigmoid layer (for binary classification) to get the output probability ytk of the task associated with layer k. So that higher level tasks can benefit, we embed the output probabilities using the fully connected label embedding LE layer, a technique used on similar scenarios (Rønning et al., 2018). The embedded label gets concatenated with the GRU output to get the activation akt that gets fed in the next layer, or the final fully connected prediction layer, as presented in figure 2. In the sequential auxiliary task, i.e. POS tagging, this is done for every timestep, while for the classification tasks the prediction is made on the final timestep. For regularization, we apply dropout on every layer of our network. sources over a period from 1990 to 2010. The corpus has been automatically annotated for part of speech and lemmatization, and we use this for our POS tagging task. We also do"
D18-1068,P18-2030,0,0.0686069,"tion last. Additionally, we decay the learning rate by a factor of 0.9 after each epoch. While this is not common with adaptive methods such as Adam, it performed better. We stop training if the accuracy on the development set stops improving. 4 6 Results Prediction of news headline popularity is an increasingly important problem, as news consumption has moved online. The insights and models described here might well be applicable to related problems of interest: for example, Balakrishnan and Parekh (2014) and Jaidka et al. (2018) study the problem of predicting clicks on email subject lines. Subramanian et al. (2018) show that a regression-based multitask approach can increase performance for the classification prediction of popularity. Their work looks at the popularity of online petitions, but the methodology applies to our subject as well, and ties in with the approaches taken in this project. Benton et al. (2017) caution that in order to evaluate MTL results properly, we need to take the number of parameters into account. Our results to some extent support this finding, by showing that a simpler linear model can fare equally well on the task. The choice of auxiliary tasks greatly influences the perfor"
D18-1070,W16-1609,0,0.0786192,"ted in the effect of homophily-inducing retrofitting on authorattribute prediction. In order to evaluate the effect, we compare the performance of author representations based on linguistic input to the performance of the same representation retrofitted to the author attribute class in question. In this section, we outline the details for the different steps. 3.1 Linguistic author representations We train Doc2Vec, a paragraph2vec (Le and Mikolov, 2014) implementation, on the corpus, inducing a 98K-by-300 matrix D, where each row represents an author. We follow the parametrization suggested in Lau and Baldwin (2016), setting the window size to 15, minimum word-frequency to 10, negative samples to 5, downsampling rate to 0.00001, and run for 1000 iterations. We use the resulting author embeddings as input to the authorattribute classifier (see 3.4). We induce the author embeddings over the entire corpus of 98K authors, without recurrence to age or gender information. As comparison, we also create a bag-of-words (BOW) representation with the same dimensionality. We use χ2 as selection criterion to find the top 300 words in the training data, separately for both age and gender classification. Contributions"
D18-1070,N15-1184,0,0.0792141,"Missing"
D18-1070,N13-1092,0,0.0199648,"Missing"
D18-1070,D17-1119,0,0.0923373,"erated content. Classifying user attributes such as age and gender is useful for a number of applications both in the public sector, where it can support the investigation of crime (in forensic linguistics) or the determination of social policies, and in the private sector, where companies want to profile a potential consumer market, targeting communication strategies and advertising to specific communities. Furthermore, recent work in NLP has shown that incorporating author attributes in various NLP tasks can also improve performance (Volkova et al., 2013; Hovy, 2015; Hovy and Søgaard, 2015; Lynn et al., 2017; Preotiuc-Pietro et al., 2016). In these tasks, authors are typically represented via their linguistic profiles, i.e., information avail671 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 671–677 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics share the same gender or age therefore get more similar vector representations (see section 3.2). This effectively increases class-separability and can thereby improve classification performance. We also experiment with a trade-off parameter α, which control"
D18-1070,D15-1162,0,0.0127155,"ender of the author, and if the review is at least 10 tokens long (shorter reviews tend to be mistokenized URLs or replies). We aggregate the reviews by users, so that each instance is a collection of texts’ from a unique user. This leaves us with 98,608 individual users, and about 8M words (roughly 80 words per instance). For each user, we use the age (discretized by decade) and gender (self-stated as binary, and augmented by Hovy et al. (2015) based on the users’ first name) as target variables. We minimally preprocess the text data, collapsing all numbers into 0s, and tokenizing via spacy (Honnibal and Johnson, 2015). 3 In our experiments, we are interested in the effect of homophily-inducing retrofitting on authorattribute prediction. In order to evaluate the effect, we compare the performance of author representations based on linguistic input to the performance of the same representation retrofitted to the author attribute class in question. In this section, we outline the details for the different steps. 3.1 Linguistic author representations We train Doc2Vec, a paragraph2vec (Le and Mikolov, 2014) implementation, on the corpus, inducing a 98K-by-300 matrix D, where each row represents an author. We fo"
D18-1070,P15-1073,1,0.8876,"t with the pervasive use of user-generated content. Classifying user attributes such as age and gender is useful for a number of applications both in the public sector, where it can support the investigation of crime (in forensic linguistics) or the determination of social policies, and in the private sector, where companies want to profile a potential consumer market, targeting communication strategies and advertising to specific communities. Furthermore, recent work in NLP has shown that incorporating author attributes in various NLP tasks can also improve performance (Volkova et al., 2013; Hovy, 2015; Hovy and Søgaard, 2015; Lynn et al., 2017; Preotiuc-Pietro et al., 2016). In these tasks, authors are typically represented via their linguistic profiles, i.e., information avail671 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 671–677 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics share the same gender or age therefore get more similar vector representations (see section 3.2). This effectively increases class-separability and can thereby improve classification performance. We also experiment"
D18-1070,D18-1469,1,0.884979,"Missing"
D18-1070,P15-2079,1,0.855577,"ervasive use of user-generated content. Classifying user attributes such as age and gender is useful for a number of applications both in the public sector, where it can support the investigation of crime (in forensic linguistics) or the determination of social policies, and in the private sector, where companies want to profile a potential consumer market, targeting communication strategies and advertising to specific communities. Furthermore, recent work in NLP has shown that incorporating author attributes in various NLP tasks can also improve performance (Volkova et al., 2013; Hovy, 2015; Hovy and Søgaard, 2015; Lynn et al., 2017; Preotiuc-Pietro et al., 2016). In these tasks, authors are typically represented via their linguistic profiles, i.e., information avail671 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 671–677 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics share the same gender or age therefore get more similar vector representations (see section 3.2). This effectively increases class-separability and can thereby improve classification performance. We also experiment with a trade-off paramet"
D18-1070,P11-1077,0,0.113578,"ct able in the text. This includes both word-based features as well as continuous representations (embeddings). Generally, linguistic features are divided into content-related and strictly stylistic features. While the first can be effectively represented by (n-grams of) words which capture the topic and meaning of a text, the second ones focus on the use of function words, expressions, pronouns, syntactic structures, etc. There is evidence in the literature that content-related text characteristics are more effective than stylistic features for gender and age prediction (Fatima et al., 2017; Rosenthal and McKeown, 2011). This effect is the consequence of a non-linguistic auto-selection process known as homophily: people within in a demographic group tend to be more similar to each other than to other groups, and subjects belonging to different groups are therefore naturally more prone to discuss different topics. Despite the large amount of available social media data (in April 2018, Facebook had more than two billion active users, YouTube and WhatsApp each one-and-a-half billion, and Twitter 330 million, see statista.com), we often encounter scenarios with limited availability of ground-truth user attribute"
D18-1070,D13-1187,0,0.09326,"ming ever more relevant with the pervasive use of user-generated content. Classifying user attributes such as age and gender is useful for a number of applications both in the public sector, where it can support the investigation of crime (in forensic linguistics) or the determination of social policies, and in the private sector, where companies want to profile a potential consumer market, targeting communication strategies and advertising to specific communities. Furthermore, recent work in NLP has shown that incorporating author attributes in various NLP tasks can also improve performance (Volkova et al., 2013; Hovy, 2015; Hovy and Søgaard, 2015; Lynn et al., 2017; Preotiuc-Pietro et al., 2016). In these tasks, authors are typically represented via their linguistic profiles, i.e., information avail671 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 671–677 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics share the same gender or age therefore get more similar vector representations (see section 3.2). This effectively increases class-separability and can thereby improve classification performance. We also"
D18-1070,L16-1478,0,0.0158376,"st studies to apply statistical NLP techniques to author attribute prediction are Koppel et al. (2002); Argamon et al. (2003), using the British National Corpus (BNC). The same authors also introduced the use of blogs as data source (Koppel et al., 2006). In recent years, predicting socio-demographic variables from text has seen increased interest, with several corpora for the classification of age and gender, covering various languages, such as English (Schler et al., 2006; Rosenthal and McKeown, 2011), Spanish, French, German, Dutch (Company and Wanner, 2015), Greek (Mikros, 2012), Chinese (Zhang et al., 2016), and Vietnamese (Pham et al., 2009). A big contribution in this field, however, was the shared tasks of the PAN workshops (Rangel et al., 2013, 2014, 2015, 2016). Research has identified a variety of linguistic features, ranging from “stylistic features with n-grams models, parts-of-speech, collocations, LDA, different readability indexes, vocabulary richness, correctness or verbosity” (Rangel et al., 2016). However, none of these papers used demographic information directly in the author representations. Closest to our method are Lopez-Monroy et al. (2013), who propose the use of second-orde"
D18-1469,P14-2134,0,0.11476,"Language Processing, pages 4383–4394 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tation learning especially useful for the study of regional language variation along several linguistic dimensions. We use a corpus of 16.8 million anonymous German online posts, cast cities as document labels, and induce document embeddings for these cities via Doc2Vec (Le and Mikolov, 2014). We first show that the resulting city embeddings capture regional linguistic variation at a more finegrained, continuous regional distinction than previous approaches (Bamman et al., 2014; Östling and Tiedemann, 2017), which operated at a state or language level.1 We also show that the embeddings can serve as input to a geolocation task, outperforming a bag-of-words model, and producing competitive results. However, such representations are susceptible to linguistic data bias, ignore geographic factors, and are hard to evaluate with respect to their fit with existing linguistic distinctions. We address these problems by including geographic information via retrofitting (Faruqui et al., 2015; Hovy and Fornaciari, 2018): we use administrative region boundaries to modify the city"
D18-1469,D18-1070,1,0.933417,"grained, continuous regional distinction than previous approaches (Bamman et al., 2014; Östling and Tiedemann, 2017), which operated at a state or language level.1 We also show that the embeddings can serve as input to a geolocation task, outperforming a bag-of-words model, and producing competitive results. However, such representations are susceptible to linguistic data bias, ignore geographic factors, and are hard to evaluate with respect to their fit with existing linguistic distinctions. We address these problems by including geographic information via retrofitting (Faruqui et al., 2015; Hovy and Fornaciari, 2018): we use administrative region boundaries to modify the city embeddings, and evaluate the resulting vectors in a clustering approach to discover larger dialect regions. In contrast to most dialectometric approaches (Nerbonne et al., 1999; Proki´c and Nerbonne, 2008), and in line with common NLP practice (Doyle, 2014; Grieve, 2016; Huang et al., 2016; Rahimi et al., 2017a), we also evaluate the clustered dialect areas quantitatively. Rather than testing the geographic extent of individual words against known dialect areas (Doyle, 2014), we compare the match of entire geographic regions to a rec"
D18-1469,W17-1202,0,0.0242746,"f socio-economic attributes with linguistic features, including regional distribution of lexical and phonological differences (Eisenstein et al., 2010; Doyle, 2014; Bamman et al., 2014), syntactic variation (Johannsen et al., 2015), diachronic variation (Danescu-Niculescu-Mizil et al., 2013; Kulkarni et al., 2015; Hamilton et al., 2016), and correlation with socio-demographic attributes (Eisenstein et al., 2011; Eisenstein, 2015). Other have further explored regional variation on social media, and showed the prevalence of regional lexical variants (Hovy et al., 2015; Hovy and Johannsen, 2016; Donoso and Sánchez, 2017). Several works include quantitative comparisons to measure the empirical fit of their findings (Peirsman et al., 2010; Han et al., 2014; Huang et al., 2016; Grieve, 2016; Kulkarni et al., 2016), albeit not on entire existing dialect maps. The use of representation learning is new and relatively limited, especially given its prevalence in other areas of NLP. Bamman et al. (2014) have shown how regional meaning differences can be learned from Twitter via distributed word representations between US states, but not for individual cities. More recently, Kulkarni et al. (2016); Rahimi et al. (2017a"
D18-1469,L16-1477,1,0.858013,"examine the correlation of socio-economic attributes with linguistic features, including regional distribution of lexical and phonological differences (Eisenstein et al., 2010; Doyle, 2014; Bamman et al., 2014), syntactic variation (Johannsen et al., 2015), diachronic variation (Danescu-Niculescu-Mizil et al., 2013; Kulkarni et al., 2015; Hamilton et al., 2016), and correlation with socio-demographic attributes (Eisenstein et al., 2011; Eisenstein, 2015). Other have further explored regional variation on social media, and showed the prevalence of regional lexical variants (Hovy et al., 2015; Hovy and Johannsen, 2016; Donoso and Sánchez, 2017). Several works include quantitative comparisons to measure the empirical fit of their findings (Peirsman et al., 2010; Han et al., 2014; Huang et al., 2016; Grieve, 2016; Kulkarni et al., 2016), albeit not on entire existing dialect maps. The use of representation learning is new and relatively limited, especially given its prevalence in other areas of NLP. Bamman et al. (2014) have shown how regional meaning differences can be learned from Twitter via distributed word representations between US states, but not for individual cities. More recently, Kulkarni et al. ("
D18-1469,E14-1011,0,0.343172,"tations are susceptible to linguistic data bias, ignore geographic factors, and are hard to evaluate with respect to their fit with existing linguistic distinctions. We address these problems by including geographic information via retrofitting (Faruqui et al., 2015; Hovy and Fornaciari, 2018): we use administrative region boundaries to modify the city embeddings, and evaluate the resulting vectors in a clustering approach to discover larger dialect regions. In contrast to most dialectometric approaches (Nerbonne et al., 1999; Proki´c and Nerbonne, 2008), and in line with common NLP practice (Doyle, 2014; Grieve, 2016; Huang et al., 2016; Rahimi et al., 2017a), we also evaluate the clustered dialect areas quantitatively. Rather than testing the geographic extent of individual words against known dialect areas (Doyle, 2014), we compare the match of entire geographic regions to a recent German dialect map (Lameli, 2013). We use cluster evaluation metrics to measure how well our clusters match the known dialect regions. The results show that our method automatically captures existing (manually determined) dialect distinctions well, and even goes beyond them in that it also allows for a more fine"
D18-1469,N13-1037,0,0.0139324,"lk to other users within a 10km-radius around them. The app was first published in 2014, and has seen substantial growth since its beginning. It has several million users in the Germanspeaking area (GSA), and is expanding to France, Italy, Scandinavia, Spain, and lately the United States. Users can post and answer to posts within the radius around their own current location. All users are anonymous. Answers to an initial post are organized in threads. The vast majority of posts in Jodel are written in standard German, but since it is conceptually spoken langauge (Koch and Oesterreicher, 1985; Eisenstein, 2013), regional and dialectal forms are common, especially in Switzerland, Austria, and rural areas in Southern Germany. The data therefore reflects current 4384 2 https://jodel.com/ developments in language dynamics to mark regionality (Purschke, 2018). We used a publicly available API to collect data between April and June 2017 from 123 initial locations: 79 German cities with a population over 100k people, all 17 major cities in Austria (“Mittel- und Oberzentren”), and 27 cities in Switzerland (the 26 cantonal capitals plus Lugano in the very south of the Italian-speaking area). Due to the 10km"
D18-1469,D10-1124,0,0.633767,"Missing"
D18-1469,P11-1137,0,0.0156601,"comparison, they rarely quantitatively evaluate against them, as we do. Recently, NLP has seen increased interest in computational sociolinguistics (Nguyen et al., 2016). These works examine the correlation of socio-economic attributes with linguistic features, including regional distribution of lexical and phonological differences (Eisenstein et al., 2010; Doyle, 2014; Bamman et al., 2014), syntactic variation (Johannsen et al., 2015), diachronic variation (Danescu-Niculescu-Mizil et al., 2013; Kulkarni et al., 2015; Hamilton et al., 2016), and correlation with socio-demographic attributes (Eisenstein et al., 2011; Eisenstein, 2015). Other have further explored regional variation on social media, and showed the prevalence of regional lexical variants (Hovy et al., 2015; Hovy and Johannsen, 2016; Donoso and Sánchez, 2017). Several works include quantitative comparisons to measure the empirical fit of their findings (Peirsman et al., 2010; Han et al., 2014; Huang et al., 2016; Grieve, 2016; Kulkarni et al., 2016), albeit not on entire existing dialect maps. The use of representation learning is new and relatively limited, especially given its prevalence in other areas of NLP. Bamman et al. (2014) have sh"
D18-1469,N15-1184,0,0.0688946,"Missing"
D18-1469,P16-1141,0,0.0378059,"English (Jones, 2015). While these papers rely on existing dialect maps for comparison, they rarely quantitatively evaluate against them, as we do. Recently, NLP has seen increased interest in computational sociolinguistics (Nguyen et al., 2016). These works examine the correlation of socio-economic attributes with linguistic features, including regional distribution of lexical and phonological differences (Eisenstein et al., 2010; Doyle, 2014; Bamman et al., 2014), syntactic variation (Johannsen et al., 2015), diachronic variation (Danescu-Niculescu-Mizil et al., 2013; Kulkarni et al., 2015; Hamilton et al., 2016), and correlation with socio-demographic attributes (Eisenstein et al., 2011; Eisenstein, 2015). Other have further explored regional variation on social media, and showed the prevalence of regional lexical variants (Hovy et al., 2015; Hovy and Johannsen, 2016; Donoso and Sánchez, 2017). Several works include quantitative comparisons to measure the empirical fit of their findings (Peirsman et al., 2010; Han et al., 2014; Huang et al., 2016; Grieve, 2016; Kulkarni et al., 2016), albeit not on entire existing dialect maps. The use of representation learning is new and relatively limited, especia"
D18-1469,K15-1011,1,0.852707,"s (Grieve et al., 2011; Huang et al., 2016) and the regional differentiation of African American Vernacular English (Jones, 2015). While these papers rely on existing dialect maps for comparison, they rarely quantitatively evaluate against them, as we do. Recently, NLP has seen increased interest in computational sociolinguistics (Nguyen et al., 2016). These works examine the correlation of socio-economic attributes with linguistic features, including regional distribution of lexical and phonological differences (Eisenstein et al., 2010; Doyle, 2014; Bamman et al., 2014), syntactic variation (Johannsen et al., 2015), diachronic variation (Danescu-Niculescu-Mizil et al., 2013; Kulkarni et al., 2015; Hamilton et al., 2016), and correlation with socio-demographic attributes (Eisenstein et al., 2011; Eisenstein, 2015). Other have further explored regional variation on social media, and showed the prevalence of regional lexical variants (Hovy et al., 2015; Hovy and Johannsen, 2016; Donoso and Sánchez, 2017). Several works include quantitative comparisons to measure the empirical fit of their findings (Peirsman et al., 2010; Han et al., 2014; Huang et al., 2016; Grieve, 2016; Kulkarni et al., 2016), albeit not"
D18-1469,W16-1609,0,0.0459518,". The objective is to maximize the log probability of the prediction, y = arg max log W N X log(p(wi |k)) i=1 where k is a city, and W = wi...N a sequence of N randomly sampled words from the thread (see Figure 1 for a schematic representation). During training, semantically similar words end up closer together in vector space, as do words “similar” to a particular city, and cities that are linguistically similar to each other. Due to the nature of our task, we unfortunately do not have gold data (i.e., verified cluster labels) to tune parameters.We therefore follow the settings described in (Lau and Baldwin, 2016) for the parameters, and set the vector dimensions to 300, window size to 15, minimum frequency to 10, negative samples to 5, downsampling to 0.00001, and run for 10 iterations. 3.2 Visualization In order to examine whether the city embeddings capture the continuous nature of dialects, we visualize them. If our assumption holds, we expect to see gradual continuous change between cities and regions. We use non-negative matrix factorization (NMF) on the 300-dimensional city representation matrix to find the first three principal components, normalize them each to values 0.0–1.0 and interpret tho"
D18-1469,W13-3512,0,0.0378765,"tructed discrete feature vectors. However, the success of such approaches depends on precise prior knowledge of variation features (Lameli, 2013). Distributed representations, as unsupervised methods, can complement these methods by capturing similarities between words and documents (here: cities) along various latent dimensions, including syntactic, semantic, and pragmatic aspects. These representations are therefore more compact, less susceptible to data sparsity than latent variable models, and allow us to represent a large number of possible clusters than featurebased representations (cf. Luong et al. (2013)). These properties also allow us to measure similarities on a continuous scale, which makes represen4383 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4383–4394 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tation learning especially useful for the study of regional language variation along several linguistic dimensions. We use a corpus of 16.8 million anonymous German online posts, cast cities as document labels, and induce document embeddings for these cities via Doc2Vec (Le and Mikolov, 201"
D18-1469,D15-1238,0,0.0146496,"hat representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dialects are not static discrete entities, but exist along a continuum in most languages. Variational linguistics and dialectology typically discretize this continuum by using a set of preselected features (Trudgill, 2000), often including outdated vocabulary. The resulting dialect areas are highly accurate, but extremely timeconsu"
D18-1469,W97-1102,0,0.576508,"static discrete entities, but exist along a continuum in most languages. Variational linguistics and dialectology typically discretize this continuum by using a set of preselected features (Trudgill, 2000), often including outdated vocabulary. The resulting dialect areas are highly accurate, but extremely timeconsuming to construct and inflexible (the largest and to date most comprehensive evaluation of German dialects, the Wenker-Atlas (Rabanus et al., 2010) is almost 150 years old and took decades to complete). Work in dialectometry has shown that computational methods, such as clustering (Nerbonne and Heeringa, 1997; Proki´c and Nerbonne, 2008; Szmrecsanyi, 2008, inter alia) and dimensionality reduction (Nerbonne et al., 1999; Shackleton Jr, 2005) can instead be used to identify dimensions of variation in manually constructed discrete feature vectors. However, the success of such approaches depends on precise prior knowledge of variation features (Lameli, 2013). Distributed representations, as unsupervised methods, can complement these methods by capturing similarities between words and documents (here: cities) along various latent dimensions, including syntactic, semantic, and pragmatic aspects. These r"
D18-1469,E17-2102,0,0.411901,"g with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dialects are not static discrete entities, but exist along a continuum in most languages. Variational linguistics and dialectology typically discretize this continuum by using a set of preselected features (Trudgill, 2000), often including outdated vocabulary. The resulting dialect areas are highly accurate, but extremely timeconsuming to construct and inflexib"
D18-1469,D17-1016,0,0.47535,"method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dialects are not static discrete entities, but exist along a continuum in most languages. Variational linguistics and dialectology typically discretize this continuum by using a set of preselected features (Trudgill, 2000), often including outdated vocabulary. The resulting dialect areas are highly accurate, but extremely timeconsuming to construct and inflexible (the largest and to date most c"
D18-1469,P17-2033,0,0.273761,"method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dialects are not static discrete entities, but exist along a continuum in most languages. Variational linguistics and dialectology typically discretize this continuum by using a set of preselected features (Trudgill, 2000), often including outdated vocabulary. The resulting dialect areas are highly accurate, but extremely timeconsuming to construct and inflexible (the largest and to date most c"
D18-1469,W17-4415,1,0.925488,", we collect 2.3 million threads, or 16.8 million posts. We treat each thread as a document in our representation learning setup, labeled with the name of the city in which the thread took place. 2.2 Preprocessing We preprocess the data to minimize vocabulary size, while maintaining regional discriminative power. We lowercase the input and restrict ourselves to content words, based on the part-ofspeech (nouns, verbs, adjectives, adverbs, and proper names), using the spacy4 tagger. Prior studies showed that many regionallydistributed content words are topically driven (Eisenstein et al., 2010; Salehi et al., 2017). People talk more about their own region than about others, so the most indicative words include place names (the own city, or specific places within that city), and other local culture terms, such as sports teams. We try to minimize the effect of such regional topics, by excluding all named entities, as well as the names of all cities in our list, to instead focus on dialectal lexical variation. We use NLTK5 to remove German stop words, and to lemmatize the words. While this step removes the inflectional patterns found in German, which could have regional differences, we focus here on lexica"
D18-1469,W17-4908,0,0.0223066,"rating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dial"
D18-1469,E17-1116,0,0.0487323,"rating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible. 1 Introduction People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation. Accounting for this variation is a challenge for NLP systems (see for example the failed attempts of people with accents trying to use dialogue systems. Accounting for variation can significantly improve performance in machine translation (Mirkin and Meunier, 2015; Östling and Tiedemann, 2017), geolocation (Rahimi et al., 2017a,b) and help personalize applications and search. However, regional variation involves a complex set of grammatical, lexical, and phonological features, all of them continuously changing. Consequently, dial"
D19-5510,P16-2057,1,0.835654,"to use spoken input to control household devices and appliances. While still relatively new, smart speakers are rapidly growing in popularity. As the Economist (2017) put it: “voice has the power to transform computing, by providing a natural means of interaction.” We use a dataset of smart speaker reviews and coherence scores as a metric to choose the number of topics, and evaluate the resulting model both in terms of human judgement and in its ability to meaningfully discriminate brands in the market. 1 People tend to over-report negative experiences, while some positive reviews are bought (Hovy, 2016). 76 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 76–83 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics Raw data # reviews # words # unique words 1,724,842 25,007 the coherence score (R¨oder et al., 2015) of the resulting topics. This metric is more useful for interpretability than choosing the number of topics on held-out data likelihood, which is a proxy and can still result in semantically meaningless topics (Chang et al., 2009). The question is: what is coherent? A set of topic descriptors are said to be coheren"
D19-5510,E12-1021,0,0.0333289,"latent topics, and each topic is a distribution over words. LDA has some limitations. The main limitations are the assumption that the number of topics is known and fixed, together with the validity of the assignments, and the interpretability of topics. LDA evaluation schemes can be categorized into intrinsic evaluation (holdout-log likelihood/ perplexity (Blei et al., 2003; Wallach et al., 2009), topic coherence (Newman et al., 2010; R¨oder et al., 2015), human-in-the-loop (word or topic intrusion (Chang et al., 2009; Lau et al., 2014)), and extrinsic evaluation (e.g., document clustering (Jagarlamudi et al., 2012), information retrieval (Wei and Croft, 2006)). Those work mainly focus on extracting meaningful high-level topic descriptors. 81 In this paper, we show that those techniques, when combined appropriately together, are useful in not only high-level topics but also in-depth insights from data. In order to do so, we address LDA limitations with topic coherence, human-in-the-loop, and incorporating human knowledge to merge topics for better quality (Boyd-Graber et al., 2014). 6 Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. Reading tea leaves: How humans in"
D19-5510,E14-1056,0,0.0272152,"et al., 2003). LDA assumes that a document is comprised of mixtures over latent topics, and each topic is a distribution over words. LDA has some limitations. The main limitations are the assumption that the number of topics is known and fixed, together with the validity of the assignments, and the interpretability of topics. LDA evaluation schemes can be categorized into intrinsic evaluation (holdout-log likelihood/ perplexity (Blei et al., 2003; Wallach et al., 2009), topic coherence (Newman et al., 2010; R¨oder et al., 2015), human-in-the-loop (word or topic intrusion (Chang et al., 2009; Lau et al., 2014)), and extrinsic evaluation (e.g., document clustering (Jagarlamudi et al., 2012), information retrieval (Wei and Croft, 2006)). Those work mainly focus on extracting meaningful high-level topic descriptors. 81 In this paper, we show that those techniques, when combined appropriately together, are useful in not only high-level topics but also in-depth insights from data. In order to do so, we address LDA limitations with topic coherence, human-in-the-loop, and incorporating human knowledge to merge topics for better quality (Boyd-Graber et al., 2014). 6 Jonathan Chang, Sean Gerrish, Chong Wang"
D19-5510,N10-1012,0,0.0590822,"0), Probabilistic LSI (pLSI) (Hofmann, 1999), and the most commonly used, Latent Dirichlet Allocation (Blei et al., 2003). LDA assumes that a document is comprised of mixtures over latent topics, and each topic is a distribution over words. LDA has some limitations. The main limitations are the assumption that the number of topics is known and fixed, together with the validity of the assignments, and the interpretability of topics. LDA evaluation schemes can be categorized into intrinsic evaluation (holdout-log likelihood/ perplexity (Blei et al., 2003; Wallach et al., 2009), topic coherence (Newman et al., 2010; R¨oder et al., 2015), human-in-the-loop (word or topic intrusion (Chang et al., 2009; Lau et al., 2014)), and extrinsic evaluation (e.g., document clustering (Jagarlamudi et al., 2012), information retrieval (Wei and Croft, 2006)). Those work mainly focus on extracting meaningful high-level topic descriptors. 81 In this paper, we show that those techniques, when combined appropriately together, are useful in not only high-level topics but also in-depth insights from data. In order to do so, we address LDA limitations with topic coherence, human-in-the-loop, and incorporating human knowledge"
D19-5528,D12-1091,0,0.0337708,"acy and median distance. Interestingly, mean distance is higher, suggesting a very long tail of far-away predictions. Experiments We carry out 8 experiments, 4 on TWITTER US and 4 on TWITTER -WORLD. For each data set, we compare the performance of multi-task (MTL) and single-task (i.e., classification) models (STL), both with the labels of Rahimi et al. (2017b) and our own label set. For each of the 8 conditions, we report results averaged over three runs to reduce the impact of the random initializations. For each condition, we compute significance between STL and MTL via bootstrap sampling (Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014). The effectiveness of MTL increases with label granularity. This makes sense, since under a more fine-grained label scheme, the correlation between coordinates and labels is higher, which is exactly what we learn in the auxiliary task. Under the broader labeling scheme by Rahimi et al. (2017b), label areas are of irregular size, and so the correlation with the coordinates varies. With the k-d tree labels, the mean distance between the coordinates and the cluster centroids is 50 Km for TWITTER US and 40 km for TWITTER -WORLD, while with our labels the mean distance is 16"
D19-5528,D18-1068,1,0.834937,"f the U.S.”. We address this point with a model which jointly solves the classification and regression problem, similar to the approach by Subramanian et al. (2018), who combine regression with a classification-like “ordinal regression” in order to predict both the number of votes for a petition as well as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates into labels. The first (Rahimi et al., 2017b) identifies irregular areas via k-d trees, and is the most common in the literature. The second (Fornaciari and Hovy, 2019b) directly identifies towns of at least 15K inhabitants and allows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together"
D19-5528,W15-1527,0,0.0176624,"eets written in English, coming from North America and from everywhere in the World. Each instance consists of a set of tweets from a single user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful recent approaches to geolocation use Deep Learning architectures for the task (Liu and Inkpen, 2015; Iso et al., 2017; Han et al., 2016). Many authors (Miura et al., 2016; Bakerman et al., 2018; Rahimi et al., 2018; Ebrahimi et al., 2018; Do et al., 2018; Fornaciari and Hovy, 2019a) follow a hybrid approach, combining the text representation with network information and further meta-data. However, recent works explore the effectiveness of purely textual data for geolocation (Tang et al., 2019). Other researchers have directly predicted the geographic coordinates associated with the texts. Eisenstein et al. (2010) was the first to formulate the problem as a regression task predicting the coo"
D19-5528,K18-1005,0,0.445174,"gle user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful recent approaches to geolocation use Deep Learning architectures for the task (Liu and Inkpen, 2015; Iso et al., 2017; Han et al., 2016). Many authors (Miura et al., 2016; Bakerman et al., 2018; Rahimi et al., 2018; Ebrahimi et al., 2018; Do et al., 2018; Fornaciari and Hovy, 2019a) follow a hybrid approach, combining the text representation with network information and further meta-data. However, recent works explore the effectiveness of purely textual data for geolocation (Tang et al., 2019). Other researchers have directly predicted the geographic coordinates associated with the texts. Eisenstein et al. (2010) was the first to formulate the problem as a regression task predicting the coordinate values as numerical values. Lourentzou et al. (2017) use very simple labels, but create a neural model which separately performs b"
D19-5528,D10-1124,0,0.719517,"Missing"
D19-5528,N13-1090,0,0.0709607,"Missing"
D19-5528,D19-5529,1,0.897873,"l as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates into labels. The first (Rahimi et al., 2017b) identifies irregular areas via k-d trees, and is the most common in the literature. The second (Fornaciari and Hovy, 2019b) directly identifies towns of at least 15K inhabitants and allows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granularity impacts the effectiveness of MTL. 2 3 Data Corpora We use two publicly available data sets commonly used for geolocation,"
D19-5528,W16-3931,0,0.209623,"the World. Each instance consists of a set of tweets from a single user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful recent approaches to geolocation use Deep Learning architectures for the task (Liu and Inkpen, 2015; Iso et al., 2017; Han et al., 2016). Many authors (Miura et al., 2016; Bakerman et al., 2018; Rahimi et al., 2018; Ebrahimi et al., 2018; Do et al., 2018; Fornaciari and Hovy, 2019a) follow a hybrid approach, combining the text representation with network information and further meta-data. However, recent works explore the effectiveness of purely textual data for geolocation (Tang et al., 2019). Other researchers have directly predicted the geographic coordinates associated with the texts. Eisenstein et al. (2010) was the first to formulate the problem as a regression task predicting the coordinate values as numerical values. Lourentzou et al. (2017) use very s"
D19-5528,D19-5530,1,0.875461,"l as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates into labels. The first (Rahimi et al., 2017b) identifies irregular areas via k-d trees, and is the most common in the literature. The second (Fornaciari and Hovy, 2019b) directly identifies towns of at least 15K inhabitants and allows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granularity impacts the effectiveness of MTL. 2 3 Data Corpora We use two publicly available data sets commonly used for geolocation,"
D19-5528,D17-1016,0,0.553899,"ion with a classification-like “ordinal regression” in order to predict both the number of votes for a petition as well as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates into labels. The first (Rahimi et al., 2017b) identifies irregular areas via k-d trees, and is the most common in the literature. The second (Fornaciari and Hovy, 2019b) directly identifies towns of at least 15K inhabitants and allows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granulari"
D19-5528,C12-1064,0,0.224037,"method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granularity impacts the effectiveness of MTL. 2 3 Data Corpora We use two publicly available data sets commonly used for geolocation, known as TWITTER -US and TWITTER -WORLD. They were released by Roller et al. (2012) and Han et al. (2012) respectively. Both data sets consist of geolocated tweets written in English, coming from North America and from everywhere in the World. Each instance consists of a set of tweets from a single user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful recent approaches to geol"
D19-5528,P18-1187,0,0.545706,"of tweets from a single user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful recent approaches to geolocation use Deep Learning architectures for the task (Liu and Inkpen, 2015; Iso et al., 2017; Han et al., 2016). Many authors (Miura et al., 2016; Bakerman et al., 2018; Rahimi et al., 2018; Ebrahimi et al., 2018; Do et al., 2018; Fornaciari and Hovy, 2019a) follow a hybrid approach, combining the text representation with network information and further meta-data. However, recent works explore the effectiveness of purely textual data for geolocation (Tang et al., 2019). Other researchers have directly predicted the geographic coordinates associated with the texts. Eisenstein et al. (2010) was the first to formulate the problem as a regression task predicting the coordinate values as numerical values. Lourentzou et al. (2017) use very simple labels, but create a neural model whic"
D19-5528,P17-2033,0,0.639527,"ion with a classification-like “ordinal regression” in order to predict both the number of votes for a petition as well as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates into labels. The first (Rahimi et al., 2017b) identifies irregular areas via k-d trees, and is the most common in the literature. The second (Fornaciari and Hovy, 2019b) directly identifies towns of at least 15K inhabitants and allows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granulari"
D19-5528,D12-1137,0,0.582917,"ows the evaluation of the method in a more realistic scenario, but results in 3–6 times more labels. Contributions 1) We propose a novel multitask CNN model, which learns geographic label prediction and coordinate regression together. 2) Based on Fornaciari and Hovy (2019b), we propose an alternative coordinate discretization, which correlates more with geocoordinates (Section 3). We find that label granularity impacts the effectiveness of MTL. 2 3 Data Corpora We use two publicly available data sets commonly used for geolocation, known as TWITTER -US and TWITTER -WORLD. They were released by Roller et al. (2012) and Han et al. (2012) respectively. Both data sets consist of geolocated tweets written in English, coming from North America and from everywhere in the World. Each instance consists of a set of tweets from a single user, associated with a pair of geographic coordinates (latitude and longitude). TWITTER -US has 449 694 instances, TWITTER -WORLD 1 386 766. Both corpora have predefined development and test sets of 10 000 records each. These corpora were used in the shared task of W-NUT 2016, providing the basis for comparison with other models in the literature. Related Work Most successful rec"
D19-5528,W14-1601,1,0.765447,"stingly, mean distance is higher, suggesting a very long tail of far-away predictions. Experiments We carry out 8 experiments, 4 on TWITTER US and 4 on TWITTER -WORLD. For each data set, we compare the performance of multi-task (MTL) and single-task (i.e., classification) models (STL), both with the labels of Rahimi et al. (2017b) and our own label set. For each of the 8 conditions, we report results averaged over three runs to reduce the impact of the random initializations. For each condition, we compute significance between STL and MTL via bootstrap sampling (Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014). The effectiveness of MTL increases with label granularity. This makes sense, since under a more fine-grained label scheme, the correlation between coordinates and labels is higher, which is exactly what we learn in the auxiliary task. Under the broader labeling scheme by Rahimi et al. (2017b), label areas are of irregular size, and so the correlation with the coordinates varies. With the k-d tree labels, the mean distance between the coordinates and the cluster centroids is 50 Km for TWITTER US and 40 km for TWITTER -WORLD, while with our labels the mean distance is 16 and 7 km, re1 We also"
D19-5528,P18-2030,0,0.0278964,"rpora of geo-tagged texts. Those corpora allow us to train supervised models to predict the geographic location for a post, relying on the post’s 217 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 217–223 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics tions would be useful for geographic prediction: “a user who mentions content in both NYC and LA is predicted to be in the centre of the U.S.”. We address this point with a model which jointly solves the classification and regression problem, similar to the approach by Subramanian et al. (2018), who combine regression with a classification-like “ordinal regression” in order to predict both the number of votes for a petition as well as the voting threshold it reaches. There is a rich literature on the use of multitask learning (Caruana, 1996; Caruana et al., 1996; Caruana, 1997) in NLP, highlighting the importance of choosing the right auxiliary tasks (Alonso and Plank, 2017; Bingel and Søgaard, 2017; Benton et al., 2017; Lamprinidis et al., 2018). dition: when there are enough classification labels. We show this by evaluating on two different schemes for discretizing coordinates int"
D19-5528,D14-1039,0,0.526503,"bocconi.it Abstract text and, possibly, users’ interaction information and other meta-data provided by the social media. While a lot of work has gone into this problem, it is still far from solved. The task is usually framed as a multi-class classification problem, but actual location information is normally given as a pair of continuous-valued latitude/longitude coordinates (e.g.: 51.5074◦ N, 0.1278◦ W). Using these coordinates in classification requires translation into labels corresponding to a geographic area (e.g., cities, states, countries). This translation is another non-trivial task (Wing and Baldridge, 2014), and necessarily loses information. Much less frequently, geolocation is framed as regression, i.e., direct prediction of the coordinates. While potentially more accurate, regression over geographic coordinates presents a host of challenges (values are continuous but bounded, can be negative, and distances are nonEuclidean, due to the Earth’s curvature). It is therefore usually less effective than classification. Ideally, we would like to combine the advantages of both approaches, i.e., let the regression over continuous-valued coordinates guide the discrete location classification. So far, h"
D19-5528,N16-1174,0,0.0449934,", as well as a research grant from CERMES to set up a GPU server, which enabled us to run these experiments. spectively. With highly granular P2C labels, MTL consistently outperforms STL; in contrast, with wider areas, STL mean distance beats MTL in TWITTER -US. The auxiliary regression adds valuable information to the classification task: MTL improves significantly over STL. 6 Ablation study In order to verify the impact of the network components on the overall performance, we carry out a brief ablation study. In particular, we are interested in the attention mechanism, implemented following Yang et al. (2016). To this end, we train a MTL model without attention mechanism. We note that they are not directly comparable to those shown in table 1, since they used different, randomly initialized embeddings, and should be interpreted with caution. The results do suggest, though, that we can expect the attention mechanism to increase performance by about 10 points percent (both for accuracy and for acc@161), and to increase median distance by about 150 km. This effect holds for both multi-task and single-task models. 7 References H´ector Mart´ınez Alonso and Barbara Plank. 2017. When is multitask learnin"
D19-5529,K18-1005,0,0.434707,"Missing"
D19-5529,N13-1090,0,0.692909,"ers by orders of magnitude, making training more expensive and increasing the risk of overfitting. Previous work has therefore usually resorted to sampling methods. While sampling addresses the space issue, it necessarily loses a large amount of information, especially in complex networks, and introduces the risk of sampling biases. Compounding the problem is the fact that adjacency matrices, despite their size, are very sparse, and do not represent information efficiently. This problem is analogous to sparse word and text representations, which were successfully replaced by dense embeddings (Mikolov et al., 2013a). We show how to incorporate dense network representations in two ways: 1) with an existing word2vec-based method based on network structure, node2vec (Grover and Leskovec, 2016), and 2) with a new, doc2vec-based method of document Prior research has shown that geolocation can be substantially improved by including user network information. While effective, it suffers from the curse of dimensionality, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this information, we therefore nee"
D19-5529,D10-1124,0,0.453794,"Missing"
D19-5529,W16-3931,0,0.0986225,"e have a larger number of connections with people who live close-by (from their school, workplace, or friend network). The 224 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 224–230 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics also be exploited, as Fornaciari and Hovy (2019a) showed in a multi-task model that jointly predicts continuous geocoordinates and discrete labels. In general, geolocation with multi-source models is becoming more popular, as indicated by their increased use in state-of-the-art performances. Miura et al. (2016, 2017) considered text, metadata and network information, modeling the last as a combination user and city embeddings. Similarly to our study, Rahimi et al. (2015) exploited the mentions, even though they used them to build undirected graphs. Ebrahimi et al. (2017, 2018) also used mentions to create an undirected graph, that they pruned and fed into an embedding layer followed by an attention mechanism, in order to create a network representation. The study of Rahimi et al. (2018) is an example of network segmentation for use in a neural model. They propose a Graph Convolutional Neural Networ"
D19-5529,D19-5528,1,0.897873,"use location-specific terms (Salehi et al., 2017). However, research has shown that text should be augmented with network information, since people interact with other from their local social circles. Even though social media allows for worldwide connections, most people have a larger number of connections with people who live close-by (from their school, workplace, or friend network). The 224 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 224–230 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics also be exploited, as Fornaciari and Hovy (2019a) showed in a multi-task model that jointly predicts continuous geocoordinates and discrete labels. In general, geolocation with multi-source models is becoming more popular, as indicated by their increased use in state-of-the-art performances. Miura et al. (2016, 2017) considered text, metadata and network information, modeling the last as a combination user and city embeddings. Similarly to our study, Rahimi et al. (2015) exploited the mentions, even though they used them to build undirected graphs. Ebrahimi et al. (2017, 2018) also used mentions to create an undirected graph, that they pru"
D19-5529,P17-1116,0,0.498932,"Missing"
D19-5529,D19-5530,1,0.594611,"use location-specific terms (Salehi et al., 2017). However, research has shown that text should be augmented with network information, since people interact with other from their local social circles. Even though social media allows for worldwide connections, most people have a larger number of connections with people who live close-by (from their school, workplace, or friend network). The 224 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 224–230 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics also be exploited, as Fornaciari and Hovy (2019a) showed in a multi-task model that jointly predicts continuous geocoordinates and discrete labels. In general, geolocation with multi-source models is becoming more popular, as indicated by their increased use in state-of-the-art performances. Miura et al. (2016, 2017) considered text, metadata and network information, modeling the last as a combination user and city embeddings. Similarly to our study, Rahimi et al. (2015) exploited the mentions, even though they used them to build undirected graphs. Ebrahimi et al. (2017, 2018) also used mentions to create an undirected graph, that they pru"
D19-5529,W98-1218,0,0.304554,"ter or equal to 10 and 5 for TWITTER -US and TWITTER WORLD, respectively. This choice is motivated by the different vocabulary size of the two data sets. Any term with frequency greater than 2, but below these thresholds, which is associated with only one label, we replace with label-representative tokens. Low-frequency terms found in more than one place are considered geographically ambiguous and discarded. This allows us to reduce remarkably the vocabulary size, maintaining the useful geographic information of the huge amount of low frequency terms. Considering the terms’ Zipf distribution (Powers, 1998), this procedure allows us to replace a small number of types, but a great number of tokens. Following Han et al. (2012), we further filter the vocabulary by applying Information Gain Ratio (IGR), selecting the terms with the highest values until we reach a manageable vocabulary size: 750K and 470K for TWITTER -US and TWITTER WORLD. Concretely, we filter from the text everything but the user mentions and apply doc2vec to the resulting “texts” (Mikolov et al., 2013b). Basically, we are representing the users according to their communicative behavior directed at other users, in the temporal orde"
D19-5529,C12-1064,0,0.90924,"application that builds effective network representations through dense vectors, with no need of sampling procedures even in large networks; • We show that the node representations can be tuned via two parameters which model the width and strength of their interactions. 2 Related work Different kinds of data sources and methods can be used for the geolocation of users in Social Media. The the most straightforward approach is to exploit the geographic information conveyed by the linguistic behavior of the user. The first studies relied on the idea of exploiting Location-Indicative Words (LIW) (Han et al., 2012, 2014). More recently, neural models have been applied to the same strategy (Rahimi et al., 2017; Tang et al., 2019), improving performance. The problem, however, can be modeled in different ways, including the different designs of the geographic areas to predict, such as grids (Wing and Baldridge, 2011), hierarchical grids (Wing and Baldridge, 2014), or different kinds of clusters (Han et al., 2012, 2014). In this paper, we test our models both on the set of geographic areas - i.e., labels - used in the shared task of the Workshop on Noisy User-generated Text - W-NUT (Han et al., 2016), and"
D19-5529,P18-1187,0,0.3125,"multi-source models is becoming more popular, as indicated by their increased use in state-of-the-art performances. Miura et al. (2016, 2017) considered text, metadata and network information, modeling the last as a combination user and city embeddings. Similarly to our study, Rahimi et al. (2015) exploited the mentions, even though they used them to build undirected graphs. Ebrahimi et al. (2017, 2018) also used mentions to create an undirected graph, that they pruned and fed into an embedding layer followed by an attention mechanism, in order to create a network representation. The study of Rahimi et al. (2018) is an example of network segmentation for use in a neural model. They propose a Graph Convolutional Neural Network (GCN), where network and text data are vertically concatenated in a single channel, rather than employed as parallel channels into the same model. Do et al. (2017, 2018) present the Multi-Entry Neural Network (MENET), a model which, similarly to our study, employs node2vec and, separately, includes doc2vec as methods for extraction of document features. These works represent the state-of-the-art benchmark with respect to the implementation of network views in the models. Other mo"
D19-5529,P15-2104,0,0.0208693,"hop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 224–230 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics also be exploited, as Fornaciari and Hovy (2019a) showed in a multi-task model that jointly predicts continuous geocoordinates and discrete labels. In general, geolocation with multi-source models is becoming more popular, as indicated by their increased use in state-of-the-art performances. Miura et al. (2016, 2017) considered text, metadata and network information, modeling the last as a combination user and city embeddings. Similarly to our study, Rahimi et al. (2015) exploited the mentions, even though they used them to build undirected graphs. Ebrahimi et al. (2017, 2018) also used mentions to create an undirected graph, that they pruned and fed into an embedding layer followed by an attention mechanism, in order to create a network representation. The study of Rahimi et al. (2018) is an example of network segmentation for use in a neural model. They propose a Graph Convolutional Neural Network (GCN), where network and text data are vertically concatenated in a single channel, rather than employed as parallel channels into the same model. Do et al. (2017"
D19-5529,P17-2033,0,0.760207,"of sampling procedures even in large networks; • We show that the node representations can be tuned via two parameters which model the width and strength of their interactions. 2 Related work Different kinds of data sources and methods can be used for the geolocation of users in Social Media. The the most straightforward approach is to exploit the geographic information conveyed by the linguistic behavior of the user. The first studies relied on the idea of exploiting Location-Indicative Words (LIW) (Han et al., 2012, 2014). More recently, neural models have been applied to the same strategy (Rahimi et al., 2017; Tang et al., 2019), improving performance. The problem, however, can be modeled in different ways, including the different designs of the geographic areas to predict, such as grids (Wing and Baldridge, 2011), hierarchical grids (Wing and Baldridge, 2014), or different kinds of clusters (Han et al., 2012, 2014). In this paper, we test our models both on the set of geographic areas - i.e., labels - used in the shared task of the Workshop on Noisy User-generated Text - W-NUT (Han et al., 2016), and the more fine-grained clusters obtained through the method of Fornaciari and Hovy (2019b). Geogra"
D19-5529,D12-1137,0,0.640586,"ic areas to predict, such as grids (Wing and Baldridge, 2011), hierarchical grids (Wing and Baldridge, 2014), or different kinds of clusters (Han et al., 2012, 2014). In this paper, we test our models both on the set of geographic areas - i.e., labels - used in the shared task of the Workshop on Noisy User-generated Text - W-NUT (Han et al., 2016), and the more fine-grained clusters obtained through the method of Fornaciari and Hovy (2019b). Geographic coordinates themselves can 3 3.1 Methods The data sets We test our methods on three data sets: GEOTEXT (Eisenstein et al., 2010), TWITTER -US (Roller et al., 2012) and TWITTER -WORLD (Han et al., 2012). They contain English tweets, concatenated by author, with geographic coordinates associated with each author. GEOTEXT contains 10K texts, TWITTER -US 450K and TWITTER -WORLD 1.390M. The corpora are each split into training, development and test sets. 3.2 Learning network representations 3.2.1 node2vec Grover and Leskovec (2016) presented node2vec, a method to obtain dense node representations through a skip-gram model. Those representations, however, are obtained through a tiered sam225 pling procedure. While that allows node2vec to explore large network"
D19-5529,W17-4415,1,0.781497,"ion-based convolutional neural network and evaluate the contribution of each component on geolocation performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our models achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to networks of virtually any size. 1 Introduction Current state-of-the-art methods for user geolocation in social media rely on a number of data sources. Text is the main source, since people use location-specific terms (Salehi et al., 2017). However, research has shown that text should be augmented with network information, since people interact with other from their local social circles. Even though social media allows for worldwide connections, most people have a larger number of connections with people who live close-by (from their school, workplace, or friend network). The 224 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 224–230 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics also be exploited, as Fornaciari and Hovy (2019a) showed in a multi-task"
D19-5529,W14-1601,1,0.642906,"Missing"
D19-5529,D14-1039,0,0.600938,"r the geolocation of users in Social Media. The the most straightforward approach is to exploit the geographic information conveyed by the linguistic behavior of the user. The first studies relied on the idea of exploiting Location-Indicative Words (LIW) (Han et al., 2012, 2014). More recently, neural models have been applied to the same strategy (Rahimi et al., 2017; Tang et al., 2019), improving performance. The problem, however, can be modeled in different ways, including the different designs of the geographic areas to predict, such as grids (Wing and Baldridge, 2011), hierarchical grids (Wing and Baldridge, 2014), or different kinds of clusters (Han et al., 2012, 2014). In this paper, we test our models both on the set of geographic areas - i.e., labels - used in the shared task of the Workshop on Noisy User-generated Text - W-NUT (Han et al., 2016), and the more fine-grained clusters obtained through the method of Fornaciari and Hovy (2019b). Geographic coordinates themselves can 3 3.1 Methods The data sets We test our methods on three data sets: GEOTEXT (Eisenstein et al., 2010), TWITTER -US (Roller et al., 2012) and TWITTER -WORLD (Han et al., 2012). They contain English tweets, concatenated by aut"
D19-5529,P11-1096,0,0.230044,"inds of data sources and methods can be used for the geolocation of users in Social Media. The the most straightforward approach is to exploit the geographic information conveyed by the linguistic behavior of the user. The first studies relied on the idea of exploiting Location-Indicative Words (LIW) (Han et al., 2012, 2014). More recently, neural models have been applied to the same strategy (Rahimi et al., 2017; Tang et al., 2019), improving performance. The problem, however, can be modeled in different ways, including the different designs of the geographic areas to predict, such as grids (Wing and Baldridge, 2011), hierarchical grids (Wing and Baldridge, 2014), or different kinds of clusters (Han et al., 2012, 2014). In this paper, we test our models both on the set of geographic areas - i.e., labels - used in the shared task of the Workshop on Noisy User-generated Text - W-NUT (Han et al., 2016), and the more fine-grained clusters obtained through the method of Fornaciari and Hovy (2019b). Geographic coordinates themselves can 3 3.1 Methods The data sets We test our methods on three data sets: GEOTEXT (Eisenstein et al., 2010), TWITTER -US (Roller et al., 2012) and TWITTER -WORLD (Han et al., 2012). T"
D19-5530,K18-1005,0,0.628247,"(2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW). R"
D19-5530,D10-1124,0,0.837749,"Missing"
D19-5530,D19-5529,1,0.608119,"ased TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW). Recently, neural models have"
D19-5530,W98-1218,0,0.701153,"Missing"
D19-5530,D19-5528,1,0.86171,"ased TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW). Recently, neural models have"
D19-5530,P18-1187,0,0.794214,"raphic regions as labels. Geodesic grids were used for the geolocation of posts on Flickr, Twitter and Wikipedia (Serdyukov et al., 2009; Wing and Baldridge, 2011). Hulden et al. (2015) noticed that “using smaller grid sizes leads to an immediate sparse data problem since very few features/words are [selectively] observed in each cell”. In order to enhance the expressiveness of the geographic cells, Wing and Baldridge (2014), constructed both flat and hierarchical grids relying on k-d tree, and testing their methods at different levels of granularity. The same labels were used in the study of Rahimi et al. (2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et"
D19-5530,C12-1064,0,0.42656,"ct of detail (precise down to a centimeter) that is both unnecessary and unrealistic for most practical applications. Collapsing coordinates to geographic categories is therefore a common step in geolocation. However, this discretization step is open to interpretation: what method should we choose? Previous work includes three different approaches to discretizing continuous values into location labels (see also Section 2): 1.) Geodesic grids are the most straightforward, but do not “lead to a natural representation of the administrative, population-based or language boundaries in the region” (Han et al., 2012). 2.) Clustering coordinates prevents the identification of (nearly) empty locations and keeps points which are geographically close together in one location. Unfortunately, in crowded regions, clusters might be too close to each other, and therefore divide cultural/linguistic areas into meaningless groups. 3.) Predefined administrative regions, like cities, can provide homogeneous interpretable areas. However, mapping coordinates to the closest city can be ambiguous. Previous work typically considered cities with a population of at least 100K (Han et al., 2012, 2014). This approach has the op"
D19-5530,P17-2033,0,0.651672,") implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW). Recently, neural models have been built with the same purpose (Rahimi et al., 2017; Tang et al., 2019). Contributions The contributions of this paper are the following: 1.) we propose P2C, a k-d tree based procedure to cluster geographic points associated with existing towns within a certain distance between town and cluster centroid. 2.) we show that P2C produces more meaningful, interpretable cultural and linguistic locations 3.) we show that P2C labels substantially improve model performance in exact, fine-grained classification 2 Related work Geolocation prediction can, in principle, be modeled both as regression and as classification problem. In practice, however, give"
D19-5530,D12-1137,0,0.773199,"nce in exact, fine-grained classification 2 Related work Geolocation prediction can, in principle, be modeled both as regression and as classification problem. In practice, however, given the difficulty of predicting continuous coordinate values, regression is often carried out in conjunction with the classification (Eisenstein et al., 2010; Lourentzou et al., 2017; Fornaciari and Hovy, 2019b). In general, however, the task is considered a classification problem, which requires solutions for the 3 Methods Data sets We apply our method to two widely used data sets for geolocation: TWITTER -US (Roller et al., 2012), and TWITTER -WORLD (Han et al., 2012). They are all collections of En1 232 http://www.geonames.org choose for d, 0.5, is about one third the distance of accuracy at 161 km (Acc@161). However, since P2C iteratively creates clusters of clusters, it is possible that the original points belonging to different clusters are further apart than the threshold of d. For this reason, we selected values of d which are about three to fifteen times smaller than 161 km/100 mi. Given d and a set of coordinate points/instances in the data set, P2C iterates over the following steps until convergence: glish tw"
D19-5530,I17-1075,0,0.0146327,"(2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-In"
D19-5530,W14-1601,1,0.871284,"Missing"
D19-5530,D14-1039,0,0.674373,"cts social and geographical distinctions in the world, and that more meaningful realworld labels help language-based prediction models to perform their task more efficiently. identification of geographic regions as labels. Geodesic grids were used for the geolocation of posts on Flickr, Twitter and Wikipedia (Serdyukov et al., 2009; Wing and Baldridge, 2011). Hulden et al. (2015) noticed that “using smaller grid sizes leads to an immediate sparse data problem since very few features/words are [selectively] observed in each cell”. In order to enhance the expressiveness of the geographic cells, Wing and Baldridge (2014), constructed both flat and hierarchical grids relying on k-d tree, and testing their methods at different levels of granularity. The same labels were used in the study of Rahimi et al. (2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions"
D19-5530,W16-3931,0,0.1938,"ting their methods at different levels of granularity. The same labels were used in the study of Rahimi et al. (2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the"
D19-5530,P11-1096,0,0.499989,"f the larger set of classes. This suggests that P2C captures more meaningful location distinctions (backed up by a qualitative analysis), and that previous labels capture only broader, linguistically mixed areas. More generally, our results show that language reflects social and geographical distinctions in the world, and that more meaningful realworld labels help language-based prediction models to perform their task more efficiently. identification of geographic regions as labels. Geodesic grids were used for the geolocation of posts on Flickr, Twitter and Wikipedia (Serdyukov et al., 2009; Wing and Baldridge, 2011). Hulden et al. (2015) noticed that “using smaller grid sizes leads to an immediate sparse data problem since very few features/words are [selectively] observed in each cell”. In order to enhance the expressiveness of the geographic cells, Wing and Baldridge (2014), constructed both flat and hierarchical grids relying on k-d tree, and testing their methods at different levels of granularity. The same labels were used in the study of Rahimi et al. (2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of citi"
D19-5530,P17-1116,0,0.388786,"dy of Rahimi et al. (2018). Han et al. (2012, 2014), who released TWITTER -WORLD, use the information provided by the Geoname dataset1 in order to identify a set of cities around the world with at least 100K inhabitants. Then they refer their geo-tagged texts to those cities, creating easily interpretable geographic places. Cha et al. (2015) proposed a voting-based grid selection scheme, with the classification referred to regions/states in US. Most works use deep learning techniques for classification (Miura et al., 2016). Often, they include multi-view models, considering different sources (Miura et al., 2017; Lau et al., 2017; Ebrahimi et al., 2018; Fornaciari and Hovy, 2019a). In particular, Lau et al. (2017) implemented a multi-channel convolutional network, structurally similar to our model. Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-"
E12-1019,P08-1090,0,0.0466414,"icant gains by exploiting context. 3. Empirical evidence illustrates that our framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. However, we do not use a restricted set of events, but focus primarily on a single temporal relation tlink instead of named relations like BEFORE, AFTER or OVERLAP (although we show that we can incorp"
E12-1019,W10-0901,0,0.0217002,"best systems achieve F1-scores of 0.76 on events and 0.72 on fluents. 1 Before his death in October, Steve Jobs led Apple for 15 years. Introduction It is a long-standing goal of NLP to process natural language content in such a way that machines can effectively reason over the entities, relations, and events discussed within that content. The applications of such technology are numerous, including intelligence gathering, business analytics, healthcare, education, etc. Indeed, the promise of machine reading is actively driving research in this area (Etzioni et al., 2007; Barker et al., 2007; Clark and Harrison, 2010; Strassel et al., 2010). Temporal information is a crucial aspect of this task. For a machine to successfully understand natural language text, it must be able to associate time points and temporal durations with relations and events it discovers in text. ∗ The first author conducted this research during an internship at IBM Research. For a machine reading system processing this sentence, we would expect it to link the fluent CEO of (Steve Jobs, Apple) to time duration “15 years”. Similarly we expect it to link the event “death” to the time expression “October”. We do not take a strong “ontol"
E12-1019,doddington-etal-2004-automatic,0,0.077992,"paper can be summarized as follows: 1. We define a common methodology to link events and fluents to timestamps. 2. We use tree kernels in combination with classical feature-based approaches to obtain significant gains by exploiting context. 3. Empirical evidence illustrates that our framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or docum"
E12-1019,W01-1313,0,0.161495,"Missing"
E12-1019,J09-4007,1,0.83103,"atures include the test for certain words indicating the event is a noun, a verb, and if so which tense it has and whether it is a reporting verb. 4.2 Tree Kernel Engineering We expect that there exist certain patterns between the entities of a temporal link, which manifest on several levels: some on the lexical level, others expressed by certain sequences of POS tags, NE labels, or other representations. Kernels provide a principled way of expanding the number of dimensions in which we search for a decision boundary, and allow us to easily model local sequences and patterns in a natural way (Giuliano et al., 2009). While it is possible to define a space in which we find a decision boundary that separates positive and negative instances with manually engineered features, these features can hardly capture the notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we use"
E12-1019,P04-1043,0,0.0358114,"notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we used an implementation of the Subtree and Subset Tree (SST) (Moschitti, 2006). The advantages of using tree kernels are two-fold: thanks to an existing implementation 188 (SVMlight with tree kernels, Moschitti (2004)), it is faster and easier than traditional feature engineering. The tree structure also allows us to use different levels of representations (POS, lemma, etc.) and combine their contributions, while at the same time taking into account the ordering of labels. We use POS, lemma, semantic type, and a representation that replaces each word with a concatenation of its features (capitalization, countable, abstract/concrete noun, etc.). We developed a shallow tree representation that captures the context of the target terms, without encoding too much structure (which may prevent generalization). In"
E12-1019,E06-1015,0,0.0195403,"a decision boundary that separates positive and negative instances with manually engineered features, these features can hardly capture the notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we used an implementation of the Subtree and Subset Tree (SST) (Moschitti, 2006). The advantages of using tree kernels are two-fold: thanks to an existing implementation 188 (SVMlight with tree kernels, Moschitti (2004)), it is faster and easier than traditional feature engineering. The tree structure also allows us to use different levels of representations (POS, lemma, etc.) and combine their contributions, while at the same time taking into account the ordering of labels. We use POS, lemma, semantic type, and a representation that replaces each word with a concatenation of its features (capitalization, countable, abstract/concrete noun, etc.). We developed a shallow tr"
E12-1019,P06-1050,0,0.109155,"r framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. However, we do not use a restricted set of events, but focus primarily on a single temporal relation tlink instead of named relations like BEFORE, AFTER or OVERLAP (although we show that we can incorporate these as well). Part of our task is similar to task C of Te"
E12-1019,strassel-etal-2010-darpa,0,0.296009,"cores of 0.76 on events and 0.72 on fluents. 1 Before his death in October, Steve Jobs led Apple for 15 years. Introduction It is a long-standing goal of NLP to process natural language content in such a way that machines can effectively reason over the entities, relations, and events discussed within that content. The applications of such technology are numerous, including intelligence gathering, business analytics, healthcare, education, etc. Indeed, the promise of machine reading is actively driving research in this area (Etzioni et al., 2007; Barker et al., 2007; Clark and Harrison, 2010; Strassel et al., 2010). Temporal information is a crucial aspect of this task. For a machine to successfully understand natural language text, it must be able to associate time points and temporal durations with relations and events it discovers in text. ∗ The first author conducted this research during an internship at IBM Research. For a machine reading system processing this sentence, we would expect it to link the fluent CEO of (Steve Jobs, Apple) to time duration “15 years”. Similarly we expect it to link the event “death” to the time expression “October”. We do not take a strong “ontological” position on what"
E12-1019,S07-1014,0,0.281389,"Missing"
E12-1019,S10-1010,0,\N,Missing
E14-1078,J09-4005,0,0.0425861,"han others, and which is thus not reflected in learned predictors. We incorporate the annotator uncertainty on certain labels by measuring annotator agreement and use it in the modified loss function of a structured perceptron. We show that this approach works well independent of regularization, both on in-sample and out-of-sample data. Moreover, when evaluating the models trained with our loss function on downstream tasks, we observe improvements on two different tasks. Our results suggest that we need to pay more attention to annotator confidence when training predictors. In a similar vein, Klebanov and Beigman (2009) divide the instance space into easy and hard cases, i.e. easy cases are reliably annotated, whereas items that are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). Acknowledgements We would like to thank the anonymous reviewe"
E14-1078,W02-1001,0,0.0419919,"and A2 another, and vice versa. We experiment with both agreement scores (F 1 and confusion matrix probabilities) to augment the loss function in our learner. The next section describes this modification in detail. 4 γ(yj , yi )) = 1 − P ({A1 (X), A2 (X)} = {yj , yi }) In both loss functions, a lower gamma value means that the tags are more likely to be confused by a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F 1- and CM-weighted below. Inter-annotator agreement loss We briefly introduce the cost-sensitive perceptron classifier. Consider the weighted perceptron loss on our ith example hxi , yi i (with learning rate α = 1), Lw (hxi , yi i): γ(sign(w · xi ), yi ) max(0, −yi w · xi ) 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions In a non-cost-sensitive classifier, the weight function γ(y"
E14-1078,P11-1061,0,0.00446396,"o disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP"
E14-1078,R13-1026,0,0.0146309,"Missing"
E14-1078,N13-1070,1,0.0361792,"2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remain"
E14-1078,J93-2004,0,0.0539354,"ussion on October 10th re the aftermath of #seanref . . . While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections o"
E14-1078,W10-0713,0,0.111643,"Missing"
E14-1078,I11-1100,0,0.0198995,"Missing"
E14-1078,N13-1039,0,0.0226252,"Missing"
E14-1078,fromreide-etal-2014-crowdsourcing,1,0.835032,", we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured problems and propose cost-sensitive lea"
E14-1078,petrov-etal-2012-universal,0,0.0301893,"l is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on sy"
E14-1078,J08-3001,0,0.0420123,"red and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted POS tag I N -H OUSE 82.58 82.77 Table 3: Downstream results for named entity recognition (F1 scores). Table 3 shows the result of using our POS models in downstream NER evaluation. Here we observe mixed results. The cost-sensitive model is 5 http://www.ark.cs.cmu.edu/TweetNLP/ http://oak.dcs.shef.ac.uk/msm2013/ie_ challenge/ 6 748 dom but systematic (Reidsma and Carletta, 2008). However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score"
E14-1078,P11-2008,0,0.080131,"Missing"
E14-1078,W08-1203,0,0.405608,"Missing"
E14-1078,D11-1141,0,0.0112468,"s (Owoputi et al., 2013).5 For NER, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured pr"
E14-1078,P11-1067,0,0.0210056,"entences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see bel"
E14-1078,N03-1028,0,0.0984998,"Missing"
E14-1078,P13-2113,1,0.933032,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1077,1,0.937341,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1013,0,0.0467928,"Missing"
E14-1078,P12-1108,0,0.0190218,"ssification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is scored at once, while we update on a per-label basis. The work most related to ours is the recent study of Song et al. (2012). They suggest that some errors made by a POS tagger are more serious than others, especially for downstream tasks. They devise a hierarchy of POS tags for the Penn treebank tag set (e.g. the class NOUN contains NN, NNS, NNP, NNPS and CD) and use that in an SVM learner. They modify the Hinge loss that can take on three values: 0, σ, 1. If an error occurred and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicte"
E14-1078,E12-1006,0,0.022199,"in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see below). when learning predi"
E14-1078,I08-3008,0,0.00695885,"on a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effect"
E14-1078,P09-1032,0,\N,Missing
E14-1078,hovy-etal-2014-pos,1,\N,Missing
E14-1078,P13-2017,0,\N,Missing
E17-1015,D13-1114,0,0.012339,"deep learning and mental health space using written social media text that people with different mental health conditions are already producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these stud"
E17-1015,W15-1201,0,0.179055,"thor, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et"
E17-1015,W15-1204,1,0.0290939,"Missing"
E17-1015,W16-0311,0,0.0256565,"redict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related ment"
E17-1015,P16-2096,1,0.145427,"Missing"
E17-1015,P15-1073,1,0.846952,"ia text that people with different mental health conditions are already producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which m"
E17-1015,Y15-1064,0,0.0234308,"ics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions"
E17-1015,W15-1202,1,0.051459,"9; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions are therefore good candidates for model"
E17-1015,W15-1206,0,0.00410294,"awgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions are therefore good candidates for modeling in a multi-task framework. In this paper, we propose m"
E17-1015,W15-2913,1,0.160894,"producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlyi"
E17-1015,W15-1203,0,0.0315559,"Missing"
E17-1015,P15-1169,0,0.0105533,"people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from m"
E17-1015,P11-1077,0,0.0365779,"benefits to clinicians and patients. We explore some of the possibilities in the deep learning and mental health space using written social media text that people with different mental health conditions are already producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post"
E17-1015,D10-1001,0,0.0344318,"2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions are therefore good candidates for modeling in a multi-task framework. In this paper, we propose multi-task learning for detecting suicide risk and mental health conditions. The tasks of our model include neuroatypicality (i.e., atypical mental health) and suicide attempt, as well as the related mental health conditions of anxiety, depression, eating disorder, panic attacks, schizophrenia, bipolar disorder, and posttraumatic stress disorder (PTSD), and we explore the effect of task selection"
E17-1015,W11-0310,0,0.0269287,"possibilities in the deep learning and mental health space using written social media text that people with different mental health conditions are already producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015)."
E17-1015,W14-3214,0,0.009618,"d in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and G"
E17-1015,P16-2038,0,0.0276998,"et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., partof-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions are therefore good candidates for modeling in a multi-task framework. In this paper, we propose multi-task learning for detecting suicide risk and mental health conditions. The tasks of our model include neuroatypicality (i.e., atypical mental health) and suicide attempt, as well as the related mental health conditions of anxiety, depression, eating disorder, panic attacks, schizophrenia, bipolar disorder, and posttraumatic stress disorder (PTSD), and we explore the effect of task selection on model performance. We additionally include the ef"
E17-1015,D13-1187,0,0.364722,"multi-task framework. In this paper, we propose multi-task learning for detecting suicide risk and mental health conditions. The tasks of our model include neuroatypicality (i.e., atypical mental health) and suicide attempt, as well as the related mental health conditions of anxiety, depression, eating disorder, panic attacks, schizophrenia, bipolar disorder, and posttraumatic stress disorder (PTSD), and we explore the effect of task selection on model performance. We additionally include the effect of modeling gender, which has been shown to improve accuracy in tasks using social media text (Volkova et al., 2013; Hovy, 2015). Predicting suicide risk and several mental health conditions jointly opens the possibility for the model to leverage a shared representation for conditions that frequently occur together, a phenomenon known as comorbidity. Further including gender reflects the fact that gender differences are found in the patterns of mental health (WHO, 2016), which may help to sharpen the model. The MTL framework we propose allows such shared information across predictions and enables the inclusion of several loss functions with a common shared underlying representation. This approach is flexib"
E17-1015,P14-1018,0,0.0615492,"Missing"
E17-1015,W11-1515,0,\N,Missing
fromreide-etal-2014-crowdsourcing,N13-1037,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P11-1097,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P11-1037,0,\N,Missing
fromreide-etal-2014-crowdsourcing,D11-1141,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P10-1040,0,\N,Missing
fromreide-etal-2014-crowdsourcing,N13-1132,1,\N,Missing
fromreide-etal-2014-crowdsourcing,W10-0713,0,\N,Missing
hovy-etal-2014-pos,zeman-2008-reusable,0,\N,Missing
hovy-etal-2014-pos,J08-3001,0,\N,Missing
hovy-etal-2014-pos,I11-1100,0,\N,Missing
hovy-etal-2014-pos,N13-1037,0,\N,Missing
hovy-etal-2014-pos,R13-1026,0,\N,Missing
hovy-etal-2014-pos,petrov-etal-2012-universal,0,\N,Missing
hovy-etal-2014-pos,D11-1141,0,\N,Missing
hovy-etal-2014-pos,P11-2008,0,\N,Missing
hovy-etal-2014-pos,N13-1039,0,\N,Missing
K15-1011,Y00-1004,0,0.701769,"vertheless soon; presently; shortly a good deal; lots; very much M F F F F M M M F F Adjectives 6 5 4 fantastic; wondrous; wonderful inexpensive; cheap; economic amazing; awesome; marvelous tinny; bum; cheap happy best (quality) professional pretty easy; convenient; simple okay; o.k.; all right F M F M F M M F F M Table 6: Gender: equivalence classes in BabelNet 7 Related Work Sociolinguistic studies investigate the relation between a speaker’s linguistic choices and socio-economic variables. This includes regional origin (Schmidt and Herrgen, 2001; Nerbonne, 2003; Wieling et al., 2011), age (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), gender (Holmes, 1997; Rickford and Price, 2013), social class (Labov, 1964; Milroy and Milroy, 1992; Macaulay, 2001; Macaulay, 2002), and ethnicity (Carter, 2013; Rickford and Price, 2013). We focus on age and gender in this work. Corpus-based studies of variation have largely been conducted either by testing for the presence or absence of a set of pre-defined words (Pennebaker et al., 2001; Pennebaker et al., 2003), or by analysis of the unigram distribution (Barbieri, 2008). This approach restricts the findings to the phenomena defined in the hypo"
K15-1011,P13-2109,0,0.0221923,"Missing"
K15-1011,D13-1114,0,0.0287284,"However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age and gender (Alowibdi et al., 2013; Ciot et al., 2013) to add meta-data on Twitter, but again, error rates are too high for use in sociolinguistic hypothesis testing. We use a new source of data, namely the user Most computational sociolinguistics studies have focused on phonological and lexical variation. We present the first large-scale study of syntactic variation among demographic groups (age and gender) across several languages. We harvest data from online user-review sites and parse it with universal dependencies. We show that several age and gender-specific variations hold across languages, for example that women are more likely to use VP"
K15-1011,E14-1011,0,0.0624453,"phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age and gender (Alowibdi et al., 2013"
K15-1011,P11-1137,0,0.0631603,"01; Macaulay, 2002), and ethnicity (Carter, 2013; Rickford and Price, 2013). We focus on age and gender in this work. Corpus-based studies of variation have largely been conducted either by testing for the presence or absence of a set of pre-defined words (Pennebaker et al., 2001; Pennebaker et al., 2003), or by analysis of the unigram distribution (Barbieri, 2008). This approach restricts the findings to the phenomena defined in the hypothesis, in this case the word list used. In contrast, our approach works beyond the lexical level, is data-driven and thus unconstrained by prior hypotheses. Eisenstein et al. (2011) use multi-output 109 Langs. BABEL N ET class bitual be, null genitive marking, etc. Our study is different from his in using full syntactic analyses, studying variation across age and gender rather than ethnicity, and in studying syntactic variation across several languages. Highest Adverbs 5 4 3 actually; really; in fact truly; genuinely; really however; nevertheless however merely; simply; just reasonably; moderately; fairly very in truth; really very; really; real very; really; real <35 <35 <35 <35 <35 <35 >45 <35 >45 <35 8 Syntax has been identified as an important factor in language vari"
K15-1011,W13-1102,0,0.0803442,"mpany Textio introduced a tool to help phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age"
K15-1011,P12-1066,0,0.020615,"ial web data, say Twitter. Expected parse performance can be estimated from the SANCL 2012 shared task on dependency parsing of web data (Petrov and McDonald, 2012). The best result on the review domain there was 83.86 LAS and 88.31 UAS, close to the average over all web domains (83.45 LAS and 87.62 UAS). From the parses, we extract all subtrees of up to three tokens (treelets). We do not distinguish between right- and left-branching relations: the representation is basically a “bag of relations”. The purpose of this is to increase comparability across languages with different word orderings (Naseem et al., 2012). A onetoken treelet is simply the POS tag of the token, e.g. N OUN or V ERB. A two-token treelet is a typed relation between head and dependent, e.g. N SUBJ V ERB−−−−→N OUN. Treelets of three tokens have two possible structures. Either the head directly dominates two tokens, or the tokens are linked together in a chain, as shown below: nsubj dobj NOUN . . . VERB . . . NOUN 3.1 poss Second, we perform feature selection using L1 randomized logistic regression models, with age or gender as target variable, and the treelets as input features. However, direct feature selection with L1 regularized"
K15-1011,N13-1037,0,0.0153699,"mpany Textio introduced a tool to help phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age"
K15-1011,P10-1023,0,0.00872615,"Highest Adverbs In German, it is mostly used to express comparisons 5 (1) in Ordnung (alright) (2) am Tag (on the day) 6 BABEL N ET class 4 Semantic variation within syntactic categories Given that a number of the indicative features are single treelets (POS tags), we wondered whether there are certain semantic categories that fill these slots. Since we work across several languages, we are looking for semantically equivalent classes. We collect the most significant adjectives and adverbs for each gender for each language and map the words to all of their possible lexical groups in BabelNet (Navigli and Ponzetto, 2010). This creates lexical equivalence classes. Table 6 shows the results. We purposefully exclude nouns and verbs here, as there is too much variation to detect any patterns. The number of languages that share lexical items from the same BabelNet class is typically smaller than the number of languages that share a treelet. Nevertheless, we observe certain patterns. The results for gender are presented in Table 6. For adverbs, the division seems to be about intensity: men use more downtoners (approximately; almost; still), while women use more intensifiers (actually; really; truly; quite; lots). T"
K15-1011,P15-2079,1,0.188287,"Missing"
K15-1011,P15-1073,1,0.673266,"Missing"
K15-1011,W15-4302,1,0.492093,"Missing"
K15-1011,E14-3004,0,0.0769129,"Missing"
K15-1011,D13-1187,0,0.366374,"Missing"
K15-1011,P13-2017,0,\N,Missing
L16-1477,Y00-1004,0,0.0484667,"riven way. Keywords: language variation, social media, web explorer, computational sociolinguistics 1. Introduction Language varies. Not only over time, but also between people who live at the same time, and within the same person between different situations. We do not expect a teenager to speak the same way as a pensioner, and we would not talk the same way at a dinner party as we would at a scientific conference, or at a six-year-old’s birthday party. We use language, sometimes actively, more often subconsciously, to mark our membership in a group, defined by sociodemographic factors: age (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), gender (Holmes, 1997; Rickford and Price, 2013), regional origin (Schmidt and Herrgen, 2001; Nerbonne, 2003; Wieling et al., 2011), social class (Labov, 1964; Milroy and Milroy, 1992; Macaulay, 2001; Macaulay, 2002), ethnicity (Carter, 2013; Rickford and Price, 2013), and many more. At the same time, we use language to distinguish ourselves from other groups within the same socio-demographic category (Silverstein, 2003; Agha, 2005): young vs. old, men vs. women, town vs. country. Since everyone is at the intersection of multiple such socio-demograph"
L16-1477,P15-2079,1,0.852513,"people discuss only a limited range of topics, and they potentially use a special register for this text genre (a “reviewese”). However, our setup is extensible, and could incorporate further demographically annotated sources (e.g., social media such as Twitter) in the future, if we have access. We also plan to enable syntactically based search, both based on word classes, as well as syntactic constructions (Johannsen et al., 2015). In order to produce reliable results, though, we require a reliable way of processing nonstandard language, namely POS taggers and parsers. As recently shown by (Hovy and Søgaard, 2015; Jørgensen et al., 2015), reliability of these tools for non-canonical data is still uneven. In the meantime, we hope for active feedback from our users and plan to incorporate their suggestions and wishes. We believe that the main drivers of such a tool’s functionality should be the practitioners who use it. 6. Related Work A number of similar online projects exist. The most wellknown one is certainly the Google Ngram corpus (Michel et al., 2011), which enables lexical search over enormous amounts of text. However, it does not include demographic factors, only time of publication, and has re"
L16-1477,K15-1011,1,0.824959,"ce data. However, as we have shown, these three factors already allow for a number of variational studies. The current source of the data – online reviews – is certainly a biasing factor: people discuss only a limited range of topics, and they potentially use a special register for this text genre (a “reviewese”). However, our setup is extensible, and could incorporate further demographically annotated sources (e.g., social media such as Twitter) in the future, if we have access. We also plan to enable syntactically based search, both based on word classes, as well as syntactic constructions (Johannsen et al., 2015). In order to produce reliable results, though, we require a reliable way of processing nonstandard language, namely POS taggers and parsers. As recently shown by (Hovy and Søgaard, 2015; Jørgensen et al., 2015), reliability of these tools for non-canonical data is still uneven. In the meantime, we hope for active feedback from our users and plan to incorporate their suggestions and wishes. We believe that the main drivers of such a tool’s functionality should be the practitioners who use it. 6. Related Work A number of similar online projects exist. The most wellknown one is certainly the Goo"
L16-1477,W15-4302,1,0.859118,"imited range of topics, and they potentially use a special register for this text genre (a “reviewese”). However, our setup is extensible, and could incorporate further demographically annotated sources (e.g., social media such as Twitter) in the future, if we have access. We also plan to enable syntactically based search, both based on word classes, as well as syntactic constructions (Johannsen et al., 2015). In order to produce reliable results, though, we require a reliable way of processing nonstandard language, namely POS taggers and parsers. As recently shown by (Hovy and Søgaard, 2015; Jørgensen et al., 2015), reliability of these tools for non-canonical data is still uneven. In the meantime, we hope for active feedback from our users and plan to incorporate their suggestions and wishes. We believe that the main drivers of such a tool’s functionality should be the practitioners who use it. 6. Related Work A number of similar online projects exist. The most wellknown one is certainly the Google Ngram corpus (Michel et al., 2011), which enables lexical search over enormous amounts of text. However, it does not include demographic factors, only time of publication, and has recently been criticized fo"
N09-3017,W04-2608,0,0.0155097,"ribe a quality apparent in a person We parsed the sentences using the Charniak parser (Charniak, 2000). Note that the Charniak parser–even though among the best availbale English parsers–occasionally fails to parse a sentence correctly. This might result in an erroneous extraction, such as an incorrect or no word. However, these cases are fairly rare, and we did not manually correct this, but rather relied on the size of the data to compensate for such an error. After this preprocessing step, we were able to extract the features. 97 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both sides of the preposition. We thus focused on specific words that are syntactically related to the preposition via the phrase structure. This has the advantage that it is not limited to a certain window size; phrases might stretch over dozens of words, so the extracted word may occur far away from the actual preposition. These words were chosen based on a manual analysis of training data. Using Tregex (Levy and Andrew, 2006), a utility for expressing “regular expressions over trees”, we created a set"
N09-3017,P98-1013,0,0.189252,"nd FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting semantic features from these words (hypernyms, synonyms, etc.) allows for a certain degree of abstraction, and thus a high level comparison. O’Hara and Wiebe (2003) also make use of high level features, in their case the Penn Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features– such as semantic roles–of words in the context substantially aids disambiguation efforts. They caution, however, that indiscriminately using collocations and neighboring words may yield high accuracy, but has the risk of overfitting. In order to mitigate this, they classify the features by their part of speech. While we made use of collocation features, we also took into account higher order aspects of the 99 context, such as the governing phrase, part of speech type, and semantic class according to WordNet. All other things"
N09-3017,J96-1002,0,0.0217296,"et sense boolean indicator for capitalization the ! following steps: For each preposition classifier, resulted in several thousand features for(Forthe weThis ranked the features using information gain prepositions. We used information gain (Foreman, man, 2003). From the resulting lists,we included at 2003) in order to find the not highest ranking features most 4000 features. Thus all classifiers used the of each class and limited our classifiers to the top same features. 4000 features in order to reduce computation time. 2.3 Classifier 2.3 ClassifierTraining Training We chose maximum entropy (Berger et al., 1996) as We chose maximum entropy (Berger, 1996) as our our primary classifier, since it had been successfully primary classifier because the highest performing applied in byboth the highest performing preposition systems in both systems the SemEval-2007 the SemEval-2007 preposition senseBaldwin, disambiguation sense disambiguation task (Ye and 2007) taskthe (Ye and Baldwin, 2007) and the general and general word sense disambiguation taskword (Tratz et al., 2007) usedtask it. We usedetthe sense disambiguation (Tratz al.,implemen2007). We tation provided by the Mallet machine learning used the imple"
N09-3017,A00-2018,0,0.0319591,"ne example of the respective preposition. What are your beliefs <head&gt;about</head&gt; these emotions ? The preposition is annotated by a head tag, and the meaning of the preposition in question is given as defined by TPP. Each preposition had between 2 and 25 different senses (on average 9.76). For the case of “about” these would be 1. on the subject of; concerning 2. so as to affect 3. used to indicate movement within a particular area 4. around 5. used to express location in a particular place 6. used to describe a quality apparent in a person We parsed the sentences using the Charniak parser (Charniak, 2000). Note that the Charniak parser–even though among the best availbale English parsers–occasionally fails to parse a sentence correctly. This might result in an erroneous extraction, such as an incorrect or no word. However, these cases are fairly rare, and we did not manually correct this, but rather relied on the size of the data to compensate for such an error. After this preprocessing step, we were able to extract the features. 97 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on b"
N09-3017,levy-andrew-2006-tregex,0,0.0152002,"to extract the features. 97 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both sides of the preposition. We thus focused on specific words that are syntactically related to the preposition via the phrase structure. This has the advantage that it is not limited to a certain window size; phrases might stretch over dozens of words, so the extracted word may occur far away from the actual preposition. These words were chosen based on a manual analysis of training data. Using Tregex (Levy and Andrew, 2006), a utility for expressing “regular expressions over trees”, we created a set of rules to extract the words in question. Each rule matched words that exhibited a specific relationship with the preposition or were within a two word window to cover collocations. An example rule is given below. IN &gt; (P P < (V P < # #!AU X)) = x& < This particular rule finds the head (denoted by x) of a verb phrase that governs the prepositional phrase containing the preposition, unless x is an auxiliary verb. Tregex rules were used to identify the following words for feature generation: • the head verb/noun that"
N09-3017,S07-1005,0,0.0555779,"tures. Introduction Classifying instances of polysemous words into their proper sense classes (aka sense disambiguation) is potentially useful to any NLP application that needs to extract information from text or build a semantic representation of the textual information. However, to date, disambiguation between preposition senses has not been an object of great study. Instead, most word sense disambiguation work has focused upon classifying noun and verb instances into their appropriate WordNet (Fellbaum, 1998) senses. Prepositions have mostly been studied in the context of verb complements (Litkowski and Hargraves, 2007). Like instances of other word classes, many prepositions are ambiguous, carrying different semantic meanings (including notions of instrumental, accompaniment, location, etc.) as in “He ran with determination”, “He ran with a broken leg”, or “He ran with Jane”. As NLP systems take more and more 96 In 2007, the SemEval workshop presented participants with a formal preposition sense disambiguation task to encourage the development of systems for the disambiguation of preposition senses (Litkowski and Hargraves, 2007). The training and test data sets used for SemEval have been released to the ge"
N09-3017,J93-2004,0,0.0309399,"ge. They used the Charniak parser and FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting semantic features from these words (hypernyms, synonyms, etc.) allows for a certain degree of abstraction, and thus a high level comparison. O’Hara and Wiebe (2003) also make use of high level features, in their case the Penn Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features– such as semantic roles–of words in the context substantially aids disambiguation efforts. They caution, however, that indiscriminately using collocations and neighboring words may yield high accuracy, but has the risk of overfitting. In order to mitigate this, they classify the features by their part of speech. While we made use of collocation features, we also took into account higher order aspects of the 99 context, such as the governing phrase, part of speech type, and semantic class accor"
N09-3017,W03-0411,0,0.13111,"Missing"
N09-3017,S07-1057,1,0.704921,"Missing"
N09-3017,S07-1051,0,0.196051,"our system. The SemEval workshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participating systems. 2 Methodology 2.1 Data Preparation We downloaded the test and training data provided by the S"
N09-3017,S07-1044,0,0.157067,"rkshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participating systems. 2 Methodology 2.1 Data Preparation We downloaded the test and training data provided by the SemEval-2007 websit"
N09-3017,S07-1040,0,\N,Missing
N09-3017,C98-1013,0,\N,Missing
N13-1132,W10-0701,0,0.033793,", both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy fo"
N13-1132,W10-1703,0,0.0107417,"ments over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equa"
N13-1132,W02-0102,0,0.0301667,"i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximization (EM) (Dempster et al., 1977), which has successfully been applied to similar problems (Dawid and Skene, 1979). We initialize EM randomly and run for 50 iterations. We perform 100 random restarts, and keep the model with the best marginal data likelihood. We smooth the M-step by adding a fixed value δ to the fractional counts before normalizing (Eisner, 2002). We find that smoothing improves accuracy, but, overall, learning is robust to varying δ, and set δ = num0.1 labels . 1122 3.1 Natural Data In order to evaluate our model, we use the datasets from (Snow et al., 2008) that use discrete label values (some tasks used continuous values, which we currently do not model). Since they compared AMT annotations to experts, gold annotations exist for these sets. We can thus evaluate the accuracy of the model as well as the proficiency of each annotator. We show results for word sense disambiguation (WSD: 177 items, 34 annotators), recognizing textual en"
N13-1132,W10-0702,0,0.17971,"cy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting"
N13-1132,D07-1031,0,0.00707372,"Variational-Bayes (VB) training with symmetric Beta priors on θj and symmetric Dirichlet priors on the strategy parameters, ξj . Setting the shape parameters of the Beta distribution to 0.5 favors the extremes of the distribution, i.e., either an annotator tried to get the right answer, or simply did not care, but (almost) nobody tried “a little”. With VB training, we observe improved correlations over all test sets with no loss in accuracy. The hyper-parameters of the Dirichlet distribution on ξj were clamped to 10.0 for all our experiments with VB training. Our implementation is similar to Johnson (2007), which the reader can refer to for details. 3 Experiments We evaluate our method on existing annotated datasets from various AMT tasks. However, we also want to ensure that our model can handle adversarial conditions. Since we have no control over the factors in existing datasets, we create synthetic data for this purpose. P (A; θ, ξ) = N XhY T,S i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximizatio"
N13-1132,D08-1027,0,0.767307,"Missing"
N13-1132,P10-1070,1,0.275755,"78 0.70 0.76 0.87 0.91 Temporal 0.73 0.80 0.73 0.88 0.90 WSD 0.81 0.13 0.81 0.44 0.90 Table 1: Correlation with annotator proficiency: Pearson ρ of different methods for various data sets. MACE-VB’s trustworthiness parameter (trained with Variational Bayes with α = β = 0.5) correlates best with true annotator proficiency. It is natural to apply some form of weighting. One approach is to assume that reliable annotators agree more with others than random annotators. Inter-annotator agreement is thus a good candidate to weigh the answers. There are various measures for inter-annotator agreement. Tratz and Hovy (2010) compute the average agreement of each annotator and use it as a weight to identify reliable ones. Raw agreement can be directly computed from the data. It is related to majority voting, since it will produce high scores for all members of the majority class. Raw agreement is thus a very simple measure. In contrast, Cohen’s κ corrects the agreement between two annotators for chance agreement. It is widely used for inter-annotator agreement in annotation tasks. We also compute the κ values for each pair of annotators, and average them for each annotator (similar to the approach in Tratz and Hov"
N13-1132,P11-1122,0,0.0091563,"ness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting to go wrong. A common and simple s"
N13-1132,P10-5004,1,\N,Missing
N15-1135,W06-1615,0,0.437228,"Missing"
N15-1135,J92-4003,0,0.658692,"obin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #williams #hook I loved that movie... Uhm... You know, Hook. With Robin Williams, uh. peter pan williams movie Table 1: Examples from source (top row) and target domains (bottom rows) spelling variations with the standard form (youuuuuuuu → you), which reduces the vocabulary size. For languages where no such normalization dictionary is available, we use word clusterings based on Brown clusters (Brown et al., 1992) to generalize tags from unambiguous words to previously unseen words in the same class. C LUSTER 01011110 01011110 01011110 01011110 01011110 01011110 01011110 T OKEN offish alreadyyy finali aleady previously already recently TAG ∈ D ADJ ??? ??? ??? ADV ADV ADV P ROJ . TAG — ADV ADV ADV — — — Figure 1: Example of a Brown cluster with unambiguous tokens, as well as projected tags for new tokens (tokens marked “—” are unchanged in D0 ). In particular, to extend the dictionary D to D0 using clusters, we first run clustering on the unlabeled data T , using Brown clustering.2 We then assign to eac"
N15-1135,P07-1033,0,0.668034,"Missing"
N15-1135,I11-1100,0,0.0495232,"Missing"
N15-1135,P11-2008,0,0.158516,"Missing"
N15-1135,P11-1038,0,0.0629429,"generalize across spelling variations and synonyms. Additionally, we evaluate our approach on Dutch, Portuguese and Spanish Twitter and present tow novel data sets for the latter two languages. 2 Data 2.1 Wiktionary In our experiments, we use the (unigram) tag dictionaries from Wiktionary, as collected by Li et al. (2012).1 The size and quality of our tag dictionaries crucially influence how much unambiguous data we can extract, and for some languages, the number of dictionary entries is small. We can resort to normalization dictionaries to extend Wiktionary’s coverage. We do so for English (Han and Baldwin, 2011). It replaces some 1 https://code.google.com/p/ wikily-supervised-pos-tagger/ 1256 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1256–1261, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics N EWSWIRE T WITTER S POKEN Q UERIES Spielberg took the helm of this big budget live action project with Robin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #will"
N15-1135,hovy-etal-2014-pos,1,0.86932,"guese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manually labeled data from Switchboard section 4 as spoken data test set. For queries, we use manually lab"
N15-1135,I05-1017,0,0.0340948,"from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al."
N15-1135,2005.mtsummit-papers.11,0,0.0607748,"r example, is only about 0.012 (or 1 in 84), and the distribution of tags in the Twitter data set is heavily skewed towards nouns, while several other labels are under-represented. Twitter We collect the unlabeled data from the Twitter streaming API.3 We collected 57m tweets for English, 8.2m for Spanish, 4.1m for Portuguese, and 0.5m for Dutch. We do not perform sentence splitting on tweets, but take them as unit sequences. Spoken language We use the Switchboard corpus of transcribed telephone conversations (Godfrey et al., 1992), sections 2 and 3, as well as the English section of EuroParl (Koehn, 2005) and CHILDES (MacWhinney, 1997). We removed all meta-data and inline annotations (gestures, sounds, etc.), as well as dialogue markers. The final joint corpus contains transcriptions of 570k spoken sentences. Search queries For search queries, we use a combination of queries from Yahoo4 and AOL. We only use the search terms and ignore any additional information, such as user ID, time, and linked URLs. The resulting data set contains 10m queries. 3 2 https://github.com/percyliang/ brown-cluster 4 1257 Unlabeled data https://github.com/saffsd/langid.py http://webscope.sandbox.yahoo.com/ 2.3 Labe"
N15-1135,D12-1127,0,0.0753564,"Missing"
N15-1135,P02-1047,0,0.0803701,"n word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al. (2013), but the approach based on Brown clusters led to the best re"
N15-1135,P09-1113,0,0.130157,"Missing"
N15-1135,N13-1039,0,0.203014,"Missing"
N15-1135,P00-1014,0,0.0692605,"omly at training time from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the ba"
N15-1135,petrov-etal-2012-universal,0,0.0266651,"ndbox.yahoo.com/ 2.3 Labeled data We train our models on newswire, as well as mined unambiguous instances. For English, we use the OntoNotes release of the WSJ section of the Penn Treebank as training data for Twitter, spoken data, and queries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For"
N15-1135,C14-1168,1,0.853249,"Missing"
N15-1135,D11-1141,0,0.0257816,"ries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manuall"
N15-1135,Q13-1001,0,0.0669642,"Missing"
N15-1135,P99-1023,0,0.0664404,"floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We extend the model by adding continuous word representations, induced from the unlabeled data using the skip-gram algorithm (Mikolov et al., 2013), to the feature representations. Our logistic regression model thus works over"
N15-1135,N03-1033,0,0.0285312,"10). 3 Experiments 3.1 Model We use a CRF10 model (Lafferty et al., 2001) with the same features as Owoputi et al. (2013) and de5 LDC2011T03. http://www.let.rug.nl/˜vannoord/trees/ 7 http://www.linguateca.pt/floresta/info_ floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We e"
N16-1130,W15-3222,0,0.0247551,"can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wid"
N16-1130,W02-1001,0,0.0947009,"the tag dictionaries are used in an ambiguously supervised setting, and one where they are used as type constraints at prediction time in a self-training setup. Ambiguous supervision Our algorithm is related to work in cross-lingual transfer (Wisniewski et al., 2014; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and domain adaptation (Hovy et al., 2015a; Plank et al., 2014a), where tag dictionaries are used to filter projected annotation. We use the tag dictionaries to obtain partial labeling of in-domain training data. Our baseline sequence labeling algorithm is the structured perceptron (Collins, 2002). This algorithm performs additive updates passing over labeled data, comparing predicted sequences to gold standard sequences. If the predicted sequence is identical to the gold standard, no update is performed. We use a cost-sensitive structured perceptron (Plank et al., 2014b) to learn from the partially labeled data. Each update for a sequence can be broken down into a series of transition and emission updates, passing over the sequence item-by-item from left to right. For a word like hooch labeled VERB/NOUN/ADJ/PRON/ADV, we perform an update proportional to the cost associated with the pr"
N16-1130,P11-1061,0,0.458493,"per, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mini"
N16-1130,W05-0708,0,0.0482738,"cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soegaard/aave-pos16. 2 Data For historical reasons, most of the manually annotated corpora available today are newswire corpora. In contrast, very little data is available for domains such as subtitles, lyrics and tweets — especially for language varieties such as AAVE. Learning robust models for AAVE-like language and other language varieties is often further complicated by the absence of standard writing systems (Boujelbane et al., 2013; Bernhard and Ligozat, 2013; Duh and Kirchhoff, 2005). In this paper, we use three manually annotated data sets, consisting of subtitles from the television series The Wire, hip-hop lyrics from black American artists and tweets posted within the southeastern corner of the United States. We do not use this data for training, but only for evaluation, so our experiments use unsupervised (or weakly supervised) domain adaptation. Although the language use in the three domains 1116 vary, they have several things in common: the register is very informal, and the subtitles, lyrics and tweets contain slang terms such as loc’d out, cheesing with and po’,"
N16-1130,N13-1014,0,0.0192334,"inguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:"
N16-1130,P11-2008,0,0.0784066,"Missing"
N16-1130,N15-1157,1,0.843891,"of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced"
N16-1130,P11-1038,0,0.0288951,"Missing"
N16-1130,W12-1905,1,0.843772,"ord embeddings from our unlabeled corpus, we use the Gensim im2 https://github.com/coastalcph/rungsted plementation of the word2vec algorithm (Mikolov et al., 2013b; Mikolov et al., 2013a). We also learn Brown clusters from a large corpus of tweets3 (Owoputi et al., 2013), and add both as additional features to our training and test sets. The word representations capture latent similarities between words, but more importantly enable our tagging model to generalize to unseen words. Partially labeled data Model performance generally benefits from additional data and constraints during training (Hovy and Hovy, 2012; T¨ackstr¨om et al., 2013). We therefore also use the unlabeled data and tag dictionaries as additional, partially labeled training data. For this purpose, we extract a tag dictionary for AAVE-like language from various crowdsourced online lexicons. Partial constraints from tag dictionaries have previously been used to filter out incorrect label sequences from projected labels from parallel corpora (Wisniewski et al., 2014; Das and Petrov, 2011; T¨ackstr¨om et al., 2013). We use a combination of a publicly available dump of Wiktionary4 (Li et al., 2012), entries from Hepster’s glossary of mus"
N16-1130,N15-1135,1,0.359389,"ers and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries"
N16-1130,W15-4302,1,0.811291,"ources of data exhibit a very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors. In this paper, we use a corpus of POS-annotated tweets recently released by CMU,1 consisting of semi-randomly sampled US tweets. We want to use this corpus to learn a POS tagger for subtitles, lyrics, and tweets, which are typically associated with African-American Vernacular English (AAVE). We believe our POS tagger can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this"
N16-1130,D12-1127,0,0.0509388,"Missing"
N16-1130,N13-1090,0,0.0250193,"V series The Wire and The Boondocks, English hip-hop lyrics, and tweets from the southeastern states of the US. None of the unlabeled data overlaps with our evaluation datasets. We use this corpus for two purposes: to induce word clusters and embeddings, and to partially annotate a portion of it automatically, which we include in the training data of our ambiguous supervision model (see Section 3 below). 3 Robust learning Word representations To learn word embeddings from our unlabeled corpus, we use the Gensim im2 https://github.com/coastalcph/rungsted plementation of the word2vec algorithm (Mikolov et al., 2013b; Mikolov et al., 2013a). We also learn Brown clusters from a large corpus of tweets3 (Owoputi et al., 2013), and add both as additional features to our training and test sets. The word representations capture latent similarities between words, but more importantly enable our tagging model to generalize to unseen words. Partially labeled data Model performance generally benefits from additional data and constraints during training (Hovy and Hovy, 2012; T¨ackstr¨om et al., 2013). We therefore also use the unlabeled data and tag dictionaries as additional, partially labeled training data. For t"
N16-1130,N13-1039,0,0.0853359,"Missing"
N16-1130,C14-1168,1,0.916716,"2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soe"
N16-1130,E14-1078,1,0.932416,"2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soe"
N16-1130,E14-3004,0,0.225998,"very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors. In this paper, we use a corpus of POS-annotated tweets recently released by CMU,1 consisting of semi-randomly sampled US tweets. We want to use this corpus to learn a POS tagger for subtitles, lyrics, and tweets, which are typically associated with African-American Vernacular English (AAVE). We believe our POS tagger can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use wo"
N16-1130,Q13-1001,0,0.0502567,"Missing"
N16-1130,D14-1187,0,0.0381948,"Missing"
N16-2013,W11-0704,0,0.071398,"Missing"
N16-2013,W12-2103,0,0.464012,"al., 2012b); this approach is limited by its reliance on lists. Chen et al. (2012b) addresses this by the use of character n-grams among other features, in order to identify various forms of bullying. Sood et al. (2012b) extend their system from static lists to incorporating edit distances to find variants of slurs. This allows for finding a better recall, but does not address the core issue of detecting offensive sentences, which do not use terms that occur in the list. Chen et al. (2012a) address this by using lexical and syntactical features along with automatically generated black lists. Warner and Hirschberg (2012) perform a similar task of detecting hate speech using a support vector machine classifier, trained on word n-grams, brown clusters, and “the occurrence of words in a 10 word window” (Warner and Hirschberg, 2012). They find that their best model produces unigrams as most indicative features, and obtains an F1 score of 63, which is similar to the F1 score we achieve using word n-grams. 8 Conclusion We presented a list of criteria based in critical race theory to identify racist and sexist slurs. These can be used to gather more data and address the problem of a small, but highly prolific number"
P11-1147,P98-1013,0,0.0200753,"at expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all people become labeled as PERSON), effectively allowing only one class per entity. Etzioni et"
P11-1147,P07-1057,0,0.0672416,"pieces. In a first step, we parse all documents with the Stanford dependency parser (De Marneffe et al., 2006) (see Figure 2, step 1). The output is lemmatized (collapsing “throws”, “threw”, etc., into “throw”), and marked for various dependencies (nsubj, amod, etc.). This enables us to extract the predicate argument structure, like subjectverb-object, or additional prepositional phrases (see Figure 2, step 2). These structures help to simplify the model by discarding additional words like modifiers, determiners, etc., which are not essential to the proposition. The same approach is used by (Brody, 2007). We also concatenate multiword names (identified by sequences of NNPs) with an underscore to form a single token (“Steve/NNP Young/NNP” → “Steve Young”). 1468 2.2 Deriving Classes To derive the classes used for entities, we do not restrict ourselves to a fixed set, but derive a domainspecific set directly from the data. This step is performed simultaneously with the corpus generation described above. We utilize three syntactic constructions to identify classes, namely nominal modifiers, copula verbs, and appositions, see below. This is similar in nature to Hearst’s lexico-syntactic patterns ("
P11-1147,de-marneffe-etal-2006-generating,0,0.0230078,"Missing"
P11-1147,W10-0915,0,0.06212,"textual enrichment. Teams beat teams Teams play teams Quarterbacks throw passes Teams win games Teams defeat teams Receivers catch passes Quarterbacks complete passes Quarterbacks throw passes to receivers Teams play games Teams lose games 2 Methods INPUT: Steve Young threw a pass to Michael Holt 1. PARSE INPUT: throw/VBD Figure 1: The ten most frequent propositions discovered by our system for the American football domain Our approach differs from verb-argument identification or Named Entity (NE) tagging in several respects. While previous work on verb-argument selection (Pardo et al., 2006; Fan et al., 2010) uses fixed sets of classes, we cannot know a priori how many and which classes we will encounter. We therefore provide a way to derive the appropriate classes automatically and include a probability distribution for each of them. Our approach is thus less restricted and can learn context-dependent, finegrained, domain-specific propositions. While a NEtagged corpus could produce a general proposition like “PERSON throws to PERSON”, our method enables us to distinguish the arguments and learn “quarterback throws to receiver” for American football and “outfielder throws to third base” for baseba"
P11-1147,W03-1007,1,0.748435,"ndex (Holley and Guilford, 1964), avoids the paradox. It assumes that expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all people become labeled as PERSO"
P11-1147,J02-3001,0,0.101954,"Another statistic, the G-index (Holley and Guilford, 1964), avoids the paradox. It assumes that expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all peop"
P11-1147,C92-2082,0,0.287696,". We also concatenate multiword names (identified by sequences of NNPs) with an underscore to form a single token (“Steve/NNP Young/NNP” → “Steve Young”). 1468 2.2 Deriving Classes To derive the classes used for entities, we do not restrict ourselves to a fixed set, but derive a domainspecific set directly from the data. This step is performed simultaneously with the corpus generation described above. We utilize three syntactic constructions to identify classes, namely nominal modifiers, copula verbs, and appositions, see below. This is similar in nature to Hearst’s lexico-syntactic patterns (Hearst, 1992) and other approaches that derive ISA relations from text. While we find it straightforward to collect classes for entities in this way, we did not find similar patterns for verbs. Given a suitable mechanism, however, these could be incorporated into our framework as well. Nominal modifier are common nouns (labeled NN) that precede proper nouns (labeled NNP), as in “quarterback/NN Steve/NNP Young/NNP”, where “quarterback” is the nominal modifier of “Steve Young”. Similar information can be gained from appositions (e.g., “Steve Young, the quarterback of his team, said...”), and copula verbs (“S"
P11-1147,W10-0903,1,0.885429,"Missing"
P11-1147,P10-1154,0,0.063488,"Missing"
P11-1147,P10-1044,0,0.296167,"position, given the input sentences, our system has to not only identify the classes, but also learn when to p abstract away from the lexical form to the appropriate class and when to keep it (cf. Figure 2, step 3). To facilitate extraction, we focus on propositions with the following predicate-argument structures: NOUN-VERB-NOUN (e.g., “quarterbacks throw passes”), or NOUN-VERB-NOUNPREPOSITION-NOUN (e.g., “quarterbacks throw passes to receivers”. There is nothing, though, that prevents the use of other types of structures as well. We do not restrict the verbs we consider (Pardo et al., 2006; Ritter et al., 2010)), which extracts a high number of hapax structures. Given a sentence, we want to find the most likely class for each word and thereby derive the most likely proposition. Similar to Pardo et al. (2006), we assume the observed data was produced by a process that generates the proposition and then transforms the classes into a sentence, possibly adding additional words. We model this as a Hidden Markov Model (HMM) with bigram transitions (see Section 2.3) and use the EM algorithm (Dempster et al., 1977) to train it on the observed data, with smoothing to prevent overfitting. 2.1 Data We use a co"
P11-1147,D08-1027,0,0.0664651,"Missing"
P11-1147,strassel-etal-2010-darpa,0,0.0270768,"oni et al. (2005) resolve the problem with a web-based approach that learns hierarchies of the NE classes in an unsupervised manner. We do not enforce a taxonomy, but include statistical knowledge about the distribution of possible classes over each entity by incorporating a prior distribution P (class, entity). This enables us to genPage 1 eralize from the lexical form without restricting ourselves to one class per entity, which helps to better fit the data. In addition, we can distinguish several classes for each entity, depending on the context (e.g., winner vs. quarterback). Ritter et al. (2010) also use an unsupervised model to derive selectional predicates from unlabeled text. They do not assign classes altogether, but group similar predicates and arguments into unlabeled clusters using LDA. Brody (2007) uses a very similar methodology to establish relations between clauses and sentences, by clustering simplified propositions. Pe˜nas and Hovy (2010) employ syntactic patterns to derive classes from unlabeled data in the context of LbR. They consider a wider range of syntactic structures, but do not include a probabilistic model to label new data. 5 Conclusion We use an unsupervised"
P11-1147,C98-1013,0,\N,Missing
P11-1147,W10-0900,0,\N,Missing
P11-2056,J09-2001,0,0.0843537,"Missing"
P11-2056,N10-1083,0,0.0588888,"Missing"
P11-2056,P07-1005,1,0.835466,"sts an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1 See (Chan et al., 2007) for how using WSD can help MT. Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These meanings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for themselves, but in the context of sequential semantic labeling. This should also improve disambiguation of the words linked by the prepositions (here, morning, shopped, and Rome). We propose using unsupervised methods in order to leverage unlabeled data, since, to our knowledge, there are no annotated data sets that include both preposition and argument senses. In this pape"
P11-2056,W02-0102,0,0.418239,"Missing"
P11-2056,C10-2052,1,0.904044,"onstrain the argument senses, we construct a dictionary that lists for each word all the possible lexicographer senses according to WordNet. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are assumed to have all possible senses applicable to their 324 b) h! p! o! h p o h! p! o! h c) h! h o p! o! o Figure 1: Graphical Models. a) 1st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p). c) incorporates further constraints on variables As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h, p, and o in a graphical model. We define a good model as one that reasonably constrains the choices, but is still tractable in terms of the number of parameters being estimated. As a starting point, we choose the standard firstorder Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposition, we can omit all arcs to p. This results in model 1b. The joint distribu"
P11-2056,D07-1031,0,0.0815467,"nitialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6 . 5.2 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, whic"
P11-2056,S07-1005,0,0.363796,"Missing"
P11-2056,W03-0411,0,0.221531,"Missing"
P11-2056,J09-2002,0,0.428983,"Missing"
P11-2056,N09-3017,1,0.700377,"Missing"
P11-2056,P10-2039,1,0.79241,"Missing"
P11-2056,S07-1051,0,0.43851,"Missing"
P11-2056,W10-0900,0,\N,Missing
P11-2056,N10-1068,1,\N,Missing
P14-2062,W10-0701,0,0.0321628,", we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerted us to an unpublished masters the"
P14-2062,R13-1026,0,0.0281052,"Missing"
P14-2062,N13-1037,0,0.0131419,", and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications. Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013). We also sho"
P14-2062,W10-0713,0,0.224342,"Missing"
P14-2062,I11-1100,0,0.0604865,"Missing"
P14-2062,P11-2008,0,0.210133,"Missing"
P14-2062,D08-1027,0,0.435201,"Missing"
P14-2062,W10-0702,0,0.140457,"s experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerte"
P14-2062,Q13-1001,0,0.0363069,"Missing"
P14-2062,D07-1031,0,0.113169,"Missing"
P14-2062,D12-1127,0,0.0605467,"Missing"
P14-2062,J94-2001,0,0.583625,"Missing"
P14-2062,N13-1039,0,0.0526092,"Missing"
P14-2062,petrov-etal-2012-universal,0,0.0651366,"and annotation effort, we can produce structured annotations of near-expert quality. We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012). Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks. 2 In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels. Our Approach We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags. We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations. See Figure 1 for an example. If the correct label is not among the annotations, we are unable to recover the correct answer. This was the case for 1497 instances in our data (cf. the token “:” in the example). We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the"
P14-2062,P09-1057,0,0.0433099,"Missing"
P14-2062,D11-1141,0,0.0374043,"Missing"
P14-2062,N03-1028,0,0.197607,"Missing"
P14-2062,hovy-etal-2014-pos,1,\N,Missing
P14-2062,N13-1132,1,\N,Missing
P14-2079,W99-0901,0,0.0335856,"e observe two points: we always deal with the same number of elements, and we have observed variables. We can thus move from a sequential model to a general graphical model by adding transitions and re-arranging the structure. Since we do not model verbs (they each have their identity as type), they act like observed variables. We can thus move them in first position and condition the subject on it (3b). Original Model Hovy et al. (2011) construct a HMM using subject-verb-object (SVO) parse triples as observations, and the type candidates as hidden variables. Similar models have been used in (Abney and Light, 1999; Pardo et al., 2006). We estimate the free model parameters with EM (Dempster et al., 1977), run for a fixed number of iterations (30) or until convergence. Note that Forward-backward EM has time complexity of O(N 2 T ), where N is the number of states, and T the number of time steps. T = 3 in the model formulations used here, but N is much larger than typically found in NLP tasks (see also Table 3). The only way to make this tractable is to restrict the free parameters the model needs to estimate to the transitions. a) y1 y2 y3 S V O c) The model is initialized by jointly normalizing 1 the d"
P14-2079,P08-1004,0,0.028536,"imore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Our contributions in this paper are: a) • we empirically evaluate an approach to learning types from unlabeled data y1 • we investigate several domains and models Montana ball y3 x3 throw ball Figure 1: Examplequarterback of input sentence x and outb) the HMM.player ballverb type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction t"
P14-2079,eichler-etal-2008-unsupervised,0,0.0304811,"e HMM.player ballverb type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transdu"
P14-2079,P10-1149,0,0.017409,"y the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transductive and can be applied to unseen data. Our approach follows Hovy et al. (2011). However, they only evaluate one model on football by collecting sensibility ratings from Mechanical Turk. Our method provides extrinsic measures of performance on several domains. 3 y2 x1 • the learned entity types can be used to p"
P14-2079,C92-2082,0,0.18122,"Missing"
P14-2079,D08-1061,0,0.0716202,"Missing"
P14-2079,P11-1147,1,0.87373,"Missing"
P14-2079,P09-1115,0,0.0149152,"nce x and outb) the HMM.player ballverb type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The mod"
P14-2079,P08-1119,0,0.0687535,"Missing"
P14-2079,D11-1135,0,0.0215458,"are: a) • we empirically evaluate an approach to learning types from unlabeled data y1 • we investigate several domains and models Montana ball y3 x3 throw ball Figure 1: Examplequarterback of input sentence x and outb) the HMM.player ballverb type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of cl"
P14-2079,J94-2001,0,0.682266,"rict the search space and improve learning, we first have to learn which types modify entities and record their cooccurrence, and use this as dictionary. Kleiman: professor:25, expert:13, (specialist:1) Tilton: executive:37, economist:17, (chairman:4, president:2) Figure 2: Examples of dictionary entries with counts. Types in brackets are not considered. Dictionary Construction The number of common nouns in a domain is generally too high to consider all of them for every entity. A common way to restrict the number of types is to provide a dictionary that lists all legal types for each entity (Merialdo, 1994; Ravi and Knight, 2009; T¨ackstr¨om et al., 2013). To construct this dictionary, we collect for each entity (i.e., a sequence of words labeled with NNP or NNPS tags) in our data all common nouns (NN, NNS) that modify it. These are Model Our goal is to find semantic type candidates in the data, and apply them in relation extraction to see which ones are best suited. We restrict ourselves to verbal relations. We build on the approach by Hovy et al. (2011), which we describe briefly below. It consists of two parts: extracting the type candidates and fitting the model. The basic idea is that sema"
P14-2079,P12-1075,0,0.088401,"rically evaluate an approach to learning types from unlabeled data y1 • we investigate several domains and models Montana ball y3 x3 throw ball Figure 1: Examplequarterback of input sentence x and outb) the HMM.player ballverb type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learni"
P14-2079,P09-1057,0,0.0200158,"space and improve learning, we first have to learn which types modify entities and record their cooccurrence, and use this as dictionary. Kleiman: professor:25, expert:13, (specialist:1) Tilton: executive:37, economist:17, (chairman:4, president:2) Figure 2: Examples of dictionary entries with counts. Types in brackets are not considered. Dictionary Construction The number of common nouns in a domain is generally too high to consider all of them for every entity. A common way to restrict the number of types is to provide a dictionary that lists all legal types for each entity (Merialdo, 1994; Ravi and Knight, 2009; T¨ackstr¨om et al., 2013). To construct this dictionary, we collect for each entity (i.e., a sequence of words labeled with NNP or NNPS tags) in our data all common nouns (NN, NNS) that modify it. These are Model Our goal is to find semantic type candidates in the data, and apply them in relation extraction to see which ones are best suited. We restrict ourselves to verbal relations. We build on the approach by Hovy et al. (2011), which we describe briefly below. It consists of two parts: extracting the type candidates and fitting the model. The basic idea is that semantic types are usually"
P14-2079,P10-1044,0,0.0550582,"Missing"
P14-2079,W10-0913,0,0.0204582,"type is put types forthrow Note that the treated as observed variable. y1 y2 y3 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transductive and can be applied"
P14-2079,Q13-1001,0,0.0349341,"Missing"
P14-2083,W07-1508,0,0.074711,"Missing"
P14-2083,P11-2008,0,0.0296121,"Missing"
P14-2083,N13-1132,1,0.642283,"Missing"
P14-2083,P14-2062,1,0.89815,"Missing"
P14-2083,jurgens-2014-analysis,0,0.0843971,"Missing"
P14-2083,J93-2004,0,0.0463277,"Missing"
P14-2083,petrov-etal-2012-universal,0,0.0307925,"Missing"
P14-2083,E14-1078,1,0.860375,"Missing"
P14-2083,C12-1149,0,0.0661935,"e hard cases. Moreover, we compare professional annotation to that of lay people. We instructed annotators to use the 12 universal POS tags of Petrov et al. (2012). We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.2 For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figure 1a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. More surprisingly, though, we also find that, as discussed next, c) roughly the same disagreements are also ob"
P14-2083,E03-1068,0,0.0104121,"probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s ρ of 0.493 and Kendall’s τ of 0.366 for WSJ). Figure 3: Disagreement on French social media of the variation. In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. The disagreements that remain are the truly hard cases. We use a modified version of the a priori algorithm introduced in Dickinson and Meurers (2003) to identify annotation inconsistencies. It works by collecting “variation n-grams”, i.e. the longest sequence of words (n-gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n-gram in the same corpus. The algorithm starts off by looking for unigrams and expands them until no longer n-grams are found. For each variation n-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present annotators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is"
P15-1073,P14-2134,0,0.270759,"e word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Section 2.1) to incorporate the information. We evaluate the effect of demographic information on classification performance in three NLP 1 Apart from the demographic factors, other factors such as mood, interpersonal relationship, authority, language attitude, etc. contribute to our perception of language. 752 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 752–762, c Beij"
P15-1073,Y00-1004,0,0.887767,"Missing"
P15-1073,D12-1091,0,0.00907632,"ically significant differences (at p < 0.05) are marked with a star (∗ ). Note that for the macro-averaged scores, we cannot perform bootstrap significance testing. 4.1 ∗ TIC ), the system is trained on embeddings and data the sample winner differs from the entire data set, divided by 10, 000, is the reported p-value. Bootstrap-sampling essentially simulates runs of the two systems on different data sets. If one system outperforms the other under most of these conditions (i.e., the test returns a low p-value), we can be reasonably sure that the difference is not due to chance. As discussed in Berg-Kirkpatrick et al. (2012) and Søgaard et al. (2014), this test is the most appropriate for NLP data, since it does not make any assumptions about the underlying distributions, and directly takes performance into account. Note that the test still depends on data size, though, so that small differences in performance on larger data sets can be significant, while larger differences on small sets might not. We test for significance with the standard cutoff of p < 0.05. However, even under a bootstrapsampling test, we can only limit the number of likely false positives. If we run enough tests, we increase the chance of rep"
P15-1073,N13-1121,0,0.0185124,"Missing"
P15-1073,W06-1615,0,0.0238024,"uniform, and information the main objective. Under these uniform conditions, the impact of demographics on performance was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language process"
P15-1073,P07-1056,0,0.0161126,"utes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following previous work on similar data (Blitzer et al., 2007; Hardt and Wulff, 2012; Elming et al., 2014), we use one, three, or five star ratings, corresponding to negative, neutral, and positive sentiment, respectively. We balance the data sets so that both training and test set contain equal amounts of all three labels. We do this in order to avoid demographicspecific label distributions (women and people over 45 tend to give more positive ratings than men and people under 35, see Section 3.1). 3 3.1 Experiments Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the"
P15-1073,D13-1114,0,0.0195464,"n to lead to better results for various tasks, including sentiment analysis (Tang et al., 2014). Inducing task-specific embeddings carries the risk of overfitting to a task and data set, though, and would make it harder to attribute performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.4 Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, n"
P15-1073,N15-1135,1,0.823344,"domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demographic information on performance in a"
P15-1073,W10-2608,0,0.0149824,"Missing"
P15-1073,E14-1011,0,0.0363445,"ENTIMENT A NALYSIS T OPIC C LASSIFICATION G ENDER C LASSIFICATION AGNOSTIC AWARE AGNOSTIC AWARE AGNOSTIC AWARE Denmark France Germany UK US 58.74 53.50 51.91 59.72 55.57 59.12 53.40 52.83 ∗ 60.83 56.00 45.11 43.54 ∗ 56.91 59.40 61.14 46.00 42.64 55.41 ∗ 60.88 61.38 58.82 54.64 54.04 57.69 60.05 58.97 54.24 54.51 ∗ 58.25 60.97 avg 55.89 56.44 53.22 53.26 57.05 57.59 COUNTRY Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. 4.2 : p < 0.05 and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work extends to more la"
P15-1073,P11-1137,0,0.0289237,"2.64 55.41 ∗ 60.88 61.38 58.82 54.64 54.04 57.69 60.05 58.97 54.24 54.51 ∗ 58.25 60.97 avg 55.89 56.44 53.22 53.26 57.05 57.59 COUNTRY Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. 4.2 : p < 0.05 and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work extends to more languages and three different text-classification tasks. We also use word representations trained on corpora from the various demographic groups, rather than incorporating the differences explicitly as features in our model. Recently, Bamman et al. (2014a"
P15-1073,W13-1102,0,0.028418,"sk (labeled AGNOS 758 S ENTIMENT A NALYSIS T OPIC C LASSIFICATION G ENDER C LASSIFICATION AGNOSTIC AWARE AGNOSTIC AWARE AGNOSTIC AWARE Denmark France Germany UK US 58.74 53.50 51.91 59.72 55.57 59.12 53.40 52.83 ∗ 60.83 56.00 45.11 43.54 ∗ 56.91 59.40 61.14 46.00 42.64 55.41 ∗ 60.88 61.38 58.82 54.64 54.04 57.69 60.05 58.97 54.24 54.51 ∗ 58.25 60.97 avg 55.89 56.44 53.22 53.26 57.05 57.59 COUNTRY Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. 4.2 : p < 0.05 and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work"
P15-1073,N13-1037,0,0.0228252,"sk (labeled AGNOS 758 S ENTIMENT A NALYSIS T OPIC C LASSIFICATION G ENDER C LASSIFICATION AGNOSTIC AWARE AGNOSTIC AWARE AGNOSTIC AWARE Denmark France Germany UK US 58.74 53.50 51.91 59.72 55.57 59.12 53.40 52.83 ∗ 60.83 56.00 45.11 43.54 ∗ 56.91 59.40 61.14 46.00 42.64 55.41 ∗ 60.88 61.38 58.82 54.64 54.04 57.69 60.05 58.97 54.24 54.51 ∗ 58.25 60.97 avg 55.89 56.44 53.22 53.26 57.05 57.59 COUNTRY Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. 4.2 : p < 0.05 and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work"
P15-1073,W14-2602,1,0.848552,"ct ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following previous work on similar data (Blitzer et al., 2007; Hardt and Wulff, 2012; Elming et al., 2014), we use one, three, or five star ratings, corresponding to negative, neutral, and positive sentiment, respectively. We balance the data sets so that both training and test set contain equal amounts of all three labels. We do this in order to avoid demographicspecific label distributions (women and people over 45 tend to give more positive ratings than men and people under 35, see Section 3.1). 3 3.1 Experiments Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the non-linguistic correlates, i.e., ratings and"
P15-1073,W11-1515,0,0.148313,"Missing"
P15-1073,P13-1147,0,0.0159938,"however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demog"
P15-1073,C14-1168,1,0.693341,"ly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demographic information o"
P15-1073,P07-1078,0,0.0170611,"er these uniform conditions, the impact of demographics on performance was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as unifo"
P15-1073,P11-1077,0,0.117603,"to a task and data set, though, and would make it harder to attribute performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.4 Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following pr"
P15-1073,W11-0310,0,0.0106281,"he risk of overfitting to a task and data set, though, and would make it harder to attribute performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.4 Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), a"
P15-1073,W14-1601,1,0.842955,"p < 0.05) are marked with a star (∗ ). Note that for the macro-averaged scores, we cannot perform bootstrap significance testing. 4.1 ∗ TIC ), the system is trained on embeddings and data the sample winner differs from the entire data set, divided by 10, 000, is the reported p-value. Bootstrap-sampling essentially simulates runs of the two systems on different data sets. If one system outperforms the other under most of these conditions (i.e., the test returns a low p-value), we can be reasonably sure that the difference is not due to chance. As discussed in Berg-Kirkpatrick et al. (2012) and Søgaard et al. (2014), this test is the most appropriate for NLP data, since it does not make any assumptions about the underlying distributions, and directly takes performance into account. Note that the test still depends on data size, though, so that small differences in performance on larger data sets can be significant, while larger differences on small sets might not. We test for significance with the standard cutoff of p < 0.05. However, even under a bootstrapsampling test, we can only limit the number of likely false positives. If we run enough tests, we increase the chance of reporting a type-I error. In"
P15-1073,P14-1146,0,0.241864,"c regularities among the words. We Since embeddings depend crucially on the 3 754 https://code.google.com/p/word2vec/ size of the available training data, and since we want to avoid modeling size effects, we balance the three corpora we use to induce embeddings such that all three contain the same number of instances.4 Note that while we condition the embeddings on demographic variables, they are not task-specific. While general-purpose embeddings are widely used in the NLP community, task-specific embeddings are known to lead to better results for various tasks, including sentiment analysis (Tang et al., 2014). Inducing task-specific embeddings carries the risk of overfitting to a task and data set, though, and would make it harder to attribute performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.4 Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2"
P15-1073,D13-1187,0,0.0310412,"OUNTRY Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. 4.2 : p < 0.05 and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work extends to more languages and three different text-classification tasks. We also use word representations trained on corpora from the various demographic groups, rather than incorporating the differences explicitly as features in our model. Recently, Bamman et al. (2014a) have shown how regional lexical differences (i.e., situated language) can be learned and represented via distributed word re"
P15-2044,I05-1075,0,0.892023,"the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manual"
P15-2044,N13-1014,0,0.0620146,"Missing"
P15-2044,D12-1127,0,0.146746,"Missing"
P15-2044,D11-1006,0,0.108197,"t al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated trainin"
P15-2044,W03-0414,0,0.043386,"tical to, overlaps, is a subset, or is a superset of the Wiktionary tags. German, and Spanish to Czech and French. The resulting annotated target language corpora enable them to train POS taggers for these languages. Yarowsky and Ngai (2001) showed similar results using just the Hansards corpus on English to French and Chinese. Our work is inspired by these approaches, yet broader in scope on both the source and target side. Das and Petrov (2011) use word-aligned text to automatically create type-level tag dictionaries. Earlier work on building tag dictionaries from word-aligned text includes Probst (2003). Their tag dictionaries contain target language trigrams to be able to disambiguate ambiguous target language words. To handle the noise in the automatically obtained dictionaries, they use label propagation on a similarity graph to smooth and expand the label distributions. Our approach is similar to theirs in using projections to obtain type-level tag dictionaries, but we keep the token supervision and type supervision apart and end up with a model more similar to that of T¨ackstr¨om et al. (2013), who combine word-aligned text with crowdsourced type-level tag dictionaries. T¨ackstr¨om et a"
P15-2044,W14-5302,0,0.0715114,"ollow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-lingual POS models. They consider a simple single-source training set-up. We also tried ranking projected training data by confidence, using an ensemble of projections from 17–99 source languages and majority voting to obtain prob"
P15-2044,Q13-1001,0,0.0845969,"Missing"
P15-2044,N01-1026,0,0.568625,"cly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of langu"
P15-2044,N10-1083,0,0.0532965,"Missing"
P15-2044,H01-1035,0,0.956542,"for 100 languages publicly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on al"
P15-2044,A00-1031,0,0.37703,"than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare against state-of-the-art performance of supervised POS taggers. ModelsWe train T N T POS taggers (Brants, 2000) using only token-level projections. We also train semi-supervised POS taggers using the approach in Garrette and Baldridge (2013) (G AR), using both projections and dictionaries, as well as the unlabelled Bible translations.3 We use the English data as development data. We train T N T and G AR 4 3 github.com/dhgarrette/ low-resource-pos-tagging-2014/ 5 270 github.com/percyliang/brown-cluster code.google.com/p/wikily-supervised-pos-tagger/ Results Our results on the 25 test languages are consistently better than the unsupervised baselines, with the exceptions of Marathi and Persian, and by a v"
P15-2044,W06-1009,0,0.0336178,"2013) constrain Viterbi search via type-level tag dictionaries, pruning all tags not licensed by the dictionary. For the remaining tags, they use high-confidence word alignments to further prune the Viterbi search. We follow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-"
P15-2044,W02-1001,0,0.0695296,"un IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated training data 1 268 github.com/clab/fast_align Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 268–272, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics l1 EN … HR DE word type in the target language as a type-level tag dictionary. We combine the tag dictionary and the token-level projections to train discriminative, type-constrained POS taggers (Collins, 2002; T¨ackstr¨om et al., 2013). Below we refer to these POS taggers as using k sources (k-S RC). These n many POS taggers can now also be used to obtain predictions for all word tokens in our tensor object. This corresponds to doing the second loop over lines 8–17 in Algorithm 1. For each of our n languages, we thus complete the tensor by projecting tags into word tokens from the n − 1 remaining source languages. For the k supervised languages, we project the tags produced by the supervised POS taggers rather than the tags obtained by projection. We can then train our final POS taggers for all n"
P15-2044,P11-1061,0,0.356124,"the NLTK corpora, the HamleDT resources, and the Universal Dependencies project. We provide a complete overview of the resources at https://bitbucket.org/lowlands/ using k or n − 1 source languages, leading to four taggers in total. Baselines Our baselines are two standard unsupervised POS induction algorithms: Brown clustering using the implementation by Percy Liang4 and second-order unsupervised HMMs using logistic regression for emission probabilities (BergKirkpatrick et al., 2010; Li et al., 2012), with and without our Bible tag dictionaries.5 Upper bounds The weakly supervised system in Das and Petrov (2011) (DAS) relies on larger volumes of more representative and perfectly tokenized parallel data than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare agains"
P15-2044,P13-2112,0,0.144625,"Missing"
P15-2079,Y00-1004,0,0.804011,"length and compute the average score for each region under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. Data Wall Street Journal and Frankfurter Rundschau The Wall Street Journal is a New York City-based newspaper, in print since 1889, with about two million readers. It employs 2,000 journalists in 85 news bureaus across 51 countries. Wall Street Journal is often considered business-friendly, but conservative. In 2007, Rupert Murdoch bought the newspaper. The English Pe"
P15-2079,P07-1033,0,0.0212688,"Missing"
P15-2079,N15-1135,1,0.916495,"st majority are news pieces.1 Frankfurter Rundschau is a German language newspaper based in Frankfurt am Main. Its first issue dates back to 1945, shortly after the end of the second world war. It has about 120,000 readers. It is often considered a left-wing liberal newspaper. According to a study conducted by the newspaper itself,2 its readers are found in “comfortable” higher jobs, well-educated, and on average in their mid-forties. While the paper is available internationally, most of its users come from the RhineMain region. 2.2 POS annotations The Trustpilot Corpus The Trustpilot Corpus (Hovy et al., 2015a) consists of user reviews scraped from the multilingual website trustpilot.com. The reviewer base has been shown to be representative of the populations in the countries for which large reviewer bases exist, at least wrt. age, gender, and geographical spread (Hovy et al., 2015a). The language is more informal than newswire, but less creative than social media posts. This is similar to the language in the reviews section of the English Web Treebank.3 For the experiments below, we annotated parts of the British and German sections 3 3.1 Experiments Training data and models As training data for"
P15-2079,P15-1073,1,0.52386,"e (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users wh"
P15-2079,K15-1011,1,0.155606,"DV–VERB, while the TIGER treebank contains more NOUN–DET, NOUN–ADP, and NOUN–NOUN. Again, the younger group is more dissimilar to the CoNLL data, but less so than for English, with CONJ–PRON, NOUN–VERB, VERB– VERB, and PRON–DET, while the older group shows more ADV–ADJ, ADP–NOUN, NOUN–ADV, and ADJ–NOUN. In all of these cases, vocabulary does not factor into the differences, since we are at the POS level. The results indicate that there exist fundamental grammatical differences between the age groups, which go well beyond mere lexical differences. These findings are in line with the results in Johannsen et al. (2015), who showed that entire (delexicalized) dependency structures correlate with age and gender, often across several languages. 4.1 Tagging Error Analysis Analyzing the tagging errors of our model can give us an insight into the constructions that differ most between groups. In German, most of the errors in the younger group occur with adverbs, determiners, and verbs. Adverbs are often confused with adjectives, because adverbs and adjectives are used as modifiers in similar ways. The taggers also frequently confused adverbs with nouns, especially sentenceinitially, presumably largely because the"
P15-2079,W15-4302,1,0.242909,"013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users whose age, gender, and loca"
P15-2079,N13-1039,0,0.0276639,"Missing"
P15-2079,D13-1187,0,0.148722,"vers of language change (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written"
P15-2079,D07-1112,0,\N,Missing
P16-2057,I08-2115,0,0.733576,"ll Can We Detect Statistically-Generated Fake Reviews – An Adversarial Study Dirk Hovy Center for Language Technology University of Copenhagen 2300 Copenhagen, Denmark dirk.hovy@hum.ku.dk Abstract line review companies therefore put considerable effort into maintaining this trust, by addressing the greatest threat to consumer trust (and therefore income)—fake reviews. Identifying fake reviews is a natural fit for NLP, since they presumably contain linguistic cues that indicate their nature. Indeed, a number of previous works have dealt with the detection of fake reviews (Jindal and Liu, 2007; Badaskar et al., 2008; Mackiewicz, 2008; Jindal et al., 2010; Ott et al., 2011; Fornaciari and Poesio, 2014). However, in those cases, human writers were producing reviews to fool a human audience, not an NLP model. The detection models were therefore able to exploit the regularities resulting from the writers’ tendency to follow a pattern to minimize their effort. Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detectors (Roberts, 2012). What if fake review writers become aware of the ways to game a detection algorithm?2 As N"
P16-2057,1993.eamt-1.1,0,0.205941,"Missing"
P16-2057,E14-1030,0,0.0981824,"k Hovy Center for Language Technology University of Copenhagen 2300 Copenhagen, Denmark dirk.hovy@hum.ku.dk Abstract line review companies therefore put considerable effort into maintaining this trust, by addressing the greatest threat to consumer trust (and therefore income)—fake reviews. Identifying fake reviews is a natural fit for NLP, since they presumably contain linguistic cues that indicate their nature. Indeed, a number of previous works have dealt with the detection of fake reviews (Jindal and Liu, 2007; Badaskar et al., 2008; Mackiewicz, 2008; Jindal et al., 2010; Ott et al., 2011; Fornaciari and Poesio, 2014). However, in those cases, human writers were producing reviews to fool a human audience, not an NLP model. The detection models were therefore able to exploit the regularities resulting from the writers’ tendency to follow a pattern to minimize their effort. Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detectors (Roberts, 2012). What if fake review writers become aware of the ways to game a detection algorithm?2 As NLP technology becomes more common, we should expect to also see fake reviews generated"
P16-2057,P16-2096,1,0.511615,"fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detectors (Roberts, 2012). What if fake review writers become aware of the ways to game a detection algorithm?2 As NLP technology becomes more common, we should expect to also see fake reviews generated by NLP models. This pits technology against technology. In this paper, we explore the impact fake review generation has on NLP models’ ability to detect them, and an ethical challenge in our development of NLP technology: the fact that it can be used for both sides (Hovy and Spruit, 2016). Online reviews are a growing market, but it is struggling with fake reviews. They undermine both the value of reviews to the user, and their trust in the review sites. However, fake positive reviews can boost a business, and so a small industry producing fake reviews has developed. The two sides are facing an arms race that involves more and more natural language processing (NLP). So far, NLP has been used mostly for detection, and works well on human-generated reviews. But what happens if NLP techniques are used to generate fake reviews as well? We investigate the question in an adversarial"
P16-2057,P13-2089,0,0.0116217,"saries against our solutions. Acknowledgments The author would like to thank the members of the C OAS TAL group and the anonymous reviewers for their detailed comments and suggestions, and Noah Smith for pointing out a missing reference. This research was funded under the ERC Starting Grant LOWLANDS No. 313695. Table 3: Human performance (F1) with different amounts of information on reviews generated by conditional model 6 Conclusion Related Work References Reviews are a rich source of studies for NLP, and a variety of recent papers (McAuley et al., 2012; Danescu-Niculescu-Mizil et al., 2013; Reschke et al., 2013; Jurafsky et al., 2014; Hovy et al., 2015) have explored it. Badaskar et al. (2008) also use real and fake reviews and LMs, but in almost exactly the opposite setup: they select features that have high discriminative power in distinguishing real from fake reviews to include in their LMs. However, they use a review corpus that is more than an order of magnitude smaller, focus on tri- and quad-gram features, and do not take meta-information into account. The work of Fornaciari and Poesio (2014) is similar in that they also deal with fake review detection. However, they do not use an adversarial"
P16-2057,N13-1132,1,0.836879,"only a minority of judges suspected fraud, irrespective of whether they had access to the metainformation or not. Whatever signal the logistic regression model picks up seems to be more subtle than what the average human can perceive. This tendency plays out in the F1 scores (see Table 3): human judges have a much lower detection rate than the logistic regression model, even though the availability of meta-information improves performance here as well. These results hold whether we treat each vote as an individual item or aggregating the five votes for each instance by an item-response model (Hovy et al., 2013). In the latter case, the performance for both conditions and the average increases, more so for the instances without meta-information, but still not reaching the same level. ACCESS TO RAW AGGREGATED words only +meta-info avg. 63.90 65.31 64.65 65.77 66.66 66.22 although the paper does not generate fake reviews, but assesses the presence of several defined measures of meretriciousness. 7 We have investigated the detectability of fake reviews generated with meta-information. We find that (1) using access to meta-information can significantly improve the detection of fake reviews, and (2) gener"
P16-2057,P15-1073,1,0.0456166,"le Markov chain with a sufficiently large horizon to generate fluent reviews. Such an n-gram language model (LM) is a function that assigns probability to any sentence S, where S is a sequence w0 , w1 , · · · , wn , and P (S) = N Y N is the sequence of preceding words in context. The model is depicted in Figure 2 a). Since this is a generative model, it can not only be used to assign probability to observed sentences, but also generate new sentences based on the model parameters. However, extra-linguistic information, if available, can improve classification performance (Volkova et al., 2013; Hovy, 2015), and fakereview detection models often also exploit metainformation about the author and their behavior (Lim et al., 2010), looking for irregularities. We therefore use a generative story that assumes that people of different age and gender review different things, which in turn influences the type of business reviewed, and the choice of words. This assumption is borne out in the data (cf. Figure 1). We extend this model by conditioning on latent variables age (A), gender (G), and review category (C).5 In the generative story of this model, we first draw a user from one of the two genders in"
P16-2057,D13-1187,0,0.0162942,"sic approach is a simple Markov chain with a sufficiently large horizon to generate fluent reviews. Such an n-gram language model (LM) is a function that assigns probability to any sentence S, where S is a sequence w0 , w1 , · · · , wn , and P (S) = N Y N is the sequence of preceding words in context. The model is depicted in Figure 2 a). Since this is a generative model, it can not only be used to assign probability to observed sentences, but also generate new sentences based on the model parameters. However, extra-linguistic information, if available, can improve classification performance (Volkova et al., 2013; Hovy, 2015), and fakereview detection models often also exploit metainformation about the author and their behavior (Lim et al., 2010), looking for irregularities. We therefore use a generative story that assumes that people of different age and gender review different things, which in turn influences the type of business reviewed, and the choice of words. This assumption is borne out in the data (cf. Figure 1). We extend this model by conditioning on latent variables age (A), gender (G), and review category (C).5 In the generative story of this model, we first draw a user from one of the tw"
P16-2057,P11-1032,0,0.158194,"ersarial Study Dirk Hovy Center for Language Technology University of Copenhagen 2300 Copenhagen, Denmark dirk.hovy@hum.ku.dk Abstract line review companies therefore put considerable effort into maintaining this trust, by addressing the greatest threat to consumer trust (and therefore income)—fake reviews. Identifying fake reviews is a natural fit for NLP, since they presumably contain linguistic cues that indicate their nature. Indeed, a number of previous works have dealt with the detection of fake reviews (Jindal and Liu, 2007; Badaskar et al., 2008; Mackiewicz, 2008; Jindal et al., 2010; Ott et al., 2011; Fornaciari and Poesio, 2014). However, in those cases, human writers were producing reviews to fool a human audience, not an NLP model. The detection models were therefore able to exploit the regularities resulting from the writers’ tendency to follow a pattern to minimize their effort. Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detectors (Roberts, 2012). What if fake review writers become aware of the ways to game a detection algorithm?2 As NLP technology becomes more common, we should expect to al"
P16-2096,I08-2115,0,0.034688,"language or scientific jargon, can hinder the expression of outsiders’ voices from certain practices. A lack of awareness or decreased attention for demographic differences in research stages can therefore lead to issues of exclusion of people along the way. 3.2 Overgeneralization Exclusion is a side-effect of the data. Overgeneralization is a modeling side-effect. As an example, we consider automatic inference of user attributes, a common and interesting NLP task, whose solution also holds promise for many useful applications, such as recommendation engines and fraud or deception detection (Badaskar et al., 2008; Fornaciari and Poesio, 2014; Ott et al., 2011; Banerjee et al., 2014). The cost of false positives seems low: we might be puzzled or amused when receiving an email addressing us with the wrong gender, or congratulating us to our retirement on our 30th birthday. In practice, though, relying on models that produce false positives may lead to bias confirmation and overgeneralization. Would we accept the same error rates if the system was used to predict sexual orientation or religious views, rather than age or gender? Given the right training data, this is just a matter of changing the target v"
P16-2096,J11-2010,0,0.0490763,"text and author, which prevented the research from directly affecting the authors’ situation. 3 The social impact of NLP research We have outlined the relation between language and individual traits above. Language is also a political instrument, though, and an instrument of power. This influence stretches into politics and everyday competition, for example for turntaking (Laskowski, 2010; Bracewell and Tomlinson, 2012; Prabhakaran and Rambow, 2013; Prab2 H´ector Mart´ınez Alonso, personal communication Except for annotation: there are a number of papers on the status of crowdsource workers (Fort et al., 2011; Pavlick et al., 2014).Couillault et al. (2014) also briefly discuss annotators, but mainly in the context of quality control. 3 592 Concretely, the consequences of exclusion for NLP research have recently been pointed out by Hovy and Søgaard (2015) and Jørgensen et al. (2015): current state-of-the-art NLP models score a significantly lower accuracy for young people and ethnic minorities vis-`a-vis the modeled demographics. Better awareness of these mechanism in NLP research and development can help prevent problems further on. Potential counter-measures to demographic bias can be as simple a"
P16-2096,P14-2134,0,0.0199265,"h upon these issues under “traceability” (i.e., whether individuals can be identified): this is undesirable for experimental subjects, but might be useful in the case of annotators. Most importantly, though: the subject of NLP— language—is a proxy for human behavior, and a strong signal of individual characteristics. People use this signal consciously, to portray themselves in a certain way, but can also be identified as members of specific groups by their use of subconscious traits (Silverstein, 2003; Agha, 2005; Johannsen et al., 2015; Hovy and Johannsen, 2016). Language is always situated (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pi"
P16-2096,D14-1155,0,0.0107849,"’ voices from certain practices. A lack of awareness or decreased attention for demographic differences in research stages can therefore lead to issues of exclusion of people along the way. 3.2 Overgeneralization Exclusion is a side-effect of the data. Overgeneralization is a modeling side-effect. As an example, we consider automatic inference of user attributes, a common and interesting NLP task, whose solution also holds promise for many useful applications, such as recommendation engines and fraud or deception detection (Badaskar et al., 2008; Fornaciari and Poesio, 2014; Ott et al., 2011; Banerjee et al., 2014). The cost of false positives seems low: we might be puzzled or amused when receiving an email addressing us with the wrong gender, or congratulating us to our retirement on our 30th birthday. In practice, though, relying on models that produce false positives may lead to bias confirmation and overgeneralization. Would we accept the same error rates if the system was used to predict sexual orientation or religious views, rather than age or gender? Given the right training data, this is just a matter of changing the target variable. To address overgeneralization, the guiding question should be"
P16-2096,D13-1114,0,0.0210435,"003; Agha, 2005; Johannsen et al., 2015; Hovy and Johannsen, 2016). Language is always situated (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), and the characteristics in turn can be detected by and influence the performance of our models (Mandel et al., 2012; Volkova et al., 2013; Hovy, 2015). As more and more language-based technologies are becoming available, the ethical implications of NLP research become more important. What research is carried out, and its quality, directly affect the functionality and impact of those technologies. The following is meant to start a discussion ad"
P16-2096,P15-1073,1,0.828886,"d (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), and the characteristics in turn can be detected by and influence the performance of our models (Mandel et al., 2012; Volkova et al., 2013; Hovy, 2015). As more and more language-based technologies are becoming available, the ethical implications of NLP research become more important. What research is carried out, and its quality, directly affect the functionality and impact of those technologies. The following is meant to start a discussion addressing ethical issues that can emerge in (and from) NLP research. Does NLP need an et"
P16-2096,P16-2057,1,0.50904,"educational applications 4 594 Thanks to Barbara Plank for the analysis! (Tetreault et al., 2015), but can re-enforce prescriptive linguistic norms when degrading on non-standard language. Stylometric analysis can shed light on the provenance of historic texts (Mosteller and Wallace, 1963), but also endanger the anonymity of political dissenters. Text classification approaches help decode slang and hidden messages (Huang et al., 2013), but have the potential to be used for censorship. At the same time, NLP can also help uncovering such restrictions (Bamman et al., 2012). As recently shown by Hovy (2016), NLP techniques can be used to detect fake reviews, but also to generate them in the first place. All these examples indicate that we should become more aware of the way other people appropriate NLP technology for their own purposes. The unprecedented scale and availability can make the consequences of NLP technologies hard to gauge. The unintended consequences of research are also linked to the incentives associated with funding sources. The topic of government and military involvement in the field deserves special attention in this respect. On the one hand, Anderson et al. (2012) show how a"
P16-2096,couillault-etal-2014-evaluating,0,0.486715,", we believe that the field of ethics can contribute a more general framework, and so this paper is an interdisciplinary collaboration between NLP and ethics researchers. To facilitate the discussion, we also provide some of the relevant terminology from the literature on ethics of technology, namely the concepts of exclusion, overgeneralization, bias confirmation, topic under- and overexposure, and dual use. 2 This situation has changed lately due to the increased use of social media data, where authors are current individuals, who can be directly affected by the results of NLP applications. Couillault et al. (2014) touch upon these issues under “traceability” (i.e., whether individuals can be identified): this is undesirable for experimental subjects, but might be useful in the case of annotators. Most importantly, though: the subject of NLP— language—is a proxy for human behavior, and a strong signal of individual characteristics. People use this signal consciously, to portray themselves in a certain way, but can also be identified as members of specific groups by their use of subconscious traits (Silverstein, 2003; Agha, 2005; Johannsen et al., 2015; Hovy and Johannsen, 2016). Language is always situa"
P16-2096,P11-1061,0,0.0111606,"y English, Arabic, Chinese, and Spanish).4 Even if there is a potential wealth of data available from other languages, most NLP tools are geared towards English (Schnoebelen, 2013; Munro, 2013). The prevalence of resources for English has created an underexposure to typological variety: both morphology and syntax of English are global outliers. Would we have focused on n-gram models to the same extent if English was as morhpologically complex as, say, Finnish? While there are many approaches to develop multi-lingual and cross-lingual NLP tools for linguistic outliers (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Søgaard, 2011; Søgaard et al., 2015; Agi´c et al., 2015), there simply are more commercial incentives to overexpose English, rather than other languages. Even if other languages are equally (or more) interesting from a linguistic and cultural point of view, English is one of the most widely spoken language and therefore opens up the biggest market for NLP tools. This focus on English may be self-reinforcing: the existence of off-the-shelf tools for English makes it easy to try new ideas, while to start exploring other languages requires a higher startup cost in terms of basic models, so rese"
P16-2096,P13-1107,0,0.0247731,"harm in our experiments, they can still have unintended consequences that negatively affect people’s lives (Jonas, 1984). Advanced analysis techniques can vastly improve search and educational applications 4 594 Thanks to Barbara Plank for the analysis! (Tetreault et al., 2015), but can re-enforce prescriptive linguistic norms when degrading on non-standard language. Stylometric analysis can shed light on the provenance of historic texts (Mosteller and Wallace, 1963), but also endanger the anonymity of political dissenters. Text classification approaches help decode slang and hidden messages (Huang et al., 2013), but have the potential to be used for censorship. At the same time, NLP can also help uncovering such restrictions (Bamman et al., 2012). As recently shown by Hovy (2016), NLP techniques can be used to detect fake reviews, but also to generate them in the first place. All these examples indicate that we should become more aware of the way other people appropriate NLP technology for their own purposes. The unprecedented scale and availability can make the consequences of NLP technologies hard to gauge. The unintended consequences of research are also linked to the incentives associated with f"
P16-2096,K15-1011,1,0.778662,"ectly affected by the results of NLP applications. Couillault et al. (2014) touch upon these issues under “traceability” (i.e., whether individuals can be identified): this is undesirable for experimental subjects, but might be useful in the case of annotators. Most importantly, though: the subject of NLP— language—is a proxy for human behavior, and a strong signal of individual characteristics. People use this signal consciously, to portray themselves in a certain way, but can also be identified as members of specific groups by their use of subconscious traits (Silverstein, 2003; Agha, 2005; Johannsen et al., 2015; Hovy and Johannsen, 2016). Language is always situated (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013;"
P16-2096,E14-1030,0,0.00701681,"jargon, can hinder the expression of outsiders’ voices from certain practices. A lack of awareness or decreased attention for demographic differences in research stages can therefore lead to issues of exclusion of people along the way. 3.2 Overgeneralization Exclusion is a side-effect of the data. Overgeneralization is a modeling side-effect. As an example, we consider automatic inference of user attributes, a common and interesting NLP task, whose solution also holds promise for many useful applications, such as recommendation engines and fraud or deception detection (Badaskar et al., 2008; Fornaciari and Poesio, 2014; Ott et al., 2011; Banerjee et al., 2014). The cost of false positives seems low: we might be puzzled or amused when receiving an email addressing us with the wrong gender, or congratulating us to our retirement on our 30th birthday. In practice, though, relying on models that produce false positives may lead to bias confirmation and overgeneralization. Would we accept the same error rates if the system was used to predict sexual orientation or religious views, rather than age or gender? Given the right training data, this is just a matter of changing the target variable. To address overgener"
P16-2096,W15-4302,1,0.465341,"t of power. This influence stretches into politics and everyday competition, for example for turntaking (Laskowski, 2010; Bracewell and Tomlinson, 2012; Prabhakaran and Rambow, 2013; Prab2 H´ector Mart´ınez Alonso, personal communication Except for annotation: there are a number of papers on the status of crowdsource workers (Fort et al., 2011; Pavlick et al., 2014).Couillault et al. (2014) also briefly discuss annotators, but mainly in the context of quality control. 3 592 Concretely, the consequences of exclusion for NLP research have recently been pointed out by Hovy and Søgaard (2015) and Jørgensen et al. (2015): current state-of-the-art NLP models score a significantly lower accuracy for young people and ethnic minorities vis-`a-vis the modeled demographics. Better awareness of these mechanism in NLP research and development can help prevent problems further on. Potential counter-measures to demographic bias can be as simple as downsampling the over-represented group in the training data to even out the distribution. The work by Mohammady and Culotta (2014) shows another approach, by using existing demographic statistics as supervision. In general, measures to address overfitting or imbalanced data"
P16-2096,W11-1515,0,0.0438085,"Missing"
P16-2096,P11-1032,0,0.00658252,"ssion of outsiders’ voices from certain practices. A lack of awareness or decreased attention for demographic differences in research stages can therefore lead to issues of exclusion of people along the way. 3.2 Overgeneralization Exclusion is a side-effect of the data. Overgeneralization is a modeling side-effect. As an example, we consider automatic inference of user attributes, a common and interesting NLP task, whose solution also holds promise for many useful applications, such as recommendation engines and fraud or deception detection (Badaskar et al., 2008; Fornaciari and Poesio, 2014; Ott et al., 2011; Banerjee et al., 2014). The cost of false positives seems low: we might be puzzled or amused when receiving an email addressing us with the wrong gender, or congratulating us to our retirement on our 30th birthday. In practice, though, relying on models that produce false positives may lead to bias confirmation and overgeneralization. Would we accept the same error rates if the system was used to predict sexual orientation or religious views, rather than age or gender? Given the right training data, this is just a matter of changing the target variable. To address overgeneralization, the gui"
P16-2096,P10-1102,0,0.0426121,"further enriching existing text which was not strongly linked to any particular author (newswire), was usually published publicly, and often with some temporal distance (novels). All these factors created a distance between text and author, which prevented the research from directly affecting the authors’ situation. 3 The social impact of NLP research We have outlined the relation between language and individual traits above. Language is also a political instrument, though, and an instrument of power. This influence stretches into politics and everyday competition, for example for turntaking (Laskowski, 2010; Bracewell and Tomlinson, 2012; Prabhakaran and Rambow, 2013; Prab2 H´ector Mart´ınez Alonso, personal communication Except for annotation: there are a number of papers on the status of crowdsource workers (Fort et al., 2011; Pavlick et al., 2014).Couillault et al. (2014) also briefly discuss annotators, but mainly in the context of quality control. 3 592 Concretely, the consequences of exclusion for NLP research have recently been pointed out by Hovy and Søgaard (2015) and Jørgensen et al. (2015): current state-of-the-art NLP models score a significantly lower accuracy for young people and e"
P16-2096,Q14-1007,0,0.0138165,"hich prevented the research from directly affecting the authors’ situation. 3 The social impact of NLP research We have outlined the relation between language and individual traits above. Language is also a political instrument, though, and an instrument of power. This influence stretches into politics and everyday competition, for example for turntaking (Laskowski, 2010; Bracewell and Tomlinson, 2012; Prabhakaran and Rambow, 2013; Prab2 H´ector Mart´ınez Alonso, personal communication Except for annotation: there are a number of papers on the status of crowdsource workers (Fort et al., 2011; Pavlick et al., 2014).Couillault et al. (2014) also briefly discuss annotators, but mainly in the context of quality control. 3 592 Concretely, the consequences of exclusion for NLP research have recently been pointed out by Hovy and Søgaard (2015) and Jørgensen et al. (2015): current state-of-the-art NLP models score a significantly lower accuracy for young people and ethnic minorities vis-`a-vis the modeled demographics. Better awareness of these mechanism in NLP research and development can help prevent problems further on. Potential counter-measures to demographic bias can be as simple as downsampling the over"
P16-2096,W15-2913,1,0.0520643,"ys situated (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), and the characteristics in turn can be detected by and influence the performance of our models (Mandel et al., 2012; Volkova et al., 2013; Hovy, 2015). As more and more language-based technologies are becoming available, the ethical implications of NLP research become more important. What research is carried out, and its quality, directly affect the functionality and impact of those technologies. The following is meant to start a discussion addressing ethical issues that can emerge in (and from) NLP research. Does NLP need an et"
P16-2096,mcenery-2002-ethical,0,0.0189562,"re important. What research is carried out, and its quality, directly affect the functionality and impact of those technologies. The following is meant to start a discussion addressing ethical issues that can emerge in (and from) NLP research. Does NLP need an ethics discussion? As discussed above, the makeup of most NLP experiments so far has not obviated a need for ethical considerations, and so, while we are aware of individual discussions (Strube, 2015), there is little discourse in the community yet. A search for “ethic*” in the ACL anthology only yields three results. One of the papers (McEnery, 2002) turns out to be a panel discussion, another is a book review, leaving only Couillault et al. (2014), who devote most of the discussion to legal and quality issues of data sets. We know social implications have been addressed in some NLP curricula,2 but until now, no discipline-wide discussion seems to take place. The most likely reason is that NLP research has not directly involved human subjects.3 Historically, most NLP applications focused on further enriching existing text which was not strongly linked to any particular author (newswire), was usually published publicly, and often with some"
P16-2096,I13-1025,0,0.0220192,"rongly linked to any particular author (newswire), was usually published publicly, and often with some temporal distance (novels). All these factors created a distance between text and author, which prevented the research from directly affecting the authors’ situation. 3 The social impact of NLP research We have outlined the relation between language and individual traits above. Language is also a political instrument, though, and an instrument of power. This influence stretches into politics and everyday competition, for example for turntaking (Laskowski, 2010; Bracewell and Tomlinson, 2012; Prabhakaran and Rambow, 2013; Prab2 H´ector Mart´ınez Alonso, personal communication Except for annotation: there are a number of papers on the status of crowdsource workers (Fort et al., 2011; Pavlick et al., 2014).Couillault et al. (2014) also briefly discuss annotators, but mainly in the context of quality control. 3 592 Concretely, the consequences of exclusion for NLP research have recently been pointed out by Hovy and Søgaard (2015) and Jørgensen et al. (2015): current state-of-the-art NLP models score a significantly lower accuracy for young people and ethnic minorities vis-`a-vis the modeled demographics. Better"
P16-2096,D14-1157,0,0.052901,"Missing"
P16-2096,P15-1169,0,0.0888142,"Missing"
P16-2096,P15-1157,0,0.00537154,"for young people and ethnic minorities vis-`a-vis the modeled demographics. Better awareness of these mechanism in NLP research and development can help prevent problems further on. Potential counter-measures to demographic bias can be as simple as downsampling the over-represented group in the training data to even out the distribution. The work by Mohammady and Culotta (2014) shows another approach, by using existing demographic statistics as supervision. In general, measures to address overfitting or imbalanced data can be used to correct for demographic bias in data. hakaran et al., 2014; Tsur et al., 2015; Khouzami et al., 2015, inter alia), . The mutual relationships between language, society, and the individual are also the source for the societal impact factors of NLP: failing to recognize group membership (Section 3.1), implying the wrong group membership (see Section 3.2), and overexposure (Section 3.3). In the following, we discuss sources of these problems in the data, modeling, and research design, and suggest possible solutions to address them. 3.1 Exclusion As a result of the situatedness of language, any data set carries a demographic bias, i.e., latent information about the demogra"
P16-2096,P11-1077,0,0.0348826,"rs of specific groups by their use of subconscious traits (Silverstein, 2003; Agha, 2005; Johannsen et al., 2015; Hovy and Johannsen, 2016). Language is always situated (Bamman et al., 2014), i.e., it is uttered in a specific situation at a particular place and time, and by an individual speaker with all the characteristics outlined above. All of these factors can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), and the characteristics in turn can be detected by and influence the performance of our models (Mandel et al., 2012; Volkova et al., 2013; Hovy, 2015). As more and more language-based technologies are becoming available, the ethical implications of NLP research become more important. What research is carried out, and its quality, directly affect the functionality and impact of thos"
P16-2096,D13-1187,0,0.0901276,"rs can therefore leave an imprint on the utterance, i.e., the texts we use in NLP carry latent information about the author and situation, albeit to varying degrees. This information can be used to predict author characteristics from text (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Plank and Hovy, 2015; Preotiuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), and the characteristics in turn can be detected by and influence the performance of our models (Mandel et al., 2012; Volkova et al., 2013; Hovy, 2015). As more and more language-based technologies are becoming available, the ethical implications of NLP research become more important. What research is carried out, and its quality, directly affect the functionality and impact of those technologies. The following is meant to start a discussion addressing ethical issues that can emerge in (and from) NLP research. Does NLP need an ethics discussion? As discussed above, the makeup of most NLP experiments so far has not obviated a need for ethical considerations, and so, while we are aware of individual discussions (Strube, 2015), the"
P16-2096,P14-1018,0,0.0213473,"Missing"
P16-2096,P15-1165,0,0.0187084,"Missing"
P16-2096,P11-2120,0,0.0102254,"nese, and Spanish).4 Even if there is a potential wealth of data available from other languages, most NLP tools are geared towards English (Schnoebelen, 2013; Munro, 2013). The prevalence of resources for English has created an underexposure to typological variety: both morphology and syntax of English are global outliers. Would we have focused on n-gram models to the same extent if English was as morhpologically complex as, say, Finnish? While there are many approaches to develop multi-lingual and cross-lingual NLP tools for linguistic outliers (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Søgaard, 2011; Søgaard et al., 2015; Agi´c et al., 2015), there simply are more commercial incentives to overexpose English, rather than other languages. Even if other languages are equally (or more) interesting from a linguistic and cultural point of view, English is one of the most widely spoken language and therefore opens up the biggest market for NLP tools. This focus on English may be self-reinforcing: the existence of off-the-shelf tools for English makes it easy to try new ideas, while to start exploring other languages requires a higher startup cost in terms of basic models, so researchers are les"
P16-2096,N01-1026,0,0.0209989,"the ACE corpus covers only English, Arabic, Chinese, and Spanish).4 Even if there is a potential wealth of data available from other languages, most NLP tools are geared towards English (Schnoebelen, 2013; Munro, 2013). The prevalence of resources for English has created an underexposure to typological variety: both morphology and syntax of English are global outliers. Would we have focused on n-gram models to the same extent if English was as morhpologically complex as, say, Finnish? While there are many approaches to develop multi-lingual and cross-lingual NLP tools for linguistic outliers (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Søgaard, 2011; Søgaard et al., 2015; Agi´c et al., 2015), there simply are more commercial incentives to overexpose English, rather than other languages. Even if other languages are equally (or more) interesting from a linguistic and cultural point of view, English is one of the most widely spoken language and therefore opens up the biggest market for NLP tools. This focus on English may be self-reinforcing: the existence of off-the-shelf tools for English makes it easy to try new ideas, while to start exploring other languages requires a higher startup cost in terms of"
P16-2096,P15-2079,1,\N,Missing
P16-2096,P15-2044,1,\N,Missing
P16-2096,W12-3202,0,\N,Missing
P16-2096,L16-1477,1,\N,Missing
P16-3016,maynard-greenwood-2014-cares,0,0.0415631,") and label prevalance (left vs. right column) on F1 and AUC scores. to be viewed in the light of the two factors outlined here. Addressing the wider context in which tweets are written, Rajadesingan et al. (2015) mapped information from the posting history of Twitter users to research on why, when, and how sarcasm tends to be used. They also tested their model on both balanced and unbalanced datasets. Bamman and Smith (2015) showed that a variety of contextual features can improve classification performance over use of textual features alone. However, like Gonz´alez-Ib´anez et al. (2011) and Maynard and Greenwood (2014), they concluded that the use of hashtags for data labelling introduced biases to their dataset. 6 In automatic sarcasm detection, use of unbalanced datasets led to large drops in F1 scores, due to this metric not taking into account true negatives. As the ratio of TNs is necessarily large for effective sarcasm detection on data in which positive examples are rare, AUC seems a more appropriate performance metric. Although more robust to class imbalance, AUC scores also varied between the balanced and unbalanced datasets. This indicates that label class balance and dataset size should be taken"
P16-3016,W10-2914,0,0.311329,"of-the-art systems fail to address the above issues. Research suggesting that verbal irony occurs in less than a fifth of conversations (Gibbs, 2000) implies that, rather than using balanced datasets, a more realistic approach may be to view sarcasm recognition as a problem of anomaly detection, in which positive examples are scarce. While convenient, obtaining labelled data from hashtags has been found to introduce both noise, in the form of incorrectly labelled examples, and bias to the datasets used – analysis suggests that only certain forms of sarcasm are likely to be tagged in this way (Davidov et al., 2010), and predominantly by certain types of Twitter users (Bamman and Smith, 2015). To address these issues, we create a novel corpus of manually annotated Twitter conversations and, using the feature classes of Bamman and Smith (2015), perform sarcasm classification experiments on both balanced and unbalanced datasets. We also compare model performance to a dataset of conversations automatically retrieved using hashtags. Sarcasm can radically alter or invert a phrase’s meaning. Sarcasm detection can therefore help improve natural language processing (NLP) tasks. The majority of prior research has"
P16-3016,swanson-etal-2014-getting,0,0.0388704,"Missing"
P16-3016,filatova-2012-irony,0,0.605001,"ores, indicating less than chance expected agreement. Two possible explanations for low rater agreement are (1) that sarcasm recognition is a difficult task for humans (Kreuz and Caucci, 2007; Gonz´alez-Ib´anez et al., ing e.g., Davidov et al. (2010) 2010, Gonz´alez-Ib´anez et al. (2011). 2 Krippendorff (2012) considers 0.67 to be lowest acceptable agreement score. 1 This was also necessary because prior sarcasm detection studies relied on self-annotation of sarcasm by Twitter users applying their own judgements of sarcastic mean108 2011), especially without access to the surrounding context (Filatova, 2012; Wallace et al., 2014), and (2) that people undertaking such tasks remotely online are often guilty of ‘spamming,’ or providing careless or random responses (Hovy et al., 2013). To mitigate the effects of unreliable raters and get an upper bound for human performance, we use two measures: (1) discard the results of the worst performing rater in each pair (in terms of F1) and use the vote of the higher scoring raters. (2) identify the least trustworthy raters and downweight their votes using scores from an itemresponse model, MACE (Hovy et al., 2013). The first requires access to the original"
P16-3016,P14-2084,0,0.0620942,"less than chance expected agreement. Two possible explanations for low rater agreement are (1) that sarcasm recognition is a difficult task for humans (Kreuz and Caucci, 2007; Gonz´alez-Ib´anez et al., ing e.g., Davidov et al. (2010) 2010, Gonz´alez-Ib´anez et al. (2011). 2 Krippendorff (2012) considers 0.67 to be lowest acceptable agreement score. 1 This was also necessary because prior sarcasm detection studies relied on self-annotation of sarcasm by Twitter users applying their own judgements of sarcastic mean108 2011), especially without access to the surrounding context (Filatova, 2012; Wallace et al., 2014), and (2) that people undertaking such tasks remotely online are often guilty of ‘spamming,’ or providing careless or random responses (Hovy et al., 2013). To mitigate the effects of unreliable raters and get an upper bound for human performance, we use two measures: (1) discard the results of the worst performing rater in each pair (in terms of F1) and use the vote of the higher scoring raters. (2) identify the least trustworthy raters and downweight their votes using scores from an itemresponse model, MACE (Hovy et al., 2013). The first requires access to the original annotated labels, the l"
P16-3016,P11-2102,0,\N,Missing
P16-3016,N13-1132,1,\N,Missing
P19-1339,P16-1231,0,0.0306712,"Missing"
P19-1339,D14-1082,0,0.0336313,"butions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field are removed, resulting in 1,258 WSJ articles with"
P19-1339,N10-1093,0,0.0285274,"dinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an"
P19-1339,P15-1073,1,0.823085,"al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender information of the authors. However, existing data sets fail to meet both criteria: data sets with gender information are either too small to train on, lack syntactic information, or are restricted to social media; sufficiently large syntactic dat"
P19-1339,P15-2079,1,0.885205,"(2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount"
P19-1339,P16-2096,1,0.880471,"n parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in m"
P19-1339,K15-1011,1,0.849754,"se grammatical features to signal the speakers’ membership in a demographic group, with a focus on gender (Vigliocco and Franck, 1999; Mondorf, 2002; Eckert and McConnell-Ginet, 2013). Mondorf (2002) shows systemic differences in the usage of various types of clauses and their positions for men and women, stating that women have a higher usage of adverbial (accordingly, consequently1 ), causal (since, because), conditional (if, when) and purpose (so, in order that) clauses, while men tend to use more concessive clauses (but, although, whereas). Similar results hold across various languages in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy a"
P19-1339,W15-4302,1,0.891018,"ges in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little unde"
P19-1339,P03-1054,0,0.0541134,"tional Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field a"
P19-1339,D17-1119,0,0.0122243,"ted demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender information of the authors. However, existing data sets fail to meet both criteria: data sets with gender information are either too small to train on, lack syntactic information, or are restricted to social media; sufficiently large syntactic data sets are not labeled with gender information and rely (at least in part) on news g"
P19-1339,J93-2004,0,0.0746908,"r gender. 3493 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3493–3498 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 192"
P19-1339,W04-2407,0,0.159302,"Missing"
P19-1339,C04-1010,0,0.0778337,"butions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field are removed, resulting in"
P19-1339,W96-0213,0,0.827935,"tics, pages 3493–3498 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1"
P19-1339,D13-1170,0,0.00622069,"important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender informatio"
P19-1339,N03-1033,0,0.0766919,"- August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retri"
P19-1339,W00-1308,0,0.425141,"Missing"
P19-1339,D13-1187,0,0.08357,"Missing"
P19-1339,D17-1323,0,0.0751563,"men tend to use more concessive clauses (but, although, whereas). Similar results hold across various languages in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015"
Q18-1040,L16-1323,1,0.878666,"le, Median, Mean, 3rd Quartile, and Max). 4.1 Datasets We evaluate on a collection of datasets reflecting a variety of use-cases and conditions: binary vs. multi-class classification; small vs. large number of annotators; sparse vs. abundant number of items per annotator / annotators per item; and varying degrees of annotator quality (statistics presented in Table 1). Three of the datasets— WSD, RTE, and TEMP, created by Snow et al. (2008)—are widely used in the literature on annotation models (Carpenter, 2008; Hovy et al., 2013). In addition, we include the Phrase Detectives 1.0 (PD) corpus (Chamberlain et al., 2016), which differs in a number of key ways from the Snow et al. (2008) datasets: It has a much larger number of items and annotations, greater sparsity, and a much greater likelihood of spamming due to its collection via a game-with-a-purpose setting. This dataset is also less artificial than the datasets in Snow et al. (2008), which were created with the express purpose of testing crowdsourcing. The data consist of anaphoric annotations, which we reduce to four general classes (DN/DO = discourse new/old, PR = property, and NR = non-referring). To ensure similarity with the Snow et al. (2008) dat"
Q18-1040,D16-1129,0,0.182711,"s prevalence π ∼ Dirichlet(1K ) • For every item i ∈ {1, 2, ..., I}: – Draw true class ci ∼ Categorical(π) – For every position n ∈ {1, 2, ..., Ni }: ∗ Draw annotation yi,n ∼ Categorical(βjj[i,n],ci )4 • For every class k ∈ {1, 2, ..., K}: Multi-Annotator Competence Estimation (MACE) This model, introduced by Hovy et al. (2013), takes into account the credibility of the annotators and their spamming preference and strategy5 (see Figure 3). This is another example of an unpooled model, and possibly the model most widely applied to linguistic data (e.g., Plank et al., 2014a; Sabou et al., 2014; Habernal and Gurevych, 2016, inter alia). Its generative process is: • For every annotator j ∈ {1, 2, ..., J}: • For every annotator j ∈ {1, 2, ..., J}: – For every class k ∈ {1, 2, ..., K}: ∗ Draw class annotator abilities βj,k,k0 ∼ Normal(ζk,k0 , Ωk,k0 ), ∀k 0 • Draw class prevalence π ∼ Dirichlet(1K ) – Draw spamming behavior j ∼ Dirichlet(10K ) – Draw credibility θj ∼ Beta(0.5, 0.5) • For every item i ∈ {1, 2, ..., I}: – Draw true class ci ∼ Categorical(π) – For every position n ∈ {1, 2, ..., Ni }: ∗ Draw annotation yi,n ∼ Categorical(softmax(βjj[i,n],ci ))7 • For every item i ∈ {1, 2, ..., I}: – Draw true class ci"
Q18-1040,N13-1132,1,0.243271,"ough, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al.,"
Q18-1040,N15-1089,0,0.133747,"th et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al., 2015, inter alia), it is not immediately obvious to potential users how these models differ or, in fact, how they should be applied at all. To our knowledge, the literature comparing models of annotation is limited, focused exclusively on synthetic data (Quoc Viet Hung et al., 2013) or using publicly available implementations that constrain the experiments almost exclusively to binary annotations (Sheshadri and Lease, 2013). Contributions • Our selection of six widely used models (Dawid and Skene, 1979; Carpenter, 2008; Hovy et al., 2013) covers models wit"
Q18-1040,felt-etal-2014-momresp,0,0.0823473,"Hill (2007). Item-response theory has also been recently applied to NLP applications (Lalor et al., 2016; Martınez-Plumed et al., 2016; Lalor et al., 2017). The models considered so far take into account only the annotations. There is work, however, that further exploits the features that can accompany items. A popular example is the model introduced by Raykar et al. (2010), where the true class of an item is made to depend both on the annotations and on a logistic regression model that are jointly fit; essentially, the logistic regression replaces the simple categorical model of prevalence. Felt et al. (2014, 2015b) introduced similar models that also modeled the predictors (features) and compared them to other approaches (Felt et al., 2015a). Kamar et al. (2015) account for task-specific feature effects on the annotations. In §6.2, we discussed the label switching problem (Stephens, 2000) that many models of annotation suffer from. Other solutions proposed in the literature include utilizing class-informative priors, imposing ordering constraints (obvious for univariate parameters; less so in multivariate cases) (Gelman et al., 2013), or applying different post-inference relabeling techniques (F"
Q18-1040,K15-1020,0,0.131074,"th et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and correct for their bias, as well as accounting for item-level effects. They have been shown to perform better than non-probabilistic alternatives based on heuristic analysis or adjudication (Quoc Viet Hung et al., 2013). But even though a large number of such models has been proposed (Carpenter, 2008; Whitehill et al., 2009; Raykar et al., 2010; Hovy et al., 2013; Simpson et al., 2013; Passonneau and Carpenter, 2014; Felt et al., 2015a; Kamar et al., 2015; Moreno et al., 2015, inter alia), it is not immediately obvious to potential users how these models differ or, in fact, how they should be applied at all. To our knowledge, the literature comparing models of annotation is limited, focused exclusively on synthetic data (Quoc Viet Hung et al., 2013) or using publicly available implementations that constrain the experiments almost exclusively to binary annotations (Sheshadri and Lease, 2013). Contributions • Our selection of six widely used models (Dawid and Skene, 1979; Carpenter, 2008; Hovy et al., 2013) covers models wit"
Q18-1040,D16-1062,0,0.0409278,"jointly estimating the ability of the individuals and the difficulty of the test items based on the correctness of their responses. The models of annotation we discussed in this paper are completely unsupervised and infer, in addition to annotator ability and/or item difficulty, the correct labels. More details on item-response models are given in Skrondal and Rabe-Hesketh (2004) and Gelman Technical Notes Posterior Curvature. In hierarchical models, a complicated posterior curvature increases the dif581 and Hill (2007). Item-response theory has also been recently applied to NLP applications (Lalor et al., 2016; Martınez-Plumed et al., 2016; Lalor et al., 2017). The models considered so far take into account only the annotations. There is work, however, that further exploits the features that can accompany items. A popular example is the model introduced by Raykar et al. (2010), where the true class of an item is made to depend both on the annotations and on a logistic regression model that are jointly fit; essentially, the logistic regression replaces the simple categorical model of prevalence. Felt et al. (2014, 2015b) introduced similar models that also modeled the predictors (features) and compa"
Q18-1040,H93-1012,0,0.0277539,"Missing"
Q18-1040,Q14-1025,1,0.848921,"of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of th"
Q18-1040,C14-1168,1,0.740603,"(spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and cor"
Q18-1040,P14-2083,1,0.89224,"(spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to characterize the accuracy of the annotators and cor"
Q18-1040,W05-0311,1,0.910634,"tasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation. 1 Introduction The standard methodology for analyzing crowdsourced data in NLP is based on majority voting (selecting the label chosen by the majority of coders) and inter-annotator coefficients of agreement, such as Cohen’s κ (Artstein and Poesio, 2008). However, aggregation by majority vote implicitly assumes equal expertise among the annotators. This assumption, though, has been repeatedly shown to be false in annotation practice (Poesio and Artstein, 2005; Passonneau and Carpenter, 2014; Plank et al., 2014b). Chanceadjusted coefficients of agreement also have many shortcomings—for example, agreements in mistake, overly large chance-agreement in datasets with skewed classes, or no annotator bias correction (Feinstein and Cicchetti, 1990; Passonneau and Carpenter, 2014). Research suggests that models of annotation can solve these problems of standard practices when applied to crowdsourcing (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010; Hovy et al., 2013; Passonneau and Carpenter, 2014). Such probabilistic approaches allow us to"
Q18-1040,D08-1027,0,0.578026,"Missing"
Q18-1040,J08-4004,1,\N,Missing
S14-1001,P05-3014,0,0.0381676,"se (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged S EM C OR.3 3.3 4 Results The results are presented in Table 2. We distinguish between three settings with various degrees of supervision: weakly supervised, which uses no domain annotated information, but solely relies on embeddings trained on unlabeled Twitter data; unsupervised domain adaptation (DA"
S14-1001,C12-1028,0,0.0175268,"rds. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work There has been relatively little previous work on supersense tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (S EM C OR and S ENS E VAL). The task of supersense tagging was first"
S14-1001,W06-1670,0,0.762458,"aptation (here, from newswire to Twitter). In We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not"
S14-1001,H94-1046,0,0.25003,"simply express the opinions of the author on some subject matter. Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE. If someone posts a message saying that some LaTeX module now supports “drawing trees”, it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the follo"
S14-1001,W02-1001,0,0.0447847,"ocial v.stative v.weather Table 1: The 41 noun and verb supersenses in WordNet Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. 2 More or less supervised models This sections covers the varying degree of supervision of our systems as well as the usage of type constraints as distant supervision. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (S EARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and S EARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. 2.1 Distant supervision Distant supervision in these experiments was implemented by only allowing a system to predict a certain supersense for a given word if that supersense had either been observed in the training data, or"
S14-1001,R13-1026,0,0.0254562,"aramita and Altun (2006).1 S EARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 3 Experiments We experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data. Note that in all our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the T"
S14-1001,E14-1078,1,0.832545,"l our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data sets, we carried out an annotation task. We first pre-annotated all data sets with WordNet’s most frequent senses. If the word was not in WordNet and a noun, we assigned it the sense n.person. All other words were labeled O. Chains of nouns were altered to give every element the sense of the head noun, and the BI tags adjusted, i.e.: we use predicted POS tags as input to the system, in order to produce a realistic est"
S14-1001,I11-1100,0,0.0237767,"Missing"
S14-1001,D11-1141,0,0.1048,"ng tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • S EM C OR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twi"
S14-1001,E14-4042,0,0.0248191,"tiv e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work Th"
S14-1001,P12-2050,0,0.379949,"rovide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets wit"
S14-1001,P05-1005,0,0.0587046,"tion sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. The data is publicly available for download. In this article we have provided, t"
S14-1001,P99-1023,0,0.187848,"Missing"
S14-1001,S10-1049,0,0.0509089,"nd Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNe"
S14-1001,D12-1127,0,0.0563414,"Missing"
S14-1001,W11-0328,0,0.0190122,"er k on development data for using the k-most frequent senses inWordNet as type constraints. Our supervised models are trained on S EM C OR +R ITTER -T RAIN or simply R ITTER -T RAIN, depending on what gave us the best performance on the held-out data. Baselines For most word sense disambiguation studies, predicting the most frequent sense (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reach"
S14-1001,W13-0906,0,0.0402627,"d Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) a"
S14-1001,tsvetkov-etal-2014-augmenting-english,1,0.22216,"n from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.). Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names. Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available"
S14-1001,S07-1051,0,0.166033,"and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers"
S14-1001,J10-1004,0,0.0158174,"ystems (DA) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + - Supervised domain adaptation systems (SU) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + + + + + + Table 2: Weighted F1 average over 41 supersenses. 7 noun .act noun .food noun .attri bute noun .relat ion verb. cogn ition verb. creat ion verb. emot ion verb. moti on verb. perce ption verb. stativ e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum,"
S14-2034,W09-1206,0,0.114151,"Missing"
S14-2034,C10-1011,0,0.164674,"Missing"
S14-2034,J93-2004,0,0.0469115,"Missing"
S14-2034,D07-1111,0,0.0867606,"Missing"
S14-2034,C08-1095,0,0.112956,"Missing"
S16-1084,attardi-etal-2010-resource,0,0.136968,"Missing"
S16-1084,S16-1143,0,0.0332991,"Missing"
S16-1084,S16-1142,0,0.0370432,"Missing"
S16-1084,J92-4003,0,0.211357,"Missing"
S16-1084,W13-0907,1,0.0637827,"Missing"
S16-1084,2012.eamt-1.60,0,0.0171689,"Missing"
S16-1084,P15-2079,1,0.0638888,"Missing"
S16-1084,W06-1670,0,0.0258617,"nal) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not suffer from these problems, as it uses a much smaller number of coarsegrained classes. However, these classes only apply to a subset of the nouns in a sentence and exclude verbs and adjectives. They therefore provide far from complete coverage in a corpus. Noun and verb supersenses (Ciaramita and Altun, 2006) offer a middle ground in granularity: they generalize named entity classes to cover all nouns (with 26 classes), but also cover verbs (15 classes)— see table 1—and provide a human-interpretable high-level clustering. WordNet supersenses for adjectives and adverbs nominally exist, but are based on morphosyntactic rather than semantic properties. There is, however, recent work on developing supersense taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION"
S16-1084,C10-2052,1,0.110925,"Missing"
S16-1084,W11-0809,0,0.114516,"ord expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized se"
S16-1084,S14-1001,1,0.837622,"V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idio"
S16-1084,S16-1140,0,0.0497829,"Missing"
S16-1084,D14-1108,1,0.144265,"Missing"
S16-1084,J93-2004,0,0.0679023,"Missing"
S16-1084,H93-1061,0,0.121824,"Missing"
S16-1084,2014.iwslt-papers.16,0,0.201582,"Missing"
S16-1084,N13-1039,1,0.0446902,"Missing"
S16-1084,W09-3531,0,0.0354613,"Missing"
S16-1084,E14-1078,1,0.043392,"Missing"
S16-1084,W12-3301,0,0.173236,"Missing"
S16-1084,W95-0107,0,0.102727,"Missing"
S16-1084,D11-1141,0,0.00536312,"Missing"
S16-1084,W15-1612,1,0.230436,"Missing"
S16-1084,S16-1141,0,0.125282,"Missing"
S16-1084,S10-1049,0,0.0152066,"evel organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION. Schneider and Smith (2015) develop this"
S16-1084,S16-1145,0,0.0357113,"Missing"
S16-1084,W13-0906,0,0.0274625,"Missing"
S16-1084,Q14-1016,1,0.485545,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,tsvetkov-etal-2014-augmenting-english,1,0.865215,"nse taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION N : SHAPE N : STATE N : SUBSTANCE N : TIME V: BODY V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation,"
S16-1084,N13-1076,1,0.85833,"Missing"
S16-1084,P12-2050,1,0.107223,"V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should"
S16-1084,schneider-etal-2014-comprehensive,1,0.119259,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,N15-1177,1,0.219831,"rds. While the main corpus with WordNet senses, SemCor (Miller et al., 1993), does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech. This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora. To address this limitation, in the DiMSUM 2016 shared task,1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following Schneider and Smith, 2015), on multiple nontraditional genres of text. By moving away from fine-grained sense inventories and lexicalized, language-specific2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis. We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres. The integrated lexical semantic representation (§2, §3) has been annotated in an extensive benchmark data set comprising several nontraditional domains (§4). Objective, controlled evaluation procedures ("
S16-1084,M95-1005,0,0.031558,"Missing"
S16-1084,W11-0817,0,0.0301138,"n with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not"
S16-1084,I13-1024,0,0.0698786,"prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and coveri"
S16-1084,S07-1051,0,0.0267622,"nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION."
S16-1084,picca-etal-2008-supersense,0,\N,Missing
S16-1084,J13-1009,0,\N,Missing
S16-1084,W15-1806,1,\N,Missing
S16-1084,S16-1144,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W10-0719,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,D08-1027,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W06-1670,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,H93-1061,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,J12-3005,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P06-2072,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P14-1024,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,P12-2050,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,N13-1132,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,I08-2105,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,peters-peters-2000-treatment,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,E14-1049,1,\N,Missing
W11-2210,P06-2005,0,0.323646,"ojamarn et al., 2010) which aims to automatically obtain bilingual lists of names written in different scripts. They also employ string-similarity measures to find similar string pairs written in different scripts. However, their input data is constrained to Wikipedia articles written in different languages, whereas we impose no constrains on our input data, and merely require a large collection thereof. Noisy text normalization, on the other hand, has recently received a lot of focus. Most works construe the problem in the metaphors of either machine translation (MT) (Bangalore et al., 2002; Aw et al., 2006; Kaufmann and Kalita, 2010), spelling correction (Choudhury et al., 2007; Cook and Stevenson, 2009), or automated speech recognition (ASR) (Kobus et al., 2008). For our evaluation, we developed an implementation of Contractor (2010) which works on the same general approach as Han (2011). 6 Conclusions and Future Work The ability to automatically extract lexical variants from large noisy corpora has many practical applications, including noisy text normalization, query spelling suggestion, fixing OCR errors, and so on. This paper developed a novel methodology for automatically mining such pair"
W11-2210,C02-1134,0,0.010798,"iteration mining’ (Jiampojamarn et al., 2010) which aims to automatically obtain bilingual lists of names written in different scripts. They also employ string-similarity measures to find similar string pairs written in different scripts. However, their input data is constrained to Wikipedia articles written in different languages, whereas we impose no constrains on our input data, and merely require a large collection thereof. Noisy text normalization, on the other hand, has recently received a lot of focus. Most works construe the problem in the metaphors of either machine translation (MT) (Bangalore et al., 2002; Aw et al., 2006; Kaufmann and Kalita, 2010), spelling correction (Choudhury et al., 2007; Cook and Stevenson, 2009), or automated speech recognition (ASR) (Kobus et al., 2008). For our evaluation, we developed an implementation of Contractor (2010) which works on the same general approach as Han (2011). 6 Conclusions and Future Work The ability to automatically extract lexical variants from large noisy corpora has many practical applications, including noisy text normalization, query spelling suggestion, fixing OCR errors, and so on. This paper developed a novel methodology for automatically"
W11-2210,P08-1077,0,0.0169111,"ributed programming paradigm, can efficiently compute allpairs distributional similarity over very large corpora (e.g., the Twitter pairs we use later were mined from a corpus of half a billion Twitter messages). Using a similar strategy as Pasca and Dienes, we define term contexts as the bigrams that appear to the left and to the right of a given word (Pasca and 84 Dienes, 2005). Following standard practice, the contextual vectors are weighted according to pointwise mutual information and the similarity between the vectors is computed using the cosine similarity metric (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). It is important to note that there are many other possible ways to compute distributional and semantic similarity, and that just about any approach can be used within our framework. The approach used here was chosen because we had an existing implementation. Indeed, other approaches may be more apt for other data sets and tasks. This approach is applied to both the common English corpus and the domain corpus. This yields two sets of semantically (distributionally) similar word pairs that will ultimately be used to distill unsupervised lexical variants. 2.2 Filtering Common English Variants G"
W11-2210,C10-2022,0,0.169031,"ics for handling specific out-ofvocabulary-but-valid tokens such as hash tags and @usernames. C ANDIDATE SELECTION involves comparing an unknown OOV word to a list of words which are deemed in-vocabulary, and producing a top-K ranked list with candidate words and their estimated probabilities of relevance as output. This process requires a function with which to compute the similarity or alternatively, distance, between two words. More traditional string-similarity functions like the simple Lehvenshtein string edit distance do not fare too well in this domain. We implement the IBM-similarity (Contractor et al., 2010) which employs a slightly more advanced similarity function. It finds the length of the longest common subsequence (LCS) between two strings s1 and s2 , normalized by the edit distance (ED) between the consonants in each string (referred to as the ‘consonant skeleton’ (CS)), thus sim(s1 , s2 ) = LCS(s1 , s2 ) ED(CS(s1 ), CS(s2 )) Finally, the DECODING step takes an input word lattice (lattice of concatenated, weighted confusion sets), and produces a new lattice by incorporating 87 the probabilities from an n-gram language model with the prior probabilities in the lattice to produce a reranked"
W11-2210,W09-2010,0,0.255495,"in different scripts. They also employ string-similarity measures to find similar string pairs written in different scripts. However, their input data is constrained to Wikipedia articles written in different languages, whereas we impose no constrains on our input data, and merely require a large collection thereof. Noisy text normalization, on the other hand, has recently received a lot of focus. Most works construe the problem in the metaphors of either machine translation (MT) (Bangalore et al., 2002; Aw et al., 2006; Kaufmann and Kalita, 2010), spelling correction (Choudhury et al., 2007; Cook and Stevenson, 2009), or automated speech recognition (ASR) (Kobus et al., 2008). For our evaluation, we developed an implementation of Contractor (2010) which works on the same general approach as Han (2011). 6 Conclusions and Future Work The ability to automatically extract lexical variants from large noisy corpora has many practical applications, including noisy text normalization, query spelling suggestion, fixing OCR errors, and so on. This paper developed a novel methodology for automatically mining such pairs from a large domainspecific corpus. The approach makes use of distributional similarity for measur"
W11-2210,W11-0704,1,0.753483,"se of a competitive heuristic text normalization method over Twitter data as a baseline, and compare its accuracy to an augmented method which makes use of an automatically induced exception dictionary (using the method described in Section 2) as a first step, before resorting to the same baseline method as a ‘back-off’ for words not found in the dictionary. As we point out in Section 5, there are various metaphors within which the noisy text normalization problem has been approached. In general, however, the problem of noisy text normalization may be approached by using a three step process (Gouws et al., 2011): 1. In the out-of-vocabulary (OOV) detection step, we detect unknown words which are candidates for normalization 2. In the candidate selection step, we find the weighted lists of most likely candidates (from a list of in-vocabulary (IV) words) for the OOV words and group them into a confusion set. The confusion sets are then appended to one another to create a confusion- network or lattice 3. Finally, in the decoding step, we use a language model to rescore the confusion network, and then find the most likely posterior path (Viterbi path) through this network. The words at each node in the r"
W11-2210,P11-1038,0,0.458159,"Missing"
W11-2210,W10-2405,0,0.0241829,"f our knowledge, we are the first to address the problem of mining pairs of lexical variants from noisy text in an unsupervised and purely statistical manner that does not require aligned noisy and clean messages. To obtain aligned clean and noisy text without annotated data implies the use of some normalizing method first. Yvon (2010) presents one such approach, where they generate exception dictionaries from their finite-state system’s normalized output. However, their method is still trained on annotated training pairs, and hence supervised. A related direction is ‘transliteration mining’ (Jiampojamarn et al., 2010) which aims to automatically obtain bilingual lists of names written in different scripts. They also employ string-similarity measures to find similar string pairs written in different scripts. However, their input data is constrained to Wikipedia articles written in different languages, whereas we impose no constrains on our input data, and merely require a large collection thereof. Noisy text normalization, on the other hand, has recently received a lot of focus. Most works construe the problem in the metaphors of either machine translation (MT) (Bangalore et al., 2002; Aw et al., 2006; Kauf"
W11-2210,C08-1056,0,0.0815597,"to find similar string pairs written in different scripts. However, their input data is constrained to Wikipedia articles written in different languages, whereas we impose no constrains on our input data, and merely require a large collection thereof. Noisy text normalization, on the other hand, has recently received a lot of focus. Most works construe the problem in the metaphors of either machine translation (MT) (Bangalore et al., 2002; Aw et al., 2006; Kaufmann and Kalita, 2010), spelling correction (Choudhury et al., 2007; Cook and Stevenson, 2009), or automated speech recognition (ASR) (Kobus et al., 2008). For our evaluation, we developed an implementation of Contractor (2010) which works on the same general approach as Han (2011). 6 Conclusions and Future Work The ability to automatically extract lexical variants from large noisy corpora has many practical applications, including noisy text normalization, query spelling suggestion, fixing OCR errors, and so on. This paper developed a novel methodology for automatically mining such pairs from a large domainspecific corpus. The approach makes use of distributional similarity for measuring semantic similarity, a novel approach for filtering comm"
W11-2210,I05-1011,0,0.00946264,"n English corpus and the domain corpus. There are many different ways to measure semantic relatedness. In this work, we use distributional similarity as our measure of semantic similarity. However, since we are taking a fully unsupervised approach, we do not know a priori which pairs of terms may be related to each other. Hence, we must compute the semantic similarity between all possible pairs of terms within the lexicon. To solve this computationally challenging task, we use a large-scale allpairs distributional similarity approach similar to the one originally proposed by Pasca and Dienes (Pasca and Dienes, 2005). Our implementation, which makes use of Hadoop’s MapReduce distributed programming paradigm, can efficiently compute allpairs distributional similarity over very large corpora (e.g., the Twitter pairs we use later were mined from a corpus of half a billion Twitter messages). Using a similar strategy as Pasca and Dienes, we define term contexts as the bigrams that appear to the left and to the right of a given word (Pasca and 84 Dienes, 2005). Following standard practice, the contextual vectors are weighted according to pointwise mutual information and the similarity between the vectors is com"
W12-1905,P07-1036,0,0.200301,"si.edu Abstract (Johnson, 2007). Running repeated restarts with random initialization can help escape local maxima, but in order to find the global optimum, we need to run a great number (100 or more) of them (Ravi and Knight, 2009; Hovy et al., 2011). However, there is another solution. Various papers have shown that the inclusion of some knowledge greatly enhances performance of unsupervised systems. They introduce constraints on the initial model and the parameters. This directs the learning algorithm towards a better parameter configuration. Types of constraints include ILP-based methods (Chang et al., 2007; Chang et al., 2008; Ravi and Knight, 2009), and posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). While those approaches are powerful and yield good results, they require us to reformulate the constraints in a certain language, and either use an external solver, or re-design parts of the maximization step. This is time-consuming and requires a certain expertise. For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and find the best model, we need to start with a good initial model. Researchers suggested repeated random restar"
W12-1905,W02-0102,0,0.12242,"position as last element For POS-tagging, we use a standard bigram HMM without back-off. For PSD, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words (see Figure 2). Since not all prepositions have the same set of labels, we train individual models for each preposition. We can thus learn different parameter settings for the different prepositions. We use EM with smoothing and random restarts to train our models. For smoothing,  is added to each fractional count before normalization at each iteration to prevent overfitting (Eisner, 2002a). We set  to 0.01. We stop training after 40 iterations, or if the perplexity change between iterations was less than 0.0001. We experimented with different numbers of random restarts (none, 10, 50, and 100). 3.3 Dealing with Partial Annotations The most direct way to constrain a specific word to only one label is to substitute it for a special token that has only that label. If we have a partially annotated example “walk on-sense5 water” as input (see Figure 1), we add an emission probability P (word = label |tag = label ) to our model. However, this is problematic in two ways. Firstly, we"
W12-1905,P02-1001,0,0.265279,"position as last element For POS-tagging, we use a standard bigram HMM without back-off. For PSD, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words (see Figure 2). Since not all prepositions have the same set of labels, we train individual models for each preposition. We can thus learn different parameter settings for the different prepositions. We use EM with smoothing and random restarts to train our models. For smoothing,  is added to each fractional count before normalization at each iteration to prevent overfitting (Eisner, 2002a). We set  to 0.01. We stop training after 40 iterations, or if the perplexity change between iterations was less than 0.0001. We experimented with different numbers of random restarts (none, 10, 50, and 100). 3.3 Dealing with Partial Annotations The most direct way to constrain a specific word to only one label is to substitute it for a special token that has only that label. If we have a partially annotated example “walk on-sense5 water” as input (see Figure 1), we add an emission probability P (word = label |tag = label ) to our model. However, this is problematic in two ways. Firstly, we"
W12-1905,W10-1701,0,0.0264235,"ances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires some modification of the EM framework. 7 Conclusion and Future Work It is obvious, and common knowledge, that providing some annotation to an unsupervised algorithm will improve accuracy and learning speed. Surprisingly, however, our literature search did not turn up any papers stating exactly how and to what degree the improvements appear. We therefore selected a very general training method, EM, and a simple approach to i"
W12-1905,P08-1085,0,0.0180592,"traints. Careful selection of the right heuristics and the tradeoff between false positives they in36 Unsupervised methods have great appeal for resource-poor languages and new tasks. They have been applied to a wide variety of sequential labeling tasks, such as POS tagging, NE recognition, etc. The most common training technique is forwardbackward EM. While EM is guaranteed to improve the data likelihood, it can get stuck in local maxima. Merialdo (1994) showed how the the initialized model influences the outcome after a fixed number of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their settin"
W12-1905,P07-1094,0,0.0716994,"Missing"
W12-1905,C10-2052,1,0.826539,"ein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires some modification of the EM framework. 7 Conclusion and Future Work It is obvious, and common knowledge, that providing some annotation to an unsupervised algorithm will improve accuracy and learning speed. Surprisingly, however, our literature search did not turn up any papers stating exactly how and to what degree the improvements appear. We therefore selected a very general training method, EM, and a simple approach to i"
W12-1905,P11-2056,1,0.851936,"emEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential nature of the problem can be exploited in unsupervised learning. They present various sequential models and training options. They compare a standard bigram HMM and a very complex model that is designed to capture mutual constraints. In contrast to them, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words. As suggested there, we use EM with smoothing and random restarts. WordNet lexicographer senses as labels for the arguments. It has 45 labels for nouns, verbs, and adjectives and is thus roughly c"
W12-1905,D07-1031,0,0.0206166,"m with another token: there are now fewer instances from which we collect C(on|sense5 ). The fractional counts for our transition parameters are not affected by this, but the counts for emission parameter are skewed. We thus essentially siphon probability mass from P (on|sense5 ) and move it to P (on : sense5 |sense5 ). Since the test data never contains labels such as sense5 , our partial annotations have moved a large amount of probability mass to a useless parameter: we are never going to use P (on : sense5 |sense5 ) during inference! Secondly, since EM tends to find uniform distributions (Johnson, 2007), other, rarer labels will also have to receive some probability. The counts for labels with partial annotations are fixed, so in order to use the rare labels (for which we have no partial annotations), their emission counts need to come from unlabeled instances. Say sense1 is a label for which we have no partial annotations. Every time EM collects emission counts from a word “on” (and not a labeled version “on:sensen ”), it assigns some of it to P (on|sense1 ). Effectively, we thus assign too much probability mass to the emission of the word from rare labels. The result of these two effects i"
W12-1905,S07-1005,0,0.0197228,"e use, we often also know which labels are applicable to which 32 While the technique is mainly useful for problems where only few labeled examples are available, we make use of a corpus of annotated data. This allows us to control the effect of the amount and type of annotated data on accuracy. We evaluate the impact of partial annotations on two tasks: preposition sense disambiguation and POS tagging. 2.2 Preposition Sense Disambiguation Prepositions are ubiquitous and highly ambiguous. Disambiguating prepositions is thus a challenging and interesting task in itself (see SemEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential natu"
W12-1905,J94-2001,0,0.305076,"dom restarts. This indicates that the same accuracy can be reached with fewer restarts, which speeds up training time considerably. Our contributions are: • we show how to include partial annotations in EM training via parameter tying • we show how the amounts and distribution of partial annotations influence accuracy • we evaluate our method on an existing data set, comparing to both supervised and unsupervised methods on two tasks 2 2.1 observations. This is encoded in a dictionary. For POS-tagging, it narrows the possible tags for each word–irrespective of context–down to a manageable set. Merialdo (1994) showed how the amount of available dictionary information is correlated with performance. However, dictionaries list all applicable labels per word, regardless of context. We can often restrict the applicable label for an observation in a specific context even more. We extend this to include constraints applied to some, but not all instances. This allows us to restrict the choice for an observation to one label. We substitute the word in case by a special token with just one label. Based on simple heuristics, we can annotate individual words in the training data with their label. For example,"
W12-1905,H94-1048,0,0.226441,"Missing"
W12-1905,P09-1057,0,0.0204689,"repeated restarts with random initialization can help escape local maxima, but in order to find the global optimum, we need to run a great number (100 or more) of them (Ravi and Knight, 2009; Hovy et al., 2011). However, there is another solution. Various papers have shown that the inclusion of some knowledge greatly enhances performance of unsupervised systems. They introduce constraints on the initial model and the parameters. This directs the learning algorithm towards a better parameter configuration. Types of constraints include ILP-based methods (Chang et al., 2007; Chang et al., 2008; Ravi and Knight, 2009), and posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). While those approaches are powerful and yield good results, they require us to reformulate the constraints in a certain language, and either use an external solver, or re-design parts of the maximization step. This is time-consuming and requires a certain expertise. For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and find the best model, we need to start with a good initial model. Researchers suggested repeated random restarts or constraints that guide the model evolu"
W12-1905,W09-3003,0,0.0303741,"to improve the data likelihood, it can get stuck in local maxima. Merialdo (1994) showed how the the initialized model influences the outcome after a fixed number of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a"
W12-1905,P05-1044,0,0.0677564,"Missing"
W12-1905,N09-3017,1,0.799359,"on and POS tagging. 2.2 Preposition Sense Disambiguation Prepositions are ubiquitous and highly ambiguous. Disambiguating prepositions is thus a challenging and interesting task in itself (see SemEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential nature of the problem can be exploited in unsupervised learning. They present various sequential models and training options. They compare a standard bigram HMM and a very complex model that is designed to capture mutual constraints. In contrast to them, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words. As s"
W12-1905,C08-1113,0,0.126176,"of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires s"
W12-1905,P10-2039,0,0.026539,"Missing"
W12-1905,C08-1142,1,0.889626,"Missing"
W13-0907,W10-0303,0,0.0373687,"ly concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related wo"
W13-0907,E06-1042,0,0.0651925,"Missing"
W13-0907,W07-0104,0,0.196907,"Missing"
W13-0907,P04-1054,0,0.358462,"istinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “Tha"
W13-0907,J09-1005,0,0.0130978,"d of 3872 instances, 1749 of them labeled as metaphors. • we annotate and release a corpus of 3872 instances for supervised metaphor classification • we are the first to use tree kernels for metaphor identification • our approach achieves an F1-score of 0.75, the best score of of all systems tested. 2 Data 2.1 Annotation We downloaded a list of 329 metaphor examples from the web4 . For each expression, we extracted sentences from the Brown corpus that contained the seed (see Figure 1 for an example). To decide 3 A similar assumption can be used to detect the literal/nonliteral uses of idioms (Fazly et al., 2009). 4 http://www.metaphorlist.com and http:// www.macmillandictionaryblog.com 53 Figure 2: Screenshot of the annotation interface on Amazon’s Mechanical Turk We divided the data into training, dev, and test sets, using a 80-10-10 split. All results reported here were obtained on the test set. Tuning and development was only carried out on the dev set. 2.2 Vector Representation of Words The same word may occur in a literal and a metaphorical usage. Lexical information alone is 5 While this is somewhat imprecise and not always easy to decide, it proved to be a viable strategy for untrained annotat"
W13-0907,W06-3506,0,0.322276,"Missing"
W13-0907,J09-4007,0,0.0284105,"ral and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could state that it was not poss"
W13-0907,E12-1019,1,0.900764,"the input sentence with the FANSE parser (Tratz and Hovy, 2011)6 . It provides the dependency structure, POS tags, and other information. To construct the different tree representations, we replace each node in the tree with its word, lemma, POS tag, dependency label, or supersense (the WordNet lexicographer name of the word’s first sense (Fellbaum, 1998)), and mark the word in question with a special node. See Figure 3 for a graphical representation. These trees are used in addition to the vectors. This approach is similar to the ones described in (Moschitti et al., 2006; Qian et al., 2008; Hovy et al., 2012). 2.4 Classification Models the sweet in Boston DT JJ IN NNP O adj.all O n.location Figure 3: Graphic demonstration of our approach. a) dependency tree over words, with node of interest labeled. b) as POS representation. c) as supersense representation The intuition behind our approach is that metaphorical use differs from literal use in certain syntactic relations. For example, the only difference between the two sentences “I like the sweet people in Boston” and “I like the sweet pies in Boston” is the head of “sweet”. Our assumption is that—given enough examples—certain patterns emerge (e.g."
W13-0907,N13-1132,1,0.381717,"on was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could state that it was not possible to decide. We paid $0.09 for each set of 10 instances. Each instance was annotated by 7 annotators. Instances where the annotators agreed that it was impossible to tell whether it is a metaphor or not were discarded. Inter-annotator agreement was 0.57, indicating a difficult task. In order to get the label for each instance, we weighted the annotator’s answers using MACE (Hovy et al., 2013), an implementation of an unsupervised item-response model. This weighted voting produces more reliable estimates than simple majority voting, since it is capable of sorting out unreliable annotators. The final corpus consisted of 3872 instances, 1749 of them labeled as metaphors. • we annotate and release a corpus of 3872 instances for supervised metaphor classification • we are the first to use tree kernels for metaphor identification • our approach achieves an F1-score of 0.75, the best score of of all systems tested. 2 Data 2.1 Annotation We downloaded a list of 329 metaphor examples from"
W13-0907,P91-1049,0,0.13536,"hich she calls recognition) and interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anom"
W13-0907,W07-0103,0,0.0363077,"nd interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syn"
W13-0907,J04-1002,0,0.0959412,"as new metaphors, approaches that try to identify them based on lexical features alone are bound to be unsuccessful. Some approaches have therefore suggested considering distributional properties and “abstractness” of the phrase (Turney et al., 2011). This nicely captures the contextual nature of metaphors, but their ubiquity makes it impossible to find truly “clean” data to learn the separate distributions of metaphorical and literal use for each word. Other approaches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do not assume any predefined mappings. We hypothesize instead that if we interpreted every word literally, metaphors will manifest themselves as unusual semantic compositions. Since these compositions most frequently occur 2 Shutova (2010) distinguishes between metaphor identification (which she calls recognition) and interpretation. We are solely concerned with th"
W13-0907,E06-1015,0,0.117392,"e compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so a"
W13-0907,R11-2018,0,0.0408968,"Missing"
W13-0907,C08-1088,0,0.208095,"ehavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could sta"
W13-0907,C10-1113,0,0.0811424,"Missing"
W13-0907,P10-1071,0,0.0229229,"ches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do not assume any predefined mappings. We hypothesize instead that if we interpreted every word literally, metaphors will manifest themselves as unusual semantic compositions. Since these compositions most frequently occur 2 Shutova (2010) distinguishes between metaphor identification (which she calls recognition) and interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. T"
W13-0907,D11-1116,1,0.806458,"essive way. We use the existing vector representation SENNA (Collobert et al., 2011) which is derived from contextual similarity. In it, semantically similar words are represented by similar vectors, without us having to define similarity or looking at the word itself. In initial tests, these vectors performed better than binary vectors straightforwardly derived from features of the word in context. 2.3 Constructing Trees a) b) VB like I people PRP c) v.emotion O NNS n.group dependency trees, and the different levels from various annotations. We parse the input sentence with the FANSE parser (Tratz and Hovy, 2011)6 . It provides the dependency structure, POS tags, and other information. To construct the different tree representations, we replace each node in the tree with its word, lemma, POS tag, dependency label, or supersense (the WordNet lexicographer name of the word’s first sense (Fellbaum, 1998)), and mark the word in question with a special node. See Figure 3 for a graphical representation. These trees are used in addition to the vectors. This approach is similar to the ones described in (Moschitti et al., 2006; Qian et al., 2008; Hovy et al., 2012). 2.4 Classification Models the sweet in Bosto"
W13-0907,D11-1063,0,0.704648,"fixed expression with the metaphorical meaning now accepted as just another sense, no longer recognized as metaphorical at all. This gradient makes it hard to determine a boundary between literal and metaphorical use of some expressions. Identifying metaphors is thus a difficult but important step in language understanding.2 Since many words can be productively used as new metaphors, approaches that try to identify them based on lexical features alone are bound to be unsuccessful. Some approaches have therefore suggested considering distributional properties and “abstractness” of the phrase (Turney et al., 2011). This nicely captures the contextual nature of metaphors, but their ubiquity makes it impossible to find truly “clean” data to learn the separate distributions of metaphorical and literal use for each word. Other approaches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do no"
W13-0907,W06-2607,0,\N,Missing
W14-1601,N13-1070,1,0.862135,"in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System A improves over System B wrt. most metrics, we obtain significance against the odds. POS taggers and dependency parsers should also be evaluated by their impact on downstream"
W14-1601,W03-0425,0,0.0235909,"Missing"
W14-1601,W05-0909,0,0.0206194,"01 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric"
W14-1601,I11-1100,0,0.0289698,"Missing"
W14-1601,D12-1091,0,0.0568125,"ata sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate ( , i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 1 In many fields, including NLP, it has become good practice to report actual p-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual p-values."
W14-1601,W06-1615,0,0.0543626,"Missing"
W14-1601,P07-1034,0,0.0536678,"Missing"
W14-1601,C00-1011,0,0.168794,"Missing"
W14-1601,W04-1013,0,0.0270518,"0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-"
W14-1601,W06-2920,0,0.0650816,"Missing"
W14-1601,C08-1015,0,0.0300507,"Missing"
W14-1601,W03-0423,0,0.0336765,"odels from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency parsing and NER. The toy example is supposed to illustrate the logic behind our reasoning and is not specific to NLP. It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions. For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next. 3.2 Standard comparisons POS t"
W14-1601,P11-2031,0,0.00842907,"0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting resu"
W14-1601,P02-1040,0,0.113383,"Twitter TA (b) 0.3445 0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers"
W14-1601,P07-1033,0,0.0204031,"Missing"
W14-1601,P11-1157,1,0.595908,"Missing"
W14-1601,Y09-1013,0,0.0190285,"acy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. UAS <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 Table 3: Parsing p-values (M ALT-L IN VS . S TANFORD -RNN) across LAS and UAS (p < 0.05 gray-shaded). than S TANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is"
W14-1601,D11-1043,0,0.0493232,"Missing"
W14-1601,W05-0908,0,0.132601,"Missing"
W14-1601,D09-1085,0,0.0396616,"Missing"
W14-1601,P11-1067,0,0.0160959,"Figure 7: PPV for different ↵ (horizontal line is PPV for p = 0.05, vertical line is ↵ for PPV=0.95). could propose a p-value cut-off at p < 0.0025. This is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System"
W14-1601,P13-1045,0,0.00363571,"ASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency"
W14-1601,N13-1068,1,0.72957,"nalysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common to report resul"
W14-1601,N03-1033,0,0.00466178,"ndency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003"
W14-1601,E12-1006,0,0.0571022,"or a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of M ALT-L IN VS . S TANFORD -RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unl"
W14-1601,W11-0328,0,0.0154577,"across 3 runs for POS and NER and 10 runs for dependency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly availab"
W14-1601,C10-2146,0,0.0272174,"Missing"
W14-1601,C00-2137,0,0.652215,"as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common"
W14-1601,E14-4014,0,\N,Missing
W14-1601,W03-0419,0,\N,Missing
W14-1601,D07-1096,0,\N,Missing
W14-2602,W10-2608,0,0.165983,"Missing"
W14-2602,S13-2053,0,0.0263725,"Missing"
W14-2602,C12-2114,0,0.0752636,"d in the corrupted training stage. This results in each data point appearing in different, corrupted versions, as visualized in Figure 1. The copying process retains more of the information in the training data, since it is unlikely that the same feature is deleted in each copy. In our experiments, we used k=5. Larger values of k resulted in longer training times without improving performance. 2. over-using certain labels (since the label distribution on the target domain might differ). One approach that has been proven to reduce overfitting is data corruption, also known as dropout training (Søgaard and Johannsen, 2012; Søgaard, 2013), which is a way of regularizing the model by randomly leaving out features. Intuitively, this approach can be viewed as coercing the learning algorithm to rely on more general, but less consistent features. Rather than learning to mainly trust the features that are highly predictive for the given training data, the algorithm is encouraged to use the less predictive features, since the highly predictive features might be deleted by the corruption. Most prior work on dropout regularization (Søgaard and Johannsen, 2012; Wang and Manning, 2012; Søgaard, 2013) has used online corru"
W14-2602,P13-2113,0,0.0304851,"tage. This results in each data point appearing in different, corrupted versions, as visualized in Figure 1. The copying process retains more of the information in the training data, since it is unlikely that the same feature is deleted in each copy. In our experiments, we used k=5. Larger values of k resulted in longer training times without improving performance. 2. over-using certain labels (since the label distribution on the target domain might differ). One approach that has been proven to reduce overfitting is data corruption, also known as dropout training (Søgaard and Johannsen, 2012; Søgaard, 2013), which is a way of regularizing the model by randomly leaving out features. Intuitively, this approach can be viewed as coercing the learning algorithm to rely on more general, but less consistent features. Rather than learning to mainly trust the features that are highly predictive for the given training data, the algorithm is encouraged to use the less predictive features, since the highly predictive features might be deleted by the corruption. Most prior work on dropout regularization (Søgaard and Johannsen, 2012; Wang and Manning, 2012; Søgaard, 2013) has used online corruptions, i.e., th"
W14-2602,H05-1044,0,0.0650229,"Missing"
W14-2602,J92-4003,0,\N,Missing
W14-2602,P07-1056,0,\N,Missing
W14-2602,S13-2052,0,\N,Missing
W15-2913,K15-1011,1,0.686596,"ur experiments show that social media data can provide sufficient linguistic evidence to reliably predict two of four personality dimensions. 1 Introduction Individual author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs hav"
W15-2913,luyckx-daelemans-2008-personae,0,0.130146,"hough. See Henrich et al. (2010). 3 4 94 Using the sklearn toolkit. Tokenizer from: http://wwbp.org/ 5 Results 0.80 Table 3 shows the prediction accuracy for a majority-class baseline and our models on the full data set (10-fold cross-validation). While the model clearly improves on the I–E and F–T distinctions, we see no improvements over the baseline for S–N, and even a slight drop for P–J. This indicates that for the latter two dimensions, we either do not have the right features, or there is not linguistic evidence for them, given that they are more related to perception. The results from Luyckx and Daelemans (2008) on Dutch essays also suggest that P–J is difficult to learn. Given the heavy gender-skew of our data, we run additional experiments in which we control for gender. The gender-controlled dataset contains 1070 authors. The results in Table 4 show the same tendency as in the previous setup. Majority System 0.70 0.65 0.60 0.55 0.50 0.55 64.1 72.5 77.5 77.4 58.4 61.2 58.8 55.4 0.50 64.9 72.1 79.6 79.5 51.8 54.0 59.4 58.2 2000 classifier I-E baseline I-E classifier T-F baseline T-F 0.60 0 500 1000 1500 2000 Figure 2: Learning curves and majority baselines for I–E and T–F on whole data set (top) and"
W15-2913,W15-1202,0,0.0409612,"300 Copenhagen S dirk.hovy@hum.ku.dk bplank@cst.dk Abstract Volkova et al., 2015). Apart from demographic features, such as age or gender, there is also a growing interest in personality types. Predicting personality is not only of interest for commercial applications and psychology, but also for health care. Recent work by Preot¸iuc-Pietro et al. (2015) investigated the link between personality types, social media behavior, and psychological disorders, such as depression and posttraumatic stress disorder. They found that certain personality traits are predictive of mental illness. Similarly, Mitchell et al. (2015) show that linguistic traits are predictive of schizophrenia. However, as pointed out by Nowson and Gill (2014), computational personality recognition is limited by the availability of labeled data, which is expensive to annotate and often hard to obtain. Given the wide array of possible personality types, limited data size is a problem, since lowprobability types and combinations will not occur in statistically significant numbers. In addition, many existing data sets are comprised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled s"
W15-2913,W11-1515,0,0.414681,"Missing"
W15-2913,D13-1114,0,0.0375757,"as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about the limited expressiveness of MBTI, and a preference for Big Five"
W15-2913,P11-1137,0,0.0233605,"author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about th"
W15-2913,N13-1037,0,0.0136457,"ata sets are comprised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. In this work, we take a data-driven approach to personality identification, to avoid both the limitation of small data samples and a limited vocabulary. We use the large amounts of personalized data voluntarily produced on social media (e.g., Twitter) to collect sufficient amounts of data. Twitter is highly non-canonical, and famous for an almost unlimited vocabulary size (Eisenstein, 2013; Fromreide et al., 2014). In order to enable data-driven personality research, we combine this data source with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), denoted MBTIs. Myers-Briggs uses four binary dimensions to classify users (I NTROVERT–E XTROVERT, Psychology research suggests that certain personality traits correlate with linguistic behavior. This correlation can be effectively modeled with statistical natural language processing techniques. Prediction accuracy generally improves with larger data samples, which also allows for more lexical features. Most e"
W15-2913,fromreide-etal-2014-crowdsourcing,1,0.690975,"ised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. In this work, we take a data-driven approach to personality identification, to avoid both the limitation of small data samples and a limited vocabulary. We use the large amounts of personalized data voluntarily produced on social media (e.g., Twitter) to collect sufficient amounts of data. Twitter is highly non-canonical, and famous for an almost unlimited vocabulary size (Eisenstein, 2013; Fromreide et al., 2014). In order to enable data-driven personality research, we combine this data source with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), denoted MBTIs. Myers-Briggs uses four binary dimensions to classify users (I NTROVERT–E XTROVERT, Psychology research suggests that certain personality traits correlate with linguistic behavior. This correlation can be effectively modeled with statistical natural language processing techniques. Prediction accuracy generally improves with larger data samples, which also allows for more lexical features. Most existing work on personali"
W15-2913,W15-1203,0,0.0203999,"Missing"
W15-2913,P11-1077,0,0.240969,"personality dimensions. 1 Introduction Individual author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discu"
W15-2913,verhoeven-daelemans-2014-clips,0,0.0225779,"Missing"
W15-2913,D13-1187,0,0.0494234,"important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about the limited expressivene"
W15-4302,K15-1011,1,0.403535,"rk City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and some fraction of these tweets are geo-located. Eisenstein (2013) an"
W15-4302,E14-1011,0,0.385375,"soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and so"
W15-4302,P11-1137,0,0.031987,"phenomena. With ”bafroom”, it seems that about 1 in 20 occurrences on Twitter are metauses. Meta-uses may also serve social functions. AAVE features are used as cultural markers by Latinos in North Carolina (Carter, 2013), for example. Some of the research hypotheses considered (H3 and H5) relate to demographic variables such as income and educational levels. While we do not have socio-economic information about the individual Twitter user, we can use the geo-located tweets to study the correlation between socio-economic variables and linguistic features at the level of cities or ZIP codes.1 Eisenstein et al. (2011) note that this level of abstraction introduces some noise. Since Twitter users do not form representative samples of the population, the mean income for a city or ZIP code is not necessarily the mean income for the Twitter users in that area. We refer to this problem as the (3) U SER P OPU LATION B IAS. Another serious methodological problem known as (4) G ALTON ’ S P ROBLEM (Naroll, 1961; Roberts and Winters, 2013), is the observation that cross-cultural associations are H8: Backing in /str/ (to /skr/) is unique to AAVE (Rickford, 1999; Thomas, 2007; Labov, 2006). Hypotheses 1–8 are investig"
W15-4302,W13-1102,0,0.640113,"hagen Njalsgade 140 DK-2300 Copenhagen S soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half"
W15-4302,P15-2079,1,0.238458,"etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and some fraction of these tweets are geo-located. Eisenstein (2013) and Doyle (2014) studied t"
W15-4302,D13-1187,0,0.124542,"0 DK-2300 Copenhagen S soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets ever"
W17-4415,D10-1124,0,0.802123,"Missing"
W17-4415,C12-1064,0,0.796157,"easure the geographic distribution of each NE type, and measure their entropy. In the second experiment, we conduct feature selection via randomized logistic regression, and, in the third experiment, we establish a baseline by using majority classes for all types. Geographic diversity We first measure the geographic distribution of each type. We extract all NEs in the WORLD training set and use the Tweet corpus to measure entropy and mean pairwise distance (in kilometers) between tweets that contain the same NEs. We compute unpredictability as entropy: Resources Data We use the WORLD dataset (Han et al., 2012), which covers 3,709 cities worldwide and consists of tweets from 1.4M users. Han et al. (2012) hold out 10,000 users as development and 10,000 as test set. For each user with at least 10 geotagged tweets, the user’s location is set to be the city in which the majority of their tweets are from. We also use Han et al. (2012)’s method to extract the nearest city to a given latitudelongitude coordinate. H(x) = − n X P(xi ) log P(xi ) i=1 1 We map the latitude and longitude coordinates to cities/regions based on Han et al. (2012). 117 GEO-LOC FACILITY SPORT-TEAM MOVIE TV-SHOW PERSON BAND PRODUCT C"
W17-4415,D15-1256,0,0.115835,"Missing"
W17-4415,P15-2104,0,0.726545,"33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a label to train the classifiers. The approach of using k-d tree is also used in Rahimi et al. (2015); Han et al. (2012) and Wing and Baldridge (2014). See Table 3 for an example of the following methods. All use logistic regression as classifier, following Rahimi et al. (2015). repetition is used to put more emphasis/weight based on frequency.3 In order to measure the effectiveness of the three top NE types discovered in Section 4, we experiment with (1) considering all NE types (shown as Our MethodallN Es in Table 4), and (2) the three most useful types (shown as Our Methodtop3 ). Evaluation metrics We use the same evaluation metrics as previous studies: accuracy depending on location granu"
W17-4415,D11-1141,0,0.168968,"Missing"
W17-4415,D12-1137,0,0.675438,"ent detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SHOW SPORT-TEAM All Exam"
W17-4415,W17-4409,1,0.339601,"ce location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, they do not explicitly study named entities. 3 4 NE types and Geolocation In Table 1, we have seen the general distribution of NE types, with PERSON, GEO-LOC and FACILITY as top three. In this section, we focus on the predictiveness of NEs (as features) for geolocation. Later, in Section 5, we will propose a method to improve geolocation by putting more emphasis on the top NEs and their hidden location information. We conduct three experiments to quantify predictiveness of NEs. In the first, we measure the geographic distributio"
W17-4415,W16-3930,0,0.0544298,"or events during the time of collection, such as an election or a disaster. In contrast to our work, most do not consider multi-word NEs. Only few text-based studies consider NEs, and if so, focus on location names using gazetteers like GeoNames, limiting the methods to the completeness of these gazetteers. Since they usually also use other text-based models, it is hard to determine how much location names contribute. These approaches depend on a namedisambiguation phase, using Wikipedia, DBPedia, or OpenStreetMap, since location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, th"
W17-4415,D14-1039,0,0.532042,"he impact of NEs and their hidden location information for geolocation. To extract the hidden location information of each NE, we collect the locations of all tweets in our tweet corpus that contain that 118 Example baseline Only NE Baseline without NE Our Method Me, my friend and the Eiffel tower Me my friend and the Eiffel tower [Eiffel tower] ME my friend and the Me my friend and the Eiffel tower Paris Paris Paris Paris [Las Vegas] [Las Vegas] Table 3: Examples and features of methods in Section 5 Method Baseline Only NE Baseline without NE Our MethodallN Es Our Methodtop3 Previous studies Wing and Baldridge (2014) Han et al. (2012) city↑ 17.6 9.3 14.8 17.5 17.8 Accuracy country↑ @161↑ 83.6 33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a l"
W17-4415,P11-1096,0,0.256809,"as technologies such as event detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SH"
W17-4606,P09-1113,0,0.0827124,"bels, we create the E2E output for the baseline models in the same way we created the E2E data sets, i.e. by chunking and extracting labels as fields. We evaluate our model and the baselines using the MUC-5 definitions of precision, recall and F1, without partial matches (Chinchor and Sundheim, 1993). We use bootstrap sampling to estimate the probability that the model with the best micro average F1 score on the entire test set is worse for a randomly sampled subset of the test data. 3.3 Other approaches have addressed the need for token-level labels when only raw output values are available. Mintz et al. (2009) introduced distant supervision, which heuristically generates the token-level labels from the output values. You do this by searching for input tokens that matches output values. The matching tokens are then assigned the labels for the matching outputs. One drawback is that the quality of the labels crucially depend on the search algorithm and how closely the tokens match the output values, which makes it brittle. Our method is trained end-to-end, thus not relying on brittle heuristics. Sutskever et al. (2014) opened up the sequenceto-sequence paradigm. With the addition of attention (Bahdana"
W17-4606,N16-1034,0,0.0321627,"he baseline models in terms of microaveraged F1 for two of the three data sets. This is a remarkable result given that the baselines are trained on token-level labels, whereas our model is trained end-to-end. For the restaurant data set, our model is slightly worse than the baseline. 4 Discussion Related work Event extraction (EE) is similar to the E2E IE task we propose, except that it can have several event types and multiple events per input. In our E2E IE task, we only have a single event type and assume there is zero or one event mentioned in the input, which is an easier task. Recently, Nguyen et al. (2016) achieved state of the art results on the ACE 2005 EE data set using a recurrent neural network to jointly model event triggers and argument roles. 51 the input and as ’17-01-2012’ in the machine readable output. While it is clear that this model does not solve all the problems present in real-life IE tasks, we believe it is an important step towards applicable E2E IE systems. In the future, we will experiment with adding character level models on top of the pointer network outputs so the model can focus on an input, and then normalize it to fit the normalized outputs. Diederik Kingma and Jimm"
W17-4606,W16-3905,0,0.0210013,"e tagging (Finkel et al., 2005; Zhai et al., 2017). Such models require sufficient amounts of training data that is labeled at the token-level, i.e., with one label for each word. The reason token-level labels are in short supply is that they are not the intended output of human IE tasks. Creating token-level labels thus requires an additional effort, essentially doubling the work required to process each item. This additional effort is expensive and infeasible for many production systems: estimates put the average cost for a sentence at about 3 dollars, and about half an hour annotator time (Alonso et al., 2016). Consequently, state-of-the-art IE approaches, relying on sequence taggers, cannot be applied to many real life IE tasks. What is readily available in abundance and at no additional costs, is the raw, unstructured input and machine-readable output to a human IE task. Consider the transcription of receipts, checks, or business documents, where the input is an unstructured PDF and the output a row in a database (due date, payable amount, etc). Another example is flight bookings, where the input is a natural language request from the user, and the output a HTTP request, sent to the airline booki"
W17-4606,H90-1020,0,0.708227,"ng for the decoders (Williams and Zipser, 1989), such that ok,j−1 = yk,j−1 . During testing we use argmax to select the most probable output for each step j and run each decoder until the first end of sentence (EOS) symbol. 3 Input cheapest airfare from tacoma to st. louis and detroit fromloc toloc airline name cost relative period of day time time relative day name day number month name Experiments 3.1 Data sets To compare our model to baselines relying on token-level labels we use existing data sets for which token level-labels are available. We measure our performance on the ATIS data set (Price, 1990) (4978 training samples, 893 testing samples) and the MIT restaurant (7660 train, 1521 test) and movie corpus (9775 train, 2443 test) (Liu et al., 2013). These data sets contains token-level labels in the Beginning-Inside-Out format (BIO). The ATIS data set consists of natural language requests to a simulated airline booking system. Each word is labeled with one of several classes, e.g. departure city, arrival city, cost, etc. The MIT restaurant and movie corpus are similar, except for a restaurant and movie domain respectively. See table 1 for samples. MIT Restaurant 2 B-Rating start I-Rating"
W17-4606,M93-1007,0,0.272319,"E2E performance reported later. We present them here so that readers familiar with the ATIS data set can evaluate the strength of our baselines using a wellknown measure. For the E2E performance measure we train the baseline models using token-level BIO labels and predict BIO labels on the test set. Given the predicted BIO labels, we create the E2E output for the baseline models in the same way we created the E2E data sets, i.e. by chunking and extracting labels as fields. We evaluate our model and the baselines using the MUC-5 definitions of precision, recall and F1, without partial matches (Chinchor and Sundheim, 1993). We use bootstrap sampling to estimate the probability that the model with the best micro average F1 score on the entire test set is worse for a randomly sampled subset of the test data. 3.3 Other approaches have addressed the need for token-level labels when only raw output values are available. Mintz et al. (2009) introduced distant supervision, which heuristically generates the token-level labels from the output values. You do this by searching for input tokens that matches output values. The matching tokens are then assigned the labels for the matching outputs. One drawback is that the qu"
W17-4606,P05-1045,0,0.175457,"Missing"
W17-4606,P16-1154,0,0.0181431,"ow closely the tokens match the output values, which makes it brittle. Our method is trained end-to-end, thus not relying on brittle heuristics. Sutskever et al. (2014) opened up the sequenceto-sequence paradigm. With the addition of attention (Bahdanau et al., 2014), these models achieved state-of-the-art results in machine translation (Wu et al., 2016). We are broadly inspired by these results to investigate E2E models for IE. The idea of copying words from the input to the output have been used in machine translation to overcome problems with out-of-vocabulary words (Gulcehre et al., 2016; Gu et al., 2016). Our model Since our decoders can only output values that are present in the input, we prepend a single comma to every input sequence. We optimize our model using Adam and use early stopping on a held-out validation set. The model quickly converges to optimal performance, usually after around 5000 updates after which it starts overfitting. For the restaurant data set, to increase performance, we double the sizes of all the parameters and use embedding and recurrent dropout following (Gal, 2015). Further, we add a summarizer LSTM to each decoder. The summarizer LSTM reads the entire encoded in"
W17-4606,P16-1014,0,0.0173025,"search algorithm and how closely the tokens match the output values, which makes it brittle. Our method is trained end-to-end, thus not relying on brittle heuristics. Sutskever et al. (2014) opened up the sequenceto-sequence paradigm. With the addition of attention (Bahdanau et al., 2014), these models achieved state-of-the-art results in machine translation (Wu et al., 2016). We are broadly inspired by these results to investigate E2E models for IE. The idea of copying words from the input to the output have been used in machine translation to overcome problems with out-of-vocabulary words (Gulcehre et al., 2016; Gu et al., 2016). Our model Since our decoders can only output values that are present in the input, we prepend a single comma to every input sequence. We optimize our model using Adam and use early stopping on a held-out validation set. The model quickly converges to optimal performance, usually after around 5000 updates after which it starts overfitting. For the restaurant data set, to increase performance, we double the sizes of all the parameters and use embedding and recurrent dropout following (Gal, 2015). Further, we add a summarizer LSTM to each decoder. The summarizer LSTM reads the"
W18-1106,P14-2134,0,0.0212037,"stic insights I had a lovely experience with them ... Compared lots of prices and ended up with them. Good value for money ... Exactly the product I wanted. Good price and speedy delivery. Representation Learning Word embeddings have been shown to be effective as input in a variety of NLP tasks, because they are able to capture similarities along a large number of latent dimensions in the data. If language is indeed a signal for socio-demographic factors, it makes sense to assume that these socio-demographic factors are captured as latent dimensions in continuous word representations. Indeed, Bamman et al. (2014) have shown that neural representations can be used to capLABELS F, 60, ID00014 ID16457 M, ID243534 Table 1: Example reviews with different amounts of available labels 43 As a result, we have representations of the word, document, and population-level. The unique document identifiers allow us to represent each training instance as a vector. The socio-demographic labels, on the other hand, are not unique, but shared among potentially many instances. In the gensim implementation of paragraph2vec, both word and documentlabel embeddings are projected into the same high-dimensional space. We can co"
W18-1106,E17-1015,1,0.798387,", 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that the recent abundance of demographically rich data sets and complex neural architectures allows us to break out of streetlamp science and to explore those two strands of demographically-based research. This shift will en"
W18-1106,D13-1114,0,0.0256023,"can provide as many labels as we want for each document (see Table 1 for examples of this). Through the training process, latent characteristics of the document labels are reflected in the learned word embeddings, while the embeddings of the demographic labels reflect the words most closely associated with them. these groups range from gender to region, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will ha"
W18-1106,D10-1124,0,0.127348,"Missing"
W18-1106,N15-1184,0,0.0464292,"Missing"
W18-1106,P15-2079,1,0.948646,"range from gender to region, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that"
W18-1106,P16-2096,1,0.878322,"Missing"
W18-1106,W15-4302,1,0.936454,"with them. these groups range from gender to region, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this posit"
W18-1106,W17-4415,1,0.845602,"xamples below).2 An interesting exception to this rule are the Clustering with structure We can cluster the document labels with agglomerative clustering. This clustering algorithm starts with each region vector in its own cluster, and recursively merges pairs until we have reached the required number of clusters. The pairs to merge are chosen as to minimize the increase in linkage distance. While a variety of distance measures exist, the most commonly used (and empirically most useful) is Ward 2 Eisenstein et al. (2010) have therefore approached dialects as regionally distributed topics, and Salehi et al. (2017) showed that using such regional terms makes individuals more likely to be correctly geo-located. 46 Note that we are not restricted to binary adjacency: if we were comparing points rather than regions (say, individual cities), we could instead use a similarity matrix with the inverse distance between cities (closer cities are therefore merged before more distant cities). This structure lets us express continuous values, which are impossible to include in the learning setup of doc2vec. Retrofitting Faruqui et al. (2015) introduced the concept of retrofitting vectors to external dictionaries. T"
W18-1106,W16-1609,0,0.0237309,"lustering) as a Python implementation on github: https: //github.com/dirkhovy/PEOPLES. 2.2 Experimental Results I preprocess the data to remove stop words and function words, replace numbers with 0s, lowercase all words and lemmatize them. I also concatenate collocations with an underscore to form a single item. This reduces the amount of noise in the data. As labels, I use the seven age decades, as well as the two genders present in the data. Overall, this results in slightly over 2M instances. See Table 1 for examples. I run the model for 100 iterations, following the settings described in (Lau and Baldwin, 2016), with the embedding dimensions to 300, window size to 15, minimum frequency to 10, negative samples to 5, downsampling to 0.00001. 1. words to words: similar to word2vec, this allows us to find words with similar meanings, i.e., words that occur in a similar context. In addition, these words representations are conditioned on the socio-demographic factors, though. 2. words to document labels and 3. document labels to words: this allows us to find the n words best describing a document label, or the n document labels most closely associated to a word 4. document labels to document labels: this"
W18-1106,D17-1119,0,0.386998,"ribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that the recent abundance of demographically rich data sets and complex neural architectures allows us to break out of streetlamp science and to explore those two strands of d"
W18-1106,P14-1018,0,0.0598898,"Missing"
W18-1106,D13-1187,0,0.118565,"phic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that the recent abundance of demographically rich data sets and complex neural architectures allows us to break out of streetlamp science and to"
W18-1106,Q17-1021,0,0.275732,"asks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that the recent abundance of demographically rich data sets and complex neural architectures allows us to break out of streetlamp science and to explore those two strands of demographically-based resear"
W18-1106,W11-1515,0,0.0792192,"Missing"
W18-1106,D17-1323,0,0.0236061,"ion, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specific demographic factors that do have an influence on NLP models (on bias and performance), and 2) based on this knowledge, we need to develop models that account for demographics to improve performance while preventing bias. In this position paper, I argue that the recent abundance"
W18-1106,W15-2913,1,0.854574,"for examples of this). Through the training process, latent characteristics of the document labels are reflected in the learned word embeddings, while the embeddings of the demographic labels reflect the words most closely associated with them. these groups range from gender to region, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton et al., 2017). In order to move forward as a field, we will have to follow two strands of research: 1) we need to identify the specif"
W18-1106,P15-1169,0,0.039978,"Missing"
W18-1106,W16-5603,0,0.0169665,"refuse, tell, apparently, akinika M ISC: srv, hendrix, marvin, bankcard, irrational T RAVEL: hotel, airport, flight, -PRON- flight, -PRON- trip great − MALE + FEMALE and great − FEMALE + MALE to see which words women and men, respectively, use with or for great. The first calculation give us fab, fabulous, lovely, love, wonderful, really pleased, fantastic, brilliant, amazing, and thrill for women and guy, decent, good, top notch, couple, new, well, gear, get good, and awesome for men. Such knowledge is interesting with respect to sociodemographic studies, but can have practical applications: Reddy and Knight (2016) have shown how gender can be obfuscated online by replacing particularly “male” or “female” words with a neutral or even opposite counterpart. The approach shown here based on vector arithmetics is a possible simple alternative. Comparing labels to labels Comparing labels to each other is again very similar to the situation we have seen above for words. In the present study, this comparison is less interesting (though we can for example see which age groups are more or less similar to each other, see Figure 2). However, we will exploit this attribute in the next section (2.3), were we explici"
W18-1106,P11-1077,0,0.023968,"document labels (rather than unique document identifiers). Crucially, we can provide as many labels as we want for each document (see Table 1 for examples of this). Through the training process, latent characteristics of the document labels are reflected in the learned word embeddings, while the embeddings of the demographic labels reflect the words most closely associated with them. these groups range from gender to region, social class, ethnicity, and occupation. This property of language has been used in NLP to predict those demographic labels from text in authorattribute prediction tasks (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2014, 2015; Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2015a,b, inter alia). However, demographics also affect NLP beyond their use as prediction target. Demographic bias in the training data can severely distort the performance of our tools (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Zhao et al., 2017), while accounting for demographic factors can actually improve performance in a variety of tasks (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Yang and Eisenstein, 2017; Benton"
