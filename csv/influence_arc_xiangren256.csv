2020.acl-demos.42,P19-3010,1,0.756742,"uly 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. This methodology of providing recommendations often proves to be unhelpful when little annotated data exists for pre-training, as is usually the case for natural language tasks being applied to domain-specific or user-provided corpora. Unlabeled Instances Explanation Parsing / Encoding Explanation UI USER Softly Matching Module Label Weak Label To resolve this issue, AlpacaTag, an annotation framework for sequence labeling (Lin et al., 2019) attempts to provide annotation recommendations from a learned sequence labeling model that is incrementally updated by batches of incoming human annotations. Its model training follows an active learning strategy (Shen et al., 2017), which is shown to be a label-efficient, thus it attempts to minimize human annotation efforts. AlpacaTag selects the most informative batches of documents for humans to annotate and thus achieves a more cost-effective way of using human efforts. While active learning allows the model to achieve higher performance earlier in the learning process, model performance"
2020.acl-demos.42,D18-1226,1,0.704497,"portant textual entailment recognition task. They demonstrate the usefulness of explanations as an additional training signal for learning more comprehensive sentence representations. Even earlier (Andreas et al., 2016) explored breaking down natural language explanation into linguistic sub-structures for learning collections of neural modules which can be assembled into neural networks. Our framework is very related to the above weak supervision methods via explanation. Another approach to weak supervision is attempting to transfer knowledge from a related source to the target domain corpus (Lin and Lu, 2018; Lan et al., 2020). Shang et al. (2018) and Yang et al. (2018) proposed using a domainAcknowledgements This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, NSF SMA 18-29268, and Snap research gift. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all th"
2020.acl-demos.42,E14-2025,0,0.0284233,"and “where the food” can aid in weakly labeling unlabeled instances like “I had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) ("
2020.acl-demos.42,D14-1093,0,0.0290362,"guage explanations for RE and SA, and trigger explanations for NER provided the best results. For the downstream model portion of our weak supervision framework, we use common supervised method for each task: (1-RE) BLSTM+ATT (Bahdanau et al., 2015) adds an attention layer onto LSTM to encode an sequence. (2-SA) ATAE-LSTM (Wang et al., 2016) combines the aspect term information into both the embedding layer and attention layer to help the model concentrate on different parts of a sentence. 376 specific dictionary for matching on the unannotated target corpus. Both efforts employ Partial CRFs (Liu et al., 2014) which assign all possible labels to unlabeled words and maximize the total probability. This approach addresses the incomplete annotation problem, but heavily relies on a domain-specific seed dictionary. label+explanations our augmented model training paradigm is given vs. how many labels the traditional label-only model training is shown. We use the F-1 metric to compare the performances. As shown in Fig. 6, we see that our model not only is more time and label efficient than the labelonly training process, but it also outperforms the label-only training process. Given these results, we beli"
2020.acl-demos.42,W16-4011,0,0.0607319,"Missing"
2020.acl-demos.42,D18-1384,1,0.812872,"nations: 1) RE: the explanation “the phrase ‘caused by’ occurs between SUBJ and OBJ” can aid in weakly labeling unlabeled instances like “The burst has been caused by water hammer pressure” with the label “cause-effect”; 2) NER: Trigger spans near the labeled restaurant such as “had lunch at” and “where the food” can aid in weakly labeling unlabeled instances like “I had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide an"
2020.acl-demos.42,P18-1175,0,0.0622988,"Missing"
2020.acl-demos.42,P16-1101,0,0.348404,"Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment analysis (SA) (Wang et al., 2016). However, they only yield such performance levels ∗ Both authors contributed equally. The source code is publicly available at http:// inklab.usc.edu/leanlife/. 1 372 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 372–379 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. This methodology of pr"
2020.acl-demos.42,N03-4009,0,0.0539328,"n aid in weakly labeling unlabeled instances like “I had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et"
2020.acl-demos.42,N16-1030,0,0.0343808,"eral previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment analysis (SA) (Wang et al., 2016). However, they only yield such performance levels ∗ Both authors contributed equally. The source code is publicly available at http:// inklab.usc.edu/leanlife/. 1 372 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 372–379 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. Th"
2020.acl-demos.42,2020.acl-main.193,1,0.82231,"tailment recognition task. They demonstrate the usefulness of explanations as an additional training signal for learning more comprehensive sentence representations. Even earlier (Andreas et al., 2016) explored breaking down natural language explanation into linguistic sub-structures for learning collections of neural modules which can be assembled into neural networks. Our framework is very related to the above weak supervision methods via explanation. Another approach to weak supervision is attempting to transfer knowledge from a related source to the target domain corpus (Lin and Lu, 2018; Lan et al., 2020). Shang et al. (2018) and Yang et al. (2018) proposed using a domainAcknowledgements This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, NSF SMA 18-29268, and Snap research gift. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in"
2020.acl-demos.42,W17-4421,1,0.761193,"e ‘caused by’ occurs between SUBJ and OBJ” can aid in weakly labeling unlabeled instances like “The burst has been caused by water hammer pressure” with the label “cause-effect”; 2) NER: Trigger spans near the labeled restaurant such as “had lunch at” and “where the food” can aid in weakly labeling unlabeled instances like “I had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human an"
2020.acl-demos.42,D18-1230,1,0.879603,"n task. They demonstrate the usefulness of explanations as an additional training signal for learning more comprehensive sentence representations. Even earlier (Andreas et al., 2016) explored breaking down natural language explanation into linguistic sub-structures for learning collections of neural modules which can be assembled into neural networks. Our framework is very related to the above weak supervision methods via explanation. Another approach to weak supervision is attempting to transfer knowledge from a related source to the target domain corpus (Lin and Lu, 2018; Lan et al., 2020). Shang et al. (2018) and Yang et al. (2018) proposed using a domainAcknowledgements This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, NSF SMA 18-29268, and Snap research gift. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in USC INK research lab"
2020.acl-demos.42,2020.acl-main.752,1,0.781214,"xplanations to generate weakly labeled data and then trains the appropriate downstream model with this augmented training data. Our weak labeling module supports both explanation formats provided to the annotator in the UI––triggers and natural language. This section first introduces how the module utilizes triggers (§4.1) and then presents how the module deals with natural language(§4.2). 4.1 Input: Natural Language Input: Trigger When a trigger is inputted into the system, we generate weak labels for our training data via softmatching between trigger representations and unlabeled sentences (Lin et al., 2020). Each sentence may contain one or more triggers, but each trigger is associated with only one label. Our framework jointly learns a mapping between triggers and their label using a linear layer with a soft-max output 375 (a) relation extraction (b) sentiment analysis Figure 5: Weakly labeling module for exploiting natural language explanation. the keyword is ‘happy’ module has four parts: String Matching Module, Distant Counting Module, Deterministic Function Module, and the Logical Calculation Module. The first three modules are responsible for evaluating if different clauses in the logical"
2020.acl-demos.42,W17-2630,0,0.103286,"o be unhelpful when little annotated data exists for pre-training, as is usually the case for natural language tasks being applied to domain-specific or user-provided corpora. Unlabeled Instances Explanation Parsing / Encoding Explanation UI USER Softly Matching Module Label Weak Label To resolve this issue, AlpacaTag, an annotation framework for sequence labeling (Lin et al., 2019) attempts to provide annotation recommendations from a learned sequence labeling model that is incrementally updated by batches of incoming human annotations. Its model training follows an active learning strategy (Shen et al., 2017), which is shown to be a label-efficient, thus it attempts to minimize human annotation efforts. AlpacaTag selects the most informative batches of documents for humans to annotate and thus achieves a more cost-effective way of using human efforts. While active learning allows the model to achieve higher performance earlier in the learning process, model performance could be improved if additional supervision existed. It is imperative that provided annotation recommendations be as accurate as possible, as inaccurate annotation recommendations from the framework can push users towards generating"
2020.acl-demos.42,D17-1004,0,0.185846,"ta, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment analysis (SA) (Wang et al., 2016). However, they only yield such performance levels ∗ Both authors contributed equally. The source code is publicly available at http:// inklab.usc.edu/leanlife/. 1 372 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 372–379 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. This methodology of providing recommendations often proves to be unhelpful when little"
2020.acl-demos.42,D17-1161,0,0.178735,"ations for labeling decisions, and a weak supervision framework that parses explanations for the creation of weakly labeled data. The framework then uses this weakly labeled data in conjunction with user-provided labels to train models for improved annotation recommendations. Our UI shows annotators unlabeled instances (can be sampled using active learning), along with annotation recommendations in an effort to reduce annotation costs. We use PyTorch to build our models and implement an API Our work is also similar to recent attempts that exploit explanations for an improved training process (Srivastava et al., 2017; Hancock et al., 2018; Zhou et al., 2020; Wang* et al., 2020), but with two main differences. First, we embed this improved process in a practical application and sec373 for communication between the web-UI and our weak supervision framework. The learned parameters of our framework are updated in an online fashion, thus improving in near real time. We will first touch on the annotation UI (§3) and then go into our weak supervision framework (§4). 3 UI for Capturing Human Explanation (a) the labels appear in the header, followed by an annotating section; tagging suggestions are shown as underl"
2020.acl-demos.42,E12-2021,0,0.137998,"Missing"
2020.acl-demos.42,D16-1058,0,0.192169,"ting open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment analysis (SA) (Wang et al., 2016). However, they only yield such performance levels ∗ Both authors contributed equally. The source code is publicly available at http:// inklab.usc.edu/leanlife/. 1 372 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 372–379 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. This methodology of providing recommendations often proves to be unhelpful when little annotated data exists for pre-training, as is usually the case for"
2020.acl-demos.42,P18-4006,0,0.140472,"had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment a"
2020.acl-demos.42,C18-1183,0,0.299519,"had a dinner at McDonalds, where the food is cheap”. in supervised learning scenarios, and in particular when human-annotated data is abundant. As we seek to apply NLP models to larger variety of domains, such as product reviews (Luo et al., 2018), social media messages (Lin et al., 2017), while reducing human annotation efforts, better annotation frameworks with label-efficient learning techniques are crucial to our progress. Annotation frameworks have been explored by several previous works (Stenetorp et al., 2012; Bontcheva et al., 2014; Morton and LaCivita, 2003; de Castilho et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment a"
2020.acl-demos.42,D19-1397,1,0.85442,"o et al., 2016; Yang et al., 2018a). These existing open-source sequence annotation tools mainly focus on optimizing user-friendly user interfaces, such as providing shortcut key functionality to allow for faster tagging. The frameworks also attempt to provide annotation recommendation to reduce human annotation efIntroduction Deep neural networks have achieved state-of-theart performance on a wide range of sequence labeling and classification tasks such as named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (RE) (Zeng et al., 2015; Zhang et al., 2017; Ye et al., 2019), and sentiment analysis (SA) (Wang et al., 2016). However, they only yield such performance levels ∗ Both authors contributed equally. The source code is publicly available at http:// inklab.usc.edu/leanlife/. 1 372 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 372–379 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics forts. However, these recommendations are provided by a pre-trained model or via dictionary look-ups. This methodology of providing recommendations often proves to be unhelpful when little annotated data exi"
2020.acl-demos.42,S16-1002,0,\N,Missing
2020.acl-demos.42,D15-1203,0,\N,Missing
2020.acl-main.193,P07-1056,0,0.188157,"0.09) 91.92(±0.21) Table 1: Performance on real-world crowd-sourced NER datasets. The best score in each column excepting Gold is marked bold. * indicates number reported by the paper. domains: academic, bio, fiction, news, voyage, wiki, and interview. For NER task, we select the English portion of the OntoNotes v5 corpus (Hovy et al., 2006). The corpus is annotated with 9 named entities with data from 6 domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), pivot text (pt), telephone conversation (tc), and web (web). MultiDomain Sentiment Dataset (MDS) v2.0 (Blitzer et al., 2007) is used for text classification, which is built on Amazon reviews from 4 domains: books, dvd, electronics, and kitchen. Since the dataset only contains word frequencies for each review without raw texts, we follow the setting in Chen and Cardie (2018) considering 5,000 most frequent words and use the raw counts as the feature vector for each review. 5.2 Experiment Setup For sequence labeling tasks, we follow Liu et al. (2018) to build the BLSTM-CRF architecture as the base model. The dimension of characterlevel, word-level embeddings and LSTM hidden layer are set as 30, 100 and 150 respective"
2020.acl-main.193,N16-1030,0,0.0945617,"omain adaptation tasks over existing works. 2 Related Work There exists three threads of related work with this paper, which are sequence labeling, crowdsourcing and unsupervised domain adaptation. Neural Sequence Labeling. Traditional approaches for sequence labeling usually need significant efforts in feature engineering for graphical models like conditional random fields (CRFs) (Lafferty, 2001). Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lample et al., 2016) can largely eliminate humancrafted features. BLSTM-CRF models have achieved promising performance (Lample et al., 2016) and are used as our base sequence tagging model in this paper. Crowd-sourced Annotation. Crowd-sourcing has been demonstrated to be an effective way of fulfilling the label consumption of neural models (Guan et al., 2017; Lin et al., 2019). It collects annotations with lower costs and a higher speed from non-expert contributors but suffers from some degradation in quality. Dawid and Skene (1979) proposes the pioneering work to aggregate crowd annotations to estimate true lab"
2020.acl-main.193,N18-1111,0,0.0221028,". For NER task, we select the English portion of the OntoNotes v5 corpus (Hovy et al., 2006). The corpus is annotated with 9 named entities with data from 6 domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), pivot text (pt), telephone conversation (tc), and web (web). MultiDomain Sentiment Dataset (MDS) v2.0 (Blitzer et al., 2007) is used for text classification, which is built on Amazon reviews from 4 domains: books, dvd, electronics, and kitchen. Since the dataset only contains word frequencies for each review without raw texts, we follow the setting in Chen and Cardie (2018) considering 5,000 most frequent words and use the raw counts as the feature vector for each review. 5.2 Experiment Setup For sequence labeling tasks, we follow Liu et al. (2018) to build the BLSTM-CRF architecture as the base model. The dimension of characterlevel, word-level embeddings and LSTM hidden layer are set as 30, 100 and 150 respectively. For text classification, each review is represented as a 5000-d vector. We use an MLP with a hidden size of 100 for encoding features and a linear classification layer for predicting labels. The dropout with a probability of 0.5 is applied to the n"
2020.acl-main.193,D18-1217,0,0.0194418,"lower costs and a higher speed from non-expert contributors but suffers from some degradation in quality. Dawid and Skene (1979) proposes the pioneering work to aggregate crowd annotations to estimate true labels, and Snow et al. (2008) shows its effectiveness with Amazon’s Mechanical Turk system. Later works (Dempster et al., 1977; Dredze et al., 2009; Raykar et al., 2010) focus on ExpectationMaximization (EM) algorithms to jointly learn the model and annotator behavior on classification. Recent research shows the strength of multitask framework in semi-supervised learning (Lan et al., 2018; Clark et al., 2018), cross-type learning (Wang et al., 2018), and learning with entity triggers (Lin et al., 2020). Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledg"
2020.acl-main.193,2020.acl-demos.42,1,0.783509,"main and clean labeled data of multiple source domains. Each domain is treated as a source. In the decoupling phase, the model is trained on source domains, and in the aggregation phase, the model is trained on combined predictions on the training data of the target domain. Our framework can also extend to new tasks other than sequence labeling and cope with different encoders. We will demonstrate this ability in experiments. Our method is also incorporated as a feature for controlling the quality of crowd-annotation in annotation frameworks such as AlpacaTag (Lin et al., 2019) and LEAN-LIFE (Lee et al., 2020). 5 Experiments We evaluate C ON N ET in the two aforementioned settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Additionally, to demonstrate the generalization of our framework, we also test our method on sequence labeling with transformer encoder in Appendix B and text classification with MLP encoder in Section 5.5. 5.1 Datasets Crowd-Annotation Datasets. We use crowdannotation datasets based on the 2003 CoNLL shared NER task (Tjong Kim Sang and De Meulder, 2003). The real-world datasets, denoted as AMT, are collected by Rodri"
2020.acl-main.193,N19-1423,0,0.0292594,"Missing"
2020.acl-main.193,N06-2015,0,0.0461767,") 89.72(±0.47) 66.00∗ 85.60∗ 64.50(±1.48) 63.55(±1.20) 59.30∗ 62.60∗ 75.03(±1.02) 74.39(±0.98) 62.40∗ C ON N ET (Ours) 84.11(±0.71) 68.61(±0.03) 75.57(±0.27) 88.77(±0.25) 72.79(±0.04) 79.99(±0.08) Gold (Upper Bound) 89.48(±0.32) 89.55(±0.06) 89.51(±0.21) 92.12(±0.31) 91.73(±0.09) 91.92(±0.21) Table 1: Performance on real-world crowd-sourced NER datasets. The best score in each column excepting Gold is marked bold. * indicates number reported by the paper. domains: academic, bio, fiction, news, voyage, wiki, and interview. For NER task, we select the English portion of the OntoNotes v5 corpus (Hovy et al., 2006). The corpus is annotated with 9 named entities with data from 6 domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), pivot text (pt), telephone conversation (tc), and web (web). MultiDomain Sentiment Dataset (MDS) v2.0 (Blitzer et al., 2007) is used for text classification, which is built on Amazon reviews from 4 domains: books, dvd, electronics, and kitchen. Since the dataset only contains word frequencies for each review without raw texts, we follow the setting in Chen and Cardie (2018) considering 5,000 most frequent words and use the raw counts as the f"
2020.acl-main.193,W17-4421,1,0.807865,"al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages (Lin et al., 2017). Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. Saito et al. (2017) and Ruder and Plank (2018) explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adaptation and does not model the differences among multiple source domains. Yang and Eisenstein (2015) represents each domain with a vector of metadata domain attributes and uses domain vectors to train the model to deal with domain shifting, which is highly dependent on prio"
2020.acl-main.193,2020.acl-main.752,1,0.852668,"quality. Dawid and Skene (1979) proposes the pioneering work to aggregate crowd annotations to estimate true labels, and Snow et al. (2008) shows its effectiveness with Amazon’s Mechanical Turk system. Later works (Dempster et al., 1977; Dredze et al., 2009; Raykar et al., 2010) focus on ExpectationMaximization (EM) algorithms to jointly learn the model and annotator behavior on classification. Recent research shows the strength of multitask framework in semi-supervised learning (Lan et al., 2018; Clark et al., 2018), cross-type learning (Wang et al., 2018), and learning with entity triggers (Lin et al., 2020). Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domai"
2020.acl-main.193,P19-3010,1,0.876071,"random fields (CRFs) (Lafferty, 2001). Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lample et al., 2016) can largely eliminate humancrafted features. BLSTM-CRF models have achieved promising performance (Lample et al., 2016) and are used as our base sequence tagging model in this paper. Crowd-sourced Annotation. Crowd-sourcing has been demonstrated to be an effective way of fulfilling the label consumption of neural models (Guan et al., 2017; Lin et al., 2019). It collects annotations with lower costs and a higher speed from non-expert contributors but suffers from some degradation in quality. Dawid and Skene (1979) proposes the pioneering work to aggregate crowd annotations to estimate true labels, and Snow et al. (2008) shows its effectiveness with Amazon’s Mechanical Turk system. Later works (Dempster et al., 1977; Dredze et al., 2009; Raykar et al., 2010) focus on ExpectationMaximization (EM) algorithms to jointly learn the model and annotator behavior on classification. Recent research shows the strength of multitask framework in semi-supervis"
2020.acl-main.193,D18-1226,1,0.82063,"s and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages (Lin et al., 2017). Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. Saito et al. (2017) and Ruder and Plank (2018) explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adaptation and does not model the differences among multiple source domains. Yang and Eisenstein (2015) represents each domain with a vector of metadata domain attributes and uses domain vectors to train the model to deal with domain shifting, which is highly dependent on prior domain knowledge. 2135 (Ghifary et al., 2016) uses an a"
2020.acl-main.193,D18-1153,1,0.902426,"Missing"
2020.acl-main.193,D17-1005,1,0.589881,"rd variance of 3 runs with different random initialization. 5.3 Compared Methods We compare our models with multiple baselines, which can be categorized in two groups: wrapper methods and joint models. To demonstrate the theoretical upper bound of performance, we also train the base model using ground-truth annotations in the target domain (Gold). A wrapper method consists of a label aggregator and a deep learning model. These two components could be combined in two ways: (1) aggregating labels on crowd-sourced training set then feeding the generated labels to a Sequence Labeling Model (SLM) (Liu et al., 2017); (2) feeding multi-source data to a Multi-Task Learning (MTL) (Wang et al., 2018) model then aggregating multiple predicted labels. We investigate multiple label aggregation strategies. CONCAT considers all crowd annotations as gold labels. MVT does majority voting on k } the token level, i.e., the majority of labels {yi,j is selected as the gold label for each token xi,j . MVS is conducted on the sequence level, addressing the problem of violating Begin/In/Out (BIO) rules. DS (Dawid and Skene, 1979), HMM (Nguyen et al., 2017) and BEA (Rahimi et al., 2019) induce consensus labels with probabi"
2020.acl-main.193,I05-3025,0,0.094486,"tiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised crossdomain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders. 1 1 Introduction Sequence labeling is a general approach encompassing various natural language processing (NLP) tasks including part-of-speech (POS) tagging (Ratnaparkhi, 1996), word segmentation (Low et al., 2005), and named entity recognition (NER) (Nadeau and Sekine, 2007). Typically, existing methods follow the supervised learning paradigm, and require high-quality annotations. While gold standard annotation is expensive and ∗ The first two authors contributed equally. Our code can be found at https://github.com/ INK-USC/ConNet . 1 time-consuming, imperfect annotations are relatively easier to obtain from crowdsourcing (noisy labels) or other domains (out-of-domain). Despite their low cost, such supervision usually can be obtained from different sources, and it has been shown that multi-source weak"
2020.acl-main.193,P16-1101,0,0.776493,"owdsourcing datasets and improves significantly in unsupervised crossdomain adaptation tasks over existing works. 2 Related Work There exists three threads of related work with this paper, which are sequence labeling, crowdsourcing and unsupervised domain adaptation. Neural Sequence Labeling. Traditional approaches for sequence labeling usually need significant efforts in feature engineering for graphical models like conditional random fields (CRFs) (Lafferty, 2001). Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lample et al., 2016) can largely eliminate humancrafted features. BLSTM-CRF models have achieved promising performance (Lample et al., 2016) and are used as our base sequence tagging model in this paper. Crowd-sourced Annotation. Crowd-sourcing has been demonstrated to be an effective way of fulfilling the label consumption of neural models (Guan et al., 2017; Lin et al., 2019). It collects annotations with lower costs and a higher speed from non-expert contributors but suffers from some degradation in quality. Dawid and Skene (1979) propose"
2020.acl-main.193,W17-4114,0,0.0164084,"coupling phase (Section 4.2) is for decoupling the model parameters into a set of annotator-invariant model parameters and a set of annotator-specific representations. Secondly, the dynamic aggregation phase (Section 4.3) learns to contextually utilize the annotator representations with a lightweight attention mechanism to find the best suitable transformation for each sentence, so that the model can achieve a context-aware consensus among all sources. The inference process is described in Section 4.4. 4.1 The Base Model: BLSTM-CRF Many recent sequence labeling frameworks (Ma and Hovy, 2016b; Misawa et al., 2017) share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF). The BLSTM encodes an input sequence x = {x1 , x2 , . . . , xn } into a sequence of hidden state vectors h1:n . The CRF takes as input the hidden state vectors and computes an emission score matrix U ∈ Rn×L where L is the size of tag set. It also maintains a trainable transition matrix M ∈ RL×L . We can consider Ui,j is the score of labeling the tag with id j ∈ {1, 2, ..., L} for ith 2136 prediction (?8 (1) ) Weighted Voting Annotator {? (1) } CRF Attention (?) prediction (?8) Consensus"
2020.acl-main.193,D08-1027,0,0.303239,"Missing"
2020.acl-main.193,P17-1028,0,0.30991,"Skene (1979) proposes the pioneering work to aggregate crowd annotations to estimate true labels, and Snow et al. (2008) shows its effectiveness with Amazon’s Mechanical Turk system. Later works (Dempster et al., 1977; Dredze et al., 2009; Raykar et al., 2010) focus on ExpectationMaximization (EM) algorithms to jointly learn the model and annotator behavior on classification. Recent research shows the strength of multitask framework in semi-supervised learning (Lan et al., 2018; Clark et al., 2018), cross-type learning (Wang et al., 2018), and learning with entity triggers (Lin et al., 2020). Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of"
2020.acl-main.193,P19-1015,0,0.0365798,"els to a Sequence Labeling Model (SLM) (Liu et al., 2017); (2) feeding multi-source data to a Multi-Task Learning (MTL) (Wang et al., 2018) model then aggregating multiple predicted labels. We investigate multiple label aggregation strategies. CONCAT considers all crowd annotations as gold labels. MVT does majority voting on k } the token level, i.e., the majority of labels {yi,j is selected as the gold label for each token xi,j . MVS is conducted on the sequence level, addressing the problem of violating Begin/In/Out (BIO) rules. DS (Dawid and Skene, 1979), HMM (Nguyen et al., 2017) and BEA (Rahimi et al., 2019) induce consensus labels with probability models. In contrast with wrapper methods, joint models incorporate multi-source data within the structure of sequential taggers and jointly model all individual annotators. CRF-MA models CRFs with Multiple Annotators by EM algorithm (Rodrigues et al., 2014). Nguyen et al. (2017) augments the LSTM 2139 Overall 0.8 (a) PER 0.6 ORG 0.4 LOC 0.2 MISC (b) 0 1 2 3 4 5 6 7 8 0.0 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 snt1(PER) 1.5 snt2(ORG) 1.0 snt3(LOC) 0.5 0.0 snt4(MISC) −0.5 0 1 2 3 4"
2020.acl-main.193,D19-1519,1,0.84404,"earch shows the strength of multitask framework in semi-supervised learning (Lan et al., 2018; Clark et al., 2018), cross-type learning (Wang et al., 2018), and learning with entity triggers (Lin et al., 2020). Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages (Lin et al., 2017). Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. Saito et al. (2017) and Ruder and Plank (2018) explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adapt"
2020.acl-main.193,W96-0213,0,0.837392,"ing the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised crossdomain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders. 1 1 Introduction Sequence labeling is a general approach encompassing various natural language processing (NLP) tasks including part-of-speech (POS) tagging (Ratnaparkhi, 1996), word segmentation (Low et al., 2005), and named entity recognition (NER) (Nadeau and Sekine, 2007). Typically, existing methods follow the supervised learning paradigm, and require high-quality annotations. While gold standard annotation is expensive and ∗ The first two authors contributed equally. Our code can be found at https://github.com/ INK-USC/ConNet . 1 time-consuming, imperfect annotations are relatively easier to obtain from crowdsourcing (noisy labels) or other domains (out-of-domain). Despite their low cost, such supervision usually can be obtained from different sources, and it"
2020.acl-main.193,N15-1069,0,0.0765639,"ims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages (Lin et al., 2017). Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. Saito et al. (2017) and Ruder and Plank (2018) explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adaptation and does not model the differences among multiple source domains. Yang and Eisenstein (2015) represents each domain with a vector of metadata domain attributes and uses domain vectors to train the model to deal with domain shifting, which is highly dependent on prior domain knowledge. 2135 (Ghifary et al., 2016) uses an auto-encoder method by jointly training a predictor for source labels, and a decoder to reproduce target input with a shared encoder. The decoder acts as a normalizer to force the model to learn shared knowledge between source and target domains. Adversarial penalty can be added to the loss function to make models learn domain-invariant feature only (Fernando et al.,"
2020.acl-main.193,P18-1096,0,0.0424423,"inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance (Wang et al., 2019). Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages (Lin et al., 2017). Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. Saito et al. (2017) and Ruder and Plank (2018) explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adaptation and does not model the differences among multiple source domains. Yang and Eisenstein (2015) represents each domain with a vector of metadata domain attributes and uses domain vectors to train the model to deal with domain shifting, which is highly dependent on prior domain knowledge. 2135 (Ghifary et al., 2016) uses an auto-encoder method by jointly training a predictor for source labels, and a decoder to reproduce target"
2020.acl-main.445,P19-1330,0,0.0146475,"se them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of existing summarization metrics (Schluter, 2017), methods (Kedzie et al., 2018; Shapira et al., 2018), and datasets (Jung et al., 2019). Notably, Kryscinski et al. (2019) conducted a comprehensive critical evaluation for summarization from various aspects. Zopf et al. (2018) investigated sentence regression approaches in a manner similar to ours but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarization methods (Paulus et al., 2017; N"
2020.acl-main.445,P18-1013,0,0.118756,"Mj } is a support group, where I1 , I2 , ..., IMj are the indices of support sentences and Mj is the number of support sentences in Sji . One illustrative example is presented in Fig. 1. The support sentences are likely to be verbose, but we consider whether the support sentences express the semantics of the facet regardless of their length.3 The reason is that we believe extractive summarization should focus on information coverage since it cannot alter the original sentences and once salient sentences are extracted, one can then compress them in an abstractive manner (Chen and Bansal, 2018; Hsu et al., 2018). since existing datasets do not provide extractive labels but only abstractive references. Our assumption that each reference sentence corresponds to one facet is similar to that during the creation of extractive labels. The major differences are that (1) We allow an arbitrary number of support sentences while extractive labels usually limit to one support sentence for each reference sentence, i.e., we do not specify Mj . For example, we would put two support sentences to one support group if they are complementary and only combining them can cover the facet. (2) We try to find multiple suppo"
2020.acl-main.445,D19-1327,0,0.0117924,"rporate embeddingbased metrics into FAR is to use them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of existing summarization metrics (Schluter, 2017), methods (Kedzie et al., 2018; Shapira et al., 2018), and datasets (Jung et al., 2019). Notably, Kryscinski et al. (2019) conducted a comprehensive critical evaluation for summarization from various aspects. Zopf et al. (2018) investigated sentence regression approaches in a manner similar to ours but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarizat"
2020.acl-main.445,D18-1208,0,0.108378,"Missing"
2020.acl-main.445,D19-1051,0,0.0203738,"Missing"
2020.acl-main.445,W04-1013,0,0.203031,"celona in 1997 where they enjoyed a successful relationship at the Camp Nou. (ROUGE Recall/F1=0, no lexical overlap at all) Table 1: Lexical overlap — finding the document sentence with the highest ROUGE against one reference sentence — could be misleading. Examples are from the CNN/Daily Mail dataset (Nallapati et al., 2016). Introduction Text summarization has enjoyed increasing popularity due to its wide applications, whereas the evaluation of text summarization remains challenging and controversial. The most commonly used evaluation metric of summarization is lexical overlap, i.e., ROUGE (Lin, 2004), which regards the system and reference summaries as sequences of tokens and measures their n-gram overlap. However, recent studies (Paulus et al., 2017; Schluter, 2017; Kryscinski et al., 2019) reveal the limitations of ROUGE and find that in many cases, it fails to reach consensus with human judgment. Since lexical overlap only captures information 1 Data can be found at https://github.com/ morningmoni/FAR. coverage at the surface (token) level, ROUGE favors system summaries that share more tokens with the reference summaries. Nevertheless, such summaries may not always convey the desired s"
2020.acl-main.445,K16-1028,0,0.103317,"Missing"
2020.acl-main.445,P18-1188,0,0.156245,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,D18-1206,0,0.383606,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,N18-1158,0,0.270366,"upport groups (N &gt; 1), as there could be more than one set of support sentences that cover the same facet. In contrast, there is no notion of support group in extractive labels as they inherently form one such group (N = 1). Also, we allow N = 0 if such a mapping cannot be found even by humans. (3) The FAMs are more accurate as they are created by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simpl"
2020.acl-main.445,N04-1019,0,0.395762,"ed by human annotators while extractive methods use sentence regression approaches (which we evaluate in Sec. 4.1) to obtain extractive labels approximately. Relation w. Extractive Labels. Extractive methods (Nallapati et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018c) typically require binary labels of every document sentence indicating whether it should be extracted during model training. Such labels are called extractive labels and usually created heuristically based on reference summaries Comparison w. SCUs. Some may mistake FAMs for Summarization Content Units (SCUs) in Pyramid (Nenkova and Passonneau, 2004), but they are different in that (1) FAMs utilize both the documents and reference summaries while SCUs ignore the documents; (2) FAMs are at the sentence level and can thus be used to automatically evaluate extractive methods once created — simply by matching sentence indices we can know how many facets are covered, while SCUs have to be manually annotated for each system (refer to Appendix B Fig. 4). 3 We ignore coreference (e.g., “he” vs. “the writer”) and short fragments when considering the semantics of one facet, as we found that the wording of the reference summaries regarding such choi"
2020.acl-main.445,D15-1222,0,0.0179456,"ture work. Method FAR AutoFAR AutoFAR-L FAR vs. AutoFAR(-L) BanditSum Lead-3 FastRL(E) NeuSum Refresh UnifiedSum(E) 44.7 50.6 50.8 51.2 51.3 54.8 44.8 51.3 51.0 49.9 51.7 54.5 44.7 45.6 43.1 44.3 46.2 46.9 Pearson’s r 97.6 (42.9) Spearman’s ρ 77.1 (54.3) Kendall’s τ 60.0 (46.7) Table 8: FAR prediction via linear regression. AutoFAR(-L) denotes the results on the humanannotated subset (entire CNN/Daily Mail dataset). 5 Related Work Evaluation Metrics for Text Summarization. ROUGE (Lin, 2004) is the most widely used evaluation metric for text summarization. Extensions of ROUGE include ROUGE-WE (Ng and Abrecht, 2015) that incorporated word embedding into ROUGE, ROUGE 2.0 (Ganesan, 2018) that considered synonyms, and ROUGE-G (ShafieiBavani et al., 2018) that applied graph analysis to WordNet for lexical and semantic matching. Nevertheless, these extensions did not draw enough attention as the original ROUGE and recent advances (Gu et al., 2020; Zhang et al., 2019a) are still primarily evaluated by the vanilla ROUGE. Another popular branch is Pyramid-based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw"
2020.acl-main.445,E17-2007,0,0.0719641,"cument sentence with the highest ROUGE against one reference sentence — could be misleading. Examples are from the CNN/Daily Mail dataset (Nallapati et al., 2016). Introduction Text summarization has enjoyed increasing popularity due to its wide applications, whereas the evaluation of text summarization remains challenging and controversial. The most commonly used evaluation metric of summarization is lexical overlap, i.e., ROUGE (Lin, 2004), which regards the system and reference summaries as sequences of tokens and measures their n-gram overlap. However, recent studies (Paulus et al., 2017; Schluter, 2017; Kryscinski et al., 2019) reveal the limitations of ROUGE and find that in many cases, it fails to reach consensus with human judgment. Since lexical overlap only captures information 1 Data can be found at https://github.com/ morningmoni/FAR. coverage at the surface (token) level, ROUGE favors system summaries that share more tokens with the reference summaries. Nevertheless, such summaries may not always convey the desired semantics. For example, in Table 1, the document sentence with the highest ROUGE score has more lexical overlap but expresses rather different semantic meaning. In contra"
2020.acl-main.445,P17-1099,0,0.256857,"Missing"
2020.acl-main.445,D18-1085,0,0.0176168,"1.2 51.3 54.8 44.8 51.3 51.0 49.9 51.7 54.5 44.7 45.6 43.1 44.3 46.2 46.9 Pearson’s r 97.6 (42.9) Spearman’s ρ 77.1 (54.3) Kendall’s τ 60.0 (46.7) Table 8: FAR prediction via linear regression. AutoFAR(-L) denotes the results on the humanannotated subset (entire CNN/Daily Mail dataset). 5 Related Work Evaluation Metrics for Text Summarization. ROUGE (Lin, 2004) is the most widely used evaluation metric for text summarization. Extensions of ROUGE include ROUGE-WE (Ng and Abrecht, 2015) that incorporated word embedding into ROUGE, ROUGE 2.0 (Ganesan, 2018) that considered synonyms, and ROUGE-G (ShafieiBavani et al., 2018) that applied graph analysis to WordNet for lexical and semantic matching. Nevertheless, these extensions did not draw enough attention as the original ROUGE and recent advances (Gu et al., 2020; Zhang et al., 2019a) are still primarily evaluated by the vanilla ROUGE. Another popular branch is Pyramid-based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw estimated FAR scores are provided in Appendix B Fig. 5 in the interest of space. 4948 FAR is related to Pyramid and HighRES (Hardy et al.,"
2020.acl-main.445,P18-1061,0,0.0144846,"all the support sentences of one document-summary pair to one single support set and define the Support-Aware Recall (SAR) as follows. SAR is used in Sec. 3.4 for the comparative analysis of extractive methods. SAR = | Automatic Evaluation with FAR By utilizing the low abstraction category on the extractive CNN/Daily Mail dataset, we revisit extractive methods to evaluate how they perform on information coverage. Specifically, we compare Lead-3 (that extracts the first three document sentences), FastRL(E) (E for extractive only) (Chen and Bansal, 2018), BanditSum (Dong et al., 2018), NeuSum (Zhou et al., 2018), Refresh (Narayan et al., 2018c), and UnifiedSum(E) (Hsu et al., 2018) using both ROUGE and FAR. For a fair comparison, each method extracts three sentences (|E |= 3).5 Results on Neural Extractive Methods. As shown in Table 3, there is almost no discrimination among the last four methods under ROUGE-1 F1, and the rankings under ROUGE-1/2/L often contradict with each other. The observations on ROUGE Precision/Recall are similar. We provide them as well as more comparative analysis under facet-aware evaluation in Sec. 3.4. For facet coverage, the upper bound of FAR by extracting 3 sentences (O"
2020.acl-main.445,N18-1161,0,0.0523793,"Missing"
2020.acl-main.445,D19-1618,0,0.017828,"based metrics (Nenkova and Passonneau, 2004; Yang et al., 2016), which annotate and compare the Summarization Content Units (SCUs) in the summaries. 7 The raw estimated FAR scores are provided in Appendix B Fig. 5 in the interest of space. 4948 FAR is related to Pyramid and HighRES (Hardy et al., 2019) in that Pyramid employs the summaries to annotate SCUs and HighRES highlights salient text fragments in the documents, while FAR considers both the summaries and documents. Beyond lexical overlap, embedding-based evaluation metrics (Zhang et al., 2019b; Zhao et al., 2019; Sun and Nenkova, 2019; Xenouleas et al., 2019) are gaining more traction along with the dominance of pre-trained language models. One straightforward way to incorporate embeddingbased metrics into FAR is to use them as similarity measures instead of the ROUGE-based approaches tested in Sec. 4.1 for automatic FAM creation (i.e., finding support sentences for each facet by the scores of embedding-based metrics). Such similarity measures are especially beneficial when the facet and its support sentences are not similar at the lexical level. Reflections on Text Summarization. There has been increasing attention and critique to the issues of e"
2020.acl-main.445,D18-1197,0,0.0127752,"urs but they could only evaluate them approximately against ROUGE as no ground-truth labels (FAMs) existed. Annotation and Analysis. Many recent studies conduct human annotation or evaluation on text summarization and other NLP tasks to gain useful insights. Hardy et al. (2019) annotated 50 documents to demonstrate the benefits of highlightbased summarization evaluation. Recent summarization methods (Paulus et al., 2017; Narayan et al., 2018c; Chen and Bansal, 2018) generally sampled 50 to 100 documents for human evaluation in addition to ROUGE in light of its limitations. Chen et al. (2016); Yavuz et al. (2018) inspected 100 samples and analyzed their category breakdown for reading comprehension and semantic parsing, respectively. We observed similar trends when analyzing different subsets of the FAMs, indicating that our findings are relatively stable. We thus conjecture that our sample size is sufficient to verify our hypotheses and benefit future research. 6 Conclusion and Future Work We propose a facet-aware evaluation setup for better assessment of information coverage in extractive summarization. We construct an extractive summarization dataset and demonstrate the effectiveness of facet-aware"
2020.acl-main.483,P19-1459,0,0.0303382,"lem found in neural hate speech classifiers is their over-sensitivity to group identifiers like “Muslim”, “gay”, and “black”, which are only hate speech when combined with the right ∗ † Authors contributed equally Code is available here context (Dixon et al., 2018). In Figure 1 we see two documents containing the word “black” that a finetuned BERT model predicted to be hate speech, while only the second occurs in a hateful context. Neural text classifiers achieve state-of-the-art performance in hate speech detection, but are uninterpretable and can break when presented with unexpected inputs (Niven and Kao, 2019). It is thus difficult to contextualize a model’s treatment of identifier words. Our approach to this problem is to use the Sampling and Occlusion (SOC) explanation algorithm, which estimates model-agnostic, posthoc feature importance (Jin et al., 2020). We apply this approach to the Gab Hate Corpus (Kennedy et al., 2020), a new corpus labeled for “hate-based rhetoric”, and an annotated corpus from the Stormfront white supremacist online forum (de Gibert et al., 2018). Based on the explanations generated via SOC, which showed models were biased towards group identifiers, we then propose a nove"
2020.acl-main.483,D18-1302,0,0.0520212,"ata. These models suffer from the highly skewed distributions of language in these datasets (Wiegand et al., 2019). Research on bias in classification models also influences this work. Dixon et al. (2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like “gay”. Similarly, annotators’ biases towards certain social groups were found to be magnified during classifier training Mostafazadeh Davani et al. (2020). Specifically within the domain of hate speech and abusive language, Park et al. (2018) and Sap et al. (2019) have defined and studied genderand racial-bias, emphasizing issues of undetected dialect variation and imbalanced training data, respectively. Techniques for bias reduction in these settings include data augmentation by training on less biased data, term swapping during training (i.e., swapping gender words), and using debiased word embeddings (Bolukbasi et al., 2016). Complementing these works, we directly manipulate models’ modeling of the context surrounding identifier terms by regularizing explanations of these terms. Specifically, we use post-hoc explanation algorit"
2020.acl-main.483,N16-3020,0,0.0297109,"we directly manipulate models’ modeling of the context surrounding identifier terms by regularizing explanations of these terms. Specifically, we use post-hoc explanation algorithms to interpret and modulate finetuned language models like BERT (Devlin et al., 2018), which achieve state of the art performance on many hate speech detection tasks (MacAvaney et al., 2019; Mandl et al., 2019). We focus on post-hoc explanation approaches, which interpret model predictions without elucidating the mechanisms by which the model works (Guidotti et al., 2019). These explanations reveal either wordlevel (Ribeiro et al., 2016; Sundararajan et al., 2017) or phrase-level importance (Murdoch et al., 2018; Singh et al., 2019) of inputs to predictions. 3 Data We selected two public corpora for our experiments which highlight the rhetorical aspects of hate speech, versus merely the usage of slurs and explicitly offensive language (see Davidson et al., 2017). The “Gab Hate Corpus” (GHC; Kennedy et al., 2020) is a large, random sample (N = 27,655) from the Pushshift.io data dump of the Gab network ∗ , which we have annotated according to a typology of “hate-based rhetoric”, a construct motivated by hate speech criminal co"
2020.acl-main.483,P19-1163,0,0.0514664,"r from the highly skewed distributions of language in these datasets (Wiegand et al., 2019). Research on bias in classification models also influences this work. Dixon et al. (2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like “gay”. Similarly, annotators’ biases towards certain social groups were found to be magnified during classifier training Mostafazadeh Davani et al. (2020). Specifically within the domain of hate speech and abusive language, Park et al. (2018) and Sap et al. (2019) have defined and studied genderand racial-bias, emphasizing issues of undetected dialect variation and imbalanced training data, respectively. Techniques for bias reduction in these settings include data augmentation by training on less biased data, term swapping during training (i.e., swapping gender words), and using debiased word embeddings (Bolukbasi et al., 2016). Complementing these works, we directly manipulate models’ modeling of the context surrounding identifier terms by regularizing explanations of these terms. Specifically, we use post-hoc explanation algorithms to interpret and m"
2020.acl-main.483,W12-2103,0,0.0332593,"of hate speech, such as dehumanizing and insulting language. In experiments on an out-of-domain test set 5435 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5435–5442 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of news articles containing group identifiers, which are heuristically assumed to be non-hate speech, we find that regularization greatly reduces the false positive rate, while in-domain, out-of-sample classification performance is either maintained or improved. 2 Related Work Our work is conceptually influenced by Warner and Hirschberg (2012), who formulated hate speech detection as disambiguating the use of offensive words from abusive versus non-abusive contexts. More recent approaches applied to a wide typology of hate speech (Waseem et al., 2017), build supervised models trained on annotated (e.g., Waseem and Hovy, 2016; de Gibert et al., 2018) or heuristically-labeled (Wulczyn et al., 2017; Olteanu et al., 2018) data. These models suffer from the highly skewed distributions of language in these datasets (Wiegand et al., 2019). Research on bias in classification models also influences this work. Dixon et al. (2018) measured an"
2020.acl-main.483,W17-3012,0,0.020332,"y 5 - 10, 2020. 2020 Association for Computational Linguistics of news articles containing group identifiers, which are heuristically assumed to be non-hate speech, we find that regularization greatly reduces the false positive rate, while in-domain, out-of-sample classification performance is either maintained or improved. 2 Related Work Our work is conceptually influenced by Warner and Hirschberg (2012), who formulated hate speech detection as disambiguating the use of offensive words from abusive versus non-abusive contexts. More recent approaches applied to a wide typology of hate speech (Waseem et al., 2017), build supervised models trained on annotated (e.g., Waseem and Hovy, 2016; de Gibert et al., 2018) or heuristically-labeled (Wulczyn et al., 2017; Olteanu et al., 2018) data. These models suffer from the highly skewed distributions of language in these datasets (Wiegand et al., 2019). Research on bias in classification models also influences this work. Dixon et al. (2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like “gay”. Similarly, annotators’ biases towards certain"
2020.acl-main.483,N16-2013,0,0.180373,"icles containing group identifiers, which are heuristically assumed to be non-hate speech, we find that regularization greatly reduces the false positive rate, while in-domain, out-of-sample classification performance is either maintained or improved. 2 Related Work Our work is conceptually influenced by Warner and Hirschberg (2012), who formulated hate speech detection as disambiguating the use of offensive words from abusive versus non-abusive contexts. More recent approaches applied to a wide typology of hate speech (Waseem et al., 2017), build supervised models trained on annotated (e.g., Waseem and Hovy, 2016; de Gibert et al., 2018) or heuristically-labeled (Wulczyn et al., 2017; Olteanu et al., 2018) data. These models suffer from the highly skewed distributions of language in these datasets (Wiegand et al., 2019). Research on bias in classification models also influences this work. Dixon et al. (2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like “gay”. Similarly, annotators’ biases towards certain social groups were found to be magnified during classifier training Mostaf"
2020.acl-main.483,N19-1060,0,0.159304,"12; Gelber and McNamara, 2016; Gagliardone et al., 2015; Mohan et al., 2017). Performance has improved with access to more data and more sophisticated algorithms (e.g., Mondal et al., 2017; Silva et al., 2016; Del Vigna12 et al., 2017; Basile et al., 2019), but the relative sparsity of hate speech requires sampling using keywords (e.g., Olteanu et al., 2018) or sampling from environments with unusually high rates of hate speech (e.g., de Gibert et al., 2018; Hoover et al., 2019). Modern text classifiers thus struggle to learn a model of hate speech that generalizes to real-world applications (Wiegand et al., 2019). A specific problem found in neural hate speech classifiers is their over-sensitivity to group identifiers like “Muslim”, “gay”, and “black”, which are only hate speech when combined with the right ∗ † Authors contributed equally Code is available here context (Dixon et al., 2018). In Figure 1 we see two documents containing the word “black” that a finetuned BERT model predicted to be hate speech, while only the second occurs in a hateful context. Neural text classifiers achieve state-of-the-art performance in hate speech detection, but are uninterpretable and can break when presented with un"
2020.acl-main.483,S19-2007,0,\N,Missing
2020.acl-main.752,P18-1175,0,0.0630884,"Missing"
2020.acl-main.752,N16-1030,0,0.151429,"two individual entity triggers: t1 (“had ... lunch at”) and t2 (“where the food”). Both are associated to the same entity mention “Rumble Fish” (starting from 7th token) typed as restaurant (RES). Named entity recognition (NER) is a fundamental information extraction task that focuses on extracting entities from a given text and classifying them using pre-defined categories (e.g., persons, locations, organizations) (Nadeau and Sekine, 2007). Recent advances in NER have primarily focused on training neural network models with an abundance of human annotations, yielding state-of-theart results (Lample et al., 2016). However, collecting human annotations for NER is expensive and time-consuming, especially in social media messages (Lin et al., 2017a) and technical domains such as biomedical publications, financial documents, legal reports, etc. As we seek to advance NER into more domains with less human effort, ∗ 5 ?&quot; = Introduction The first two authors contributed equally. The code, data, and a longer version of the paper are at http://github.com/INK-USC/TriggerNER 1 2,5,6 → 7 how to learn neural models for NER in a costeffective way becomes a crucial research problem. The standard protocol for obtainin"
2020.acl-main.752,2020.acl-main.193,1,0.907905,"Missing"
2020.acl-main.752,2020.acl-demos.42,1,0.845213,"Missing"
2020.acl-main.752,P19-3010,1,0.890539,"Missing"
2020.acl-main.752,D18-1226,1,0.914389,"Missing"
2020.acl-main.752,P19-1524,0,0.0624096,"Missing"
2020.acl-main.752,D19-1003,0,0.045672,"Missing"
2020.acl-main.752,D15-1166,0,0.0426664,"tag sequence y. Following the most common design of neural NER architecture, BLSTMCRF (Ma and Hovy, 2016), we incorporate the entity triggers as attention queries to train a triggerenhanced sequence tagger for NER. Note that the BLSTM used in the the TrigEncoder and TrigMatcher modules is the same BLSTM we 8505 use in the SeqTagger to obtain H, the matrix containing the hidden vectors of all of the tokens. Given a sentence x, we use the previously trained TrigMatcher to compute the mean of all the trigger vectors gˆt associated with this sentence. Following the conventional attention method (Luong et al., 2015), we incorporate the mean trigger vector as the query, creating a sequence of attention-based token representations, H0 .  >  T α ~ = SoftMax v tanh U1 H + U2 gˆt T ORG MISC LOC Total 958 1,970 2.05 1.46 787 2,057 2.61 1.4 1,781 3,456 1.94 1.44 5,134 10,938 2.13 1.43 BC5CDR D ISEASE C HEMICAL Total # of Entities # of Triggers Avg. # of Trig. / Ent. Avg. Trig. Length 906 2,130 2.35 2.00 1,085 1,640 1.51 1.99 1,991 3,770 1.89 2.00 4.1 U1 , U2 , and v are trainable parameters for computing the trigger-enhanced attention scores for each token. Finally, we concatenate the original token represent"
2020.acl-main.752,P16-1101,0,0.0963656,"upervision to guide the trigger representation. Thus, the trigger vector gt is further fed into a multi-class classifier to predict the type of the associated entity e (such as PER, LOC, etc) which we use type(e) to denote. The loss of the trigger d = kgs − gt k2 1 = (1 − 1matched ) (d)2 2 1 +1matched {max (0, m − d)}2 2 The joint loss of the first stage is thus L = LT C + λLSM , where λ is a hyper-parameter to tune. 3.2 Trigger-Enhanced Sequence Tagging The learning objective in this stage is to output the tag sequence y. Following the most common design of neural NER architecture, BLSTMCRF (Ma and Hovy, 2016), we incorporate the entity triggers as attention queries to train a triggerenhanced sequence tagger for NER. Note that the BLSTM used in the the TrigEncoder and TrigMatcher modules is the same BLSTM we 8505 use in the SeqTagger to obtain H, the matrix containing the hidden vectors of all of the tokens. Given a sentence x, we use the previously trained TrigMatcher to compute the mean of all the trigger vectors gˆt associated with this sentence. Following the conventional attention method (Luong et al., 2015), we incorporate the mean trigger vector as the query, creating a sequence of attention"
2020.acl-main.752,W17-4421,1,0.95523,"Fish” (starting from 7th token) typed as restaurant (RES). Named entity recognition (NER) is a fundamental information extraction task that focuses on extracting entities from a given text and classifying them using pre-defined categories (e.g., persons, locations, organizations) (Nadeau and Sekine, 2007). Recent advances in NER have primarily focused on training neural network models with an abundance of human annotations, yielding state-of-theart results (Lample et al., 2016). However, collecting human annotations for NER is expensive and time-consuming, especially in social media messages (Lin et al., 2017a) and technical domains such as biomedical publications, financial documents, legal reports, etc. As we seek to advance NER into more domains with less human effort, ∗ 5 ?&quot; = Introduction The first two authors contributed equally. The code, data, and a longer version of the paper are at http://github.com/INK-USC/TriggerNER 1 2,5,6 → 7 how to learn neural models for NER in a costeffective way becomes a crucial research problem. The standard protocol for obtaining an annotated NER dataset involves an annotator selecting token spans in a sentence as mentions of entities, and labeling them with a"
2020.acl-main.752,D14-1162,0,0.0809908,"Missing"
2020.acl-main.752,W17-2630,0,0.0625834,"Missing"
2020.acl-main.752,C18-1183,0,0.0710943,"Missing"
2020.acl-main.752,W03-0419,0,\N,Missing
2020.acl-main.752,D18-1230,1,\N,Missing
2020.emnlp-main.158,2020.emnlp-main.703,0,0.0199765,"in VisCOLL Model ... Some fresh MASK MASK on the grass. A MASK MASK traveling down ... Training Data Stream A green with a bunch of MASK MASK hanging from it. (b) Predicting Novel Composition Figure 1: Illustration of the proposed VisCOLLtask. The end-task is Masked-Token Prediction: given an image, a model predicts the masked tokens of an associated caption in an online continual learning setup (cf. (a) in the figure). Additionally, we test composition generalization by evaluating on novel compositions (b) which are not encountered at train time. experience to semantically interpret symbols (Bisk et al., 2020; Harnad, 1990; Vigliocco et al., 2014). Introduction Modern NLP systems, including ones that build on pre-trained language models (Devlin et al., 2019; Radford et al., 2019), excel on a wide variety of tasks. These systems rely on offline (batch) training and have drawn recent criticism due to their inability to adapt to new contexts (Linzen, 2020). In contrast, humans acquire language from evolving environments, require a small memory footprint (McClelland et al., 1995), and can generalize their knowledge to newer tasks (Sprouse et al., 2013). It has been suggested that humans ground percept"
2020.emnlp-main.158,N19-1423,0,0.0225726,"from it. (b) Predicting Novel Composition Figure 1: Illustration of the proposed VisCOLLtask. The end-task is Masked-Token Prediction: given an image, a model predicts the masked tokens of an associated caption in an online continual learning setup (cf. (a) in the figure). Additionally, we test composition generalization by evaluating on novel compositions (b) which are not encountered at train time. experience to semantically interpret symbols (Bisk et al., 2020; Harnad, 1990; Vigliocco et al., 2014). Introduction Modern NLP systems, including ones that build on pre-trained language models (Devlin et al., 2019; Radford et al., 2019), excel on a wide variety of tasks. These systems rely on offline (batch) training and have drawn recent criticism due to their inability to adapt to new contexts (Linzen, 2020). In contrast, humans acquire language from evolving environments, require a small memory footprint (McClelland et al., 1995), and can generalize their knowledge to newer tasks (Sprouse et al., 2013). It has been suggested that humans ground perceptual 1 Code and data: https://github.com/INK-USC/ VisCOLL Model model the challenge, we propose VisCOLL, a Visually-grounded ContinuaL Learning setup, t"
2020.emnlp-main.158,P19-1350,0,0.0587781,"Missing"
2020.emnlp-main.158,K19-1009,0,0.0181616,"es from the official validation set as our test set, and the rest as the test set. For Flickr-shift, we use the official train, validation and the test split. Note that the “task” is only used as an identifier of data distribution for constructing the dataset; the task identities are not revealed to models and the way we construct the data streams ensures there are no clear task boundaries. Test Split of Novel Compositions. We measure compositional generalization by evaluating on a disjoint set of noun-adjective or noun-verb compositions. We use the compositional test split of COCO dataset by Nikolaus et al. (2019) and remove images related to predefined 24 concept pairs (e.g., black cat, standing child) from the training, validation and the regular test set. The test split is referred to as the compositional test set, and the rest is referred to as the regular test set. 5 Methods To benchmark on VisCOLL and study the challenges it poses on model learning, we establish several continual learning baselines. We use visuallanguage encoder models (Sec. 5.1) for masked token predictions. These models are trained from scratch (i.e., randomly initialized) with continual learning algorithms (Sec. 5.2) to dissua"
2020.emnlp-main.158,P02-1040,0,0.107196,"Missing"
2020.emnlp-main.158,2020.acl-main.465,0,0.025022,"ption in an online continual learning setup (cf. (a) in the figure). Additionally, we test composition generalization by evaluating on novel compositions (b) which are not encountered at train time. experience to semantically interpret symbols (Bisk et al., 2020; Harnad, 1990; Vigliocco et al., 2014). Introduction Modern NLP systems, including ones that build on pre-trained language models (Devlin et al., 2019; Radford et al., 2019), excel on a wide variety of tasks. These systems rely on offline (batch) training and have drawn recent criticism due to their inability to adapt to new contexts (Linzen, 2020). In contrast, humans acquire language from evolving environments, require a small memory footprint (McClelland et al., 1995), and can generalize their knowledge to newer tasks (Sprouse et al., 2013). It has been suggested that humans ground perceptual 1 Code and data: https://github.com/INK-USC/ VisCOLL Model model the challenge, we propose VisCOLL, a Visually-grounded ContinuaL Learning setup, to acquire compositional phrases from streaming visual-linguistic data. Models receive a stream of paired image-caption data which has a shifting object distribution. As the end task, we employ masked"
2020.emnlp-main.158,2020.acl-demos.14,0,0.0263793,"ge datasets: COCO-captions (Chen et al., 2015) and Flickr30k Entities (Plummer et al., 2015) which provide multiple captions for each image in MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014) respectively. We call the resulting datasets COCO-shift and Flickr-shift (see Table 1 for dataset statistics). Constructing a dataset for VisCOLL involves two key steps: (i) identify the phrase to be masked which involves a noun and associated verbs and adjectives (ii) create a non-stationary data-stream. Masking Tokens. First, we append part-of-speech tags (POS) to each caption using Stanza (Qi et al., 2020). For Flickr30k Entities, we use the annotated noun-phrases as mask tokens. For COCO-captions, we identify text spans with a regular expression chunker with the following regular expression. CHUNK: &lt;DT&gt;?&lt;JJ|VBG|VBN&gt;*&lt;NN|NNS&gt;+ &lt;VB|VBD|VBG|VBN|VBP|VPZ&gt;* The resulting text span always includes a noun, and optionally include a determinator and an adjective and verb before or after the noun. To construct a data-stream, we define a “task” as the object being referred to in the textual input data. For Flickr30k Entities, this is simply the lemmatized noun in the masked span. For COCOcaptions, we furt"
2020.emnlp-main.158,D19-1514,0,0.176235,"arch space makes it infeasible to view all possible combinations of atomic words at train time. Therefore, to succeed on VisCOLL, models should generalize to novel compositions at test time (also called composition generalization) (Lake and Baroni, 2017; Keysers et al., 2020). In this work, we extensively study the challenges associated with VisCOLL. To facilitate the research, we construct a continuously shifting data distribution to closely resemble real-word datastream and contribute COCO-shift and Flickr-shift. We benchmark these datasets using multi-modal language modeling architectures (Tan and Bansal, 2019; Su et al., 2020) which achieve state-of-art performance on multiple vision-language tasks. In particular, we don’t use any pre-training, instead train randomly initialized models on streaming data using continual learning algorithms (Robins, 1995; Rolnick et al., 2019; Aljundi et al., 2019a) and evaluate their resistance to forgetting and compositional generalization. We quantify the performance and forgetfulness of trained models and evaluate on a novel test split to measure compositional generalization, as shown in Fig. 1(b). Our proposed VisCOLL benchmark reveals that the gains observed i"
2020.emnlp-main.158,N19-1086,0,0.031459,"compositionality and continual learning. To test compositionality, we choose visually grounded masked language modeling where the model needs to compose atomic words to describe complex and novel visual scenes. To simulate a realistic continual learning setup, we construct a dynamic environment where the training data comes in as a non-stationary data stream without clear “task” boundaries. Since the goal is to simulate lanRelated Works Continual Learning. A major challenge in continual learning is to alleviate catastrophic forgetting (Robins, 1995). Several recent works (Greco et al., 2019; Wang et al., 2019; de Masson d’Autume et al., 2019) study the challenge in the context of NLP. Existing continual learning algorithms can be broadly classified into memorybased approaches (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2019b), pseudo-replay based apModeling Language Compositionality. Capturing compositionality in language has been a long challenge (Fodor et al., 1988) for neural networks. Recent works explore the problem with compositional generalization on synthetic instruction following (Lake and Baroni, 2017), text-based games (Yuan et al., 2019), visual question answering (Bahdanau et al., 2"
2020.emnlp-main.158,Q14-1006,0,0.0307401,"tasets COCO-shift and Flickr-shift. 4 0 100000 200000 300000 Time step 400000 Figure 4: Probability distributions of 50 tasks (the noun in the masked tokens) in Flickr-shift data stream. Each curve corresponds to a task. x-axis shows the time step, and y-axis shows the probability of visiting the given task at a specific time step. Dataset Construction We construct our data streams using two popular vision-language datasets: COCO-captions (Chen et al., 2015) and Flickr30k Entities (Plummer et al., 2015) which provide multiple captions for each image in MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014) respectively. We call the resulting datasets COCO-shift and Flickr-shift (see Table 1 for dataset statistics). Constructing a dataset for VisCOLL involves two key steps: (i) identify the phrase to be masked which involves a noun and associated verbs and adjectives (ii) create a non-stationary data-stream. Masking Tokens. First, we append part-of-speech tags (POS) to each caption using Stanza (Qi et al., 2020). For Flickr30k Entities, we use the annotated noun-phrases as mask tokens. For COCO-captions, we identify text spans with a regular expression chunker with the following regular expressi"
2020.emnlp-main.158,D19-1280,0,0.0643827,"Missing"
2020.emnlp-main.158,C92-1042,0,\N,Missing
2020.emnlp-main.541,D18-1225,0,0.253257,"rts study reasoning on standard knowledge graphs, where each fact is represented as a triple of subject entity, object entity and the relation between them. However, in practice, each fact may not be true forever, https://github.com/INK-USC/RE-Net ? Threaten Praise South Korea Introduction 1 Protestor (Iran) Given a temporal knowledge graph with timestamps varying from t0 to tT , TKG reasoning primarily has two settings - interpolation and extrapolation. In the interpolation setting, new facts are predicted for time t such that t0 ≤ t ≤ tT (Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018). In contrast, extrapolation reasoning, as a less studied setting, focuses on predicting new facts (e.g., unseen events) over timestamps t that are greater than tT (i.e., t > tT ). The extrapolation setting is of particular interests in TKG reasoning as it helps populate the knowledge graph over future timestamps and facilitates forecasting emerging events (Muthiah et al., 2015; Phillips et al., 2017; Korkmaz et al., 2015). Recent attempts to solve the extrapolation TKG reasoning problem are Know-Evolve (Trivedi et al., 2017) and its extension DyRep (Trivedi 6669 Proceedings of the 2020 Confer"
2020.emnlp-main.541,D18-1516,0,0.1192,"Missing"
2020.emnlp-main.557,N19-1246,0,0.108854,"Missing"
2020.emnlp-main.557,P19-1388,0,0.127809,"an retrieve paths from ConceptNet that aid in interpreting the decision made by the PTLMs on the CommonsenseQA dataset (Talmor et al., 2019b). Lin et al. (2019b) probe the commonsense knowledge in pre-trained language generation models via a constrained text generation task. However, they do not consider numerical commonsense knowledge, which is relatively under-explored area. Numerical Commonsense Knowledge. Forbes and Choi (2017) and Goel et al. (2019) studied commonsense comparisons between two physical objects (e.g., a house is usually bigger than a person) in pre-trained word embeddings. Elazar et al. (2019) and Yamane et al. (2020) propose to induce the commonsense distribution of quantitative attributes (e.g., mass, length, and currency) of objects. Their goal is to extract or crowd-source such numerical attributes, and then obtain distributions that reflect commonsense knowledge. N U MER S ENSE, however, mainly focuses on exact numerical commonsense facts (e.g., a bird has two legs) instead of a range of values (e.g., a tiger weighs around 120kg), and have a larger number of arguments besides physical attributes. Encoding Numerics for Computation. Wallace et al. (2019) probe PTLMs in terms of"
2020.emnlp-main.557,P17-1025,0,0.0623223,"ized various existing language understanding datasets targeting commonsense knowledge to test if PTLMs can capture certain commonsense knowledge. Lin et al. (2019a) also show that PTLMs can retrieve paths from ConceptNet that aid in interpreting the decision made by the PTLMs on the CommonsenseQA dataset (Talmor et al., 2019b). Lin et al. (2019b) probe the commonsense knowledge in pre-trained language generation models via a constrained text generation task. However, they do not consider numerical commonsense knowledge, which is relatively under-explored area. Numerical Commonsense Knowledge. Forbes and Choi (2017) and Goel et al. (2019) studied commonsense comparisons between two physical objects (e.g., a house is usually bigger than a person) in pre-trained word embeddings. Elazar et al. (2019) and Yamane et al. (2020) propose to induce the commonsense distribution of quantitative attributes (e.g., mass, length, and currency) of objects. Their goal is to extract or crowd-source such numerical attributes, and then obtain distributions that reflect commonsense knowledge. N U MER S ENSE, however, mainly focuses on exact numerical commonsense facts (e.g., a bird has two legs) instead of a range of values"
2020.emnlp-main.557,2020.acl-main.89,0,0.223879,"Missing"
2020.emnlp-main.557,D19-6016,0,0.101237,"age understanding datasets targeting commonsense knowledge to test if PTLMs can capture certain commonsense knowledge. Lin et al. (2019a) also show that PTLMs can retrieve paths from ConceptNet that aid in interpreting the decision made by the PTLMs on the CommonsenseQA dataset (Talmor et al., 2019b). Lin et al. (2019b) probe the commonsense knowledge in pre-trained language generation models via a constrained text generation task. However, they do not consider numerical commonsense knowledge, which is relatively under-explored area. Numerical Commonsense Knowledge. Forbes and Choi (2017) and Goel et al. (2019) studied commonsense comparisons between two physical objects (e.g., a house is usually bigger than a person) in pre-trained word embeddings. Elazar et al. (2019) and Yamane et al. (2020) propose to induce the commonsense distribution of quantitative attributes (e.g., mass, length, and currency) of objects. Their goal is to extract or crowd-source such numerical attributes, and then obtain distributions that reflect commonsense knowledge. N U MER S ENSE, however, mainly focuses on exact numerical commonsense facts (e.g., a bird has two legs) instead of a range of values (e.g., a tiger weighs a"
2020.emnlp-main.557,W19-4828,0,0.0251046,"our” as long as the adjacent word after the [MASK] is ‘legs’? To investigate if the bias exists, we show some case studies in Table 3. As 1,000 different randomly generated words fill the ‘[x]’s we see that both BERT and RoBERTa have a bias towards a certain answer, evidenced by the existence of a dominant answer in the softmax distribution. However, it seems that RoBERTa’s (Liu et al., 2019) modified pre-training strategy helps it have less bias. We argue that future studies should further control the bias in masked language modeling. Attention distribution. Following the prior probing work (Clark et al., 2019) on the relationship 5 Open-Domain ‘How-Many’ Questions The examples in the N UMER S ENSE can be also seen as open-domain questions targeting ‘howmany’ commonsense––“how many legs does a fly usually have?” Answering these open-domain numerical commonsense questions is a practical downstream application of models that are successful in the N UMER S ENSE. Thus, as a side note, we also report the performance of the state-of-the-art open-domain QA model (Asai et al., 2020). We use the model that is trained on the Natural Question (NQ) dataset (Kwiatkowski et al., 2019), where we replace the ‘[MASK"
2020.emnlp-main.557,N16-1136,0,0.0523854,"Missing"
2020.emnlp-main.557,D19-1109,0,0.11351,"Missing"
2020.emnlp-main.557,Q19-1026,0,0.0599279,"Missing"
2020.emnlp-main.557,N19-1423,0,0.0916229,"Missing"
2020.emnlp-main.557,D19-1282,1,0.869472,"he ability of PTLMs to capture linguistic knowledge via self-supervised learning from unlabeled data. We are interested in the numerical commonsense knowledge of PTLMs. Probing Commonsense Knowledge. Besides the works that we have discussed in Section 1, Zhou et al. (2020) and Talmor et al. (2019a) also proposed to probe the commonsense knowledge of pretrained language models, following the prior work by Trinh and Le (2018a and 2018b). They both utilized various existing language understanding datasets targeting commonsense knowledge to test if PTLMs can capture certain commonsense knowledge. Lin et al. (2019a) also show that PTLMs can retrieve paths from ConceptNet that aid in interpreting the decision made by the PTLMs on the CommonsenseQA dataset (Talmor et al., 2019b). Lin et al. (2019b) probe the commonsense knowledge in pre-trained language generation models via a constrained text generation task. However, they do not consider numerical commonsense knowledge, which is relatively under-explored area. Numerical Commonsense Knowledge. Forbes and Choi (2017) and Goel et al. (2019) studied commonsense comparisons between two physical objects (e.g., a house is usually bigger than a person) in pre-"
2020.emnlp-main.557,2021.ccl-1.108,0,0.204324,"Missing"
2020.emnlp-main.557,D19-1250,0,0.0832449,"Missing"
2020.emnlp-main.557,D19-1534,0,0.049925,"-trained word embeddings. Elazar et al. (2019) and Yamane et al. (2020) propose to induce the commonsense distribution of quantitative attributes (e.g., mass, length, and currency) of objects. Their goal is to extract or crowd-source such numerical attributes, and then obtain distributions that reflect commonsense knowledge. N U MER S ENSE, however, mainly focuses on exact numerical commonsense facts (e.g., a bird has two legs) instead of a range of values (e.g., a tiger weighs around 120kg), and have a larger number of arguments besides physical attributes. Encoding Numerics for Computation. Wallace et al. (2019) probe PTLMs in terms of the ability to represent numeracy tokens by a regression task (e.g., “71” → 71.0), and also find that BERT is not good at encoding numerical tokens. Some works focus on incorporate algebra computation ability in PTLMs (Zou and Lu, 2019; Geva et al., 2020), thus making them able to answer math reasoning tasks such as MAWPS (Koncel-Kedziorski et al., 2016) and DROP (Dua et al., 2019). Note that these models and tasks are not targeting numerical commonsense knowledge but mainly the numerical-related computation within text. 7 Conclusion We present a probing task, N UMER S"
2020.emnlp-main.557,D19-1536,0,0.0144841,"then obtain distributions that reflect commonsense knowledge. N U MER S ENSE, however, mainly focuses on exact numerical commonsense facts (e.g., a bird has two legs) instead of a range of values (e.g., a tiger weighs around 120kg), and have a larger number of arguments besides physical attributes. Encoding Numerics for Computation. Wallace et al. (2019) probe PTLMs in terms of the ability to represent numeracy tokens by a regression task (e.g., “71” → 71.0), and also find that BERT is not good at encoding numerical tokens. Some works focus on incorporate algebra computation ability in PTLMs (Zou and Lu, 2019; Geva et al., 2020), thus making them able to answer math reasoning tasks such as MAWPS (Koncel-Kedziorski et al., 2016) and DROP (Dua et al., 2019). Note that these models and tasks are not targeting numerical commonsense knowledge but mainly the numerical-related computation within text. 7 Conclusion We present a probing task, N UMER S ENSE, to induce numerical commonsense knowledge from pretrained language models. We collect a new diagnostic dataset carefully verified by human annotators, which covers 8 different topics. Powerful pre-trained models such as BERT and RoBERTa perform surprisi"
2020.emnlp-main.557,N19-1421,0,0.0881014,"Missing"
2020.emnlp-main.557,P19-1452,0,0.0585876,"Missing"
2020.emnlp-main.666,P19-1421,0,0.351222,"set of entities that belong to the same semantic class (i.e., Country). Entity synonym discovery (ESD) intends to group all terms in a vocabulary that refer to the same realworld entity (e.g., “America” and “USA” refer to the same country) into a synonym set (hence called a synset). Those discovered entities and synsets include rich knowledge and can benefit many downstream applications such as semantic search (Xiong Contributions. TX xiangren@usc.edu derives Land of Lincoln Introduction * Equal IL Vocabulary V 4 et al., 2017), taxonomy construction (Shen et al., 2018a), and online education (Yu et al., 2019a). Previous studies regard ESE and ESD as two independent tasks. Many ESE methods (Mamou et al., 2018b; Yan et al., 2019; Huang et al., 2020; Zhang et al., 2020; Zhu et al., 2020) are developed to iteratively select and add the most confident entities into the set. A core challenge for ESE is to find those infrequent long-tail entities in the target semantic class (e.g., “Lone Star State” in the class US_States) while filtering out false positive entities from other related classes (e.g., “Austin” and “Dallas” in the class City) as they will cause semantic shift to the set. Meanwhile, various"
2020.emnlp-main.666,2020.acl-main.725,1,0.906301,"o the same realworld entity (e.g., “America” and “USA” refer to the same country) into a synonym set (hence called a synset). Those discovered entities and synsets include rich knowledge and can benefit many downstream applications such as semantic search (Xiong Contributions. TX xiangren@usc.edu derives Land of Lincoln Introduction * Equal IL Vocabulary V 4 et al., 2017), taxonomy construction (Shen et al., 2018a), and online education (Yu et al., 2019a). Previous studies regard ESE and ESD as two independent tasks. Many ESE methods (Mamou et al., 2018b; Yan et al., 2019; Huang et al., 2020; Zhang et al., 2020; Zhu et al., 2020) are developed to iteratively select and add the most confident entities into the set. A core challenge for ESE is to find those infrequent long-tail entities in the target semantic class (e.g., “Lone Star State” in the class US_States) while filtering out false positive entities from other related classes (e.g., “Austin” and “Dallas” in the class City) as they will cause semantic shift to the set. Meanwhile, various ESD methods (Qu et al., 2017; Ustalov et al., 2017a; Wang et al., 2019; Shen et al., 2019) combine stringlevel features with embedding features to find a query"
2020.emnlp-main.688,D18-1362,0,0.551489,", which leverages high-quality rules generated by symbolicbased methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets show that RuleGuider improves the performance of walk-based models without losing interpretability. 1 1 Introduction While knowledge graphs (KGs) are widely adopted in natural language processing applications, a major bottleneck hindering its usage is the sparsity of facts (Min et al., 2013), leading to extensive studies on KG completion (or reasoning) (Trouillon et al., 2016; Dettmers et al., 2018; Das et al., 2017; Xiong et al., 2017; Lin et al., 2018; Meilicke et al., 2019). Many traditional approaches on the KG reasoning task are based on logic rules (Landwehr et al., 2007, 2010; Gal´arraga et al., 2013, 2015). These methods are referred to as symbolic-based methods. Although they showed good performance (Meilicke et al., 2019, 2020), they are inherently limited by their representations and generalizability of the associated relations of the given rules. To ameliorate such limitations, embeddingbased methods (Bordes et al., 2013; Socher et al., ∗ Equal contributions. https://github.com/derenlei/ KG-RuleGuider 1 2013; Wang et al., 2014; Y"
2020.emnlp-main.688,D15-1174,0,\N,Missing
2020.emnlp-main.688,N13-1095,0,\N,Missing
2020.emnlp-main.688,D17-1060,0,\N,Missing
2020.emnlp-main.688,N18-1165,0,\N,Missing
2020.emnlp-main.99,D19-5804,0,0.0326225,"sample questions from CommonsenseQA with the reasoning paths output by MHGRN. 7 Related Work Knowledge-Aware Methods for NLP Various work have investigated the potential to empower NLP models with external knowledge. Many attempt to extract structured knowledge, either in the form of nodes (Yang and Mitchell, 2017; Wang et al., 2019), triples (Weissenborn et al., 2017; Mihaylov and Frank, 2018), paths (Bauer et al., 2018; Kundu et al., 2019; Lin et al., 2019), or subgraphs (Li and Clark, 2015), and encode them to augment textual understanding. Recent success of pre-trained LMs motivates many (Pan et al., 2019; Ye et al., 2019; Zhang et al., 2018; Li et al., 2019; Banerjee et al., 2019) to probe LMs’ potential as latent knowledge bases. This line of work turn to textual knowledge (e.g. Wikipedia) to directly impart knowledge to pre-trained LMs. They generally fall into two paradigms: 1) Finetuning LMs on large-scale general-domain datasets (e.g. RACE (Lai et al., 2017)) or on knowledge-rich text. 2) Providing LMs with evidence via information retrieval techniques. However, these models cannot provide explicit reasoning and evidence, thus hardly trustworthy. They are also subject to the availability"
2020.emnlp-main.99,N19-1421,0,0.0592102,"ility and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, 1 with the code for experiments released . Desires Learn Schoolroom Classroom Where does a child likely sit at a desk? A Schoolroom B Furniture store C Patio D Office building E Library Introduction Many recently proposed question answering tasks require not only machine comprehension of the question and context, but also relational reasoning over entities (concepts) and their relationships by referencing external knowledge (Talmor et al., 2019; Sap et al., 2019; Clark et al., 2018; Mihaylov et al., 2018). For example, the question in Fig. 1 requires a model to perform relational reasoning over mentioned entities, i.e., to infer latent relations among the concepts: {C HILD, S IT, D ESK, S CHOOLROOM}. Background knowledge such as “a child is likely to appear in a schoolroom” may not be readily contained in the questions themselves, but are commonsensical to humans. Despite the success of large-scale pre-trained language models (PTLMs) (Devlin et al., 2019; ò Desk AtLocation 1 Child The first two authors contributed equally. The major"
2020.emnlp-main.99,N16-1170,0,0.0344087,"ll our implemented models, we use pre-trained LMs as text encoders for s for fair comparison. We do compare our models with those (Ma et al., 2019; Lv et al., 2019; Khashabi et al., 2020) augmented by other text-form external knowledge (e.g., Wikipedia), although we stick to our focus of encoding structured KG. Specifically, we fine-tune B ERT-BASE, B ERTL ARGE (Devlin et al., 2019), and RO BERTA (Liu et al., 2019b) for multiple-choice questions. We 8 take RGCN (Eq. 2 in §3), RN (Eq. 3 in §3), KagNet (Eq. 4 in §3) and GconAttn (Wang et al., 2019) as baselines. GconAttn generalizes match-LSTM (Wang and Jiang, 2016) and achieves success in language inference tasks. 6 Results and Discussions In this section, we present the results of our models in comparison with baselines as well as methods on the leaderboards for both CommonsenseQA and OpenbookQA. We also provide analysis of models’ components and characteristics. 6.1 Main Results For CommonsenseQA (Table 3), we first use the in-house data split (IH) (see §5.2) to compare our models with implemented baselines. This is different from the official split used in the leaderboard methods. Almost all KG-augmented models achieve performance gain over vanilla p"
2020.emnlp-main.99,D15-1236,0,\N,Missing
2020.emnlp-main.99,D17-1082,0,\N,Missing
2020.emnlp-main.99,P17-1132,0,\N,Missing
2020.emnlp-main.99,D18-1260,0,\N,Missing
2020.emnlp-main.99,D18-1454,0,\N,Missing
2020.emnlp-main.99,P19-1615,0,\N,Missing
2020.emnlp-main.99,P19-1226,0,\N,Missing
2020.emnlp-main.99,P19-1263,0,\N,Missing
2020.emnlp-main.99,N19-1270,0,\N,Missing
2020.emnlp-main.99,D19-1454,0,\N,Missing
2020.findings-emnlp.145,N19-1245,0,0.0312933,"weak model with labeled data; then use model prediction on unlabeled data as supervision (Carlson et al., 2009; Yang et al., 2018a). Our proposed method is non-conflicting with semi-supervised strategies and we enhance NMTeacher with these strategies to achieve the best performance. Neural Module Networks. Neural module networks (NMNs) are dynamically composed of individual modules of different capabilities. It was first proposed for VQA tasks (Andreas et al., 2016b,a; Hu et al., 2017). Recently in NLP community, reading comprehension requiring reasoning (Yang et al., 2018b; Dua et al., 2019; Amini et al., 2019) are proposed and widely studied. Recent works (Jiang and Bansal, 2019; Gupta et al., 2020) generally adopt a parser that gives a sequence of operations to derive the final answer. Our work differs in that (1) operations are constructed from explanations instead of questions; (2) NMTeacher provides supervision, instead of being used as final MRC model and trained in a fully-supervised manner. We limit our scope to SQuAD-style MRC tasks in this paper and leave other challenging tasks as future work. Unsupervised and Few-shot Learning for MRC. Several lines of work share the same goal of reducin"
2020.findings-emnlp.145,N16-1181,0,0.0285777,"be enforced through temporal ensemble (Laine and Aila, 2017; Tarvainen and Valpola, 2017). Another line of work uses bootstrapping – first training a weak model with labeled data; then use model prediction on unlabeled data as supervision (Carlson et al., 2009; Yang et al., 2018a). Our proposed method is non-conflicting with semi-supervised strategies and we enhance NMTeacher with these strategies to achieve the best performance. Neural Module Networks. Neural module networks (NMNs) are dynamically composed of individual modules of different capabilities. It was first proposed for VQA tasks (Andreas et al., 2016b,a; Hu et al., 2017). Recently in NLP community, reading comprehension requiring reasoning (Yang et al., 2018b; Dua et al., 2019; Amini et al., 2019) are proposed and widely studied. Recent works (Jiang and Bansal, 2019; Gupta et al., 2020) generally adopt a parser that gives a sequence of operations to derive the final answer. Our work differs in that (1) operations are constructed from explanations instead of questions; (2) NMTeacher provides supervision, instead of being used as final MRC model and trained in a fully-supervised manner. We limit our scope to SQuAD-style MRC tasks in this pa"
2020.findings-emnlp.145,N19-1423,0,0.218037,"ule execution (Sec. 3.1), discuss how variables and rules are obtained from explanations (Sec. 3.2), and present how a neural module teacher derives answers (Sec. 3.3). Problem Formulation Our goal is to efficiently train an extractive MRC model F, which takes as input a tuple (q, c) of question q and context c, and extracts an answer span a within the context c. We assume a lowresource situation where a large set S of (q, c) pairs 3 1600 Neural Module Teacher top k such constituents along with their score. To generate phrase representations, we first encode the sentence with BERT-base model (Devlin et al., 2019) and get representations [h1 , h2 , ..., hm ] for each token. We then apply pooling over all tokens in span p to get the phrase representation e. We considered both mean pooling and attentive pooling (Bahdanau et al., 2014). The similarity score between e and e′ can be calculated using either cosine similarity or learned bilinear similarity, i.e., Sim(e, e′ ) = tanh(eAe′ + b), where A is a learnable matrix. We discuss pretraining and design choices for softened F ILL module in Sec. 4.1. F ILL Module: (sref , pref , s) → p Description: Select the span p in a given sentence s that plays the same"
2020.findings-emnlp.145,N18-2092,0,0.0311112,". Our work differs in that (1) operations are constructed from explanations instead of questions; (2) NMTeacher provides supervision, instead of being used as final MRC model and trained in a fully-supervised manner. We limit our scope to SQuAD-style MRC tasks in this paper and leave other challenging tasks as future work. Unsupervised and Few-shot Learning for MRC. Several lines of work share the same goal of reducing dependency on human annotation for MRC. This goal can be approached from different perspectives. (1) “Distant” Supervision: to generate “proxy” training examples automatically (Dhingra et al., 2018; Lewis et al., 2019; Li et al., 2020); (2) Learning Efficiency: a model learns quickly with minimal supervision (Radford et al., 2019; Chan et al., 2019); (3) Annotation Efficiency: to create a dataset efficiently with time limit/budget; our work falls into this category. We believe these perspectives are non-conflicting with each other. It would be interesting to see whether and how methods from these perspectives can be integrated, and we leave this as future work. 7 Conclusion In this paper, we propose to teach extractive MRC with explanations, with a focus on annotation efficiency. We bel"
2020.findings-emnlp.145,2020.acl-main.497,0,0.031327,"2020) proposed NE X T to improve generalization of explanations with softened rule execution. Both BABBLE L AB BLE and NE X T highlight annotation efficiency in low-resource settings. To the best of our knowledge, we are the first to study soliciting explanations for MRC, which is intrinsically more challenging than classification tasks in existing works. Concurrent with our work, Lamm et al. (2020) proposed QED, a linguistically-grounded framework for QA explanations, which decomposes the steps to answer questions into discrete steps associated with linguistic phenomena. Related to our work, Dua et al. (2020) collect context spans that “should be aggregated to answer a question” and use these annotations as auxiliary supervision. Learning from Unlabeled data. A notable line of work focuses on enforcing consistency on unlabeled data by regularizing model predictions to be invariant to noise-augmented data (Xie et al., 2019; Yu et al., 2018). Consistency can also be enforced through temporal ensemble (Laine and Aila, 2017; Tarvainen and Valpola, 2017). Another line of work uses bootstrapping – first training a weak model with labeled data; then use model prediction on unlabeled data as supervision ("
2020.findings-emnlp.145,C18-1183,0,0.0275648,"that “should be aggregated to answer a question” and use these annotations as auxiliary supervision. Learning from Unlabeled data. A notable line of work focuses on enforcing consistency on unlabeled data by regularizing model predictions to be invariant to noise-augmented data (Xie et al., 2019; Yu et al., 2018). Consistency can also be enforced through temporal ensemble (Laine and Aila, 2017; Tarvainen and Valpola, 2017). Another line of work uses bootstrapping – first training a weak model with labeled data; then use model prediction on unlabeled data as supervision (Carlson et al., 2009; Yang et al., 2018a). Our proposed method is non-conflicting with semi-supervised strategies and we enhance NMTeacher with these strategies to achieve the best performance. Neural Module Networks. Neural module networks (NMNs) are dynamically composed of individual modules of different capabilities. It was first proposed for VQA tasks (Andreas et al., 2016b,a; Hu et al., 2017). Recently in NLP community, reading comprehension requiring reasoning (Yang et al., 2018b; Dua et al., 2019; Amini et al., 2019) are proposed and widely studied. Recent works (Jiang and Bansal, 2019; Gupta et al., 2020) generally adopt a"
2020.findings-emnlp.145,D18-1259,0,0.0417556,"that “should be aggregated to answer a question” and use these annotations as auxiliary supervision. Learning from Unlabeled data. A notable line of work focuses on enforcing consistency on unlabeled data by regularizing model predictions to be invariant to noise-augmented data (Xie et al., 2019; Yu et al., 2018). Consistency can also be enforced through temporal ensemble (Laine and Aila, 2017; Tarvainen and Valpola, 2017). Another line of work uses bootstrapping – first training a weak model with labeled data; then use model prediction on unlabeled data as supervision (Carlson et al., 2009; Yang et al., 2018a). Our proposed method is non-conflicting with semi-supervised strategies and we enhance NMTeacher with these strategies to achieve the best performance. Neural Module Networks. Neural module networks (NMNs) are dynamically composed of individual modules of different capabilities. It was first proposed for VQA tasks (Andreas et al., 2016b,a; Hu et al., 2017). Recently in NLP community, reading comprehension requiring reasoning (Yang et al., 2018b; Dua et al., 2019; Amini et al., 2019) are proposed and widely studied. Recent works (Jiang and Bansal, 2019; Gupta et al., 2020) generally adopt a"
2020.findings-emnlp.145,P14-5010,0,\N,Missing
2020.findings-emnlp.145,D17-1161,0,\N,Missing
2020.findings-emnlp.145,Q19-1026,0,\N,Missing
2020.findings-emnlp.145,N19-1246,0,\N,Missing
2020.findings-emnlp.145,P19-1487,0,\N,Missing
2020.findings-emnlp.145,Q18-1021,0,\N,Missing
2020.findings-emnlp.145,D19-1455,0,\N,Missing
2020.findings-emnlp.145,2020.tacl-1.5,0,\N,Missing
2020.findings-emnlp.158,P16-1141,0,0.0162414,"NRC discipline subject codes (sociology, math, economics, etc.), and engineering relation is 4 While these features are not exhaustive, to the best of our knowledge they are the key factors most salient to the knowledge diffusion as discussed in literature (a) Discipline diversity (b) Engineering Focus Figure 4: Bridge positioning features. Ideational Conditions. This group of features represents the semantic context and expression of a concept. How the concept is related to other concepts and the style with which the concept is expressed can both influence the diffusion and transfer process (Hamilton et al., 2016). Here we select emotionality, and accessibility in this group, and calculated them through LIWC and Dale Chall metric (details in Appendix C). We found transferred concepts are embedded in more emotional context, and described in more difficult language, compared to non-transferred counterparts. In a similar way, we plot ideational condition features over time for transferred concepts and non-transferred concepts in Fig. 5. We found that transferred concepts were consistently placed in increasingly positive contexts and conveyed in more difficult language over time, compared to nontransferred"
2020.findings-emnlp.158,W93-0231,0,0.732337,"epresenting the case when predicting whether a concept will transfer in next 1 year, 3 years or 5 years, respectively. To compare them fairly, we fix both training and testing cutoff years to keep time interval from training set to test set unchanged, which is different from the setting in previous experiments. As can be observed, prediction performance is consistently best when prediction window is 1 year, indicating the increasing difficulty in capturing long-term tempo6 Related Work Knowledge Diffusion and Transfer. Extensive studies have been dedicated to study the diffusion of knowledge (Kuhn, 1962; Rogers Everett, 1995; Hallett et al., 2019), and the transfer of knowledge from science to more applicable domains like technology (Narin and Noma, 1985; Tijssen, 2001). The majority of these studies focus on identifying contributing factors to knowledge diffusion and transfer (Rossiter, 1993; Azoulay et al., 2010; Shi et al., 2010; Kim et al., 2017). However, this line of work falls short in that (a) they focus primarily on successful / post-hoc knowledge diffusion and transfer, and little comparison of successful with unsuccessful transfer are presented, and (b) poorly specify what idea is"
2020.findings-emnlp.165,W19-2008,0,0.0758442,"Missing"
2020.findings-emnlp.165,P19-1294,0,0.0165422,". We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. In"
2020.findings-emnlp.165,P18-1082,0,0.0386358,"such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), v"
2020.findings-emnlp.165,P16-1154,0,0.240259,"onnections among conceptpairs in the dev and test examples in Fig. 8. To better summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation"
2020.findings-emnlp.165,P17-4012,0,0.0313062,"etter summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation have an emerging interest on (edit-based) non-autoregressive generation mode"
2020.findings-emnlp.165,W04-1013,0,0.0731965,"with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similarities. We report the concept Coverage, which is the average percentage of input concepts that are present in lemmatizatized outputs. In addition, we argue that it is more suitable to use evaluation metrics specially design for caption4 The used hyper-parameters are reported in the appendix. ing task, such as CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). They usually assume system generations and human references use similar concepts, and thus focus on evaluate the associations between mention"
2020.findings-emnlp.165,P19-1194,0,0.0158507,"scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019)"
2020.findings-emnlp.165,D15-1166,0,0.0159945,"ferent relation types of the one/two-hop connections among conceptpairs in the dev and test examples in Fig. 8. To better summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2"
2020.findings-emnlp.165,P02-1040,0,0.106943,". We also report their results with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similarities. We report the concept Coverage, which is the average percentage of input concepts that are present in lemmatizatized outputs. In addition, we argue that it is more suitable to use evaluation metrics specially design for caption4 The used hyper-parameters are reported in the appendix. ing task, such as CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). They usually assume system generations and human references use similar concepts, and thus focus on evaluate the associati"
2020.findings-emnlp.165,N18-1119,0,0.15875,"he best models are bold and second best ones are underlined within each metric. We highlight the metrics that we used in our official leaderboard. (Results on dev set are at Table. 7.) before the input text, we prepend the input concept set with a simple prompt: “generate a sentence with:” and fine-tune the model with the source sentence on the format “generate a sentence with c1 c2 . . . ck .” For decoding, we employ the standard beam search with a beam size of 5 for all compared models. We also report their results with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similaritie"
2020.findings-emnlp.165,E17-1061,0,0.0193372,"entary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a fe"
2020.findings-emnlp.165,D19-1454,1,0.907809,"Missing"
2020.findings-emnlp.165,P18-1238,0,0.0654374,"Missing"
2020.findings-emnlp.165,2020.acl-main.325,0,0.166295,"coder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation have an emerging interest on (edit-based) non-autoregressive generation models, which iteratively refine generated sequences. We assume that these models potentially would have better performance because of their explicit modeling on iterative refinements, and thus study the most recent such model Levenshtein Transformer (LevenTrans) by Gu et al. (2019). We also include a recent enhanced version, ConstLeven (Susanto et al., 2020), which incorporates lexical constraints in LevenTrans. Pre-trained Language Generation Models. We also employ various pre-trained language generation models, including GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), BERT-Gen (Bao et al., 2020), BART (Lewis et al., 2019), and T5 (Raffel et al., 2019), to tackle this task and test their generative commonsense reasoning ability. We fine-tuned all the above models on our training data with a seq2seq format. Specifically, to use GPT-2 for this sequence-tosequence task, we condition the language model on the fo"
2020.findings-emnlp.165,N19-1421,0,0.125624,"n knowledge-based decoding 1829 Model  Metrics T5-large+DBA T5-base+DBA GPT-2+DBA BART+DBA ROUGE-2/L BLEU-3/4 16.8 15.07 17.56 18.15 27.3 24.8 29.4 28.3 36.71 34.82 39.45 37.02 18.7 16 20.6 19.1 METEOR CIDEr SPICE Coverage 25.3 23.5 24.9 25.5 8.62 9.31 10.85 9.82 24.3 21.3 26.8 25.1 83.98 76.81 79.51 84.78 Table 5: Experimental results of models with DBA decoding method on the test set. or re-ranking as future directions. 5.3 Transferring CommonGen Models One may wonder how fine-tuned C OMMON G EN models can benefit commonsense-centric downstream tasks such as Commonsense Question Answering (Talmor et al., 2019) (CSQA) with their generative commonsense reasoning ability. To this end, we use the models trained with the C OMMON G EN dataset for generating useful context. We extract the nouns and verbs in questions and all choices respectively, and combine the concepts of the question q and each choice ci to build five concept-sets. Then, we use these concept-sets as inputs to a trained C OMMON G EN model (e.g., T5) for generating scenario a sentence gi for each as choice-specific contexts. Finally, we prepend the outputs in front of the questions, i.e., “<s>G: gi ∣ Q: q </s> C: ci </s>”. Note that the"
2020.findings-emnlp.165,P18-2016,1,0.836861,"easonable and natural sentence for correct choices while noisy sentences for wrong choices. For example with CG (T5), q=“What do people aim to do at work?”, ci =‘complete job’ (3) with gi =“people work to complete a job aimed at achieving a certain goal.”; cj =‘wear hats’ (7) gj =“people wearing hats aim their guns at each other while working on a construction site.” The used question concepts and choice concepts are underlined. 6 Related Work Commonsense benchmark datasets. There are many emerging datasets for testing machine commonsense from different angles, such as commonsense extraction (Xu et al., 2018; Li et al., 2016), next situation prediction (SWAG (Zellers et al., 2018), CODAH (Chen et al., 2019), HellaSWAG (Zellers et al., 2019b)), cultural and social understanding (Lin et al., 2018; Sap et al., 2019a,b), visual scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel"
2020.findings-emnlp.165,P19-1193,0,0.0214423,"l for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), video storytelling (Yang et al., 2019b), and conversational systems (Zhang et al., 2020a). These works suggest that generative commonsense reasoning has a great potential to benefit downstream applications. Our proposed C OMMON G EN, to the best of our knowledge, is the very first constrained sentence generation dataset for assessing and conferring generative machine commonsense and we hope it can benefit such applications. Our transferring study in Sec. 5.3 also shows the potential benefits of CommonGen-generated contexts. 7 Conclusion Our major contrib"
2020.findings-emnlp.165,Q14-1006,0,0.0414473,"t co-occurrences Multiple Caption Corpora dev/test train Human References Actively Monitored Crowd-sourcing diversity-based sampling (Concept-Set, Sents) Concept-Sets Figure 3: Dataset construction workflow overview. in everyday situations. As web images and video clips capture diverse everyday scenarios, we use their caption text as a natural resource for collecting concept-sets and their corresponding descriptions of commonsense scenarios. More specifically, we collect visually-grounded sentences from several existing caption datasets, including image captioning datasets, such as Flickr30k (Young et al., 2014), MSCOCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), as well as video captioning datasets including LSMDC (Rohrbach et al., 2017), ActivityNet (Krishna et al., 2017), and VATEX (Wang et al., 2019b). We first conduct part-of-speech tagging over all sentences in the corpora such that words in sentences can be matched to the concept vocabulary of ConceptNet. Then, we compute the sentence frequency of concept-sets consisting of 3∼5 concepts. That is, for each combination of three/four/five concepts in the vocabulary, we know how many sentences are in the corpora covering all con"
2020.findings-emnlp.165,2020.semeval-1.39,0,0.0666593,"Missing"
2020.findings-emnlp.165,P19-1393,0,0.119283,"in everyday situations. As web images and video clips capture diverse everyday scenarios, we use their caption text as a natural resource for collecting concept-sets and their corresponding descriptions of commonsense scenarios. More specifically, we collect visually-grounded sentences from several existing caption datasets, including image captioning datasets, such as Flickr30k (Young et al., 2014), MSCOCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), as well as video captioning datasets including LSMDC (Rohrbach et al., 2017), ActivityNet (Krishna et al., 2017), and VATEX (Wang et al., 2019b). We first conduct part-of-speech tagging over all sentences in the corpora such that words in sentences can be matched to the concept vocabulary of ConceptNet. Then, we compute the sentence frequency of concept-sets consisting of 3∼5 concepts. That is, for each combination of three/four/five concepts in the vocabulary, we know how many sentences are in the corpora covering all concepts. Ideally, we want the selected concept-sets in our dataset to reflect the natural distribution of conceptsets in the real world. At first glance, a reasonable solution may seem to sample from the distribution"
2020.findings-emnlp.165,D18-1009,1,0.815176,"nces for wrong choices. For example with CG (T5), q=“What do people aim to do at work?”, ci =‘complete job’ (3) with gi =“people work to complete a job aimed at achieving a certain goal.”; cj =‘wear hats’ (7) gj =“people wearing hats aim their guns at each other while working on a construction site.” The used question concepts and choice concepts are underlined. 6 Related Work Commonsense benchmark datasets. There are many emerging datasets for testing machine commonsense from different angles, such as commonsense extraction (Xu et al., 2018; Li et al., 2016), next situation prediction (SWAG (Zellers et al., 2018), CODAH (Chen et al., 2019), HellaSWAG (Zellers et al., 2019b)), cultural and social understanding (Lin et al., 2018; Sap et al., 2019a,b), visual scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine"
2020.findings-emnlp.165,P19-1472,1,0.917682,"v accuracy) by generating additional context. 1 [Machines] UniLM: Two dogs are throwing frisbees at each other . Introduction Commonsense reasoning, the ability to make acceptable and logical assumptions about ordinary scenes in our daily life, has long been acknowledged as a critical bottleneck of artificial intelligence and natural language processing (Davis and Marcus, 2015). Most recent commonsense reasoning challenges, such as CommonsenseQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019b), WinoGrande (Sakaguchi et al., 2019) and HelConcept-Set: exercise |rope |wall |tie |wave laSwag (Zellers et al., 2019b), have been framed - A man in a gym exercises by waving ropes tied to a wall. as- The discriminative tasks – i.e. AI systems are regym owner decided to tie a rope to the wall so people could quired to choose the correct option from [Humans] a set of make a wave in it for exercise. choices based on a given context. While signifiGPT2: A woman is tied up in a rope and swinging a wave at a wall. cant progress has been made on these discriminaUniLM: A man with a rope and tie is doing some exercise on a wall. tive tasks, argue that reasoning BART: A manwe is tied to a rope andcommonsense is waving"
2020.findings-emnlp.165,2020.acl-main.184,0,0.0619565,"nerating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), video storytelling (Yang et al., 2019b), and conversational systems (Zhang et al., 2020a). These works suggest that generative commonsense reasoning has a great potential to benefit downstream applications. Our proposed C OMMON G EN, to the best of our knowledge, is the very first constrained sentence generation dataset for assessing and conferring generative machine commonsense and we hope it can benefit such applications. Our transferring study in Sec. 5.3 also shows the potential benefits of CommonGen-generated contexts. 7 Conclusion Our major contribution in this paper are threefold: • we present C OMMON G EN, a novel constrained text generation task for generative commonsen"
2020.findings-emnlp.165,J15-3005,0,0.0268136,"n a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address thes"
2020.findings-emnlp.369,2020.emnlp-main.99,1,0.524801,"Missing"
2020.findings-emnlp.369,D19-1269,1,0.698938,"018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge paths dynamically, even when these are absent in the KG. Dynamic Knowledge Path Generation. Several methods generate knowledge paths instead of extracting them from static KGs. Asai et al. (2019) learn reasoning paths by forming sequences of evidence documents, however, their approach relies on the inter-document hyperlinks to establish relations in the constructed KG. The extractor of Fu et al. (2019) retrieves missing facts in order to address the sparsity of KGs. Unlike our work, their setting is limited to knowledge graph completion, where both a query entity and a single query relation are given. The most similar existing work to ours is that by Bosselut and Choi (2019), which also leverages GPT-2 to dynamically generate knowledge paths. We see two key differences between this method and ours: (1) they expand their paths gradually by predicting the next entity one at a time, while we generate the paths in an end-to-end manner; (2) their method is restricted to a setting where the conte"
2020.findings-emnlp.369,D19-1243,0,0.0628846,"Missing"
2020.findings-emnlp.369,P16-1137,0,0.0502898,"ted explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 16 - 20, 2020. 2020 Association for Computational Linguistics work on commonsense KG completion (Li et al., 2016; Bosselut et al., 2019; Bosselut and Choi, 2019) is limited to predicting the tail of a statement with known head and relation, or a single-hop"
2020.findings-emnlp.369,D19-1282,1,0.84587,"ternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 16 - 20, 2020. 2020 Association for"
2020.findings-emnlp.369,2021.ccl-1.108,0,0.177303,"Missing"
2020.findings-emnlp.369,D19-6003,0,0.111195,"Missing"
2020.findings-emnlp.369,D18-1260,0,0.549363,"contributions are: 1. We propose a method to generate task-relevant knowledge paths that may not exist in the original KG, thus addressing the contextualization and sparsity challenges of KGs. 2. We design and implement a framework with three variants of our PG, to understand the role of local and global graph information. 3. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our method compared to previous methods, as well as its robustness to limited training data. 2 Preliminaries Our multiple-choice commonsense QA setup follows prior work (Talmor et al., 2018; Mihaylov et al., 2018; Bisk et al., 2020): given a question q, a system selects exactly one of the choices a as an answer. To experiment with contextualized background knowledge, we adopt a general framework (Figure 2) consisting of a context module, a knowledge module and a reasoning module. The context module encodes both the question q and a choice a as unstructured evidence, while the knowledge module encodes external facts as structured evidence. Both the unstructured and the structured evidence are fed to the reasoning module, which produces a score for a question-choice pair. The choice with a highest score"
2020.findings-emnlp.369,P18-1076,0,0.0441303,"ly uninterpretable (Mitra et al., 2019). Alternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 1"
2020.findings-emnlp.369,P19-1459,0,0.0290101,"s, such as caves, and that a cave is a type of geological feature. Such commonsense knowledge is obvious for humans but most existing QA systems do not have it or cannot reason with it. Although recent advances in pre-trained language models (LMs) have resulted in impressive performance on commonsense-related benchmarks (Zellers et al., 2018; Bhagavatula et al., 2019; 1 AtLocation IsA The code is available at https://github.com/ wangpf3/Commonsense-Path-Generator. Huang et al., 2019), it is unclear whether this is due to commonsense reasoning or to capturing spurious correlations in the data (Niven and Kao, 2019). Pre-trained LMs may answer a question correctly for wrong reasons, making them highly uninterpretable (Mitra et al., 2019). Alternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher inter"
2020.findings-emnlp.369,N19-1368,0,0.066864,"its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge paths dynamically, even when these are absent in the KG. Dynamic Knowledge Path Generation. Several methods generate knowledge paths instead of extracting them from static KGs. Asai et al. (2019) learn reasoning paths by forming sequences of evidence documents, however, their approach relies on the inter-document hyperlinks to establish relations in"
2020.findings-emnlp.369,P16-1162,0,0.0222642,"orpora. We foresee two benefits of combining a pre-trained model such as GPT-2 and a static KG: (1) the language model would be able to generate commonsense knowledge paths, by being enriched with relevant structured knowledge; (2) the unstructured knowledge encoded in the language model would help to alleviate the sparsity challenge of the static KGs. Unlike COMET (Bosselut et al., 2019) which fine-tunes GPT (an earlier version of GPT-2) with independent triplets, we fine-tune GPT-2 with consecutive triplets that form paths (see Section 3.1). To do so, we first use GPT-2’s BytePair Encoding (Sennrich et al., 2016) to convert each symbolic path p to its textual form as a sequence {x0 , y0 , x1 , y1 , ..., yT −1 , xT }, where xt = ∣e ∣ 1 2 {xt , xt , ..., xt t } are phrase tokens of the entity et ∣r ∣ 1 2 and yt = {yt , yt , ..., yt t } are phrase tokens of the Table 1: Example Transformation of a Symbolic Path into Text. {predator, DistinctFrom, prey, IsA, animal} → { animal, [SEP], predator , distinct, from, prey, is, a, animal} relation rt . The reverse relations are represented by adding a special prefix token “ ”. The resulting paths mimic natural language sentences to facilitate optimal usage of th"
2020.findings-emnlp.369,Q18-1021,0,0.0329985,"by committing perjury. In Q3, the path from our Global generator is able to predict the relevant property of an entity and realizes that a 1-hop relation suffices in this case. Our Scratch variant, however, predicts a less precise relation ( HasContext). These cases show the path generalization ability of the fine-tuned pre-trained GPT-2, owed to its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge"
2020.findings-emnlp.369,D18-1259,0,0.0382914,"ontains incorrect information: peace is caused by committing perjury. In Q3, the path from our Global generator is able to predict the relevant property of an entity and realizes that a 1-hop relation suffices in this case. Our Scratch variant, however, predicts a less precise relation ( HasContext). These cases show the path generalization ability of the fine-tuned pre-trained GPT-2, owed to its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a s"
2020.findings-emnlp.369,D18-1009,0,\N,Missing
2020.findings-emnlp.369,P19-1263,0,\N,Missing
2020.nuse-1.2,P14-1035,0,0.0396758,"Missing"
2020.nuse-1.2,P08-1090,0,0.034808,"we devised domainspecific features to achieve our objective. (4) We tested that for long document classification, a simple feature-based approach can work better than state-of-the-art models. 2 Data and Problem Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screenplay that receives nominations by critical reviewers should have had higher chance of getting th"
2020.nuse-1.2,P10-1015,0,0.0394701,"tural knowledge of screenplay narratology, we developed a simple narratologyinspired model for our task. (3) Motivated by industry opinions and narratology, we devised domainspecific features to achieve our objective. (4) We tested that for long document classification, a simple feature-based approach can work better than state-of-the-art models. 2 Data and Problem Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assu"
2020.nuse-1.2,P13-1035,0,0.0695123,"Missing"
2020.nuse-1.2,N15-1011,0,0.0213426,"nematic success criteria lack evaluative consensus (Simonton, 2009) — previous works on evaluation of movies have largely focused on forecasting revenue or profit of movies using production, distribution, and advertising data (Ghiassi et al., 2015; Lash et al., 2015) or basic textual and human annotated features (Eliashberg et al., 2014). Challenges. By nature, a movie should be tough to be cleanly categorized, due to its length, complex storyline and turns, and the lack of evaluative criteria. Prior works in document classification (Yang et al., 2016; Liu et al., 2017; Adhikari et al., 2019; Johnson and Zhang, 2015) evaluated on datasets with small document size (Reuters, IMDB, Yelp, etc.). However, our document size on average is at least 65 times longer, which may be challenging for NN-based models to train due to long sequences and the associated computational burden. Besides, the number of training data we have is at most 1000 times smaller than other datasets. With our datasets being long, fewer and skewed, state-ofthe-art deep learning techniques may not work well. Summary of the comparisons is shown in Table 1. The main differences between our work and previous works are: (1) our approach aims to"
2020.nuse-1.2,P17-1153,1,0.90107,"Missing"
2020.nuse-1.2,W12-2502,0,0.217211,"(2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screenplay that receives nominations by critical reviewers should have had higher chance of getting through green-lighting. A noteworthy attempt in measuring quality of literary works we know of is made by Kao and Jurafsky (2012), who quantitatively analyze various indicators for discerning professional poems from amateurs’. However, in script writing, the cinematic success criteria lack evaluative consensus (Simonton, 2009) — previous works on evaluation of movies have largely focused on forecasting revenue or profit of movies using production, distribution, and advertising data (Ghiassi et al., 2015; Lash et al., 2015) or basic textual and human annotated features (Eliashberg et al., 2014). Challenges. By nature, a movie should be tough to be cleanly categorized, due to its length, complex storyline and turns, and t"
2020.nuse-1.2,P19-1353,0,0.0236572,"ures to achieve our objective. (4) We tested that for long document classification, a simple feature-based approach can work better than state-of-the-art models. 2 Data and Problem Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screenplay that receives nominations by critical reviewers should have had higher chance of getting through green-lighting"
2020.nuse-1.2,D19-1564,0,0.0309492,"em Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screenplay that receives nominations by critical reviewers should have had higher chance of getting through green-lighting. A noteworthy attempt in measuring quality of literary works we know of is made by Kao and Jurafsky (2012), who quantitatively analyze various indicators for discerning profession"
2020.nuse-1.2,D18-1464,0,0.0231771,"a simple feature-based approach can work better than state-of-the-art models. 2 Data and Problem Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screenplay that receives nominations by critical reviewers should have had higher chance of getting through green-lighting. A noteworthy attempt in measuring quality of literary works we know of is made by Kao an"
2020.nuse-1.2,P18-1017,0,0.0412051,"Missing"
2020.nuse-1.2,N16-1174,0,0.0455561,"nal poems from amateurs’. However, in script writing, the cinematic success criteria lack evaluative consensus (Simonton, 2009) — previous works on evaluation of movies have largely focused on forecasting revenue or profit of movies using production, distribution, and advertising data (Ghiassi et al., 2015; Lash et al., 2015) or basic textual and human annotated features (Eliashberg et al., 2014). Challenges. By nature, a movie should be tough to be cleanly categorized, due to its length, complex storyline and turns, and the lack of evaluative criteria. Prior works in document classification (Yang et al., 2016; Liu et al., 2017; Adhikari et al., 2019; Johnson and Zhang, 2015) evaluated on datasets with small document size (Reuters, IMDB, Yelp, etc.). However, our document size on average is at least 65 times longer, which may be challenging for NN-based models to train due to long sequences and the associated computational burden. Besides, the number of training data we have is at most 1000 times smaller than other datasets. With our datasets being long, fewer and skewed, state-ofthe-art deep learning techniques may not work well. Summary of the comparisons is shown in Table 1. The main differences"
2020.nuse-1.2,L18-1027,0,0.0642509,"Missing"
2020.nuse-1.2,D19-1180,0,0.0145331,"narratologyinspired model for our task. (3) Motivated by industry opinions and narratology, we devised domainspecific features to achieve our objective. (4) We tested that for long document classification, a simple feature-based approach can work better than state-of-the-art models. 2 Data and Problem Setting Related Works Literary works-related research has gained interest in recent years. Bamman et al. (2013, 2014) have succeeded to learn latent character types in film and novels; Iyyer et al. (2016); Chaturvedi et al. (2016); Elson et al. (2010) try to model character relations in novels. Papalampidi et al. (2019) analyze narrative structure of movies by using turning points, and Chambers and Jurafsky (2008); Sims et al. (2019) seek to detect events in narratives. On text quality assessment, Mesgar and Strube (2018) encode local change patterns to assess readability and score essays; Toledo et al. (2019) collect argument pairs that was originally built for an automatic quality assessor for debate. Problem Setup. Our work focuses on measuring quality as whether or not a movie would be nominated at a peer-reviewed venue. The basic assumption for using this approach as success metrics is simple — a screen"
2020.nuse-1.2,N16-1180,0,\N,Missing
2020.nuse-1.2,N18-1160,0,\N,Missing
2020.nuse-1.2,N19-1423,0,\N,Missing
2020.nuse-1.2,N19-1408,0,\N,Missing
2021.acl-long.102,2020.acl-main.747,0,0.53071,"at might prevent future methods from generalizing beyond English (Ponti et al., 2020). 1274 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1274–1287 August 1–6, 2021. ©2021 Association for Computational Linguistics It is of pressing urgency for the community to develop NLU systems that can serve all languages in the world to bridge the gap between different cultures and eliminate language barriers (Hu et al., 2020), and multilingual language models (MLLMs), such as XLM-R (Conneau et al., 2020), are among the most promising tools to achieve this ambitious goal. Although ML-LMs have been evaluated in a few NLU tasks, e.g., XNLI (Conneau et al., 2018) and XTEMRE (Hu et al., 2020), it is still relatively unclear how ML-LMs perform in commonsense reasoning tasks, due to the lack of 1) dedicated methods for probing common sense in ML-LMs and 2) multilingual benchmark datasets for commonsense reasoning. To analyze how much common sense MLLMs already have without any tuning, we propose M ICKEY P ROBE, a zero-shot probing task. It tasks a ML-LM to rank a set of contrastive assertions (i.e.,"
2021.acl-long.102,D17-1090,0,0.0201598,"Missing"
2021.acl-long.102,P18-1066,1,0.833079,"le for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. 3 https://github.com/commonsense/ conceptnet5/wiki/Downloads 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH 1282 Cultural Bias Reduction Like most most multilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural differences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like “what do people usually drink in the morning? (coffee/tea/milk)” or “when does a wedding usually start? (morning/afternoon/evening)” might be answered very differently by people from different backgrounds and cultures, not to mention different languages. The prior English commonsense resources which our datasets are built on are already possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either"
2021.acl-long.102,2020.emnlp-main.479,0,0.0321684,", wt+1 , . . . , w|s |. as the input to rank all tokens in the vocabulary with the probability P wt |s	 via zero-shot inference. One can evaluate the performance of recalling common sense by measuring the position of a correct token “wing” in the ranked list. That is, the LAMA probe method uses token-level probability as a proxy to probe for common sense in LMs via ranking all tokens in their vocabularies. This intuitive method, however, has several inherent limitations. First, in many other languages, multi-token concepts are ubiquitous, for example, “˛fÜ” (“library” in Simplified Chinese). Jiang et al. (2020) present several methods to decode multi-token entities so that they can adapt the LAMA probe to probe a LM for language-specific analysis. It is however infeasible to use tokenlevel probing tasks if we want to analyze ML-LMs across languages. In addition, the evaluation metric of the LAMA probe could be unfair, because there can be many correct words for a masked position (e.g., “birds have legs/eyes”). The ranking metrics of the LAMA probe, however, tend to ignore these facts, resulting in a less trustworthy analysis. The vocabulary-specific ranking is unfair when comparing across different"
2021.acl-long.102,P18-4020,0,0.0257579,"Missing"
2021.acl-long.102,2020.emnlp-main.500,0,0.025566,"ltilingual parallel dataset M for the probing task M ICKEY P ROBE. Our collected corpus, named MickeyCorpus , has 561k sentences in 11 languages (T =10.2k, K=5, |L|=11). 4.1 Creating English Probes For the correct commonsense assertions in English, we have an existing resource, the OMCS corpus (Singh et al., 2002) which contains humanwritten sentences in English that describe commonsense facts. Each assertion can be used as a Mten and we perform perturbations on it to create the other K 1 distractor assertions (i.e., false candidates), yielding an M en example. Inspired by BERT-attack method (Li et al., 2020), we use a simple method to generate false assertions that are semantically related and syntactically similar to the truth assertions. Given a correct assertion, we first randomly sample a few (1 ⇠ 3) words with a part-of-speech tag as noun, verb, or adjective, and replace them with [mask]. Then, we use a beam-search style method to decode the [mask] tokens one by one from left to right. To ensure that the distractors are less plauFigure 3: The M ICKEY P ROBE results in hit@1-acc. A larger version of this figure is in Appendix (Fig. 6). sible, we limit the decoding steps to only sample tokens"
2021.acl-long.102,2020.acl-main.240,0,0.142149,"s, e.g., XNLI (Conneau et al., 2018) and XTEMRE (Hu et al., 2020), it is still relatively unclear how ML-LMs perform in commonsense reasoning tasks, due to the lack of 1) dedicated methods for probing common sense in ML-LMs and 2) multilingual benchmark datasets for commonsense reasoning. To analyze how much common sense MLLMs already have without any tuning, we propose M ICKEY P ROBE, a zero-shot probing task. It tasks a ML-LM to rank a set of contrastive assertions (i.e., declarative sentences) in the same language by their commonsense plausibility, for which we use pseudo-likelihood (PLL) (Salazar et al., 2020) as a proxy. Unlike the LAMA probe, it can study multi-token concepts which are ubiquitous in some non-English languages. In addition, it fairly compares performance across different languages via a language-invariant evaluation protocol. Alongside the probing task, we also create MickeyCorpus, a large-scale multilingual dataset, consisting of 561k sentences in 11 different languages. Our experiments reveal that there are always large discrepancies across different languages in the tested ML-LMs, and different MLLMs show very different language preferences. Beyond supervision-free analysis of"
2021.acl-long.102,N19-1421,0,0.541246,"people communicate with each other using languages. It is thus of vital importance to evaluate and improve the commonsense reasoning capability of language models (LMs), towards building general natural language understanding (NLU) systems (Davis and Marcus, 2015). 1 We release our code and data at the project website: https://inklab.usc.edu/XCSR/. Many recent benchmark datasets and probing methods have been proposed to evaluate machine common sense. As shown in Figure 1, the LAMA probe (Petroni et al., 2019) is for analyzing LMs’ zero-shot commonsense recalling ability; CommonsenseQA (CSQA) (Talmor et al., 2019) is instead a multiple-choice QA task that needs fine-tuning; CODAH (Chen et al., 2019) and SWAG (Zellers et al., 2018) focus on the ability to complete the most plausible scenes. However, all these works have been limited only to English. Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al., 2019). Such English-centric trend of commonsense reasoning studies not only limits our research scope, but also tends to exacerbate English-specific bias that might prevent future me"
2021.acl-long.102,2016.eamt-2.8,0,0.035619,"@1-acc. A larger version of this figure is in Appendix (Fig. 6). sible, we limit the decoding steps to only sample tokens that ranks between 200th⇠300th. We repeat the above procedure multiple times with different sets of [mask] tokens. Then, we use Stanza (Qi et al., 2020) to remove distractors that have sequences of POS tags or morphological features different from the truth assertions. Finally, we sample K 1 of them as the distractors. 4.2 Scaling to Ten Other Languages. We use bidirectional translation with the MarianMT models (Junczys-Dowmunt et al., 2018) pretrained on the OPUS corpora (Tiedemann, 2016). We translate all English probes to the 25 languages that has models in both directions and then translate them back to English. As the outputs from these models might contain noise and errors, we compute the semantic similarities (i.e., cosine similarity) between the original M en and the backtranslated M x-en via the SentenceBERT (Reimers and Gurevych, 2019) model. To ensure the quality and fair comparisons, we set a similarity threshold as 0.75 and keep the intersections of probes in all languages. Considering some languages tend to have translations of lower quality, we finally choose the"
2021.acl-long.102,2021.naacl-main.41,0,0.0343044,"RT model (Devlin et al., 2019) trained on multilingual corpora without specific designs about multilinguality. The distil-mBERT (d-mBERT) (Sanh et al., 2019) is a smaller mBERT trained by knowledge distillation. Conneau and Lample (2019) proposed XLM(-100), which is pretrained with both masked language modeling (MLM) and translation language modeling (TLM). Conneau et al. (2020) further proposed XLM-R, which improves the XLM with a better sub-token vocabulary and highquality multilingual corpora (CC100). We leave the analysis of recent seq2seq ML-LMs, such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), as future work, because their architectures are significantly different from the other ML-LMs. Note that the above ML-LMs are pretrained only with token-level training objectives such as MLM (i.e., recovering masked tokens in monolingual text) and TLM (i.e., recovering masked tokens in a pair of parallel sentences in two different languages). However, most NLU tasks, including commonsense reasoning, highly rely on sentence-level representations. We argue that a well-designed sentence-level pre-training objective should improve ML-LMs for NLU tasks. This intuition motivates us to propose a se"
2021.acl-long.102,D18-1009,0,0.299875,"ense reasoning capability of language models (LMs), towards building general natural language understanding (NLU) systems (Davis and Marcus, 2015). 1 We release our code and data at the project website: https://inklab.usc.edu/XCSR/. Many recent benchmark datasets and probing methods have been proposed to evaluate machine common sense. As shown in Figure 1, the LAMA probe (Petroni et al., 2019) is for analyzing LMs’ zero-shot commonsense recalling ability; CommonsenseQA (CSQA) (Talmor et al., 2019) is instead a multiple-choice QA task that needs fine-tuning; CODAH (Chen et al., 2019) and SWAG (Zellers et al., 2018) focus on the ability to complete the most plausible scenes. However, all these works have been limited only to English. Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al., 2019). Such English-centric trend of commonsense reasoning studies not only limits our research scope, but also tends to exacerbate English-specific bias that might prevent future methods from generalizing beyond English (Ponti et al., 2020). 1274 Proceedings of the 59th Annual Meeting of the Associa"
2021.acl-long.357,P17-1171,0,0.0228832,"oder along with Q and C which then produce the [CLS] embedding for making a prediction. We name this method “MDS.” Integrated Approach. To take the best of both worlds in (Q, C) and (Q, C, A) settings, we integrate two architectures (the leftmost and middle ones in Fig. 5). We concatenate the last two hidden representations of each architecture before passing the concatenated representation through a shared MLP layer. We use BERTLARGE as f in both architectures, AGG (GRU) for g and call this model “BERTLARGE ++ (integrated)” in Table 3. Other Baselines. We also consider other baselines: ESIM (Chen et al., 2017b), B I DAF++ (Clark and Gardner, 2018), prepending extracted open event triples (Liu et al., 2019a) to BERT input, and a script learning approach, SAM-Net (Lv et al., 2019). We modify the approaches to fit into our setup. Detailed descriptions of each baseline method are included in Sec. E.3 of appendix. 6 Experiments 6.1 Experimental Setup We adopt two types of settings: the closed-book setting (Q, C) and the constrained open-domain setting (Q, C, A). In the constrained open-domain setting, we use BM25 (Robertson et al., 1995; Qi et al., 2019) as our IR method8 to obtain A, 10 retrieved arti"
2021.acl-long.357,P19-1433,0,0.0226316,"le or evidence sentence. We thus convert F ORECAST QA to a reading comprehension task and examine the answerability of the questions. stronger retrieval methods are required to identify useful evidence; 2) complex forecasting abilities may be a bottleneck of current systems. Ablation on Timestamp Modeling. We conduct an ablation study on modeling time information (publication date) of the retrieved articles, as seen in Table 5. We test: a) pre-pending date string as BERT input, b) using binary encodings of dates9 and concatenate with article encoding before aggregation, and c) using char-RNN (Goyal and Durrett, 2019) for encoding date string before aggregation10 . We find that using binary encodings of dates improves the accuracy for the maxpool aggregator. However, the GRU aggregator’s accuracy decreases when given date information. We conjecture that our modeling for the time information of each article is not strong enough to help forecasting. We leave more sophisticated modeling for future work. Answerability of Questions. To validate that the questions in F ORECAST QA are indeed answerable, we convert our setup into a machine reading comprehension (MRC) task — find an answer given an assumed appropri"
2021.acl-long.357,2020.findings-emnlp.171,0,0.0167889,"uestion. (1) Temporal Aggregation: This aggregator utilizes temporal ordering of the retrieved articles. Articles are sorted by their timestamps and their [CLS] token representation from f are aggregated by a Gated Recurrent Unit (GRU) (Cho et al., 2014) with a MLP head to make final predictions. (2) Set Aggregation: Alternatively, we ignore the temporal ordering of articles and use a maxpooling operation 7 We did not include more recent pre-trained language models (e.g., RoBERTa (Liu et al., 2019b), ALBERT (Lan et al., 2020), T5 (Raffel et al., 2020)) or pre-trained QA models like UnifiedQA (Khashabi et al., 2020), as these models are trained using text data published after the earliest timestamp in our dataset (2019-01-01), meaning information leakage could occur (and violates the forecasting setup). We tested more LMs in Sec. E.5 of appendix. on the [CLS] token representations of each article. This pooled representation is passed to an MLP layer to make a prediction. Comparison between these aggregations helps understand the effect of modeling temporal order of evidence. These two aggregation modules are denoted by “AGG (GRU)” and “AGG (Maxpool),” respectively. Multi-document Summarization (MDS). Rat"
2021.acl-long.357,Q19-1026,0,0.143871,"and translate the questions and answer choices into their format, which limits the expressiveness of natural text. However, unlike these datasets and approaches, F ORECAST QA does not provide any structured data to a model. The model must learn how to extract, keep track of, and link pertinent events from unstructured text to solve forecasting questions. QA and Temporal Reasoning on Text. There are several approaches for QA using unstructured text. Extractive QA approaches rely on finding answer spans from the text that best answer a question (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Kwiatkowski et al., 2019; Huang et al., 2019). 4637 Who will be German chancellor by November 2019? Will the James Bond actor arrive Italy in September 2019? Will the Public charge rule impact US taxpayers by August 2019? Will the Mona Lisa be missing in the Louvre by October 2019? Will the Wright family blame Boris Johnson for its failure in September 2019? Will the Duke of Sussex refuse to tour Africa in September 2019? Will there Will there be electricity in Canada despite hurricane Dorian in September 2019? What will be the budget of Terminator Dark Fate in October 2019? Who will be wanted to execute by Saudi pro"
2021.acl-long.357,P19-1276,0,0.107655,". Two architectures are used when aggregating information from multiple, time-stamped articles A retrieved for a question. (1) Temporal Aggregation: This aggregator utilizes temporal ordering of the retrieved articles. Articles are sorted by their timestamps and their [CLS] token representation from f are aggregated by a Gated Recurrent Unit (GRU) (Cho et al., 2014) with a MLP head to make final predictions. (2) Set Aggregation: Alternatively, we ignore the temporal ordering of articles and use a maxpooling operation 7 We did not include more recent pre-trained language models (e.g., RoBERTa (Liu et al., 2019b), ALBERT (Lan et al., 2020), T5 (Raffel et al., 2020)) or pre-trained QA models like UnifiedQA (Khashabi et al., 2020), as these models are trained using text data published after the earliest timestamp in our dataset (2019-01-01), meaning information leakage could occur (and violates the forecasting setup). We tested more LMs in Sec. E.5 of appendix. on the [CLS] token representations of each article. This pooled representation is passed to an MLP layer to make a prediction. Comparison between these aggregations helps understand the effect of modeling temporal order of evidence. These two a"
2021.acl-long.357,2021.ccl-1.108,0,0.0604135,"Missing"
2021.acl-long.357,2020.emnlp-main.88,0,0.0217448,"articles before “2019-09-01.” With the addition of this timestamp constraint, our query becomes a question about a future event in “September, 2019” based on articles from the “past”; the model is now being tested for its forecasting ability2 . To answer the question, the model must find pertinent events from “past” information, resolve the temporal and causal relations between them, and finally make a forecasting judgement based on its interpretation of past information to answer the question. Our task differs from that of other works that require an understanding of temporal relationships (Ning et al., 2020) and temporal commonsense reasoning (Zhou et al., 2019), as our task forces a model to make a forecasting judgement. In support of the proposed F ORECAST QA formulation, we construct a dataset of 10,392 yes-no and multiple-choice questions. This data is collected via crowdsourcing based on news articles, where workers are shown articles and asked to come up with yes-no and multiple-choice questions. We also crowdsourced appropriate timestamps for each question. Finally, we design a method based on pre-trained language models to deal with retrieved articles for our task. In our experiments, the"
2021.acl-long.357,N18-1202,0,0.0420581,"Missing"
2021.acl-long.357,D19-1261,0,0.0375194,"Missing"
2021.acl-long.362,2020.coling-main.449,0,0.0400285,"extraction setup in our model as most of previous contributions, using sequence labeling architecture. But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding. Dynamic Parameter Generation. Our model proposes an adaptive-based decoding setup, parameterized with attribute embeddings through a Mixture-of-Experts module and a hypernetwork. Jacobs et al. (1991) first propose a system composed of several different “expert” networks and use a gating network that decides how to assign different training instances to different “experts”. Alshaikh et al. (2020); Guo et al. (2018); Le et al. (2016); Peng et al. (2019) all use domain/knowledge experts, and combine the predictions of each expert with a gating network. Unlike these works, we combine the weights of each expert to parameterize a network layer given an input embedding. Ha et al. (2017) propose the general idea of generating the parameters of a network by another network. The proposed model in Cai et al. (2019) generates the parameters of an encoderdecoder architecture by referring to the contextaware and topic-aware input. Suarez (2017) uses a hypernetwork to scale the weights of the main"
2021.acl-long.362,D19-1188,0,0.0146423,") first propose a system composed of several different “expert” networks and use a gating network that decides how to assign different training instances to different “experts”. Alshaikh et al. (2020); Guo et al. (2018); Le et al. (2016); Peng et al. (2019) all use domain/knowledge experts, and combine the predictions of each expert with a gating network. Unlike these works, we combine the weights of each expert to parameterize a network layer given an input embedding. Ha et al. (2017) propose the general idea of generating the parameters of a network by another network. The proposed model in Cai et al. (2019) generates the parameters of an encoderdecoder architecture by referring to the contextaware and topic-aware input. Suarez (2017) uses a hypernetwork to scale the weights of the main recurrent network. Platanios et al. (2018) tackle neural machine translation between multiple languages using a universal model with a contextual parameter generator. 7 Conclusion In this work we propose a multi-attribute value extraction model that performs joint modeling of many attributes using an adaptive CRF-based decoder. Our model has a high capacity to derive attribute-specific network parameters while fac"
2021.acl-long.362,N19-1423,0,0.283402,"cream scent Y B O B E O B I E O Table 1: An example of the tag sequence for attribute “Scent” annotated with the BIOE scheme. 2.2 BiLSTM-CRF Architecture The BiLSTM-CRF architecture (Huang et al., 2015) consists of a BiLSTM-based text encoder, and a CRF-based decoder. This architecture has been proven to be effective for the attribute value extraction task (Zheng et al., 2018; Xu et al., 2019; Karamanolakis et al., 2020). We build our AdaTag model based on the BiLSTM-CRF architecture as we find that the BiLSTM-CRF-based models generally perform better than their BiLSTM-based, 4695 BERT-based (Devlin et al., 2019) and BERT-CRFbased counterparts, as shown in §5. We introduce the general attribute-agnostic BiLSTM-CRF architecture, which our model is based on, in this subsection. Given a text sequence X = [x1 , . . . , xn ]. We obtain the sequence of word embeddings X = [x1 , . . . , xn ] using an embedding matrix Wword . We get the hidden representation of each word by feeding X into a bi-directional Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer with hidden size dh : [h1 , . . . , hn ] = BiLSTM([x1 , . . . , xn ]). (1) We use a CRF-based decoder to decode the sequence of hidden r"
2021.acl-long.362,D18-1498,0,0.0641358,"Missing"
2021.acl-long.362,2020.acl-main.751,1,0.921978,"of natural texts describing products and their main features. Key product features are conveyed in unstructured texts, with limited impact on machine-actionable applications, like search (Ai et al., 2017), recommendation (Kula, 2015), and question answering (Kulkarni et al., 2019), among others. Automatic attribute value extraction aims to obtain structured product features from product profiles. The input is a textual ∗ Most of the work was done during an internship at Amazon. Most existing studies on attribute value extraction use neural sequence labeling architectures (Zheng et al., 2018; Karamanolakis et al., 2020; Xu et al., 2019). To handle multiple attributes, one line of previous contributions develops a set of “attribute-specific” models (i.e., one model per attribute). The goal is to construct neural networks with (partially) separate model parameters for different attributes. For example, one can construct an independent sequence labeling model for each attribute and make predictions with all the models collectively (e.g., the vanilla OpenTag model (Zheng et al., 2018)). Instead of totally separate models, one can also use different tag sets corresponding to different attributes. These networks"
2021.acl-long.362,W16-1611,0,0.0261804,"revious contributions, using sequence labeling architecture. But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding. Dynamic Parameter Generation. Our model proposes an adaptive-based decoding setup, parameterized with attribute embeddings through a Mixture-of-Experts module and a hypernetwork. Jacobs et al. (1991) first propose a system composed of several different “expert” networks and use a gating network that decides how to assign different training instances to different “experts”. Alshaikh et al. (2020); Guo et al. (2018); Le et al. (2016); Peng et al. (2019) all use domain/knowledge experts, and combine the predictions of each expert with a gating network. Unlike these works, we combine the weights of each expert to parameterize a network layer given an input embedding. Ha et al. (2017) propose the general idea of generating the parameters of a network by another network. The proposed model in Cai et al. (2019) generates the parameters of an encoderdecoder architecture by referring to the contextaware and topic-aware input. Suarez (2017) uses a hypernetwork to scale the weights of the main recurrent network. Platanios et al. ("
2021.acl-long.362,N19-1263,0,0.0202079,"ons, using sequence labeling architecture. But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding. Dynamic Parameter Generation. Our model proposes an adaptive-based decoding setup, parameterized with attribute embeddings through a Mixture-of-Experts module and a hypernetwork. Jacobs et al. (1991) first propose a system composed of several different “expert” networks and use a gating network that decides how to assign different training instances to different “experts”. Alshaikh et al. (2020); Guo et al. (2018); Le et al. (2016); Peng et al. (2019) all use domain/knowledge experts, and combine the predictions of each expert with a gating network. Unlike these works, we combine the weights of each expert to parameterize a network layer given an input embedding. Ha et al. (2017) propose the general idea of generating the parameters of a network by another network. The proposed model in Cai et al. (2019) generates the parameters of an encoderdecoder architecture by referring to the contextaware and topic-aware input. Suarez (2017) uses a hypernetwork to scale the weights of the main recurrent network. Platanios et al. (2018) tackle neural"
2021.acl-long.362,D14-1162,0,0.0868406,". Taking the context into consideration helps get embeddings that can more accurately represent the semantics of the word. Here we use the contextualized representations provided by BERT (Devlin et al., 2019) to generate the embedding. We use BERT to encode Xi and get vi ’s phrase embedding (the averaged embedding of each word in the phrase) as rvalue . By i replacing vi with “[BOA] r˜ [EOA]”1 and encoding the modified sequence with BERT, we get the phrase embedding for “[BOA] r˜ [EOA]” as rname . i Uncontextualized Embeddings. Static embeddings like Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) can be more stable to use under noisy contexts. We use Glove (50d) to get the phrase embedding for vi as rvalue and the i name phrase embedding for r˜ as ri . 3.4 Model Training As we parameterize the CRF-based decoder with the attribute embedding through MoE and hypernetwork, the learnable parameters in our model includes θencoder = {Wword , θbi-lstm }, w b b θhyper = {Whyper , bw hyper , Whyper , bhyper }, θmoe = {Wmoe , bmoe , {T(i) }ki=1 }. We freeze the attribute embeddings Watt as it gives better performance, which is also discussed in §5.3. The whole model is trained end-to-end by maxi"
2021.acl-long.362,D18-1039,0,0.0287826,"; Le et al. (2016); Peng et al. (2019) all use domain/knowledge experts, and combine the predictions of each expert with a gating network. Unlike these works, we combine the weights of each expert to parameterize a network layer given an input embedding. Ha et al. (2017) propose the general idea of generating the parameters of a network by another network. The proposed model in Cai et al. (2019) generates the parameters of an encoderdecoder architecture by referring to the contextaware and topic-aware input. Suarez (2017) uses a hypernetwork to scale the weights of the main recurrent network. Platanios et al. (2018) tackle neural machine translation between multiple languages using a universal model with a contextual parameter generator. 7 Conclusion In this work we propose a multi-attribute value extraction model that performs joint modeling of many attributes using an adaptive CRF-based decoder. Our model has a high capacity to derive attribute-specific network parameters while facilitating knowledge sharing. Incorporated with pretrained attribute embeddings, our model shows marked improvements over previous methods. Acknowledgments This work has been supported in part by NSF SMA 18-29268. We would lik"
2021.acl-long.362,P19-1514,0,0.300099,"products and their main features. Key product features are conveyed in unstructured texts, with limited impact on machine-actionable applications, like search (Ai et al., 2017), recommendation (Kula, 2015), and question answering (Kulkarni et al., 2019), among others. Automatic attribute value extraction aims to obtain structured product features from product profiles. The input is a textual ∗ Most of the work was done during an internship at Amazon. Most existing studies on attribute value extraction use neural sequence labeling architectures (Zheng et al., 2018; Karamanolakis et al., 2020; Xu et al., 2019). To handle multiple attributes, one line of previous contributions develops a set of “attribute-specific” models (i.e., one model per attribute). The goal is to construct neural networks with (partially) separate model parameters for different attributes. For example, one can construct an independent sequence labeling model for each attribute and make predictions with all the models collectively (e.g., the vanilla OpenTag model (Zheng et al., 2018)). Instead of totally separate models, one can also use different tag sets corresponding to different attributes. These networks can also share the"
2021.acl-long.362,2020.emnlp-main.166,0,0.0263792,"rld scenarios. 6 Related Work Attribute Value Extraction. OpenTag (Zheng et al., 2018) formulates attribute value extraction as a sequence tagging task, and proposes a BiLSTMSelfAttention-CRF architecture to address the problem. Xu et al. (2019) propose an “attribute-aware” setup, by utilizing one set of BIO tags and attribute name embedding with an attention mechanism, to enforce the extraction network to be attribute comprehensive. Karamanolakis et al. (2020) additionally incorporate the product taxonomy into a multitask learning setup, to capture the nuances across different product types. Zhu et al. (2020) introduce a multi-modal network to combine text and visual information with a cross-modality attention to leverage image rich information that is not conveyed in text. Wang et al. (2020) use a question answering formulation to tackle attribute value extraction. We adopt the extraction setup in our model as most of previous contributions, using sequence labeling architecture. But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding. Dynamic Parameter Generation. Our model proposes an adaptive-based decoding setup, parameterized with"
2021.acl-short.82,2020.acl-main.421,0,0.0326287,"al., 2021) can be considered as a sub-category in zeroshot learning, with the goal of generalizing to unseen tasks during inference. Adapters for Transformers. Houlsby et al. (2019) proposed adapter layers for parameterefficient transfer learning in NLP. Adapter layers, which adopt a bottleneck architecture with two linear layers, are added after each multi-headed attention layer and each feed-foward layer in a pretrained transformer. Adapters have been recently applied to multi-lingual settings, with successes in NER, QA and commonsense reasoning (Pfeiffer et al., 2020; Philip et al., 2020; Artetxe et al., 2020). 7 Conclusion In this paper, we introduced H YPTER, a framework to improve text-to-text transformer’s generalization ability to unseen tasks. H YPTER enhances taskspecific abilities by inserting adapters generated with a hypernetwork, meanwhile it maintains the model’s general task-solving ability by freezing main model parameters. We demonstrated the effectiveness of H YPTER on two datasets. Future work may explore teaching models with compositional instructions using H YPTER, or propose robust fine-tuning methods that help the model generalize to unseen data. It is also necessary to constru"
2021.acl-short.82,P19-1470,0,0.023667,"arning from (source, target) examples, in this paper we study the problem of learning from task descriptions (Weller et al., 2020). The train set contains M tasks, and the i-th task contains Ni examples of (s, t) pairs in text format. During test time, the learned model is required to directly make inferences on a new task given a task description. Introduction Pre-trained text-to-text models (Raffel et al., 2020; Lewis et al., 2020) provide a unified formulation and off-the-shelf weights for a variety of NLP tasks, such as question answering (Khashabi et al., 2020) and commonsense reasoning (Bosselut et al., 2019). In addition to their strong performance, text-totext models naturally support generalizing to novel tasks, by incorporating task description as part of the source sequence and fine-tuning the model with (source, target) examples (Weller et al., 2020). At inference time, the model is required to perform 1 Code and data can be found at https://github.com/ INK-USC/hypter. unseen tasks with the source sequence containing new task descriptions. While this initial attempt shows positive results, there are two potential limitations for the direct finetuning approach. (1) Predictions can be sensitiv"
2021.acl-short.82,D19-5801,0,0.0332509,"Missing"
2021.acl-short.82,2020.tacl-1.28,0,0.0314865,"port generalizing to novel tasks, by incorporating task description as part of the source sequence and fine-tuning the model with (source, target) examples (Weller et al., 2020). At inference time, the model is required to perform 1 Code and data can be found at https://github.com/ INK-USC/hypter. unseen tasks with the source sequence containing new task descriptions. While this initial attempt shows positive results, there are two potential limitations for the direct finetuning approach. (1) Predictions can be sensitive to the task descriptions (or “prompts”) that are heuristically designed (Jiang et al., 2020). Paraphrasing the task description may lead to performance downgrade. (2) The model still learns from individual (source, target) examples, instead of learning to solve tasks at a higher level, by explicitly taking multiple examples within a task as a whole (see Fig. 1). Meanwhile, applying existing zero-shot learning methods that supports task-level learning to text-to-text transformers is non-trivial. Methods designed specifically for classification problems, such as prototypical networks (Snell et al., 2017), cannot be directly applied to text-to-text models. Moreover, given the large size"
2021.acl-short.82,2020.acl-main.625,0,0.0333752,"earns from individual (source, target) examples, instead of learning to solve tasks at a higher level, by explicitly taking multiple examples within a task as a whole (see Fig. 1). Meanwhile, applying existing zero-shot learning methods that supports task-level learning to text-to-text transformers is non-trivial. Methods designed specifically for classification problems, such as prototypical networks (Snell et al., 2017), cannot be directly applied to text-to-text models. Moreover, given the large size of text-to-text models, generating parameters for a whole model from the task description (Jin et al., 2020) is infeasible. In this work, we follow the settings in (Weller et al., 2020) and aim to improve a model’s generalization ability to unseen tasks by better incorporating task descriptions and using a task-level training procedure. We introduce H YPTER, a frame646 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 646–653 August 1–6, 2021. ©2021 Association for Computational Linguistics Hypernetwork (part of) Adapter i + Layer Norm Transformer + Layer i Generated"
2021.acl-short.82,2020.findings-emnlp.171,0,0.0110196,", (Rajpurkar et al., 2016) Figure 1: Instead of learning from (source, target) examples, in this paper we study the problem of learning from task descriptions (Weller et al., 2020). The train set contains M tasks, and the i-th task contains Ni examples of (s, t) pairs in text format. During test time, the learned model is required to directly make inferences on a new task given a task description. Introduction Pre-trained text-to-text models (Raffel et al., 2020; Lewis et al., 2020) provide a unified formulation and off-the-shelf weights for a variety of NLP tasks, such as question answering (Khashabi et al., 2020) and commonsense reasoning (Bosselut et al., 2019). In addition to their strong performance, text-totext models naturally support generalizing to novel tasks, by incorporating task description as part of the source sequence and fine-tuning the model with (source, target) examples (Weller et al., 2020). At inference time, the model is required to perform 1 Code and data can be found at https://github.com/ INK-USC/hypter. unseen tasks with the source sequence containing new task descriptions. While this initial attempt shows positive results, there are two potential limitations for the direct fi"
2021.acl-short.82,2020.acl-main.519,0,0.0245919,"ot with fewer examples. This suggests sufficient number of examples per task is necessary for H YPTER to generate effective adapters. 6 Related Work Zero-shot Learning with Transformers. Zeroshot learning (ZSL) has been explored for various NLP tasks, including text classification (Yin et al., 2019), entity linking (Logeswaran et al., 2019) and entity typing (Obeidat et al., 2019). Several works study cross-task transfer by unifying the inputoutput format, e.g., relation extraction as machine reading comprehension (Levy et al., 2017), named entity recognition as machine reading comprehension (Li et al., 2020). Such formulation allows generalization to unseen relation or named entity types at test time. Learning from task descriptions (Weller et al., 2020) and instructions (Mishra et al., 2021) can be considered as a sub-category in zeroshot learning, with the goal of generalizing to unseen tasks during inference. Adapters for Transformers. Houlsby et al. (2019) proposed adapter layers for parameterefficient transfer learning in NLP. Adapter layers, which adopt a bottleneck architecture with two linear layers, are added after each multi-headed attention layer and each feed-foward layer in a pretrai"
2021.acl-short.82,2021.ccl-1.108,0,0.0671096,"Missing"
2021.acl-short.82,P19-1335,0,0.0489377,"Missing"
2021.acl-short.82,N19-1087,0,0.0554608,"Missing"
2021.acl-short.82,Q19-1026,0,0.0275744,"Missing"
2021.acl-short.82,K17-1034,0,0.0558016,"Missing"
2021.acl-short.82,2020.emnlp-main.617,0,0.046106,"Missing"
2021.acl-short.82,2020.acl-main.703,0,0.215272,"inter months N N M (a) Zero-shot Learning from Task Description, ZEST dataset (Weller et al., 2020) M (b) Synthetic Version of SQuAD, (Rajpurkar et al., 2016) Figure 1: Instead of learning from (source, target) examples, in this paper we study the problem of learning from task descriptions (Weller et al., 2020). The train set contains M tasks, and the i-th task contains Ni examples of (s, t) pairs in text format. During test time, the learned model is required to directly make inferences on a new task given a task description. Introduction Pre-trained text-to-text models (Raffel et al., 2020; Lewis et al., 2020) provide a unified formulation and off-the-shelf weights for a variety of NLP tasks, such as question answering (Khashabi et al., 2020) and commonsense reasoning (Bosselut et al., 2019). In addition to their strong performance, text-totext models naturally support generalizing to novel tasks, by incorporating task description as part of the source sequence and fine-tuning the model with (source, target) examples (Weller et al., 2020). At inference time, the model is required to perform 1 Code and data can be found at https://github.com/ INK-USC/hypter. unseen tasks with the source sequence con"
2021.acl-short.82,D16-1264,0,0.117883,"Missing"
2021.acl-short.82,W17-2623,0,0.0512059,"Missing"
2021.acl-short.82,2020.emnlp-main.180,0,0.0420921,"Missing"
2021.acl-short.82,2020.emnlp-main.105,0,0.452848,"lve tasks at a higher level, by explicitly taking multiple examples within a task as a whole (see Fig. 1). Meanwhile, applying existing zero-shot learning methods that supports task-level learning to text-to-text transformers is non-trivial. Methods designed specifically for classification problems, such as prototypical networks (Snell et al., 2017), cannot be directly applied to text-to-text models. Moreover, given the large size of text-to-text models, generating parameters for a whole model from the task description (Jin et al., 2020) is infeasible. In this work, we follow the settings in (Weller et al., 2020) and aim to improve a model’s generalization ability to unseen tasks by better incorporating task descriptions and using a task-level training procedure. We introduce H YPTER, a frame646 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 646–653 August 1–6, 2021. ©2021 Association for Computational Linguistics Hypernetwork (part of) Adapter i + Layer Norm Transformer + Layer i Generated Param. ? i Linear (Wi,2, bi,2) Up-projection (Wiu, biu) Data Trainable Param"
2021.acl-short.82,D19-1404,0,0.0355291,"Missing"
2021.econlp-1.8,H05-1044,0,0.100161,"policy-related and the reference words. While we can calculate the cosine similarity between the word embedding vectors of the words of interest as (Hamilton et al., 2016), these similarities only describe the degree of semantic changes but not “how” the semantics of policy words change. Instead, we capture the semantic changes of the policy words by calculating how the semantic of policy words are close to the reference words as (Garg et al., 2018). For the reference words, we use the set of neutral words and the set of positive and negative words from the Subjectivity Lexicon (Wilson, 2005; Wilson et al., 2005). The Subjectivity Lexicon is the annotated polarity dictionary of subjective words: words with explicit polarity (positive and negative), and neutral words 1 . We use these annotated words to calculate the relative distance difference between the semantic of policy words and explicit semantic (positive and negative) words or neutral semantic (neutral) words. We also try another setting of the reference words calculating the relative distance difference between the semantic of policy words and positive semantic words or negative semantic words. The role of these reference words in this paper i"
2021.econlp-1.8,P16-1141,0,0.199319,"tion when they change their 2 2.1 Methods and Data Word embedding To extract the policy explanation from the policy documents, we use a dynamic embedding model, focusing on the transition of the semantic of the important words in the policy. The dynamic embedding calculates the semantic differences of the words in the documents over time. Hence, it allows us to study the differences in the semantics of specific policy-related words between two consecutive periods. Word embeddings highlight the semantic movements hidden in corpora, such as gender biases (Garg et al., 2018) or law-of semantics (Hamilton et al., 2016). Comparing the embedding vectors from the corpora over time can yield fallacious analysis because low-dimension embed56 Proceedings of the Third Workshop on Economics and Natural Language Processing, pages 56–61 Punta Cana and online, November 11. ©2021 Association for Computational Linguistics ding vectors can yield arbitrary orthogonal transformations (Hamilton et al., 2016). To avoid this pitfall, we use the state-of-the-art dynamic wordembedding method that efficiently yields temporal word embeddings (Di Carlo et al., 2019). We construct word-embedding vectors for each time, wt (30 dimens"
2021.emnlp-main.132,2020.coling-main.381,0,0.0212,"the discovery of §4.2 can be taken Figure 4: Performance on the original language pair advantage of is mitigating catastrophic forgetting. after transfer. The original Fr–En parent model scores Catastrophic forgetting refers to the loss of pre- 35.0 BLEU on the Fr–En test set. {src,tgt}+xattn outperforms {src,tgt}+body on the parent task. viously acquired knowledge in the model during transfer to a new task. To the best of our knowledge, catastrophic forgetting in MT models has only been studied within the context of inter-domain adapta- 5.2 Zero-Shot Translation tion (Thompson et al., 2019; Gu and Feng, 2020), Another area where well-aligned embeddings from and not inter-lingual adaptation. the {src,tgt}+xattn setting can come in handy The effectiveness of the cross-lingual embed- is zero-shot translation. Since the source embed1760 dings are aligned, we, for instance, can replace the French embeddings in the Fr–Es model learned via tgt+xattn with German embeddings from the De–En model learned via src+xattn and form a De–Es translation model with no De–Es training or direct De–Fr alignment. We additionally build two more zero-shot systems in the same manner: Ro–Es (using transferred Ro–En and Fr–E"
2021.emnlp-main.132,2021.acl-long.381,1,0.730015,"{src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We leave a comparison of the relative advantages and disadvantages of these approaches to future work. Module Freezing. In terms of restrictions introduced, our work is related to a group of recent works that freeze certain"
2021.emnlp-main.132,W18-6325,0,0.0165642,"ta and model that we use to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es}, respectively, all param"
2021.emnlp-main.132,D18-2012,0,0.0162025,"target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pairs based on methods described in §3.1: scratch, {src,tgt}, {src,"
2021.emnlp-main.132,2021.emnlp-main.243,0,0.0723281,"Missing"
2021.emnlp-main.132,2020.acl-main.703,0,0.0293962,"vitskiy et al., 2021). In particular, transfer learn- three questions: 1) How powerful is cross-attention ing from large pretrained Transformer-based lan- alone in terms of adapting to the new language pair guage models has been widely adopted to train new while other modules are frozen? 2) How crucial are the cross-attention layers pretrained values with models: adapting models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) regard to successful adaptation to the new task? and 3) Are there any qualitative differences in the for encoder-only tasks and models such as BART (Lewis et al., 2020) and mBART (Liu et al., 2020) learned representations when cross-attention is the only module that gets updated? for encoder-decoder tasks like machine translation To answer these questions, we compare mul(MT). This transfer learning is predominantly pertiple strategies of fine-tuning towards a new lanformed in the form of fine-tuning: using the values of several hundred million parameters from the pre- guage pair from a pretrained translation model that shares one language with the new pair. These are trained model to initialize a model and start training depicted in Figure 1: a) Ignoring the"
2021.emnlp-main.132,2021.acl-long.353,0,0.0250061,"edge by contrasting {src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We leave a comparison of the relative advantages and disadvantages of these approaches to future work. Module Freezing. In terms of restrictions introduced, our work is related to a group of recen"
2021.emnlp-main.132,2020.tacl-1.47,0,0.254967,"icular, transfer learn- three questions: 1) How powerful is cross-attention ing from large pretrained Transformer-based lan- alone in terms of adapting to the new language pair guage models has been widely adopted to train new while other modules are frozen? 2) How crucial are the cross-attention layers pretrained values with models: adapting models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) regard to successful adaptation to the new task? and 3) Are there any qualitative differences in the for encoder-only tasks and models such as BART (Lewis et al., 2020) and mBART (Liu et al., 2020) learned representations when cross-attention is the only module that gets updated? for encoder-decoder tasks like machine translation To answer these questions, we compare mul(MT). This transfer learning is predominantly pertiple strategies of fine-tuning towards a new lanformed in the form of fine-tuning: using the values of several hundred million parameters from the pre- guage pair from a pretrained translation model that shares one language with the new pair. These are trained model to initialize a model and start training depicted in Figure 1: a) Ignoring the pretrained from there. 1 par"
2021.emnlp-main.132,R19-1136,0,0.0471395,"Missing"
2021.emnlp-main.132,D18-1103,0,0.0138335,"e to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es}, respectively, all parameters are reused exce"
2021.emnlp-main.132,I17-2050,0,0.0174523,"ur experiments and the data and model that we use to materialize the analysis outlined in §2.2. 3.1 Methods We first provide the details of our transfer setup, and then describe the specific fine-tuning baselines and variants used in our experiments. General Setup. An important concern when transferring is initializing the embeddings of the new language. When initializing parameters in the child model, there are several ways to address the vocabulary mismatch between the parent and the child model: frequency-based assignment, random assignment (Zoph et al., 2016), joint (shared) vocabularies (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Neubig and Hu, 2018; Gheini and May, 2019; Liu et al., 2020), and no assignment at all, which results in training randomly initialized embeddings (Aji et al., 2020). In our experiments, we choose to always use new random initialization for the new embeddings (including token embeddings, positional embeddings, and corresponding layer norm parameters). This decision is made to later let us study what happens to embeddings under each of the settings, independent of any pretraining artifacts that exist in them. For instance, when transferring from Fr–En to {Ro–En, Fr–Es},"
2021.emnlp-main.132,N19-4009,0,0.0179706,"additionally include Ha–En, Fr–Es, and Fr–De. We designate Fr–En as the parent language pair and Ro–En, Ja–En, De–En, Ha–En (new source), Fr–Es, Fr–De (new target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Anal"
2021.emnlp-main.132,2020.emnlp-main.361,0,0.0303184,"ime of fine-tuning, they are not able to reveal anything about the importance of pretrained modules. Our approach, however, enables highlighting the crucial role of the encoded translation knowledge by contrasting {src,tgt}+xattn and {src,tgt}+randxattn. Bapna and Firat (2019) devise adapters for MT by inserting language pairspecific adapter parameters in the Transformer architecture. In the multilingual setting, they show that by fine-tuning adapters in a shared pretrained multilingual model, they can compensate for the performance drop of high-resource languages incurred by shared training. Philip et al. (2020) replace language pair-specific adapters with monolingual adapters, which enables adapting under the zero-shot setting. Another family of lightweight fine-tuning approaches (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021), inspired by prompt tuning (Brown et al., 2020), also relies on updating a set of additional new parameters from scratch towards each downstream task. Such sets of parameters equal a very small fraction of the total parameters in the pretrained model. By contrast, our approach updates a subset of the model’s own parameters instead of adding new ones. We l"
2021.emnlp-main.132,W18-6319,0,0.0185147,"We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pairs based on methods described in §3.1: scratch, {src,tgt}, {src,tgt}+body, {src,tgt}+xattn, and {src,tgt}+randxattn. Our core results, which rely on transferring from the Fr–En parent under each setting, are reported in Table 2. All scores are detokenized cased BLEU computed using S ACREBLEU (Post, 2018).6 across all six language pairs, suggesting that cross-attention is capable of taking advantage of encoded generic translation knowledge in the Transformer body to adapt to each child task. Performance gain from {src,tgt} and drop from {src,tgt}+body when changing the target language (i.e., Fr–Es and Fr–De) are more pronounced than when transferring the source. This is expected—when changing the target, two out of three cross-attention matrices (key and value matrices) are now exposed to a new language. When transferring source, only the query matrix is exposed to the new language. Storage. W"
2021.emnlp-main.132,N18-2084,0,0.0246657,") and Ro–De (using transferred Ro–En and Fr–De models). To put zero-shot scores in context, for each pair we also train a model from scratch: for De– Es using 294,216-sentence News Commentary v14 corpus, and for Ro–Es and Ro–De using 387,653sentence and 385,663-sentence Europarl corpora respectively. All scores are provided in Table 3. Zero-shot BLEU Supervised BLEU De–Es Ro–Es Ro–De 9.2 18.3 14.7 18.6 9.8 13.4 Table 3: Performance of zero-shot systems for three language pairs. De–Es is evaluated on newstest2013 test set. Ro–Es and Ro–De are evaluated on respective TED talks corpus test sets (Qi et al., 2018). In the case of De–Es, we train two additional models from scratch on 50,000- and 100,000- sentence subsets of the training corpus. These respectively score 7.2 and 12.0 BLEU on the newstest2013 De–Es test set (v.s. zero-shot performance of 9.2). Taken together, these results show that the zero-shot methods we obtain from crossattention-based transfer can yield reasonable translation models in the absence of parallel data. 6 Related Work Studying Cross-attention. Several recent works consider the importance of self- and cross-attention heads in the Transformer architecture (Voita et al., 2019"
2021.emnlp-main.132,P16-1162,0,0.0269261,"Ro–En, Ja–En, De–En, Ha–En (new source), Fr–Es, Fr–De (new target) as child language pairs. Our Fr–En parent model is trained on the Europarl + Common Crawl subset of WMT14 Fr–En,5 which comprises 5,251,875 sentences. Details and statistics of the data for the child language pairs are provided in Table 1. Model Details. We use the Transformer base architecture (6 layers of encoder and decoder with model dimension of 512 and 8 attention heads) for all models, (Vaswani et al., 2017) and the Fairseq (Ott et al., 2019) toolkit for all our experiments. All models rely on BPE subword vocabularies (Sennrich et al., 2016) processed through the SentencePiece (Kudo and Richardson, 2018) BPE implementation. The vocabulary for the parent model consists of 32K French subwords on the source side, and 32K English subwords on the target side. The sizes of the vocabularies for child models are also reported in Table 1. We follow the advice from Gowda and May (2020) when deciding what vocabulary size to choose, i.e., we choose the maximum number of operations to ensure a minimum of 100 tokens per type. 4 Results and Analysis Our preliminary empirical results consist of five experiments for each of the child language pai"
2021.emnlp-main.132,P19-1580,0,0.401579,"ay}@isi.edu Abstract Fine-tuning pretrained models often involves updating all parameters of the model without making We study the power of cross-attention in the a distinction between them based on their imporTransformer architecture within the context of transfer learning for machine translation, tance. However, copious recent studies have looked and extend the findings of studies into crossinto the relative cruciality of multi-headed self- and attention when training from scratch. We cross- attention layers when training an MT model conduct a series of experiments through finefrom scratch (Voita et al., 2019; Michel et al., 2019; tuning a translation model on data where eiYou et al., 2020). Cross-attention (also known as ther the source or target language has changed. encoder-decoder attention) layers are more imporThese experiments reveal that fine-tuning only tant than self-attention layers in the sense that they the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the result in more degradation in quality when pruned, entire translation model). We provide insights and hence, are more sensitive to pruning (Voita into why this is the case and observe that li"
2021.emnlp-main.132,N15-1104,0,0.0547712,"Missing"
2021.emnlp-main.132,2020.acl-main.687,0,0.374037,"eters of the model without making We study the power of cross-attention in the a distinction between them based on their imporTransformer architecture within the context of transfer learning for machine translation, tance. However, copious recent studies have looked and extend the findings of studies into crossinto the relative cruciality of multi-headed self- and attention when training from scratch. We cross- attention layers when training an MT model conduct a series of experiments through finefrom scratch (Voita et al., 2019; Michel et al., 2019; tuning a translation model on data where eiYou et al., 2020). Cross-attention (also known as ther the source or target language has changed. encoder-decoder attention) layers are more imporThese experiments reveal that fine-tuning only tant than self-attention layers in the sense that they the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the result in more degradation in quality when pruned, entire translation model). We provide insights and hence, are more sensitive to pruning (Voita into why this is the case and observe that limet al., 2019; Michel et al., 2019). Also, crossiting fine-tuning in this manner yi"
2021.emnlp-main.132,D16-1163,1,0.901483,"that is needed in our case relative to having to store a full new model for each adapted task. Our contributions are: 1) We empirically show the competitive performance of exclusively finetuning the cross-attention layers when contrasted with fine-tuning the entire Transformer body; 2) We show that when fine-tuning only the cross-attention layers, the new embeddings get aligned with the respective embeddings in the pretrained model. The same effect does not hold when fine-tuning the en2 tire Transformer body; 3) we demonstrate effective Freezing shared language embeddings is common practice (Zoph et al., 2016). application of this aligning artifact in mitigating 1755 catastrophic forgetting (Goodfellow et al., 2014) and zero-shot translation. 2 Cross-Attention Fine-Tuning for MT Fine-tuning pretrained Transformer models towards downstream tasks has pushed the limits of NLP, and MT has been no exception (Liu et al., 2020). Despite the prevalence of using pretrained Transformers, recent studies focus on investigating the importance of self- and cross- attention heads while training models from scratch (Voita et al., 2019; Michel et al., 2019; You et al., 2020). These studies verify the relative impor"
2021.emnlp-main.302,C18-1139,0,0.078285,"Missing"
2021.emnlp-main.302,D19-5531,0,0.026856,"and its produced attacks (e.g., the OntoRock benchmark) can benefit the community working to increase the robustness and out-of-distribution generalization of NER.8 There are other recent works which also turn their attention from achieving a new state-of-the-art of NER model towards studying NER models’ robustness and generalization ability. Agarwal et al. (2020a) create entity-switched datasets by replacing entities with others of the same type but different national origin. They find that NER models Acknowledgements perform worse on entities from certain countries. Mayhew et al. (2020) and Bodapati et al. (2019) This research is supported in part by the Office focus on the robustness when inputs are not writ- of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity ten in the standard casing (e.g., “he is from us” ! (IARPA), via Contract No. 2019-19051600007, “US”). Fu et al. (2020) analyze the generalization the DARPA MCS program under Contract No. ability of current NER models by evaluating them across datasets. Agarwal et al. (2020b) further an- N660011924033, the Defense Advanced Research alyze the roles of context and names in entity pre- Projects Agency"
2021.emnlp-main.410,2020.acl-main.485,0,0.441218,"ises the urgent need to quantify biases both in the knowledge resources and in the downstream models that use these resources. We present the first study on measuring bias in two large CSKBs, namely ConceptNet (Speer et al., 2017), the most widely used knowledge graph in commonsense reasoning tasks, and GenericsKB (Bhakthavatsalam et al., 2020), which expresses knowledge in the form of natural language sentences and has gained increasing usage. We formalize a new quantification of “representational harms,” i.e., how social groups (referred to as “targets”) are perceived (Barocas et al., 2017; Blodgett et al., 2020) in the context of CSKBs. We consider two types of such harms in the context of CSKBs. One is intra-target overgeneralization, indicating that “common sense” in these resources may unfairly attribute a polarized (nega1 ConceptNet also includes knowledge from expert-created sources such as WordNet (Miller, 1995) 5016 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5016–5033 c November 7–11, 2021. 2021 Association for Computational Linguistics tive or positive) characteristic to all members of a target class such as “lawyers are dishonest.” The other"
2021.emnlp-main.410,P19-1470,0,0.367824,"icsKB, we find harmful overgeneralizations of both negative and positive perceptions over many target groups, indicating that human biases have been conflated with “common sense” in these resources. We find severe disparities across the targets in demographic categories such as professions and genders, both in the number of statements and the polarized perceptions about the targets. We then examine two generative downstream tasks and the corresponding models that use ConceptNet. Specifically, we focus on automatic knowledge graph construction and story generation and quantify biases in COMeT (Bosselut et al., 2019) and CommonsenseStoryGen (CSG) (Guan et al., 2020). We find that these models also contain the harmful overgeneralizations and disparities found in ConceptNet. We then design a simple mitigation method that filters unwanted triples according to our measures in ConceptNet. We retrain COMeT using filtered ConceptNet and show that our proposed mitigation approach helps in reducing both overgeneralization and disparity issues in the COMeT model but leads to a performance drop in terms of the quality of triples generated according to human evaluations. We open-source our data and prompts to evaluat"
2021.emnlp-main.410,2020.acl-main.711,0,0.0884487,"ving room for future work to build fairer and stronger commonsense models. GenericsKB (Chinese people are very reclusive group of people) (Lawyers are registered menace to society) Table 1: Biased cases in ConceptNet and GenericsKB. Introduction Commonsense knowledge is important for a wide range of natural language processing (NLP) tasks as a way to incorporate information about everyday situations necessary for human language understanding. Numerous models have included knowledge resources such as ConceptNet (Speer et al., 2017) for question answering (Lin et al., 2019), sarcasm generation (Chakrabarty et al., 2020), and dialogue response generation (Zhou et al., 2018, 2021), among others. However, commonsense knowledge resources are mostly human-generated, either crowdsourced from the public (Speer et al., 2017; Sap et al., 2019) or crawled from massive web corpora (Bhakthavatsalam et al., 2020). For example, ConceptNet originated from the Open ∗ equal contribution Mind Common Sense project that collects commonsense statements online from web users (Singh et al., 2002)1 and GenericsKB consists of crawled text from public websites. One issue with this approach is that the crowdsourcing workers and web pa"
2021.emnlp-main.410,2020.deelio-1.9,1,0.759631,"is work primarily advocates for having more ethspite the aforementioned extensive research in this ical commonsense reasoning resources and modarea, not much attention has been given to the rep- els. In the near future, there will likely be more resentational harms in tools and models used for efforts to incorporate commonsense in NLP modcommonsense reasoning. els. Conflating human biases with commonsense is Injecting commonsense knowledge into NLP harmful. Thus, pointing out the existing problems tasks is gaining attention (Storks et al., 2019; and proposing simple solutions to them can have Chang et al., 2020). In our work, we study a significant broad impact to the community. We two downstream tasks in this area and show how acknowledge that our paper had disturbing content, they are affected by existing biases in upstream but these egregious examples are representative of commonsense knowledge resources like Concept- the knowledge supplied to NLP models. Our goal Net. Although Sweeney and Najafian (2019) have is not to devalue any work or any target group, previously shown that ConceptNet word embed- but to raise awareness of these problems in the AI dings (Speer, 2017) are less biased compared t"
2021.emnlp-main.410,2020.emnlp-main.373,0,0.0547023,"Missing"
2021.emnlp-main.410,P19-1159,0,0.0185948,"y with award W911NF-19-20271, NSF IIS 2048211, and NSF SMA 182926. This material is based upon work supported, in part, by the Defense Advanced Research Projects Agency (DARPA) and Army Research Office (ARO) under Contract No. W911NF-21-C-0002. Work on fairness in NLP has expanded to different applications and domains including coreference resolution (Zhao et al., 2018a), named entity recognition (Mehrabi et al., 2020), machine translation (Font and Costa-jussà, 2019), word embedding (Bolukbasi et al., 2016; Zhao et al., 2018b; Zhou Ethics and Broader Impact et al., 2019), as well as surveys (Sun et al., 2019; Blodgett et al., 2020; Mehrabi et al., 2021). De- This work primarily advocates for having more ethspite the aforementioned extensive research in this ical commonsense reasoning resources and modarea, not much attention has been given to the rep- els. In the near future, there will likely be more resentational harms in tools and models used for efforts to incorporate commonsense in NLP modcommonsense reasoning. els. Conflating human biases with commonsense is Injecting commonsense knowledge into NLP harmful. Thus, pointing out the existing problems tasks is gaining attention (Storks et al.,"
2021.emnlp-main.410,P19-1162,0,0.0240173,"commonsense is Injecting commonsense knowledge into NLP harmful. Thus, pointing out the existing problems tasks is gaining attention (Storks et al., 2019; and proposing simple solutions to them can have Chang et al., 2020). In our work, we study a significant broad impact to the community. We two downstream tasks in this area and show how acknowledge that our paper had disturbing content, they are affected by existing biases in upstream but these egregious examples are representative of commonsense knowledge resources like Concept- the knowledge supplied to NLP models. Our goal Net. Although Sweeney and Najafian (2019) have is not to devalue any work or any target group, previously shown that ConceptNet word embed- but to raise awareness of these problems in the AI dings (Speer, 2017) are less biased compared to community. We also acknowledge that we do not other embeddings, we demonstrate that destruc- cover all the possible existing target groups in each tive biases still exist in ConceptNet that need to be category, such as non-binary gender groups. Howcarefully studied. ever, we incorporated groups from (Nadeem et al., 5024 2020) and made extensions to fill gaps in these groups. Additionally, during our"
2021.emnlp-main.410,N18-2003,0,0.0580469,"Missing"
2021.emnlp-main.410,D18-1521,0,0.0534714,"Missing"
2021.emnlp-main.413,2021.naacl-main.384,0,0.113457,"the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5063–5074 c November 7–11, 2021. 2021 Association for Computational Linguistics issue use soft constraints, such as copy mechanism or additional attention, to focus on (certain parts of) the source input (See et al., 2017; Dinan et al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang"
2021.emnlp-main.413,P17-1123,0,0.052269,"Missing"
2021.emnlp-main.413,2020.acl-main.703,0,0.0181177,"We take two widely used news sum- model needs to focus on. We use copy mechamarization datasets, CNN/Daily Mail (CNN/DM) nism as a representative example for this purpose. (Nallapati et al., 2016) and XSum (Narayan et al., Copy mechanism estimates the importance of to2018), for evaluation, where CNN/DM is more kens in the source input and learns to copy them extractive (Mao et al., 2020a) and XSum more ab- when appropriate, which is useful for preserving stractive (Maynez et al., 2020) in nature. the important concepts in the input, especially rare 5065 2.4 Experimental Settings We use BART (Lewis et al., 2020) as the major base model for analysis. We use spaCy (Honnibal and Montani, 2017) to extract the entities from the target output as the gold concepts, the quality of which is shown to be reasonably good for the source-target alignment in summarization (Nan et al., 2021). We mainly use exact matching to be consistent with the current automatic metrics that generally consider lexical overlap, while also manually analyzing the missing concepts to address the limitation of exact matching. More implementation details can be found in App. A. 2.5 Results and Analysis Concept Availability. We first exa"
2021.emnlp-main.413,D19-1387,0,0.0226509,"Missing"
2021.emnlp-main.413,2020.acl-main.445,1,0.928399,"nce-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to identify what to attend to in the source problem with a broad set of applications, such as input and what to include in the model output, with dialog response generation (Dinan et al., 2019), access to only parallel training data. headline generation (Gu et al., 2020), and sumHowever, as seq2seq learning does not explicitly marization (Mao et al., 2020b). A distinct feature focus on key concepts (e.g., named entities) and of text-to-text generation (vs. free-form text gen- commonly used evaluation metrics (e.g., BLEU eration) is that it is often desired to preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.413,2020.emnlp-main.136,1,0.930371,"nce-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to identify what to attend to in the source problem with a broad set of applications, such as input and what to include in the model output, with dialog response generation (Dinan et al., 2019), access to only parallel training data. headline generation (Gu et al., 2020), and sumHowever, as seq2seq learning does not explicitly marization (Mao et al., 2020b). A distinct feature focus on key concepts (e.g., named entities) and of text-to-text generation (vs. free-form text gen- commonly used evaluation metrics (e.g., BLEU eration) is that it is often desired to preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.413,2020.acl-main.123,0,0.0432268,"Missing"
2021.emnlp-main.413,2020.acl-main.173,0,0.287024,"ow Hollywood got its name? It's a pretty funny story! [Headline Generation] [Source Input] Israel's air forces on Monday launched a third missile strike on Gaza city in less than four hours, Palestinian witnesses said. [Target Output] Israel launches third airstrike on Gaza city. Figure 1: Examples of text-to-text generation tasks where preserving important input concepts is crucial for producing satisfactory results. concepts in the source input (see Fig. 1 for an illustration). On one hand, concept preservation is crucial for maintaining the factual consistency between the input and output (Maynez et al., 2020; Nan et al., 2021). On the other hand, encouraging the model to focus on important input concepts may also improve its generation quality (Yao et al., 2019; Li et al., 2020). Mainstream text-to-text generation methods are mostly data-driven, which “hope” to learn meaningful mappings between source input and target output via sequence-to-sequence (seq2seq) learning – this is particularly the case for the recent pre-trained language models (PLMs) (Lewis et al., 1 Introduction 2020; Raffel et al., 2020), where seq2seq learning is Text-to-text generation is an important research expected to ident"
2021.emnlp-main.413,P17-1054,0,0.0170284,"pts or the dataset involves abundant extrinsic information. 4.3 4.4 Evaluation on Concept Preservation As sequence-level metrics consider all the tokens equally important and can only measure overall generation quality, we further conduct conceptlevel evaluation to measure model performance on concept preservation in particular. We first examine the quality of extracted concepts (automatic constraints) in Table 7. We observe that the F1 scores on different datasets are largely in the range of 0.4 to 0.5, which is on par with the state-of-the-art performance on keyphrase extraction benchmarks (Meng et al., 2017). Task SQuAD WoW Gigaword CNN/DM XSum Precision Recall F1 29.1 35.4 41.1 52.0 38.5 62.9 53.1 47.8 50.1 58.3 39.8 42.5 44.2 51.0 46.4 Table 7: Quality of automatically extracted concepts (constraints) on different text-to-text generation tasks. Takeaways From the analysis on concept preservation and model performance when automatic constraints are used, we can see that EDE is likely to improve overall generation quality when the upper bound performance using available gold concepts is high. EDE may not be very effective when many gold concepts cannot be found from the source input, which is an"
2021.emnlp-main.413,K16-1028,0,0.0676565,"Missing"
2021.emnlp-main.413,W12-3018,0,0.0514483,"Missing"
2021.emnlp-main.413,D18-1206,0,0.0265979,"Missing"
2021.emnlp-main.413,N18-1119,0,0.190733,"al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and Liu, 2017; Post and Vilar, 2018; Zhang et al., 2020b) focus on scenarios where gold (ground-truth) constraints are given (e.g., generate a story using user-specified keywords), while in generic text-to-text generation tasks the constraints (target output) are unavailable. In this paper, we present a systematic analysis on generic text-to-text generation to understand (1) the abilities of seq2seq models, especially the PLMs, for preserving important input concepts and (2) whether more explicit guidance that uses important input concepts as lexical constraints can complement seq2seq learning. We select four representative tas"
2021.emnlp-main.413,N19-1269,0,0.0204546,"on and also preserves the input concepts better on Gigaword. Table 5: Performance comparison of abstractive summarization on CNN/DM and XSum. counterpart on both datasets. Better performance may be achieved when the constraint extraction method is improved. Apart from sequence-level evaluation (ROUGE), we observe that EDE is consistently better at concept preservation in conceptlevel evaluation (Sec. 4.3). 4.2 Human Evaluation 300k training 10k training In addition to automatic evaluation, we further conduct human evaluation with the following three aspects: closeness, relevancy, and fluency (Prabhumoye et al., 2019). Closeness measures the similarity between the model output and target output. Relevancy considers the quality of model output directly with the source input, since there are usually more valid outputs than the target output for generation tasks. Fluency of the model output is on a scale of 1 (unreadable) to 4 (perfect). Method R-1 R-2 R-L We randomly sample 50 examples on each task Transformer (Vaswani et al., 2017) 10.97 2.23 10.42 and conduct pairwise comparisons between BART, MASS (Song et al., 2019) 25.03 9.48 23.48 UniLMlarge (Dong et al., 2019) 32.96 14.68 30.56 EDE (DBA), and EDE (DDB"
2021.emnlp-main.413,D16-1264,0,0.0205961,"the enforced task of generating a question given a passage and constraints are noisy (inappropriate). We choose the corresponding answer. Ideally, a seq2seq model from LCGen methods that enforce constraints by would learn to focus on the relevant information constraining beam search (Hokamp and Liu, 2017; surrounding the answer span and reuse some of Post and Vilar, 2018), as they function at the inthe concepts in the source input as part of the gen- ference stage and can be easily combined with diferated question. We use the SQuAD 1.1 dataset ferent seq2seq models, making our analysis more (Rajpurkar et al., 2016), which is repurposed by Du generalizable. In contrast, sampling-based and et al. (2017) for question generation. insertion-based approaches are not easily applicaKnowledge-Grounded Dialog. Knowledge- ble to generic text-to-text generation, as they typgrounded dialog involves utterances with ground- ically involve specialized training schemes or do not accept other inputs than the constraints (Miao ings to specific knowledge sources. It is used as et al., 2019; Zhang et al., 2020b; Sha, 2020). another test case for our analysis as the model is Specifically, we take dynamic beam allocation supp"
2021.emnlp-main.413,D15-1044,0,0.0468808,"ith the training set. output, as the <EOS> token is only allowed when Headline Generation. Headline generation fits all the constraints are met. At each decoding step, our analysis as a headline usually consists of the there are three sources of candidates: (1) the top-k most important concepts in the input article. We tokens across all hypotheses as in standard beam use the English Gigaword dataset (Napoles et al., search; (2) all unfulfilled constraint tokens for each 2012) for evaluation. As training on the full train- hypothesis; and (3) the single-best token for each ing set of Gigaword (Rush et al., 2015) is compu- hypothesis. The banks are trimmed by the sequence tationally prohibitive, we adopt two low-resource probability if the total number of candidate tokens settings where the 10k training examples in Dong is beyond capacity (beam size). et al. (2019) and the first 300k of the 3.8M training Soft Constrained Generation. We additionally examples are used. examine the effectiveness of soft constrained genAbstractive Summarization. Abstractive summa- eration for comparison. There are various types rization is used as another testbed as a summary of soft constraints and we particularly consid"
2021.emnlp-main.413,D19-1253,0,0.0503546,"Missing"
2021.emnlp-main.413,P17-1099,0,0.317824,"preserve the and ROUGE) also treat all the tokens in a sequence equally important, it is unclear how many of the * Equal contribution 1 important input concepts can be (or have been) Our code is available at https://github.com/ morningmoni/EDE preserved. Existing attempts to alleviate the above 5063 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5063–5074 c November 7–11, 2021. 2021 Association for Computational Linguistics issue use soft constraints, such as copy mechanism or additional attention, to focus on (certain parts of) the source input (See et al., 2017; Dinan et al., 2019; Dou et al., 2021). Nevertheless, they still lack explicit guidance and resort to seq2seq learning itself to figure out what is important, without any guarantee of the model output or evaluation on the input concept preservation. Explicit guidance for text generation can be achieved by lexically (hard) constrained generation (LCGen), which specifies lexical constraints (tokens) that must be included in the model output. However, to what extent guiding text-totext generation with lexical constraints works in general remains unknown, as existing studies on LCGen (Hokamp and"
2021.emnlp-main.413,2020.emnlp-main.698,0,0.0298321,"Missing"
2021.emnlp-main.413,2020.emnlp-main.701,0,0.182136,"d question. We use the SQuAD 1.1 dataset ferent seq2seq models, making our analysis more (Rajpurkar et al., 2016), which is repurposed by Du generalizable. In contrast, sampling-based and et al. (2017) for question generation. insertion-based approaches are not easily applicaKnowledge-Grounded Dialog. Knowledge- ble to generic text-to-text generation, as they typgrounded dialog involves utterances with ground- ically involve specialized training schemes or do not accept other inputs than the constraints (Miao ings to specific knowledge sources. It is used as et al., 2019; Zhang et al., 2020b; Sha, 2020). another test case for our analysis as the model is Specifically, we take dynamic beam allocation supposed to extract important concepts from the (DBA) (Post and Vilar, 2018) for our analysis, as grounded knowledge when appropriate. We use the Wizard of Wikipedia (WoW) dataset (Dinan et al., it has higher efficiency than other LCGen methods that constrain beam search (Hokamp and Liu, 2019) for evaluation. As we focus on generation 2017). DBA revises the process of beam search by instead of knowledge retrieval, we adopt the gold dividing the beam into a number of groups (named knowledge settin"
2021.emnlp-main.413,2020.acl-main.125,0,0.0685332,"Missing"
2021.emnlp-main.436,S15-2136,0,0.05928,"Missing"
2021.emnlp-main.436,S17-2093,0,0.0408444,"Missing"
2021.emnlp-main.436,Q14-1022,0,0.0219478,"ction, we describe details of implementing ECONET, datasets and evaluation metrics, and discuss compared methods reported in Section 4. 3.1 Implementation Details Event Detection Model. As mentioned briefly in Section 2, we train a highly accurate event preAfter training with ECONET, we fine-tune the up- diction model to mask event (triggers). We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to s"
2021.emnlp-main.436,glavas-etal-2014-hieve,0,0.0200926,". We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to show both models produce comparable results. Continual Pretraining. We randomly selected only 200K out of 10 million samples to speed up our experiments and found the results can be as good as using a lot more data. We used half of these 200K samples for temporal masked samples and the other half for the event masked samples. We ensure none"
2021.emnlp-main.436,W16-1007,0,0.0355074,"Missing"
2021.emnlp-main.436,2020.acl-main.678,0,0.244347,"rence between ERE and QA / MRC samples of event temporal reasoning. Bottom: our targeted masking strategy for ECONET v.s. random masking in PTLMs. about event temporal relations is presented, and models need to provide correct answers using the information in a given passage. Recent approaches leveraging large pre-trained language models (PTLMs) achieved state-of-the1 Introduction art results on a range of event temporal reasoning Reasoning event temporal relations is crucial for tasks (Ning et al., 2020; Pereira et al., 2020; Wang natural language understanding, and facilitates et al., 2020; Zhou et al., 2020c; Han et al., 2019b). many real-world applications, such as tracking Despite the progress, vanilla PTLMs do not focus biomedical histories (Sun et al., 2013; Bethard on capturing event temporal knowledge that can et al., 2015, 2016, 2017), generating stories (Yao be used to infer event relations. For example, in et al., 2019; Goldfarb-Tarrant et al., 2020), and Figure 1, an annotator of the QA sample can easily forecasting social events (Li et al., 2020; Jin et al., infer from the temporal indicator “following” that 2020). In this work, we study two prominent event “transfer” happens BEFORE “"
2021.emnlp-main.436,S13-2001,0,0.0365703,"mask event (triggers). We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to show both models produce comparable results. Continual Pretraining. We randomly selected only 200K out of 10 million samples to speed up our experiments and found the results can be as good as using a lot more data. We used half of these 200K samples for temporal masked samples and the other half for the event masked sa"
2021.emnlp-main.436,2020.emnlp-main.51,0,0.0295574,"pposing feedback that trains the overall model to better capture indicators with similar temporal signals. 5 Related Work with concepts randomly shuffled or generated by models, which enables language models to capture large-scale commonsense knowledge. Event Temporal Reasoning. There has been a surge of attention to event temporal reasoning research recently. Some noticeable datasets include ERE samples: TB-Dense (Chambers et al., 2014), M ATRES (Ning et al., 2018) and RED (O’Gorman et al., 2016). Previous SOTA systems on these data leveraged PTLMs and structured learning (Han et al., 2019c; Wang et al., 2020; Zhou et al., 2020c; Han et al., 2020) and have substantially improved model performances, though none of them tackled the issue of lacking event temporal knowledge in PTLMs. TORQUE (Ning et al., 2020) and MCTACO (Zhou et al., 2019) are recent MRC datasets that attempt to reason about event temporal relations using natural language rather than ERE formalism. Zhou et al. (2020a) and Zhao et al. (2020) are two recent works that attempt to incorporate event temporal knowledge in PTLMs. The formal one focuses on injecting temporal commonsense with targeted event time, frequency and duration masks"
2021.emnlp-main.572,N16-1181,0,0.0641608,"Missing"
2021.emnlp-main.572,2020.coling-main.448,0,0.0293796,"g research questions: • Q1. Can we teach cross-task generalization ability to pre-trained models with existing methods? • Q2. During upstream learning, is it better to be “well-rounded” (learning from diverse tasks) or be “specialized and targeted” (learning from tasks in the same category with unseen tasks)? • Q3. Does it help if we have more labelled data for seen tasks during upstream learning? 2 Related Work Meta-learning in NLP. Recent works have explored meta-learning methods for relation classification (Han et al., 2018; Gao et al., 2019), general text classification (Dou et al., 2019; Bansal et al., 2020a,b), low-resource machine translation (Gu et al., 2018), cross-lingual NLI/QA (Nooralahzadeh et al., 2020). In general, these works apply meta-learning algorithms to a set of sub-tasks; however the sub-tasks are either synthetic (e.g., classifying a new set of five relations is a new sub-task) or drawn from a rather narrow distribution (e.g., QA in one language is a sub-task). In our work, we explore a more realistic setting – learning from a set of NLP tasks with diverse goals: classification, question answering, conditional generation, etc. This setting is attracting attention in NLP commun"
2021.emnlp-main.572,2020.emnlp-main.38,0,0.0424745,"g research questions: • Q1. Can we teach cross-task generalization ability to pre-trained models with existing methods? • Q2. During upstream learning, is it better to be “well-rounded” (learning from diverse tasks) or be “specialized and targeted” (learning from tasks in the same category with unseen tasks)? • Q3. Does it help if we have more labelled data for seen tasks during upstream learning? 2 Related Work Meta-learning in NLP. Recent works have explored meta-learning methods for relation classification (Han et al., 2018; Gao et al., 2019), general text classification (Dou et al., 2019; Bansal et al., 2020a,b), low-resource machine translation (Gu et al., 2018), cross-lingual NLI/QA (Nooralahzadeh et al., 2020). In general, these works apply meta-learning algorithms to a set of sub-tasks; however the sub-tasks are either synthetic (e.g., classifying a new set of five relations is a new sub-task) or drawn from a rather narrow distribution (e.g., QA in one language is a sub-task). In our work, we explore a more realistic setting – learning from a set of NLP tasks with diverse goals: classification, question answering, conditional generation, etc. This setting is attracting attention in NLP commun"
2021.emnlp-main.572,2020.findings-emnlp.148,0,0.0754036,"Missing"
2021.emnlp-main.572,2020.tacl-1.43,0,0.0438231,"Missing"
2021.emnlp-main.572,D13-1160,0,0.0836422,"Missing"
2021.emnlp-main.572,2020.emnlp-main.85,0,0.024282,"Missing"
2021.emnlp-main.572,D18-1080,0,0.052886,"Missing"
2021.emnlp-main.572,S19-2005,0,0.0304484,"Missing"
2021.emnlp-main.572,2020.emnlp-main.528,0,0.0565399,"Missing"
2021.emnlp-main.572,N19-1300,0,0.0505584,"Missing"
2021.emnlp-main.572,N19-1361,0,0.0597939,"Missing"
2021.emnlp-main.572,D19-1606,0,0.0571744,"Missing"
2021.emnlp-main.572,W19-8652,0,0.0442259,"Missing"
2021.emnlp-main.572,L18-1544,0,0.0264371,"Missing"
2021.emnlp-main.572,P19-1102,0,0.0285268,"Missing"
2021.emnlp-main.572,K17-1034,0,0.0535931,"Missing"
2021.emnlp-main.572,2020.acl-main.703,0,0.0205035,"ion tasks, or half-and-half. These three partitions help us to understand the influence brought by different task distribution in Ttrain . The remaining four partitions still focus on crossing task boundaries, but in a finer granularity: seen and unseen tasks are in the same category, but not the same sub-category. For example, Partition 3.1 has 57 non-NLI classification tasks as Ttrain , and 8 NLI tasks as Ttest . These partitions help us to understand whether cross-task generalization in this finer granularity is easier for model to acquire. 5 Methods to C ROSS F IT We mainly use BART-Base (Lewis et al., 2020) as the text-to-text transformer for our analysis in the C ROSS F IT setup. We leave confirmatory experiments with T5-v1.1-Base and BART-Large model in Appendix C. Direct Fine-tuning on Test Tasks. This serves as the basic baseline method for the C ROSS F IT challenge, which does not make use of Ttrain or Tdev , or go through the upstream learning stage. For each task T ∈ Ttest , we directly fine-tune the text-to-text model with its Dtrain , tune the hyperparameters with Ddev , and assess its performance with the test set Dtest . We use the performance of direct fine-tuning as the base for com"
2021.emnlp-main.572,2020.findings-emnlp.165,1,0.845772,"Missing"
2021.emnlp-main.572,D19-5808,0,0.0330489,"Missing"
2021.emnlp-main.572,P17-1015,0,0.0381527,"Missing"
2021.emnlp-main.572,2020.acl-main.465,0,0.0175097,"d with small, harmless perturbations (Ribeiro et al., 2020). Triantafillou et al., 2020), but is relatively underIn retrospect, researchers have advocated for build- explored in NLP. Pruksachatkun et al. (2020) and ing more human-like, general linguistic intelli- Vu et al. (2020) study transferability between one gence that can “reuse previously acquired knowl- intermediate task and a given target task, while it’s possible to further improve performance with multiedge about a language and adapt to a new task ple intermediate tasks. Han et al. (2018) and Bansal quickly” (Yogatama et al., 2019; Linzen, 2020). et al. (2020a) focus on cross-task generalization 1 Our code is at https://github.com/INK-USC/CrossFit. within the scope of classification tasks, whereas hu7163 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189 c November 7–11, 2021. 2021 Association for Computational Linguistics mans can generalize across different task formats (classification, multiple choice, generation, etc.), goals (question answering, fact checking, etc.) and domains (biomedical, social media, etc.). intelligence, and we hope C ROSS F IT serves as a useful testbed f"
2021.emnlp-main.572,2020.emnlp-main.601,0,0.0612198,"Missing"
2021.emnlp-main.572,P11-1015,0,0.0257467,"Missing"
2021.emnlp-main.572,marelli-etal-2014-sick,0,0.0382605,"Missing"
2021.emnlp-main.572,2020.emnlp-main.368,0,0.0229871,"th existing methods? • Q2. During upstream learning, is it better to be “well-rounded” (learning from diverse tasks) or be “specialized and targeted” (learning from tasks in the same category with unseen tasks)? • Q3. Does it help if we have more labelled data for seen tasks during upstream learning? 2 Related Work Meta-learning in NLP. Recent works have explored meta-learning methods for relation classification (Han et al., 2018; Gao et al., 2019), general text classification (Dou et al., 2019; Bansal et al., 2020a,b), low-resource machine translation (Gu et al., 2018), cross-lingual NLI/QA (Nooralahzadeh et al., 2020). In general, these works apply meta-learning algorithms to a set of sub-tasks; however the sub-tasks are either synthetic (e.g., classifying a new set of five relations is a new sub-task) or drawn from a rather narrow distribution (e.g., QA in one language is a sub-task). In our work, we explore a more realistic setting – learning from a set of NLP tasks with diverse goals: classification, question answering, conditional generation, etc. This setting is attracting attention in NLP community rapidly and is also explored in very recent work (Zhong et al., 2021; Mishra et al., 2021; Bragg et al."
2021.emnlp-main.572,P05-1015,0,0.259983,"Missing"
2021.emnlp-main.572,2020.bionlp-1.15,0,0.0494664,"Missing"
2021.emnlp-main.572,2020.acl-main.467,0,0.0344679,"Missing"
2021.emnlp-main.572,D12-1071,0,0.0887983,"Missing"
2021.emnlp-main.572,P19-1487,0,0.0310409,"Missing"
2021.emnlp-main.572,D16-1264,0,0.0390701,"Missing"
2021.emnlp-main.572,P19-1534,0,0.0210478,"Missing"
2021.emnlp-main.572,2020.acl-main.442,0,0.0234962,"g ability also benefits dant task-specific data have become the predomifrom task-level generalization, or cross-task genernant recipe for state-of-the-art results in NLP. Howalization, i.e., how to learn a new task efficiently ever, these approaches are heavily dependent on given experiences of learning previous tasks. large-scale labeled datasets that are expensive to Such ability has been widely studied in computer create, and the resulting models still generalize vision and robotics community (Yu et al., 2020; poorly to out-of-distribution inputs created with small, harmless perturbations (Ribeiro et al., 2020). Triantafillou et al., 2020), but is relatively underIn retrospect, researchers have advocated for build- explored in NLP. Pruksachatkun et al. (2020) and ing more human-like, general linguistic intelli- Vu et al. (2020) study transferability between one gence that can “reuse previously acquired knowl- intermediate task and a given target task, while it’s possible to further improve performance with multiedge about a language and adapt to a new task ple intermediate tasks. Han et al. (2018) and Bansal quickly” (Yogatama et al., 2019; Linzen, 2020). et al. (2020a) focus on cross-task generaliz"
2021.emnlp-main.572,P18-1156,0,0.0303936,"Missing"
2021.emnlp-main.572,2020.gebnlp-1.9,0,0.0263932,"Missing"
2021.emnlp-main.572,2020.emnlp-main.346,0,0.245683,"encoderdecoder language models used in our study. 7 Conclusion and Future Work meta-learning; confirmed that the selection of seen tasks would influence the few-shot performance on unseen tasks. We have highlighted several unexpected or undesired observations in our analysis, for which we invite future work in understanding and combating related issues. In addition, we envision the C ROSS F IT Challenge and the NLP Few-shot Gym to serve as the testbed for many interesting “meta-problems”, such as (1) learning to generate prompt for diverse task formats and further improve learning efficiency (Shin et al., 2020; Gao et al., 2020); (2) learning to select appropriate source tasks to learn from during upstream learning (Zamir et al., 2018; Standley et al., 2020), potentially with task2vec methods (Achille et al., 2019; Vu et al., 2020); (3) applying task augmentation strategies to prevent over-fitting (Murty et al., 2021); (4) learning to accumulate knowledge and avoid catastrophic forgetting in an continual learning setup (Jin et al., 2021); (5) decomposing complex tasks into atomic tasks and exploring cross-task generalization through the lens of compositionality (Andreas et al., 2016; Khot et al., 2"
2021.emnlp-main.572,N19-1351,0,0.0356472,"Missing"
2021.emnlp-main.572,D13-1170,0,0.0177411,"Missing"
2021.emnlp-main.572,D19-1608,0,0.0199339,"Missing"
2021.emnlp-main.572,N19-1421,0,0.133514,"Missing"
2021.emnlp-main.572,2020.tacl-1.25,0,0.0430661,"Missing"
2021.emnlp-main.572,Q19-1040,0,0.0647084,"Missing"
2021.emnlp-main.572,W17-4413,0,0.0594466,"Missing"
2021.emnlp-main.572,N18-1101,0,0.0822951,"Missing"
2021.emnlp-main.572,2020.tacl-1.13,0,0.0267813,"Missing"
2021.emnlp-main.572,D18-1425,0,0.0290715,"Missing"
2021.emnlp-main.572,P19-1496,0,0.0257401,"Missing"
2021.emnlp-main.572,D18-1009,0,0.0597398,"Missing"
2021.emnlp-main.572,D15-1237,0,0.064349,"Missing"
2021.emnlp-main.572,P19-1472,0,0.150082,"Missing"
2021.emnlp-main.572,D18-1259,0,0.0509604,"Missing"
2021.emnlp-main.572,2020.coling-main.411,0,0.022457,"Missing"
2021.emnlp-main.573,D11-1072,0,0.0914873,"Missing"
2021.emnlp-main.573,2020.tacl-1.5,0,0.0216494,"tion between mask frequency and token frequency for masking policies learned from TQA, Self-supervised Pre-training. Pre-trained lanalong with random masking and SSM for reference. guage models has shown its capability on a wide Mask frequency is computed as the number of oc- variety of NLP tasks. Current self-supervised obcurrences that a token was masked divided by the jectives are mostly heuristic, including masked lannumber of all masked tokens. For random mask- guage modeling (Devlin et al., 2019), span bounding, the datapoints approximate a Zipfian distribu- ary representation learning (Joshi et al., 2020), cortion (Zipf, 1999), with some noise due to random rupted sentence reconstruction (Lewis et al., 2020), sampling of words. Secondly, for SSM, most dat- etc. Raffel et al. (2020) systematically studied the apoints fall on a curve above the random masking self-supervised objectives used in previous literaline, while a small portion of tokens are less likely ture. Related to our goal of exploring pre-training to be masked, formulating line segments in the bot- objectives, ELECTRA (Clark et al., 2020) propose tom area. These observations indicate that SSM a replaced token prediction task which"
2021.emnlp-main.573,P17-1147,0,0.0178085,", whereby learning to unmask spans in context is analogous to memorizing facts about the span. In the absence of such understanding, heuristic policies may be sub-optimal. This motivates us to explore whether automating the discovery of optimal masking policies is possible. We design methods to learn a masking policy with supervised learning (§4.2) or meta-learning (§4.3), and compare downstream task performance using the same protocol in our previous analysis. Notably, we observe that masking policies learned with supervised learning and meta-learning outperforms the SSM policy for TriviaQA (Joshi et al., 2017), and these policies learned from TriviaQA also help improve performance on Web Questions (Berant et al., 2013). We also discuss the pros and cons of learned masking policies, such as downstream task learning efficiency, risks of over-fitting and learning instability. Finally, in hopes to better understand the heuristic and learned masking policies, we provide quantitative analysis on the masks produced by these policies. We visualize the distribution of part-ofspeech tags among masked tokens, and their relation to token frequency in the corpus (§5.3). We find that the masking policies learned"
2021.emnlp-main.573,2020.emnlp-main.493,0,0.0259897,"7 propose to mask n-grams according to Pointwise Mutual Information (PMI). These works typically consider the efficiency of an objective when pretraining from scratch and without preconceived focus on a given problem; while we focus on encoding knowledge or adapting the model during intermediate pre-training with a given task in mind. Domain/Task-specific Pre-training. Gururangan et al. (2020) experiment on four domains (biomedical, computer science, news, reviews) and eight different datasets, where they discover that pre-training with in-domain corpus leads to better downstream performance. Kang et al. (2020) propose to learn a mask generator via reinforcement learning. Closely related to us, Gu et al. (2020) propose task-guided pre-training by learning to predict importance score for each token in pre-train corpus. Vu et al. (2020); Pruksachatkun et al. (2020) studies knowledge transfer from intermediate-task fine-tuning, while we focus on a different problem setting of intermediate pre-training with generic corpus (e.g., Wikipedia). We believe both settings have practical utility in real-world applications. 7 Conclusion Acknowledgments We would like to thank anonymous reviewers for their constru"
2021.emnlp-main.573,K17-1034,0,0.0627149,"Missing"
2021.emnlp-main.573,2020.acl-main.703,0,0.479697,"a language model f (.; θ), parameterized by θ. Formally, given a sequence of tokens x = [x1 , x2 , ..., xm ], g(x; φ) generates a sequence of binary decisions d = [d1 , d2 , ..., dm ], where di = 1 indicates the token xi will be masked. The source sequence for pre-training, x(src) , is formulated by replacing the selected tokens with a special &lt;mask> (src) (src) (src) token, i.e., x(src) = [x1 , x2 , ..., xm ], where (src) (src) xi = xi if di = 0 and xi = &lt;mask> if di = 1. We denote this operation as x(src) = x ⊕ d. The target sequence x(tar) can be either the full original sequence x (BART, Lewis et al. 2020), or the sequence of masked tokens (T5, Raffel et al. 2020). 3 Analysis Setup In this section we introduce the analysis pipeline (§3.1) and downstream datasets we use (§3.2). We defer the details of learned masking policies to §4. 3.1 Experiment Procedure Our goal is to analyze the influence in downstream task performance brought by different masking policies g(.; φ) during intermediate pre-training. Towards this goal, we ensure that the only variable is the masking policy, while all other aspects are controlled, so that the downstream performance reveal the influence we aim to study. We first"
2021.emnlp-main.573,D19-5808,0,0.0472364,"Missing"
2021.emnlp-main.573,2021.ccl-1.108,0,0.0653848,"Missing"
2021.emnlp-main.573,N19-4009,0,0.0645197,"Missing"
2021.emnlp-main.573,D19-1250,0,0.0567999,"Missing"
2021.emnlp-main.573,2020.acl-main.467,0,0.0373087,"Missing"
2021.emnlp-main.573,2020.emnlp-main.437,0,0.0229287,"Missing"
2021.emnlp-main.573,D19-1608,0,0.0402163,"Missing"
2021.emnlp-main.573,D19-1629,0,0.0561887,"Missing"
2021.emnlp-main.598,P19-1470,0,0.0292472,"of thousands self-contained commonsense statements involving tomorrow,” you may reply reassuringly, “Deep breaths, you’ll do great!” Implicit to this com- novel entities (“Prindag is going to perform in front of a crowd, so prindag is more likely to feel nermunication is a commonsense logical inference vous.”) and adapt them to two evaluation settings. that a person performing in front of a crowd may feel anxious, and that a reassuring remark helps Unfortunately, these two capabilities have ease anxiety (Figure 1). A growing body of liter- largely been overlooked by existing natural lanature (Bosselut et al., 2019; Petroni et al., 2019) guage inference (NLI) benchmarks (Williams 1 Our code and data are public at https://sites. et al., 2018) and knowledge probing studies for google.com/usc.edu/rica. transformer-based PTLMs (Vaswani et al., 2017; 7560 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7560–7579 c November 7–11, 2021. 2021 Association for Computational Linguistics Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020; Petroni et al., 2019). Most existing commonsense reasoning-focused datasets (Zhang et al., 2017; Williams et al., 2018; Osterm"
2021.emnlp-main.598,2020.acl-main.703,0,0.148028,"patterns in the training set. 4. Raw Large-Scale Training: Finally, to analyze the effects of training on an even larger but noisier set with the similar format. Starting from the raw set of 257k crawled statements, we sample 100k statements from 17k axioms ensuring no overlap with the test set. 3.4 Baseline Methods We evaluate multiple state-of-the-art transformerbased PTLMs covering both masked and generative language models. For the masked word prediction task, we consider BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ERNIE, a knowledge enhanced LM (Zhang et al., 2019), and BART (Lewis et al., 2020b). For sentence probability, we consider GPT-2 (Radford et al.), a unidirectional language model for left-to-right language generation. 4 Results and Analysis We examine the performance of multiple language models on each evaluation setting on our probe 1. Zero-Shot: In the zero-shot setting, we test data, including zero-shot and fine-tuning on varimodels without any exposure to training data. ous splits, and present ablation studies to analyze 7565 89.9 Avg Accuracy 80 60 58.7 45.8 60.5 49.8 50.948.3 40 20 0 12.5 100 &quot;Pos&quot; Words &quot;Neg&quot; Words 77.7Random Guess 12.2 20.9 Human RB-base RB-base-ft"
2021.emnlp-main.598,2020.blackboxnlp-1.12,0,0.0366188,"n common names instead of novel entities, producing and Sabharwal, 2020; Richardson et al., 2020). probes containing only previously-seen words. As Figure 5b shows, the performance of all models in Robustness to Linguistic Variations Previous three settings did not change significantly, strongly work has also examined model robustness against suggesting that novel entities are not critical to paraphrases by producing linguistically-varied senPTLM performance. We conclude novel entities tences for different tasks such as NLI (Liu et al., do not introduce helpful or distracting sub-words. 2020; Li et al., 2020), question answering (Weller et al., 2020), and sentiment analysis (Ribeiro et al., Impact of Linguistic Perturbations Before 2018), just to name a few. Our work distinguishes fine-tuning, a heavy bias for positive valence words from them as we look into robustness in regard to 2 https://books.google.com/ngrams commonsense reasoning and develops a systematic 7567 procedure to generate paraphrases using first-order logic. Probing PTLMs Prior works in analyzing the (commonsense) reasoning ability of PTLMs have primarily focused on creating probing tasks by generating ad-hoc masked sentences eith"
2021.emnlp-main.598,2020.emnlp-main.557,1,0.80909,"ibeiro et al., Impact of Linguistic Perturbations Before 2018), just to name a few. Our work distinguishes fine-tuning, a heavy bias for positive valence words from them as we look into robustness in regard to 2 https://books.google.com/ngrams commonsense reasoning and develops a systematic 7567 procedure to generate paraphrases using first-order logic. Probing PTLMs Prior works in analyzing the (commonsense) reasoning ability of PTLMs have primarily focused on creating probing tasks by generating ad-hoc masked sentences either from knowledge bases (Petroni et al., 2019; Davison et al., 2019; Lin et al., 2020) or existing datasets (Zhou et al., 2020; Talmor et al., 2020; Kwon et al., 2019; Zhou et al., 2021b). This first line of works aim to test if PTLMs can work as knowledge bases, i.e. can they retrieve factual knowledge; our work focuses on implicit commonsense relations, not facts. We differ from the second line of work by proposing a systematic procedure to generate probes and evaluate for robustness. Clark et al. (2020) shows that PTLMs can emulate deductive reasoning given explicit rules, but we focus on unstated commonsense relations. 6 Conclusion We design RICA as an AI challenge to test"
2021.emnlp-main.598,2021.acl-long.102,1,0.771653,", the composite perturbation types such as N EGATION A NTONYM are not necessarily harder for PTLMs, even though performance on A NTONYM is the lowest. We speculate that the model is exploiting some pattern in N EGATION A NTONYM that is not present for just A NTONYM. 5 Related Work Commonsense Reasoning has a long history in AI, with classical work primarily focusing on executing symbolic rules as hand-crafted programs for machines to learn (Mccarthy, 1960). The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b; Lin et al., 2021c,a,b; Sakaguchi et al., 2020) test a model’s ability to choose the correct option given a context and a question; PTLMs have reached high performance on these benchmarks after fine-tuning. We differ from these benchmarks by focusing on robustness to linguistic variation via our linguistically-varied commonsense statements. RICA also challenges PTLMs on two evaluation tasks to better probe the PTLMs’ representations. Reasoning-focused Inference There have been Ablation of Novel Entities In order to ensure many benchmarks that focus on reasoning abilinovel entities used in RICA did not impact P"
2021.emnlp-main.598,2021.naacl-main.366,1,0.744855,", the composite perturbation types such as N EGATION A NTONYM are not necessarily harder for PTLMs, even though performance on A NTONYM is the lowest. We speculate that the model is exploiting some pattern in N EGATION A NTONYM that is not present for just A NTONYM. 5 Related Work Commonsense Reasoning has a long history in AI, with classical work primarily focusing on executing symbolic rules as hand-crafted programs for machines to learn (Mccarthy, 1960). The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b; Lin et al., 2021c,a,b; Sakaguchi et al., 2020) test a model’s ability to choose the correct option given a context and a question; PTLMs have reached high performance on these benchmarks after fine-tuning. We differ from these benchmarks by focusing on robustness to linguistic variation via our linguistically-varied commonsense statements. RICA also challenges PTLMs on two evaluation tasks to better probe the PTLMs’ representations. Reasoning-focused Inference There have been Ablation of Novel Entities In order to ensure many benchmarks that focus on reasoning abilinovel entities used in RICA did not impact P"
2021.emnlp-main.598,N19-1421,0,0.055355,"Missing"
2021.emnlp-main.598,P18-1079,0,0.0684777,"Missing"
2021.emnlp-main.598,2020.tacl-1.37,0,0.0794854,"Missing"
2021.emnlp-main.598,D19-3002,0,0.051765,"Missing"
2021.emnlp-main.598,2020.emnlp-main.105,0,0.0208953,"es, producing and Sabharwal, 2020; Richardson et al., 2020). probes containing only previously-seen words. As Figure 5b shows, the performance of all models in Robustness to Linguistic Variations Previous three settings did not change significantly, strongly work has also examined model robustness against suggesting that novel entities are not critical to paraphrases by producing linguistically-varied senPTLM performance. We conclude novel entities tences for different tasks such as NLI (Liu et al., do not introduce helpful or distracting sub-words. 2020; Li et al., 2020), question answering (Weller et al., 2020), and sentiment analysis (Ribeiro et al., Impact of Linguistic Perturbations Before 2018), just to name a few. Our work distinguishes fine-tuning, a heavy bias for positive valence words from them as we look into robustness in regard to 2 https://books.google.com/ngrams commonsense reasoning and develops a systematic 7567 procedure to generate paraphrases using first-order logic. Probing PTLMs Prior works in analyzing the (commonsense) reasoning ability of PTLMs have primarily focused on creating probing tasks by generating ad-hoc masked sentences either from knowledge bases (Petroni et al., 2"
2021.emnlp-main.598,N18-1101,0,0.0395835,"nature (Bosselut et al., 2019; Petroni et al., 2019) guage inference (NLI) benchmarks (Williams 1 Our code and data are public at https://sites. et al., 2018) and knowledge probing studies for google.com/usc.edu/rica. transformer-based PTLMs (Vaswani et al., 2017; 7560 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7560–7579 c November 7–11, 2021. 2021 Association for Computational Linguistics Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020; Petroni et al., 2019). Most existing commonsense reasoning-focused datasets (Zhang et al., 2017; Williams et al., 2018; Ostermann et al., 2019; Zhou et al., 2021a; Talmor et al., 2019) do not systematically evaluate robustness against linguistic variations, meaning we cannot preclude the possibility that models are learning spurious patterns to solve the needed task. To fill this gap, we introduce RICA, a challenge to evaluate a model’s Robust Inference using Commonsense Axioms in English. RICA draws on linguistic and cognitive science research (Schank and Abelson, 1977; Alshawi and van Eijck, 1989) suggesting humans translate language to logical representations and reason using these abstract representations"
2021.emnlp-main.598,D18-1009,0,0.0195344,"sense probes, while suggesting models do not comprehend the axioms. Interestingly, the composite perturbation types such as N EGATION A NTONYM are not necessarily harder for PTLMs, even though performance on A NTONYM is the lowest. We speculate that the model is exploiting some pattern in N EGATION A NTONYM that is not present for just A NTONYM. 5 Related Work Commonsense Reasoning has a long history in AI, with classical work primarily focusing on executing symbolic rules as hand-crafted programs for machines to learn (Mccarthy, 1960). The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b; Lin et al., 2021c,a,b; Sakaguchi et al., 2020) test a model’s ability to choose the correct option given a context and a question; PTLMs have reached high performance on these benchmarks after fine-tuning. We differ from these benchmarks by focusing on robustness to linguistic variation via our linguistically-varied commonsense statements. RICA also challenges PTLMs on two evaluation tasks to better probe the PTLMs’ representations. Reasoning-focused Inference There have been Ablation of Novel Entities In order to ensure many benchma"
2021.findings-acl.131,2020.emnlp-main.85,0,0.0299064,"Missing"
2021.findings-acl.131,2020.acl-main.711,0,0.0188894,"(A) coal (B) hole (C) cd player (D) sunlight (×) (E) ink (√) Figure of speech (ink → writing → knowledge → light of wisdom) + Counterfactual (without burning) I have hundreds of legs, but I can only lean. What am I? (A) chair (×) (B) sock (C) pleopod (D) pants (E) broom (√) Counterfactual (many legs but cannot stand) + Metaphor (bristles) Figure 5: Case studies of the error by UnifiedQA-3B model on the test set of R IDDLE S ENSE. et al., 2018), pun generation (He et al., 2019; Luo et al., 2019), creative story generation, and humor detection (Weller and Seppi, 2019, 2020), sarcasm generation (Chakrabarty et al., 2020), etc. Riddling, as a way to use creative descriptions to query a common concept, are relatively underexplored. Previous works (Tan et al., 2016; Gonc¸alo Oliveira and Rodrigues, 2018) focus on the generation of riddles in specific languages and usually rely on language-specific features (e.g., decomposing a Chinese character into multiple smaller pieces). There is few datasets or public resources for studying riddles as a reasoning task, to the best of our knowledge. The proposed R ID DLE S ENSE is among the very first works connecting commonsense reasoning and computational creative, and pro"
2021.findings-acl.131,W19-2008,0,0.0445084,"Missing"
2021.findings-acl.131,P19-1388,0,0.0239915,"ich prevents them from effectively reasoning about the subtle differences between options. 5 Related Work Benchmarking Machine Common Sense The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNearRE (Xu et al., 2018) on mining spatial commonsense knowledge; DoQ (Elazar et al., 2019) and NumerSense (Lin et al., 2020a) on numerical common sense; CommonGen (Lin et al., 2020b) for generative commonsense reasoning, and many others; OpenCSR (Lin et al., 2021) and ProtoQA (Boratko et al., 2020) aim to test commonsense reasoning ability in an open-ended setting. CommonsenseQA (Talmor et al., 2019) has the same format as our proposed R IDDLE S ENSE, and both target general commonsense knowledge via multiple-choice question answering. However, CSQA focuses more on straightforward questions where the description of the answer concept is easy to understand and retrieval over Concept"
2021.findings-acl.131,2020.emnlp-main.99,1,0.92656,"we conclude that the dominant reasoning chains in RS are much more implicit, and consequently RS is more challenging to reason with using commonsense knowledge resources like ConceptNet. 4 Experiments We first introduce three types of popular baseline methods for commonsense reasoning (Section 4.1), then we present our main experimental results with analysis (Section 4.2), and finally show case studies for error analysis (Section 4.3). 1508 [CLS] question [SEP] choice1 [CLS] question [SEP] choice2 [CLS] question [SEP] choice3 LMs + Graph Reasoning Modules KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) are two typical graph-based language reasoning models. They both extract a schema graph from ConceptNet, i.e., a subgraph of ConceptNet consisting of Q-A paths in Figure 2, by incorporating them with a graph encoding module. They finally fuse the external commonsense knowledge with a text encoder (e.g., a pretrained LM). KagNet uses heuristics to prune irrelevant paths and then encode them with path-based LSTM and hierarchical attention to select the most important paths for improving commonsense reasoning. In contrast, the recent MHGRN explicitly encodes multi-hop paths at scale using graph"
2021.findings-acl.131,D18-1060,0,0.0343155,"Missing"
2021.findings-acl.131,W18-6604,0,0.063618,"Missing"
2021.findings-acl.131,N19-1172,0,0.0244825,"ing onions → taking off my skin. (D) plant (E) body (×) What is that which, though black itself, enlightens the world without burning? (A) coal (B) hole (C) cd player (D) sunlight (×) (E) ink (√) Figure of speech (ink → writing → knowledge → light of wisdom) + Counterfactual (without burning) I have hundreds of legs, but I can only lean. What am I? (A) chair (×) (B) sock (C) pleopod (D) pants (E) broom (√) Counterfactual (many legs but cannot stand) + Metaphor (bristles) Figure 5: Case studies of the error by UnifiedQA-3B model on the test set of R IDDLE S ENSE. et al., 2018), pun generation (He et al., 2019; Luo et al., 2019), creative story generation, and humor detection (Weller and Seppi, 2019, 2020), sarcasm generation (Chakrabarty et al., 2020), etc. Riddling, as a way to use creative descriptions to query a common concept, are relatively underexplored. Previous works (Tan et al., 2016; Gonc¸alo Oliveira and Rodrigues, 2018) focus on the generation of riddles in specific languages and usually rely on language-specific features (e.g., decomposing a Chinese character into multiple smaller pieces). There is few datasets or public resources for studying riddles as a reasoning task, to the best"
2021.findings-acl.131,N19-1423,0,0.63208,"an-written riddles and then aggregating, verifying, and correcting these examples using a combination of human rating and NLP tools to create a dataset consisting of 5.7k high-quality examples. Finally, we use Amazon Mechanical Turk to crowdsource quality distractors to create a challenging benchmark. We show that our riddle questions are more challenging than CommonsenseQA by analyzing graph-based statistics over ConceptNet (Speer et al., 2017), a large knowledge graph for common sense reasoning. Recent studies have demonstrated that finetuning large pretrained language models, such as BERT (Devlin et al., 2019a), RoBERTa, and ALBERT (Lan et al., 2020), can achieve strong results on current commonsense reasoning benchmarks. Developed on top of these language models, graph-based language reasoning models such as KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) show superior performance. Most recently, UnifiedQA (Khashabi et al., 2020) proposes to unify different QA tasks and train a text-to-text model for learning from all of them, which achieves state-of-the-art performance on many commonsense benchmarks. To provide a comprehensive benchmarking analysis, we systematically compare the above me"
2021.findings-acl.131,2020.findings-emnlp.171,0,0.235545,"g. In contrast, the recent MHGRN explicitly encodes multi-hop paths at scale using graph networks with relational attention, improving efficiency and performance over KagNet and other models. A unique merit of such graph-based models is their interpretibility due to the neural attention over the symbolic structures of KGs. SoftMax (1) Fine-Tuning BERT/RoBERTa/ALBERT, etc. (2) Symbolic KG-based Language Reasoning KagNet (Lin et al. 2019) & MHGRN (Feng et al. 2020) Input: question 
 A: choice1 B: choice2 C: choice 3 Output: choice2 (3) Fine-Tuning T5 with a text-to-text task format. UnifiedQA (Khashabi et al., 2020) Figure 3: Three types of baseline methods: 1) finetuning pre-trained LMs, 2) incorporating graph-based reasoner, 3) fine-tuning a unified text-to-text LM. 4.1 Baseline Methods Given a riddle question q, there are 5 different choices {c1 , . . . , c5 }, where only one of them is the correct choice and the others are distractors. The model needs to rank all choices and select the best one as the final answer. There are three major types of models for commonsense reasoning tasks in this format: 1) fine-tuning pretrained language models, 2) incorporating relevant knowledge graphs for reasoning, 3"
2021.findings-acl.131,D19-1282,1,0.916808,"out 0.7 ∼ 1.8 for CSQA. Thus, we conclude that the dominant reasoning chains in RS are much more implicit, and consequently RS is more challenging to reason with using commonsense knowledge resources like ConceptNet. 4 Experiments We first introduce three types of popular baseline methods for commonsense reasoning (Section 4.1), then we present our main experimental results with analysis (Section 4.2), and finally show case studies for error analysis (Section 4.3). 1508 [CLS] question [SEP] choice1 [CLS] question [SEP] choice2 [CLS] question [SEP] choice3 LMs + Graph Reasoning Modules KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) are two typical graph-based language reasoning models. They both extract a schema graph from ConceptNet, i.e., a subgraph of ConceptNet consisting of Q-A paths in Figure 2, by incorporating them with a graph encoding module. They finally fuse the external commonsense knowledge with a text encoder (e.g., a pretrained LM). KagNet uses heuristics to prune irrelevant paths and then encode them with path-based LSTM and hierarchical attention to select the most important paths for improving commonsense reasoning. In contrast, the recent MHGRN explicitly encodes multi-h"
2021.findings-acl.131,2020.emnlp-main.557,1,0.892232,"Missing"
2021.findings-acl.131,2021.naacl-main.366,1,0.550311,"se reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNearRE (Xu et al., 2018) on mining spatial commonsense knowledge; DoQ (Elazar et al., 2019) and NumerSense (Lin et al., 2020a) on numerical common sense; CommonGen (Lin et al., 2020b) for generative commonsense reasoning, and many others; OpenCSR (Lin et al., 2021) and ProtoQA (Boratko et al., 2020) aim to test commonsense reasoning ability in an open-ended setting. CommonsenseQA (Talmor et al., 2019) has the same format as our proposed R IDDLE S ENSE, and both target general commonsense knowledge via multiple-choice question answering. However, CSQA focuses more on straightforward questions where the description of the answer concept is easy to understand and retrieval over ConceptNet, while RS makes use of riddle questions to test higher-order commonsense reasoning ability. More detailed comparisions between them are in Section 3, which shows that the"
2021.findings-acl.131,2020.findings-emnlp.165,1,0.94194,"ue that future research should aim to address the creative use of language in commonsense reasoning and general understanding of language, as creativity is a critical feature of natural language. We list several promising directions as follows. First of all, we should mine (semi-)structured knowledge of metaphors, so that concepts can connect via metaphorical links (e.g., “tail” → “thread”). Second, to prevent false inferences, we need more complete, precise commonsense knowledge of concepts. For example, in Figure 5, a model should know a chair only has exactly four legs instead of hundreds (Lin et al., 2020a); ink can be black or red, but it won’t change over time. However, current KGs only have (leg, PartOf, chair) and (ink, HasProperty, black/red). In addition, the reasoning methods should incorporate more symbolic logic rules, so that the multi-hop conditions and counterfactual “but-no” negations will be handled better. Finally, we think the graphaugmented methods should be improved to compare multiple options in a schema graph, e.g., QAGNN (Yasunaga et al., 2021). Both K AG N ET and MHGRN consider only a single option at a time which prevents them from effectively reasoning about the subtle"
2021.findings-acl.131,2021.ccl-1.108,0,0.0689627,"Missing"
2021.findings-acl.131,W94-0310,0,0.390225,"such as Wikipedia and Wiktionary. A few recent methods also aim to generate relevant triples via language generation models so that the context graph is more beneficial for reasoning (Wang et al., 2020; Yan et al., 2020). Our experiments in this paper aim to compare the most typical and popular methods which have open-source implementations, which we believe are beneficial for understanding the limitation of these methods in higherorder commonsense reasoning — R IDDLE S ENSE. Computational Creativity and NLP Creativity has been seen as a central property of the human use of natural language (McDonald and Busa, 1994). Text should not be always taken at face value, however, higher-order use of language and figurative devices such as metaphor can communicate richer meanings and needs deeper reading and more complicated reasoning skills (Veale, 2011). Recent works on processing language with creative use focus on metaphor detection (Gao 1511 Riddle Questions Choices (√=truth; ×=model’s choice) Explanation I am black when you buy me, red when you use me. When I turn white, you know it's time to throw me away. What am I? (A) charcoal (√) (B) rose flower (C) ink (×) (D) fruit (E) shoe Describing multiple condit"
2021.findings-acl.131,D18-1260,0,0.0599827,"Missing"
2021.findings-acl.131,D19-1410,0,0.0146029,"nd misspelled words, we process riddles through careful data cleaning as well as human verification. First, we use an open-source tool for detecting typos3 and then refine the sentences. Then we continuously sample (riddle, answer) pairs and recognize errors, for which we iteratively improve our program with a set of conditions to filter out noisy examples that are not readable or have ambiguous answers. Also, we merge the riddles from different sources while removing duplicate riddle questions with similar answers. For detecting duplicate riddles with minor word changes, we use SentenceBERT (Reimers and Gurevych, 2019) to find clusters with high cosine similarities. 2 We use “riddle” and “riddle-style commonsense question” interchangeably in this paper. 1505 3 github.com/phatpiglet/autocorrect 2.2 Distractor Collection from AMT We consider a multi-choice question answering format rather than the open-ended format, as it is easier to meaningfully compare the performance of different models in a more controlled manner — there is a limited range of of options. For such a dataset, given a riddle-style question and 5 answer options, the model should select the best one as the predicted answer. This format offers"
2021.findings-acl.131,2020.acl-main.240,0,0.020062,"ch serve as an effective proxy to analyze the differences between the two datasets. smaller than CSQA, we argue that RS is complementary to the CSQA dataset and introduces novel challenges for the commonsense reasoning community. As they share the same format, we can test different methods by training on either CSQAonly, RS-only, or the concatenation of CSQA and RS, as we show later in Section 4. Moreover, there is a greater number of long questions (i.e., containing more than 20 words) in RS than in CSQA. Additionally, we find that RS questions have a lower normalized pseudolikelihood (PLL) (Salazar et al., 2020), a proxy of estimating sentence probability, suggesting that RS questions are more puzzling (i.e., the words are less frequently co-occurring). We also use a RoBERTa model fine-tuned on MNLI (Williams et al., 2018) to perform natural language inference between CSQA/RS questions and their answers. There is a much greater proportion of questions in RS that have conflicting relations with their correct answers than compared to CSQA. This is indicative of RS’s complexity due to the selfcontradictory and perplexing nature of riddles. Interestingly, we also find that although there are about twice"
2021.findings-acl.131,D19-1454,0,0.0624668,"Missing"
2021.findings-acl.131,N19-1421,0,0.48557,"ge gap between the bestsupervised model and human performance — suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems. 1 D B I have five fingers, but I am not alive. What am I? My life can be measured in hours. I serve by being devoured. Thin, I am quick; Fat, I am slow. Wind is my foe. What am I? (A) piano (B) computer (C) glove (D) claw (E) hand (A) paper (B) candle (C) lamp (D) clock (E) worm RiddleSense C Figure 1: The top example is a trivial commonsense question from the CommonsenseQA (Talmor et al., 2019) dataset. The two bottom examples are from our proposed R IDDLE S ENSE challenge. The right-bottom question is a descriptive riddle that implies multiple commonsense facts about candle, and it needs understanding of figurative language such as metaphor; The left-bottom one additionally needs counterfactual reasoning ability to address the ‘but-no’ cues. These riddle-style commonsense questions require NLU systems to have higher-order reasoning skills with the understanding of creative language use. Introduction “ The essence of a riddle is to express true facts under impossible combinations.”"
2021.findings-acl.131,D16-1081,0,0.0258132,"ning) I have hundreds of legs, but I can only lean. What am I? (A) chair (×) (B) sock (C) pleopod (D) pants (E) broom (√) Counterfactual (many legs but cannot stand) + Metaphor (bristles) Figure 5: Case studies of the error by UnifiedQA-3B model on the test set of R IDDLE S ENSE. et al., 2018), pun generation (He et al., 2019; Luo et al., 2019), creative story generation, and humor detection (Weller and Seppi, 2019, 2020), sarcasm generation (Chakrabarty et al., 2020), etc. Riddling, as a way to use creative descriptions to query a common concept, are relatively underexplored. Previous works (Tan et al., 2016; Gonc¸alo Oliveira and Rodrigues, 2018) focus on the generation of riddles in specific languages and usually rely on language-specific features (e.g., decomposing a Chinese character into multiple smaller pieces). There is few datasets or public resources for studying riddles as a reasoning task, to the best of our knowledge. The proposed R ID DLE S ENSE is among the very first works connecting commonsense reasoning and computational creative, and provides a large dataset to train and evaluate models for answering riddle questions. Acknowledgements This research is supported in part by the Of"
2021.findings-acl.131,P11-1029,0,0.0210281,"paper aim to compare the most typical and popular methods which have open-source implementations, which we believe are beneficial for understanding the limitation of these methods in higherorder commonsense reasoning — R IDDLE S ENSE. Computational Creativity and NLP Creativity has been seen as a central property of the human use of natural language (McDonald and Busa, 1994). Text should not be always taken at face value, however, higher-order use of language and figurative devices such as metaphor can communicate richer meanings and needs deeper reading and more complicated reasoning skills (Veale, 2011). Recent works on processing language with creative use focus on metaphor detection (Gao 1511 Riddle Questions Choices (√=truth; ×=model’s choice) Explanation I am black when you buy me, red when you use me. When I turn white, you know it's time to throw me away. What am I? (A) charcoal (√) (B) rose flower (C) ink (×) (D) fruit (E) shoe Describing multiple conditions of a common object. Only charcoal applies to all. I have a long tail that I let fly. Every time I go through a gap, I leave a bit of my tail in the trap. What am I? (A) monkey (B) basketball (C) fishing pole (×) (D) comet (E) need"
2021.findings-acl.131,2020.findings-emnlp.369,1,0.839231,"reasoning methods that are popular in many benchmarks: fine-tuning pretrained LMs (Devlin et al., 2019a; Liu et al., 2019; Lan et al., 2020), graph-based reasoning with external KGs (Lin et al., 2019; Feng et al., 2020), and finetuning unified text-to-text QA models (Khashabi et al., 2020). Apart from ConceptNet, There are also some methods (Lv et al., 2020; Xu et al., 2020) using additional knowledge resources such as Wikipedia and Wiktionary. A few recent methods also aim to generate relevant triples via language generation models so that the context graph is more beneficial for reasoning (Wang et al., 2020; Yan et al., 2020). Our experiments in this paper aim to compare the most typical and popular methods which have open-source implementations, which we believe are beneficial for understanding the limitation of these methods in higherorder commonsense reasoning — R IDDLE S ENSE. Computational Creativity and NLP Creativity has been seen as a central property of the human use of natural language (McDonald and Busa, 1994). Text should not be always taken at face value, however, higher-order use of language and figurative devices such as metaphor can communicate richer meanings and needs deeper re"
2021.findings-acl.131,D19-1372,0,0.0235236,"lack itself, enlightens the world without burning? (A) coal (B) hole (C) cd player (D) sunlight (×) (E) ink (√) Figure of speech (ink → writing → knowledge → light of wisdom) + Counterfactual (without burning) I have hundreds of legs, but I can only lean. What am I? (A) chair (×) (B) sock (C) pleopod (D) pants (E) broom (√) Counterfactual (many legs but cannot stand) + Metaphor (bristles) Figure 5: Case studies of the error by UnifiedQA-3B model on the test set of R IDDLE S ENSE. et al., 2018), pun generation (He et al., 2019; Luo et al., 2019), creative story generation, and humor detection (Weller and Seppi, 2019, 2020), sarcasm generation (Chakrabarty et al., 2020), etc. Riddling, as a way to use creative descriptions to query a common concept, are relatively underexplored. Previous works (Tan et al., 2016; Gonc¸alo Oliveira and Rodrigues, 2018) focus on the generation of riddles in specific languages and usually rely on language-specific features (e.g., decomposing a Chinese character into multiple smaller pieces). There is few datasets or public resources for studying riddles as a reasoning task, to the best of our knowledge. The proposed R ID DLE S ENSE is among the very first works connecting com"
2021.findings-acl.131,2021.naacl-main.45,0,0.0302046,"commonsense knowledge of concepts. For example, in Figure 5, a model should know a chair only has exactly four legs instead of hundreds (Lin et al., 2020a); ink can be black or red, but it won’t change over time. However, current KGs only have (leg, PartOf, chair) and (ink, HasProperty, black/red). In addition, the reasoning methods should incorporate more symbolic logic rules, so that the multi-hop conditions and counterfactual “but-no” negations will be handled better. Finally, we think the graphaugmented methods should be improved to compare multiple options in a schema graph, e.g., QAGNN (Yasunaga et al., 2021). Both K AG N ET and MHGRN consider only a single option at a time which prevents them from effectively reasoning about the subtle differences between options. 5 Related Work Benchmarking Machine Common Sense The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNe"
2021.findings-acl.131,D18-1009,0,0.0461187,"orate more symbolic logic rules, so that the multi-hop conditions and counterfactual “but-no” negations will be handled better. Finally, we think the graphaugmented methods should be improved to compare multiple options in a schema graph, e.g., QAGNN (Yasunaga et al., 2021). Both K AG N ET and MHGRN consider only a single option at a time which prevents them from effectively reasoning about the subtle differences between options. 5 Related Work Benchmarking Machine Common Sense The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNearRE (Xu et al., 2018) on mining spatial commonsense knowledge; DoQ (Elazar et al., 2019) and NumerSense (Lin et al., 2020a) on numerical common sense; CommonGen (Lin et al., 2020b) for generative commonsense reasoning, and many others; OpenCSR (Lin et al., 2021) and ProtoQA (Boratko et al., 2020) aim to test commonsense reasoning ability in an"
2021.findings-acl.131,P19-1472,0,0.0241738,"o that the multi-hop conditions and counterfactual “but-no” negations will be handled better. Finally, we think the graphaugmented methods should be improved to compare multiple options in a schema graph, e.g., QAGNN (Yasunaga et al., 2021). Both K AG N ET and MHGRN consider only a single option at a time which prevents them from effectively reasoning about the subtle differences between options. 5 Related Work Benchmarking Machine Common Sense The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNearRE (Xu et al., 2018) on mining spatial commonsense knowledge; DoQ (Elazar et al., 2019) and NumerSense (Lin et al., 2020a) on numerical common sense; CommonGen (Lin et al., 2020b) for generative commonsense reasoning, and many others; OpenCSR (Lin et al., 2021) and ProtoQA (Boratko et al., 2020) aim to test commonsense reasoning ability in an open-ended setting. CommonsenseQA"
2021.findings-acl.131,2020.lrec-1.753,0,0.0803428,"Missing"
2021.findings-acl.131,N18-1101,0,0.0230305,"ning community. As they share the same format, we can test different methods by training on either CSQAonly, RS-only, or the concatenation of CSQA and RS, as we show later in Section 4. Moreover, there is a greater number of long questions (i.e., containing more than 20 words) in RS than in CSQA. Additionally, we find that RS questions have a lower normalized pseudolikelihood (PLL) (Salazar et al., 2020), a proxy of estimating sentence probability, suggesting that RS questions are more puzzling (i.e., the words are less frequently co-occurring). We also use a RoBERTa model fine-tuned on MNLI (Williams et al., 2018) to perform natural language inference between CSQA/RS questions and their answers. There is a much greater proportion of questions in RS that have conflicting relations with their correct answers than compared to CSQA. This is indicative of RS’s complexity due to the selfcontradictory and perplexing nature of riddles. Interestingly, we also find that although there are about twice as many examples in CSQA as RS, there are more distinct words in the questions and answer choices of RS than CSQA, suggesting that RS covers more diverse topics than CSQA. 3.2 3.1 Key Statistics Table 1 presents the"
2021.findings-acl.131,P18-2016,1,0.821127,"K AG N ET and MHGRN consider only a single option at a time which prevents them from effectively reasoning about the subtle differences between options. 5 Related Work Benchmarking Machine Common Sense The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning: SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), CODAH (Chen et al., 2019), aNLI (Bhagavatula et al., 2020) for situation-based reasoning; Physical IQA (Bisk et al., 2020) on physical knowledge; Social IQA (Sap et al., 2019) on social psychology knowledge; LocatedNearRE (Xu et al., 2018) on mining spatial commonsense knowledge; DoQ (Elazar et al., 2019) and NumerSense (Lin et al., 2020a) on numerical common sense; CommonGen (Lin et al., 2020b) for generative commonsense reasoning, and many others; OpenCSR (Lin et al., 2021) and ProtoQA (Boratko et al., 2020) aim to test commonsense reasoning ability in an open-ended setting. CommonsenseQA (Talmor et al., 2019) has the same format as our proposed R IDDLE S ENSE, and both target general commonsense knowledge via multiple-choice question answering. However, CSQA focuses more on straightforward questions where the description of"
2021.findings-acl.322,P19-1470,0,0.244036,"edge capacity by multi-task learning, (2) transferability by transfer learning and (3) induction by controlled low-resource learning. Introduction Large-scale commonsense knowledge graphs (CKGs), like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), store structured knowledge that can benefit various knowledge-driven applications. Given the usefulness of CKGs, but also their inability to flexibly provide information, (Paulheim, 2018), recent work has paid much attention to populating CKGs with commonsense knowledge mined from pretrained language models (LMs) (Wang et al., 2020c; Bosselut et al., 2019). Enhancing the knowledge of CKGs is essential to support reasoning on downstream tasks (Talmor et al., 2019; Wang et al., 2020b; Young et al., 2018). The task of completing CKGs has typically been posed as commonsense knowledge inference, where the goal is to predict the object of a fact triplet, given its subject and a relation (predicate) (Petroni 1 The code is avaiable at https://github.com/ wangpf3/LM-for-CommonsenseInference. et al., 2019; Bosselut et al., 2019). Commonsense inference techniques, such as COMET (Bosselut et al., 2019), typically fine-tune an LM, like GPT (Radford et al.,"
2021.findings-acl.322,2020.emnlp-main.524,0,0.0203479,"r, o ) before the input tuple. ent metrics and choose to report METEOR in the main text and other metrics in the appendix. 2.4 Connections to Prior Studies Earlier works (Li et al., 2016; Jastrzebski et al., 2018; Davison et al., 2019) poses the CKG completion task as triplet classification, where the goal is to score the plausibility of a complete triplet. COMET (Bosselut et al., 2019) is the first to cast this task as commonsense inference with LMs. Follow-up contributions utilize COMET as a commonsense provider in various downstream tasks (Bosselut and Choi, 2021; Ammanabrolu et al., 2021; Chakrabarty et al., 2020), thus providing evidence for LM’s generalization to previously unseen scenarios. Further efforts include Hwang et al. (2020), which show that the quality of the training triplets is a key factor of adapting LMs, and (Da et al., 2021), which investigates how to learn COMET in a few-shot learning setting. Meanwhile, the study by Wang et al. (2020a) indicates the limited generalization of COMET. Ma et al. (2021) also adapt LMs simultaneously on multiple CKGs, albeit their goal is to improve downstream performance rather than CKG inference. In this paper, we aim to provide a more comprehensive st"
2021.findings-acl.322,2020.emnlp-main.634,0,0.0258343,"ent adaptation of LMs (Da et al., 2021). Note that we do not explicitly provide the LMs with the information about the source CKG of the triplet as input (e.g., prepending a related special token to the triplet). Adapting LMs with Commonense Knowledge The training objectives for adapting LMs is to maximize the probability of generating the object phrase xo given the tuple (xs , xr ). During inference, we adopt greedy decoding to obtain the predicted object from the adapted LM. There have been various techniques developed for adapting pretrained LMs to downstream tasks (Howard and Ruder, 2018; Chen et al., 2020). Moreover, previously only the vanilla Fine-tuning, i.e., updating the whole LM architecture during training, has been employed to adapt LMs for commonsense inference (Bosselut et al., 2019; Hwang et al., 2020; Da et al., 2021). To obtain comprehensive results that are not specific to one particular way of fine-tuning, here we investigate two more alternatives, each of which has their own advantage when considered in different contexts. While a set of pretrained LMs exists, we adopt a widely used generative model, GPT2 (Radford et al., 2019), as our baseline LM. The investigation of other gen"
2021.findings-acl.322,P16-1137,0,0.187611,"(go to a concert, MotivatedByGoal) as input, and listen to music as output. Assuming that a CKG is given, the goal is to leverage the commonsense triplets in the CKG as training examples to adapt the LM for commonsense inference. 2.2 CKG Datasets We consider three large and popular CKGs, with different foci:(1) ConceptNet’s broad set of commonsense knowledge includes taxonomic (e.g., IsA), utility (e.g., UsedFor), and temporal knowledge (e.g., HasPrerequisite). It combines crowdsourced knowledge with that from existing sources, such as WordNet. We use its ConceptNet-100K subset, collected by Li et al. (2016). (2) TupleKB (Dalvi Mishra et al., 2017) focuses on scientific commonsense knowledge like (salt, dissolve in, water). It is constructed through an information extraction pipeline. (3) ATOMIC (Sap et al., 2019) has social commonsense knowledge about causes and effects of everyday events, and mental states (e.g., xIntent) of their participants. It is created by crowdsourcing. As indicated by Jastrzebski et al. (2018), a large proportion of the subjects in the test set of ConceptNet-100K overlap with its training set, while TupleKB does not provide an official split. Thus, we (re-)split these tw"
2021.findings-acl.322,2021.acl-long.353,0,0.0581453,"Missing"
2021.findings-acl.322,W04-1013,0,0.142556,"Missing"
2021.findings-acl.322,P02-1040,0,0.114263,"Missing"
2021.findings-acl.322,D19-1250,0,0.0462078,"Missing"
2021.findings-acl.322,D19-1410,0,0.0291597,"fine-tuning and in-context learning (Brown et al., 2020), this technique (Gao et al., 2020) adds a demonstration to each input as additional context and fine-tunes the whole LM as usual. Incorporating demonstrations is shown to boost performance when the amount of training data is extremely limited. In our case, a demon0 0 stration is a top-1 training triplet (s , r, o ), ranked according to the cosine similarity between the embedding of the input tuple (s, r) and the embeddings of the training tuples with the same relation type r. The tuple embeddings are given by a pretrained Sentence-BERT (Reimers and Gurevych, 2019). For instance, a demonstration (go to restaurant, UsedFor, eat out) would be added before the input (go to pub, UsedFor). With the demonstrated triplets, the LM could learn to understand the schema of the CKG instead of simply learning the knowledge from the training data. Commonsense Inference with LMs Given a training triplet (s,r,o), we represent s and o as sequences of tokens, xs and xo , which is trivial given that they are already expressed as phrases. As for the relaAdapter Tuning (AT) Unlike fine-tuning, adapter tuning (Houlsby et al., 2019) fixes the entire LM and adds one trainable"
2021.findings-acl.322,N19-1421,0,0.0246985,"low-resource learning. Introduction Large-scale commonsense knowledge graphs (CKGs), like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), store structured knowledge that can benefit various knowledge-driven applications. Given the usefulness of CKGs, but also their inability to flexibly provide information, (Paulheim, 2018), recent work has paid much attention to populating CKGs with commonsense knowledge mined from pretrained language models (LMs) (Wang et al., 2020c; Bosselut et al., 2019). Enhancing the knowledge of CKGs is essential to support reasoning on downstream tasks (Talmor et al., 2019; Wang et al., 2020b; Young et al., 2018). The task of completing CKGs has typically been posed as commonsense knowledge inference, where the goal is to predict the object of a fact triplet, given its subject and a relation (predicate) (Petroni 1 The code is avaiable at https://github.com/ wangpf3/LM-for-CommonsenseInference. et al., 2019; Bosselut et al., 2019). Commonsense inference techniques, such as COMET (Bosselut et al., 2019), typically fine-tune an LM, like GPT (Radford et al., 2018), over the training set from a single CKG. While such methods are able to dynamically enhance the compl"
2021.findings-acl.322,2020.emnlp-main.51,1,0.881881,"izability: (1) knowledge capacity by multi-task learning, (2) transferability by transfer learning and (3) induction by controlled low-resource learning. Introduction Large-scale commonsense knowledge graphs (CKGs), like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), store structured knowledge that can benefit various knowledge-driven applications. Given the usefulness of CKGs, but also their inability to flexibly provide information, (Paulheim, 2018), recent work has paid much attention to populating CKGs with commonsense knowledge mined from pretrained language models (LMs) (Wang et al., 2020c; Bosselut et al., 2019). Enhancing the knowledge of CKGs is essential to support reasoning on downstream tasks (Talmor et al., 2019; Wang et al., 2020b; Young et al., 2018). The task of completing CKGs has typically been posed as commonsense knowledge inference, where the goal is to predict the object of a fact triplet, given its subject and a relation (predicate) (Petroni 1 The code is avaiable at https://github.com/ wangpf3/LM-for-CommonsenseInference. et al., 2019; Bosselut et al., 2019). Commonsense inference techniques, such as COMET (Bosselut et al., 2019), typically fine-tune an LM, l"
2021.findings-acl.322,2020.findings-emnlp.369,1,0.920725,"izability: (1) knowledge capacity by multi-task learning, (2) transferability by transfer learning and (3) induction by controlled low-resource learning. Introduction Large-scale commonsense knowledge graphs (CKGs), like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), store structured knowledge that can benefit various knowledge-driven applications. Given the usefulness of CKGs, but also their inability to flexibly provide information, (Paulheim, 2018), recent work has paid much attention to populating CKGs with commonsense knowledge mined from pretrained language models (LMs) (Wang et al., 2020c; Bosselut et al., 2019). Enhancing the knowledge of CKGs is essential to support reasoning on downstream tasks (Talmor et al., 2019; Wang et al., 2020b; Young et al., 2018). The task of completing CKGs has typically been posed as commonsense knowledge inference, where the goal is to predict the object of a fact triplet, given its subject and a relation (predicate) (Petroni 1 The code is avaiable at https://github.com/ wangpf3/LM-for-CommonsenseInference. et al., 2019; Bosselut et al., 2019). Commonsense inference techniques, such as COMET (Bosselut et al., 2019), typically fine-tune an LM, l"
2021.findings-acl.354,2020.emnlp-main.99,1,0.725326,"s (KGs) can provide structured commonsense facts (edges) of the form (concept1, relation, concept2) (Speer et al., 2017). Hence, many recent CSR models augment the PLM with a KG, 1 Our code and data can be found at https://github. com/INK-USC/HGN. allowing such KG-augmented models to make predictions via multi-hop reasoning over the KG (Lin et al., 2019; Bosselut and Choi, 2019). Despite the growing success of KG-augmented models, obtaining helpful KG facts for a given task instance remains challenging. Existing models assume using either KG-extracted edges (Lin et al., 2019; Ma et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), PLM-generated edges (to address KG edge sparsity) (Bosselut and Choi, 2019), or a late fusion of both (Wang et al., 2020) is sufficient. Both extraction and generation can produce unhelpful edges, so the model must decide which edges to focus on during reasoning. Since extracted and generated edges are derived from the same set of concepts (nodes), modeling the interactions between extracted and generated edges jointly within a shared KG structure could provide stronger signal for identifying contextually relevant edges. However, current models do not leverage this in"
2021.findings-acl.354,2020.findings-emnlp.171,0,0.104013,"20) learns a path generator from paths collected through random walks on the KG. The learned generator is used to generate paths connecting question and answer concepts. g is calculated as the concatenation of the pooled vector over the generated paths and the pooled vector over the extracted paths. Our Model’s Variants. As described in §3.2, the edge embedding can be computed either as a relation embedding or a path embedding. We name these two variants as HGN (w/ RelGen edges) and HGN (w/ PathGen edges) respectively. Methods ALBERT+DESC-KCR (Xu et al., 2020) ALBERT+KD ALBERT+KCR Unified QA (Khashabi et al., 2020) ALBERT+KRD T5-3B (Raffel et al., 2020) ALBERT+HGN (w/ RelGen edges) TeGBERT ALBERT+PathGenerator (Wang et al., 2020) ALBERT (Lan et al., 2020) Single Ensemble 80.7 80.3 79.5 79.1 78.4 78.1 77.3 76.8 75.6 - 83.3 80.9 80.0 78.2 76.5 Table 2: Leaderboard of CommonsenseQA. HGN ranks first among comparable systems, especially achieving remarkable improvement over PathGenerator (Wang et al., 2020). 4.3 Results Performance Comparisons. Tables 1, 3, 4 show performance comparisons between our models and baseline models on CommonsenseQA, CODAH, OpenBookQA and QASC. We clearly find that models with stro"
2021.findings-acl.354,D19-1282,1,0.87671,"to humans, it is rarely stated in natural language (Gunning, 2018). This makes it hard for neural pre-trained language models (PLMs) (Devlin et al., 2019) to learn commonsense knowledge from corpora alone (Marcus, 2018). Unlike raw text corpora, knowledge graphs (KGs) can provide structured commonsense facts (edges) of the form (concept1, relation, concept2) (Speer et al., 2017). Hence, many recent CSR models augment the PLM with a KG, 1 Our code and data can be found at https://github. com/INK-USC/HGN. allowing such KG-augmented models to make predictions via multi-hop reasoning over the KG (Lin et al., 2019; Bosselut and Choi, 2019). Despite the growing success of KG-augmented models, obtaining helpful KG facts for a given task instance remains challenging. Existing models assume using either KG-extracted edges (Lin et al., 2019; Ma et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), PLM-generated edges (to address KG edge sparsity) (Bosselut and Choi, 2019), or a late fusion of both (Wang et al., 2020) is sufficient. Both extraction and generation can produce unhelpful edges, so the model must decide which edges to focus on during reasoning. Since extracted and generated edges are derived"
2021.findings-acl.354,2021.ccl-1.108,0,0.0193703,"Missing"
2021.findings-acl.354,D19-6003,0,0.117283,", knowledge graphs (KGs) can provide structured commonsense facts (edges) of the form (concept1, relation, concept2) (Speer et al., 2017). Hence, many recent CSR models augment the PLM with a KG, 1 Our code and data can be found at https://github. com/INK-USC/HGN. allowing such KG-augmented models to make predictions via multi-hop reasoning over the KG (Lin et al., 2019; Bosselut and Choi, 2019). Despite the growing success of KG-augmented models, obtaining helpful KG facts for a given task instance remains challenging. Existing models assume using either KG-extracted edges (Lin et al., 2019; Ma et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), PLM-generated edges (to address KG edge sparsity) (Bosselut and Choi, 2019), or a late fusion of both (Wang et al., 2020) is sufficient. Both extraction and generation can produce unhelpful edges, so the model must decide which edges to focus on during reasoning. Since extracted and generated edges are derived from the same set of concepts (nodes), modeling the interactions between extracted and generated edges jointly within a shared KG structure could provide stronger signal for identifying contextually relevant edges. However, current models do n"
2021.findings-acl.354,D18-1260,0,0.0462142,"ned to generate embeddings for the subgraph’s missing edges to form a “hybrid” graph, then reason over the graph (to update model parameters) while filtering out context-irrelevant edges. HGN achieves this primarily through edge reweighting, which downweights irrelevant edges, and edge-weighted message passing, which attenuates irrelevant edges’ impact on reasoning. Our extensive experiments demonstrate that HGN improves performance over all baselines across four CSR benchmarks. In particular, among comparable methods, HGN ranks first on the CommonsenseQA (Talmor et al., 2019) and OpenbookQA (Mihaylov et al., 2018) leaderboards. Plus, our user studies show that humans find HGNfiltered edges to be more valid and helpful than the heuristically extracted edges used in prior work. 2 Problem Statement We consider CSR tasks, like question answering (QA), which can benefit from commonsense KGs. To solve CSR tasks, we focus on KG-augmented models, where a PLM is augmented with a commonsense KG. Given a CSR task, let x be the task’s text input, f be the model, and f (x) be the model output. We denote a KG as G = (V, R, E). V, R, and E are the sets of nodes (concepts), relations, and edges (facts), respectively,"
2021.findings-acl.354,P15-1016,0,0.0244378,"s spred = [v˜i , r˜, v˜j ]. Let [x1 , x2 , ..., xT ] = [sinput , spred ]. The edge embedding for (vi , vj ) is then computed as fgen (vi , vj ) = 1 PT i=1 hi , where hi is the GPT-2 hidden state for T xi . See Appendix §A for more details. Alternatively, we consider another edge generation approach proposed by Wang et al. (2020). Here, fgen (·, ·) is trained to generate a relational path connecting vi to vj , then pool the path into an edge embedding. The rationale for this approach is that such paths have been shown to contain useful semantic information about the relation between vi and vj (Neelakantan et al., 2015; Das et al., 2017; Wang et al., 2020). Adjacency Matrix. Before edge generation, G 0 has binary adjacency matrix Aextract , where A(i,j) = 1 ⇔ ∃r, s.t. (vi , r, vj ) ∈ E. After getting embeddings for all edges (vi , vj ) ∈ E 0 , Aextract becomes A0 , a denser binary adjacency matrix in which A0(i,j) = 1 ⇔ (vi , vj ) ∈ E 0 . 3.3 Hybrid Graph Reasoning The procedure described in §3.2 yields a hybrid graph, containing unweighted edges between all question-answer node pairs. Constructing this hybrid graph may improve edge recall, but does not address precision. Some edges in the initial hybrid gr"
2021.findings-acl.354,D19-1250,0,0.0543541,"Missing"
2021.findings-acl.354,D19-1242,0,0.0128299,"passing and edge reweighting. 4045 Graph Structure Learning. Instead of assuming a fixed graph structure, a number of graph models learn the graph structure with respect to the downstream task. Some models learn to discretely select edges for the graph (i.e., hard pruning). Kipf et al. (2018) and Franceschi et al. (2019) sample the graph structure from a predicted probabilistic distribution with differentiable approximations. Norcliffe-Brown et al. (2018) calculate the relatedness between any pair of nodes and only keep the top-k strongest connections for each node to construct the edge set. Sun et al. (2019) start with a small graph and iteratively expand it with retrieving operations. Others learn to reweight edges in a fully connected graph (i.e., soft pruning). Jiang et al. (2019) and Yu et al. (2019) propose heuristics for regularizing edge weights. Hu et al. (2019) use the question embedding to help predict edge weights. Unlike other edge reweighting models, HGN operates over a hybrid graph of both extracted and generated edges, while updating edge weights with respect to node, edge, and text features. 6 Conclusion In this paper, we propose HGN, a KG-augmented model for CSR. To address KG ed"
2021.findings-acl.354,N19-1421,0,0.0388126,"an extracted KG subgraph, HGN is trained to generate embeddings for the subgraph’s missing edges to form a “hybrid” graph, then reason over the graph (to update model parameters) while filtering out context-irrelevant edges. HGN achieves this primarily through edge reweighting, which downweights irrelevant edges, and edge-weighted message passing, which attenuates irrelevant edges’ impact on reasoning. Our extensive experiments demonstrate that HGN improves performance over all baselines across four CSR benchmarks. In particular, among comparable methods, HGN ranks first on the CommonsenseQA (Talmor et al., 2019) and OpenbookQA (Mihaylov et al., 2018) leaderboards. Plus, our user studies show that humans find HGNfiltered edges to be more valid and helpful than the heuristically extracted edges used in prior work. 2 Problem Statement We consider CSR tasks, like question answering (QA), which can benefit from commonsense KGs. To solve CSR tasks, we focus on KG-augmented models, where a PLM is augmented with a commonsense KG. Given a CSR task, let x be the task’s text input, f be the model, and f (x) be the model output. We denote a KG as G = (V, R, E). V, R, and E are the sets of nodes (concepts), relat"
2021.findings-acl.354,2020.findings-emnlp.369,1,0.105762,"odels augment the PLM with a KG, 1 Our code and data can be found at https://github. com/INK-USC/HGN. allowing such KG-augmented models to make predictions via multi-hop reasoning over the KG (Lin et al., 2019; Bosselut and Choi, 2019). Despite the growing success of KG-augmented models, obtaining helpful KG facts for a given task instance remains challenging. Existing models assume using either KG-extracted edges (Lin et al., 2019; Ma et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), PLM-generated edges (to address KG edge sparsity) (Bosselut and Choi, 2019), or a late fusion of both (Wang et al., 2020) is sufficient. Both extraction and generation can produce unhelpful edges, so the model must decide which edges to focus on during reasoning. Since extracted and generated edges are derived from the same set of concepts (nodes), modeling the interactions between extracted and generated edges jointly within a shared KG structure could provide stronger signal for identifying contextually relevant edges. However, current models do not leverage this information. 4038 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4038–4051 August 1–6, 2021. ©2021 Association for"
2021.findings-acl.354,2021.naacl-main.45,0,0.147473,"structured commonsense facts (edges) of the form (concept1, relation, concept2) (Speer et al., 2017). Hence, many recent CSR models augment the PLM with a KG, 1 Our code and data can be found at https://github. com/INK-USC/HGN. allowing such KG-augmented models to make predictions via multi-hop reasoning over the KG (Lin et al., 2019; Bosselut and Choi, 2019). Despite the growing success of KG-augmented models, obtaining helpful KG facts for a given task instance remains challenging. Existing models assume using either KG-extracted edges (Lin et al., 2019; Ma et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), PLM-generated edges (to address KG edge sparsity) (Bosselut and Choi, 2019), or a late fusion of both (Wang et al., 2020) is sufficient. Both extraction and generation can produce unhelpful edges, so the model must decide which edges to focus on during reasoning. Since extracted and generated edges are derived from the same set of concepts (nodes), modeling the interactions between extracted and generated edges jointly within a shared KG structure could provide stronger signal for identifying contextually relevant edges. However, current models do not leverage this information. 4038 Findings"
2021.findings-emnlp.302,N19-1423,0,0.0736118,"Missing"
2021.findings-emnlp.302,2020.findings-emnlp.372,0,0.0320737,"n various distillation approaches (Q5). To this end, we use Conventional KD (Hinton et al., 2015) as a base distillation approach for MSD. In addition, we include several distillation baselines including Conventional KD (Hinton et al., 2015), FitNet (Romero et al., 2015), RKD (Park et al., 2019), and SP (Tung and Mori, 2019) for comparison. Other distillation approaches are applicable to MSD and we will discuss the results using other KD approaches in our experiments. To perform analysis, we adopt VisualBERT (Li et al., 2019), a pre-trained multimodal model, as the teacher model and TinyBERT (Jiao et al., 2020) as a student model. VisualBERT consists of 12 layers and a hidden size of 768, and has 109 million parameters, while TinyBERT consists of 4 layers and a hidden size of 312, and has 14.5 million parameters. We use the region features from images for both the teacher and the student and fine-tune the student on each dataset. For training the weight learner we use the datasets’ validation set as meta data. We find the best hyperparameters on the validation set. 3.3 Datasets and Evaluation Metrics To answer the questions, we select four multimodal datasets: Hateful-Memes (Kiela et al., 2020) MMIM"
2021.findings-emnlp.302,Q14-1006,0,0.0270789,"768, and has 109 million parameters, while TinyBERT consists of 4 layers and a hidden size of 312, and has 14.5 million parameters. We use the region features from images for both the teacher and the student and fine-tune the student on each dataset. For training the weight learner we use the datasets’ validation set as meta data. We find the best hyperparameters on the validation set. 3.3 Datasets and Evaluation Metrics To answer the questions, we select four multimodal datasets: Hateful-Memes (Kiela et al., 2020) MMIMDB (Arevalo et al., 2017), Visual Entailment (SNLI-VE) (Xie et al., 2019; Young et al., 2014), 3.2 Experimental Setup and VQA2 (Goyal et al., 2017). Through our empirical analysis, we aim to answer The Hateful-Memes dataset consists of 10K multhe following questions: timodal memes. The task is a binary classification 3559 problem, which is to detect hate speech in multimodal memes. We use Accuracy (ACC) and AUC as evaluation metrics for hateful memes. The MM-IMDB (Multimodal IMDB) dataset consists of 26K movie plot outlines and movie posters. The task involves assigning genres to each movie from a list of 23 genres. This is a multi-label prediction problem, i.e., one movie can have mu"
2021.findings-emnlp.349,2020.acl-main.463,0,0.0264189,"nts, which is key to smooth communication (Clark and Schaefer, 1989; Response generation (RG) systems, which have the Clark and Brennan, 1991). basic goal of mimicking human conversation, have as of yet an unmeasured ability to understand comFor example, consider a conversation between municative intents. In general, standard neural lan- two friends shown in Figure 1. The reason the guage models build correlative models of linguistic person on the right (responder) is happy is not stimuli rather than deep understanding of human- indicated explicitly, but it is common sense that level meaning (Bender and Koller, 2020). As such, finding a buyer for the house (that the responder is there is reason to suspect that, while RG systems to- likely aiming to sell) makes one happy, which exday have impressive performance on common met- plains the response “I’m so happy”. Motivated by rics (Zhang et al., 2020b; Roller et al., 2021), they how humans communicate, we ask a main research achieve this performance without truly understand- question: do RG models understand the implicit ing human communication. Commonsense reason- CSR that explains why a response makes sense? ing (CSR), defined as “the basic level of practi"
2021.findings-emnlp.349,2020.acl-main.130,0,0.060011,"ons that provides causal knowledge about what justifies a sentence. Second, it provides fine-grained causal explanations along different dimensions. Last but not least, we have conducted multiple rounds of pilot studies to directly ask workers to write out commonsense explanations for a response, but the subjectivity of this open-ended task led to large variations in quality. Instead we ask workers to verify explanations generated from a model. We sample 1,200 dialogues from 4 dialogue datasets (300 from each): DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020), and SocialIQAprompted dialogues (Zhou et al., 2021). We generate 6k commonsense causal explanations (5 dimensions for each dialogue), using the last turn as the response and the previous turns as dialogue history (after filtering short turns). We follow Zhou et al. (2021)’s approach to select dialogues that contain at least a one-hop triple from ConceptNet (Liu and Singh, 2004). We use the same hyperparameters and weights from the best-performing 770M T5 model from Mostafazadeh et al. (2020). 3.1 3.2 Attribution Probing Here we examine if P (E|H, R) &gt; P (E 0 |H, R), i.e., can RG models perfo"
2021.findings-emnlp.349,2020.acl-main.703,0,0.0627528,"Missing"
2021.findings-emnlp.349,I17-1099,0,0.021445,"several reasons. First, it generates contextual commonsense explanations that provides causal knowledge about what justifies a sentence. Second, it provides fine-grained causal explanations along different dimensions. Last but not least, we have conducted multiple rounds of pilot studies to directly ask workers to write out commonsense explanations for a response, but the subjectivity of this open-ended task led to large variations in quality. Instead we ask workers to verify explanations generated from a model. We sample 1,200 dialogues from 4 dialogue datasets (300 from each): DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020), and SocialIQAprompted dialogues (Zhou et al., 2021). We generate 6k commonsense causal explanations (5 dimensions for each dialogue), using the last turn as the response and the previous turns as dialogue history (after filtering short turns). We follow Zhou et al. (2021)’s approach to select dialogues that contain at least a one-hop triple from ConceptNet (Liu and Singh, 2004). We use the same hyperparameters and weights from the best-performing 770M T5 model from Mostafazadeh et al. (2020). 3.1 3.2 Attribution Probing H"
2021.findings-emnlp.349,2021.acl-long.102,1,0.82179,"Missing"
2021.findings-emnlp.349,2021.naacl-main.366,1,0.757557,"Missing"
2021.findings-emnlp.349,2021.findings-acl.131,1,0.842741,"Missing"
2021.findings-emnlp.349,2020.findings-emnlp.165,1,0.852682,"Missing"
2021.findings-emnlp.349,2021.emnlp-main.410,1,0.7796,"Missing"
2021.findings-emnlp.349,2020.emnlp-main.370,0,0.540698,"planations that justify dialogue responses. Each annotation is a dialogue-specific explanation that explicitly describes what might cause the response in one of the five dimensions: event, emotion, location, possession, and attribute, inspired by human cognitive psychology (Kintsch and Van Dijk, 1978). We find through pilot studies that directly asking people to annotate result in explanations with high variation and subjectivity, to account for this, we first generate candidate explanations by adopting a large textto-text language model trained on a story explanation dataset, namely GLUCOSE (Mostafazadeh et al., 2020), under the dialogue setting. Next, we conduct a carefully designed two-stage human verification process with a qualification test and the main annotation task. We present our findings from verifying 6k generated explanations on 1,200 dialogues sampled from four public dialogue datasets. sational communication. Our probing setup contrasts valid explanations with corrupted version. Corruptions are generated via two methods: logical corruptions that disrupt logical coherence, and complete corruption where we disrupt the grammatical naturalness of the sentence. We find that the models fail to und"
2021.findings-emnlp.349,P19-1534,0,0.0627781,"ontextual commonsense explanations that provides causal knowledge about what justifies a sentence. Second, it provides fine-grained causal explanations along different dimensions. Last but not least, we have conducted multiple rounds of pilot studies to directly ask workers to write out commonsense explanations for a response, but the subjectivity of this open-ended task led to large variations in quality. Instead we ask workers to verify explanations generated from a model. We sample 1,200 dialogues from 4 dialogue datasets (300 from each): DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020), and SocialIQAprompted dialogues (Zhou et al., 2021). We generate 6k commonsense causal explanations (5 dimensions for each dialogue), using the last turn as the response and the previous turns as dialogue history (after filtering short turns). We follow Zhou et al. (2021)’s approach to select dialogues that contain at least a one-hop triple from ConceptNet (Liu and Singh, 2004). We use the same hyperparameters and weights from the best-performing 770M T5 model from Mostafazadeh et al. (2020). 3.1 3.2 Attribution Probing Here we examine if P (E|H, R) &gt; P (E 0 |H, R)"
2021.findings-emnlp.349,2020.tacl-1.37,0,0.0318746,"rating explanations with negated relations in it. 6 Related Work Commonsense Reasoning The majority of recent CSR benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019; Lin et al., 2021c,a, 2020) test a model’s abilModel size does not help with understand- ity to choose the correct option given a context ing common sense. Comparing BART-base and and a question. Recent work also aims to probe BART-large in Figure 5, we find that except for the models in these tasks to see if reasoning is actuattribution setting with complete corruptions, size ally achieved (Richardson and Sabharwal, 2020; does not change probing results (even lower accu- Richardson et al., 2020; Zhou et al., 2020; Lin et al., racy against logical corruptions), indicating that 2021b). Arabshahi et al. (2020) focuses on if-thenthe size of RG model is not the key to understand because reasoning in conversations and design a commonsense explanations for dialogue responses. theorem prover. In RG, several works have tried 4139 to incorporate commonsense (Zhou et al., 2018; Zhang et al., 2020a) using ConceptNet, a commonsense knowledge graph (Liu and Singh, 2004) to make responses more natural-sounding. Dialogue Res"
2021.findings-emnlp.349,D11-1054,0,0.147924,"Missing"
2021.findings-emnlp.349,2021.eacl-main.24,0,0.405941,"eral, standard neural lan- two friends shown in Figure 1. The reason the guage models build correlative models of linguistic person on the right (responder) is happy is not stimuli rather than deep understanding of human- indicated explicitly, but it is common sense that level meaning (Bender and Koller, 2020). As such, finding a buyer for the house (that the responder is there is reason to suspect that, while RG systems to- likely aiming to sell) makes one happy, which exday have impressive performance on common met- plains the response “I’m so happy”. Motivated by rics (Zhang et al., 2020b; Roller et al., 2021), they how humans communicate, we ask a main research achieve this performance without truly understand- question: do RG models understand the implicit ing human communication. Commonsense reason- CSR that explains why a response makes sense? ing (CSR), defined as “the basic level of practi- This will help us analyze whether the RG models cal knowledge and reasoning concerning everyday that seem to produce human-like responses really 1 understand the reasoning process that justifies the Our code and data are on our project page: https:// sites.google.com/usc.edu/cedar. response, which is impor"
2021.findings-emnlp.349,P19-1004,0,0.0157833,"onse from a reason that is similarly natural in terms of grammar but with totally different and invalid logical implications for the dialogue. Humans, from our sampled dialogues, again show much higher accuracy in this setting. 80 Accuracy Even gibberish does not change response probability much. Surprisingly, we find that even when corrupting the explanation so completely that it becomes unnatural English, most seq2seq RG models still generate responses with a roughly equal likelihood (left2right models perform better but still lag human performance) as shown in the right portion of Table 1. Sankar et al. (2019) find that the increase in perplexity of the response is tiny when they perturb the dialogue context, but here we find that there might even not be any increase in perplexity when conditioned on gibberish compared to a valid explanation expressed in English, while humans can identify the natural explanation perfectly. Model Size Effects on Probing 60 40 91.0 Models BART-base BART-large 56.0 53.0 51.0 41.0 42.0 41.0 51.0 20 0 Inference-LC Inference-CC Attribution-LC Attribution-CC Probing-Corruption Settings Figure 5: Model size Effects on the two probing settings for BART aggregated across fou"
2021.findings-emnlp.349,D19-1454,0,0.0474933,"Missing"
2021.findings-emnlp.349,2020.acl-tutorials.7,0,0.0348158,"pabilities. Probing results show that models fail to capture the logical relations between commonsense explanations and responses and fine-tuning on in-domain data and increasing model sizes do not lead to understanding of CSR for RG. We hope our study motivates more research in making RG models emulate the human reasoning process in pursuit of smooth human-AI communication 1 . Figure 1: A motivating example for our study. We want to know whether RG models understand the implicit common sense that justifies dialogue responses. situations and events that are commonly shared among most people” (Sap et al., 2020), is critical in human communication. Specifically, CSR helps establish a common ground consisting of “mutual 1 Introduction knowledge” between participants, which is key to smooth communication (Clark and Schaefer, 1989; Response generation (RG) systems, which have the Clark and Brennan, 1991). basic goal of mimicking human conversation, have as of yet an unmeasured ability to understand comFor example, consider a conversation between municative intents. In general, standard neural lan- two friends shown in Figure 1. The reason the guage models build correlative models of linguistic person on"
2021.findings-emnlp.349,2021.naacl-main.60,0,0.0838506,"Missing"
2021.findings-emnlp.349,N15-1020,0,0.076704,"Missing"
2021.findings-emnlp.349,N19-1421,0,0.053918,"Missing"
2021.findings-emnlp.349,D18-1009,0,0.0682839,"Missing"
2021.findings-emnlp.349,2020.acl-main.184,0,0.550172,"ative intents. In general, standard neural lan- two friends shown in Figure 1. The reason the guage models build correlative models of linguistic person on the right (responder) is happy is not stimuli rather than deep understanding of human- indicated explicitly, but it is common sense that level meaning (Bender and Koller, 2020). As such, finding a buyer for the house (that the responder is there is reason to suspect that, while RG systems to- likely aiming to sell) makes one happy, which exday have impressive performance on common met- plains the response “I’m so happy”. Motivated by rics (Zhang et al., 2020b; Roller et al., 2021), they how humans communicate, we ask a main research achieve this performance without truly understand- question: do RG models understand the implicit ing human communication. Commonsense reason- CSR that explains why a response makes sense? ing (CSR), defined as “the basic level of practi- This will help us analyze whether the RG models cal knowledge and reasoning concerning everyday that seem to produce human-like responses really 1 understand the reasoning process that justifies the Our code and data are on our project page: https:// sites.google.com/usc.edu/cedar. r"
2021.findings-emnlp.349,P18-1205,0,0.0232969,"esponses. theorem prover. In RG, several works have tried 4139 to incorporate commonsense (Zhou et al., 2018; Zhang et al., 2020a) using ConceptNet, a commonsense knowledge graph (Liu and Singh, 2004) to make responses more natural-sounding. Dialogue Response Generation Recent work focused on fine-tuning large pre-trained transformer models (Radford et al., 2019; Zhang et al., 2020b) on dialogue data. Many dialogue datasets have been collected with different focuses such as incorporating knowledge (Gopalakrishnan et al., 2019; Dinan et al., 2019b), empathy (Rashkin et al., 2019), personality (Zhang et al., 2018) and reasoning (Cui et al., 2020) within dialog systems. There has also been work on combining a variety of datasets to exhibit multiple attributes (Roller et al., 2021). 7 Conclusion We study commonsense reasoning in dialogue response generation aiming to close the gap between current RG models and human communication abilities. Specifically we formalize the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual form of commonsense. We design an explanation collection procedure for RG and propose two probing settings to evaluate RG m"
2021.findings-emnlp.349,2020.acl-demos.30,0,0.545641,"ative intents. In general, standard neural lan- two friends shown in Figure 1. The reason the guage models build correlative models of linguistic person on the right (responder) is happy is not stimuli rather than deep understanding of human- indicated explicitly, but it is common sense that level meaning (Bender and Koller, 2020). As such, finding a buyer for the house (that the responder is there is reason to suspect that, while RG systems to- likely aiming to sell) makes one happy, which exday have impressive performance on common met- plains the response “I’m so happy”. Motivated by rics (Zhang et al., 2020b; Roller et al., 2021), they how humans communicate, we ask a main research achieve this performance without truly understand- question: do RG models understand the implicit ing human communication. Commonsense reason- CSR that explains why a response makes sense? ing (CSR), defined as “the basic level of practi- This will help us analyze whether the RG models cal knowledge and reasoning concerning everyday that seem to produce human-like responses really 1 understand the reasoning process that justifies the Our code and data are on our project page: https:// sites.google.com/usc.edu/cedar. r"
2021.findings-emnlp.349,2021.sigdial-1.13,1,0.709892,"ies a sentence. Second, it provides fine-grained causal explanations along different dimensions. Last but not least, we have conducted multiple rounds of pilot studies to directly ask workers to write out commonsense explanations for a response, but the subjectivity of this open-ended task led to large variations in quality. Instead we ask workers to verify explanations generated from a model. We sample 1,200 dialogues from 4 dialogue datasets (300 from each): DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020), and SocialIQAprompted dialogues (Zhou et al., 2021). We generate 6k commonsense causal explanations (5 dimensions for each dialogue), using the last turn as the response and the previous turns as dialogue history (after filtering short turns). We follow Zhou et al. (2021)’s approach to select dialogues that contain at least a one-hop triple from ConceptNet (Liu and Singh, 2004). We use the same hyperparameters and weights from the best-performing 770M T5 model from Mostafazadeh et al. (2020). 3.1 3.2 Attribution Probing Here we examine if P (E|H, R) &gt; P (E 0 |H, R), i.e., can RG models perform causal attribution as humans by assigning a higher"
2021.findings-emnlp.62,2020.findings-emnlp.148,0,0.0734193,"Missing"
2021.findings-emnlp.62,S19-2005,0,0.0259703,"Missing"
2021.findings-emnlp.62,2021.acl-long.295,0,0.0318823,"regularized adapter generation approach. We find that catastrophic forgetting affects generalization ability to a lesser degree than performance on seen tasks; while continual learning algorithms can still bring considerable benefit to the generalization ability1 . 1 Introduction The ability to recall acquired knowledge for learning new tasks quickly and efficiently over time has been seen as a crucial metric of general linguistic intelligence (Yogatama et al., 2019). Progress on this research problem has led to remarkable improvements in recent works on few-shot learning (Brown et al., 2020; Gao et al., 2021). However, these methods have primarily focused on learning from a static set of tasks (datasets) in an offline manner, without dynamically expanding the acquired Training Task 1 CoLA Task 2 … Task 3 MRPC SST-2 e.g., GLUE tasks Evaluation Predicting on seen tasks Without adaptation Adapting to new tasks With few-shot adaption Figure 1: Overview of the Training and Evaluation setup in CLIF. The model learns over a number of training tasks sequentially and is evaluated over all the seen tasks. We also evaluate its ability to adapt to new tasks with only a small number of labeled examples. knowle"
2021.findings-emnlp.62,H01-1069,0,0.0148183,"Missing"
2021.findings-emnlp.62,2021.naacl-main.218,0,0.0370166,"ing continual learning algorithms that are compatible with PTLM fine-tuning. 5 Related Work Continual Learning The primary challenge that is addressed in CL literature is overcoming catastrophic forgetting. Generally, existing CL methods encompass memory and generative replaybased approaches (Robins, 1995; Lopez-Paz and Ranzato, 2017; Shin et al., 2017), regularization based approaches (Kirkpatrick et al., 2017; Nguyen et al., 2018) and model expansion based approaches (Shin et al., 2017). Recently, continual learning has drawn attention in the NLP field (Sun et al., 2020; Wang et al., 2019b; Huang et al., 2021). Q4: Sensitivity Analysis: how do models perform under various number of few-shot training examples. Figure 6 summarizes few-shot performance of different methods under different number of training examples per class on CLIF-26 and CLIF-55. We observe BiHNet-Reg always Continual Meta-Learning There exists literaachieves the best performance and the improve- ture that studies continual meta-learning outside ment is generally more significant when the train- NLP application, with various definition of the 721 problem. Some prior works (Xu et al., 2019; de Masson d’Autume et al., 2019; Wang et a"
2021.findings-emnlp.62,2020.acl-main.709,0,0.0577634,"Missing"
2021.findings-emnlp.62,2020.emnlp-main.623,0,0.040264,"Missing"
2021.findings-emnlp.62,2020.acl-main.703,0,0.0294702,"taset combinations, referred to as CLIF-26 and CLIF-55 tasks, summarized in Table 1. In the first combination, following Bansal et al. (2020), we use the GLUE (Wang et al., 2019a) bench716 2 https://huggingface.co/datasets regularized bi-level adapter generation framework to better address the CLIF problem (Sec. 3.3). 3.1 Base NLP Models BART and BART-Adapter. As we formulate the NLP tasks in the CLIF problem in a unified text-to-text format, we use pre-trained language models (LMs) as the architecture of the model f and fine-tune the entire model during training. We mainly use the BART-base (Lewis et al., 2020) model for our experiments. We also include Adapter training (Houlsby et al., 2019) as an alternative to fine-tuning the entire BART model. Here, adapters (Houlsby et al., 2019) are two-layer MultiLayer Perceptrons (MLPs) plugged after each layer of BART. Given the output h` at the `-th layer of the transformer, the adapted output is computed as h0` = h` + f`a (h` ), where f`a is the adapter layer at layer `. Only adapters are learned during training, while the BART model is frozen. We note two approaches BART and BART-Adapter respectively. Hyper-Networks for Adapter Generation. In addition to"
2021.findings-emnlp.62,C02-1150,0,0.203655,"Missing"
2021.findings-emnlp.62,2020.emnlp-main.601,0,0.0655237,"Missing"
2021.findings-emnlp.62,P11-1015,0,0.155161,"Missing"
2021.findings-emnlp.62,marelli-etal-2014-sick,0,0.0542155,"Missing"
2021.findings-emnlp.62,P05-1015,0,0.346792,"Missing"
2021.findings-emnlp.62,N19-1351,0,0.0448081,"Missing"
2021.findings-emnlp.62,D13-1170,0,0.00997394,"Missing"
2021.findings-emnlp.62,N18-1074,0,0.0198988,"Missing"
2021.findings-emnlp.62,W18-0535,0,0.0217344,"Missing"
2021.findings-emnlp.62,D18-1404,0,0.0627157,"Missing"
2021.findings-emnlp.62,P17-2067,0,0.0753298,"Missing"
2021.findings-emnlp.62,N19-1131,0,0.0595407,"Missing"
2021.findings-emnlp.62,2020.emnlp-main.39,0,0.314372,"training, so that we can use such performance to assess how well a CLIF method improves the generalization ability. Continual Learning Algorithms As a straightforward baseline method, we use Vanilla to denote simply training the model f sequentially on the upstream tasks. Specifically, it trains the model f on Tui until its performance converges and then continually train f on the data of Tui+1 . Note that the access of the data on previous tasks is not allowed in CL. We also consider CL algorithms such as EWC (Kirkpatrick et al., 2017), MbPA++ (de Masson d’Autume et al., 2019) and meta-MbPA (Wang et al., 2020) in our experiments. We use an online variant of EWC (Schwarz et al., 2018). EWC regularizes the change of important model parameters during training. The MbPA++ method performs test-time adaptation over a few training examples stored in the memory. The meta-MbPA method includes a meta-learning objective to adapt fast. As a comparator that does not suffer from forgetting, we also report the results of multi-task learning over upstream tasks (MTL) for reference. Hyper-Networks for CL. von Oswald et al. (2020) proposed a hypernetwork-based continual learning algorithm, where the high-level idea"
2021.findings-emnlp.62,Q19-1040,0,0.0631169,"Missing"
2021.findings-emnlp.62,N18-1101,0,0.0814952,"Missing"
2021.findings-emnlp.62,N19-1242,0,0.0235828,"(Sun et al., 2020; Wang et al., 2019b; Huang et al., 2021). Q4: Sensitivity Analysis: how do models perform under various number of few-shot training examples. Figure 6 summarizes few-shot performance of different methods under different number of training examples per class on CLIF-26 and CLIF-55. We observe BiHNet-Reg always Continual Meta-Learning There exists literaachieves the best performance and the improve- ture that studies continual meta-learning outside ment is generally more significant when the train- NLP application, with various definition of the 721 problem. Some prior works (Xu et al., 2019; de Masson d’Autume et al., 2019; Wang et al., 2020) aim to develop algorithms that allows fast recovery of previous performance when a few training examples of an early task are available again at the test time. Caccia et al. (2020) proposed a setup where models visit a sequence of potentially re-occuring tasks and measured online cumulative performance as metrics. Antoniou et al. (2020) assumes the model visits a sequence of few-shot classification tasks while the test tasks consist of seen classes at training. The problem setup of Jerfel et al. (2019) is most related to ours which learns t"
2021.findings-emnlp.62,D15-1237,0,0.0662131,"Missing"
2021.maiworkshop-1.7,2021.mrl-1.1,0,0.0382161,"Missing"
2021.maiworkshop-1.7,N19-1423,0,0.0374871,"improving a student model (Hinton et al., 2015; Park et al., 2019; Romero et al., 2014; Tian et al., 2019; M¨uller et al., 2020) and improving a teacher model itself by self-distillation (Xie et al., 2020; Kim et al., 2020; Furlanello et al., 2018). There has been a surge of interest in distillation in a multimodal setup such as cross-modal Introduction Recent advances in computer vision and natural language processing are attributed to deep neural networks with large number of layers. Current state-of-the-art architectures are getting wider and deeper with billions of parameters, e.g., BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020). In addition to their huge sizes, such wide and deep models suffer from high computational costs and latencies at inference. These shortcomings greatly ∗ The work in progress was mainly done during internship at Facebook AI. 42 Proceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 42–53 June 6, 2021. ©2021 Association for Computational Linguistics distillation (Gupta et al., 2016; Tian et al., 2019). Multimodal problems involve relating information from multiple sources. For example, visual question answering (VQA) requires answering que"
2021.maiworkshop-1.7,Q14-1006,0,0.0282755,"3). Then, we update the student’s parameter along the descent direction of the student’s loss on a mini-batch training data (line 4). Note that the student’s parameter is parameterized by the meta learner’s parameter. With the updated parameter, the meta leaner can be updated by moving the current parameter Θ(t) along the objective gradient of equation (9) on a mini-batch meta data (line 5). After updating the meta-learner, the student’s parameter can be updated on a mini-batch training data (line 6). 4 2020) MM-IMDB (Arevalo et al., 2017), and Visual Entailment (SNLI-VE) (Xie et al., 2019a; Young et al., 2014). The Hateful-Memes dataset (Kiela et al., 2020) consists of 10K multimodal memes. The task is a binary classification problem, which is to detect hate speech in multimodal memes. We use Accuracy (ACC), and AUC as evaluation metrics of choice for hateful memes (Kiela et al., 2020). The MM-IMDB (Multimodal IMDB) dataset consists of 26K movie plot outlines and movie posters. The task involves assigning genres to each movie from a list of 23 genres. This is a multi-label prediction problem, i.e., one movie can have multiple genres and we use Macro F1 and Micro F1 as evaluation metrics following ("
2021.naacl-main.283,2020.acl-main.421,0,0.0358265,". (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3617–3632 June 6–11, 2021. ©2021 Association for Computational Linguistics tasks have gained less attention, with smaller benchmark datasets that cover a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This f"
2021.naacl-main.283,2020.repl4nlp-1.1,0,0.0435506,"learning is a technique used et al., 2019). to adapt a model trained on a downstream task in a The generalization of such representations has source language to directly generalize to the task in been extensively evaluated on traditional tasks such new languages. It aims to come up with common as Part-of-Speech (POS) tagging, Named Entity cross-lingual representations and leverages them to Recognition (NER) and Cross-lingual Document bridge the divide between resources to make any Classification (CLDC) (Ahmad et al., 2019; Wu NLP application scale to multiple languages. This and Dredze, 2019; Bari et al., 2020a; Schwenk is particularly useful for data-scarce scenarios, as it and Li, 2018), with ever-growing open commureduces the need for API calls implied by machine nity annotation efforts like Universal Dependentranslation or costly task-specific annotation for cies (Nivre et al., 2020) and CoNLL shared tasks new languages. (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of t"
2021.naacl-main.283,D18-1398,0,0.115069,"oss-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making i"
2021.naacl-main.283,D19-1252,0,0.0471437,"Missing"
2021.naacl-main.283,2020.clssts-1.5,0,0.161057,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,2020.acl-main.653,0,0.0411571,"l alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and is more typologically diverse. TyDiQA is not a classification task, but we show how meta-learning can be applied usefully to it. We also show"
2021.naacl-main.283,2020.emnlp-main.484,0,0.0460222,"Missing"
2021.naacl-main.283,D19-1129,0,0.0634266,"Missing"
2021.naacl-main.283,K19-1061,1,0.840535,"Missing"
2021.naacl-main.283,D19-1575,0,0.0235775,"a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019)."
2021.naacl-main.283,2020.acl-main.348,0,0.292112,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,D19-1077,0,0.046422,"Missing"
2021.naacl-main.296,D19-1588,0,0.0457666,"Missing"
2021.naacl-main.296,2020.acl-main.483,1,0.910078,"e-tuning Figure 1: Comparison between the focus of our study (d) and previous works (a,b,c). We study the viability of obtaining an upstream model that could reduce bias in a number of downstream classifiers when fine-tuned. as models may learn to associate certain features with positive or negative labels spuriously (Dixon et al., 2018), or propagate bias encoded in PTLMs to downstream classifiers (Caliskan et al., 2017; Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates gender-bias in the pronoun resolution task when models are trained using BERT embeddings, and Kennedy et al. (2020) shows that hate speech classifiers finetuned from BERT result in more frequent false positive predictions for certain group identifier mentions (e.g., “muslim”, “black”). Approaches for bias mitigation are mostly applied during fine-tuning to reduce bias in a specific downstream task or dataset (Park et al., 2018; Zhang et al., 2018; Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches 1 Introduction reduce the influence of spurious features in the original dataset (Dixon et al., 2018; Zhao et al., The practice of fine-tuning pretrained language 2018; Park et al.,"
2021.naacl-main.296,D19-1425,0,0.0265958,"Zhang et al., 2018; Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches 1 Introduction reduce the influence of spurious features in the original dataset (Dixon et al., 2018; Zhao et al., The practice of fine-tuning pretrained language 2018; Park et al., 2018), and adversarial learnmodels (PTLMs or LMs), such as BERT (Devlin ing approaches generate debiased data represenet al., 2019), has improved prediction performance in a wide range of NLP tasks. However, fine- tations that are exclusive to the downstream model tuned LMs may exhibit biases against certain pro- (Kumar et al., 2019; Zhang et al., 2018). These tected groups (e.g., gender and ethnic minorities), techniques act on biases particular to the given dataset, domain, or task, and require new bias miti1 Code and data: https://github.com/INK-U gation when switching to a new downstream task SC/Upstream-Bias-Mitigation 2 or dataset. This can require auxiliary training obThe work was partially done when Xisen Jin was an intern at Snap Inc. jectives, the definition of task-specific fairness met3770 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human"
2021.naacl-main.296,W19-3823,0,0.0934002,"Model Datasets ?$&quot; , ?$# … Dataset ?!# Model (c) Removing sensitive attributes from data … (d) Upstream bias mitigation for fine-tuning Figure 1: Comparison between the focus of our study (d) and previous works (a,b,c). We study the viability of obtaining an upstream model that could reduce bias in a number of downstream classifiers when fine-tuned. as models may learn to associate certain features with positive or negative labels spuriously (Dixon et al., 2018), or propagate bias encoded in PTLMs to downstream classifiers (Caliskan et al., 2017; Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates gender-bias in the pronoun resolution task when models are trained using BERT embeddings, and Kennedy et al. (2020) shows that hate speech classifiers finetuned from BERT result in more frequent false positive predictions for certain group identifier mentions (e.g., “muslim”, “black”). Approaches for bias mitigation are mostly applied during fine-tuning to reduce bias in a specific downstream task or dataset (Park et al., 2018; Zhang et al., 2018; Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches 1 Introduction reduce the influence of spurious featu"
2021.naacl-main.296,N19-1063,0,0.0274316,"7) proposed training a classifier together with an adversarial predictor for sensitive attributes. Madras et al. (2018) further studied re-usable de-biased representations by training a new downstream classifier (potentially with a different classification task) using the learned representations. However, this practice relies on frozen representations (rather than models themselves), which precludes the possibility of generating predictions for new data. Mitigating bias in pretrained models. Another line of work addresses bias in pretrained models (e.g., word vectors, BERT, Zhou et al., 2019; May et al., 2019; Bhardwaj et al., 2020; Liang et al., 2020). Many such studies again focus on bias in frozen data representations, and do not study their effects on downstream classifiers. Others alternatively assess the propagation of bias from pretrained models to downstream classifiers: Ravfogel et al. (2020) study algorithms for mitigating bias in pretrained models by de-biasing the learned representations, which can subsequently be used in classifiers as frozen representations. 4.5 Investigating Why UBM Reduces Bias We attempt to interpret why fine-tuning from a debiased upstream model remains less bias"
2021.naacl-main.296,D18-1302,0,0.0207082,"(Dixon et al., 2018), or propagate bias encoded in PTLMs to downstream classifiers (Caliskan et al., 2017; Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates gender-bias in the pronoun resolution task when models are trained using BERT embeddings, and Kennedy et al. (2020) shows that hate speech classifiers finetuned from BERT result in more frequent false positive predictions for certain group identifier mentions (e.g., “muslim”, “black”). Approaches for bias mitigation are mostly applied during fine-tuning to reduce bias in a specific downstream task or dataset (Park et al., 2018; Zhang et al., 2018; Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches 1 Introduction reduce the influence of spurious features in the original dataset (Dixon et al., 2018; Zhao et al., The practice of fine-tuning pretrained language 2018; Park et al., 2018), and adversarial learnmodels (PTLMs or LMs), such as BERT (Devlin ing approaches generate debiased data represenet al., 2019), has improved prediction performance in a wide range of NLP tasks. However, fine- tations that are exclusive to the downstream model tuned LMs may exhibit biases against certain pro-"
2021.naacl-main.296,W19-3810,0,0.0348288,"olution — we explore whether upstream bias mitigation of a LM followed by downstream fine-tuning reduces bias for the downstream model. Though previous work has addressed biases in frozen PTLM or word embeddings (Bolukbasi et al., 2016; Zhou et al., 2019; Bhardwaj et al., 2020; Liang et al., 2020; Ravfogel et al., 2020), for example by measuring associations between gender and occupations in an embedding space, they do not study their effect on downstream classifiers (Fig. 1 (b)), while some of them study the effects while keeping the embeddings frozen (Zhao et al., 2019; Kurita et al., 2019; Prost et al., 2019). Bias in these frozen representations can also be directly corrected by removing associations between feature and sensitive attributes (Elazar and Goldberg, 2018; Madras et al., 2018) (Fig. 1 (c)), but this does not allow predictions to be generated for new data. two stages: first, in the upstream bias mitigation stage, a LM is fine-tuned with bias mitigation objectives on one or several “upstream” tasks, and subsequently the classification layer is re-initialized; then, in the downstream fine-tuning stage the encoder from the upstream model, jointly with the new classification layer, are aga"
2021.naacl-main.296,2020.acl-main.647,0,0.292884,", we suggest that LMs that undergo bias mitigation acquire inductive bias that is helpful for reducing harmful biases when fine-tuned on new domains and tasks. In four tasks with known bias factors — hate speech detection, toxicity detection, occupation prediction from short bios, and coreference resolution — we explore whether upstream bias mitigation of a LM followed by downstream fine-tuning reduces bias for the downstream model. Though previous work has addressed biases in frozen PTLM or word embeddings (Bolukbasi et al., 2016; Zhou et al., 2019; Bhardwaj et al., 2020; Liang et al., 2020; Ravfogel et al., 2020), for example by measuring associations between gender and occupations in an embedding space, they do not study their effect on downstream classifiers (Fig. 1 (b)), while some of them study the effects while keeping the embeddings frozen (Zhao et al., 2019; Kurita et al., 2019; Prost et al., 2019). Bias in these frozen representations can also be directly corrected by removing associations between feature and sensitive attributes (Elazar and Goldberg, 2018; Madras et al., 2018) (Fig. 1 (c)), but this does not allow predictions to be generated for new data. two stages: first, in the upstream bi"
2021.naacl-main.296,P19-1163,0,0.0285069,"-tuning. speech. We include two datasets for study, namely 2. Cross-Domain and Cross-Task Fine-Tuning. the Gab Hate Corpus (GHC; Kennedy et al., 2018) and the Stormfront corpus (de Gibert et al., 2018). Similar to how LMs are fine-tuned for various tasks Both datasets contain binary labels for hate and and domains, in a more practical setup, we test non-hate instances, though with differences in the whether transfer of bias mitigation effects is viable labeling schemas and domains. across domains and tasks. To achieve this, we apply bias mitigation while fine-tuning a LM on AAVE Dialect Bias. Sap et al. (2019) show that one dataset and perform fine-tuning on another. offensive and hate speech classifiers yield a higher 3. Multiple Bias Factors. In the most challenging false positive rate on text written in African Amerisetup, we train a single upstream model to address can Vernacular English (AAVE). This bias brings multiple bias factors (e.g., both dialect bias and significant harm to the communities that uses gender bias). Such upstream models can be trained AAVE, for example, by leading to the disproporwith multi-task learning (i.e., jointly training over tionate removal of the text written in A"
2021.naacl-main.296,2020.socialnlp-1.2,0,0.0136092,"ixon et al. (2018) and Zhang et al. (2020), we incorporate the Identity Phrase Templates Test Sets (reported as IPTTS), which consists of 77k hate and non-hate examples mentioning group identifiers, generated with templates. Following P these works, for IPTTS we compute FPRD as z |FPRz − FPRoverall |, where FPRz is false positive rate on sentences with the group identifier z, and FPRoverall is the overall false positive rate. of AAVE examples in the datasets and the noisy outputs of AAVE classifier (Blodgett et al., 2016), we expect the in-domain FPRD metrics to be noisy. Therefore, following Xia et al. (2020), we incorporate the BROD (Blodgett et al., 2016) dataset, which is a large unlabeled collection of Twitter posts written in l. Since in practice only a small portion of texts are toxic or spam, we treat all examples from BROD as normal, and report the accuracy (which equals 1−FPR) on the dataset. Gender Stereotype Metrics. We employ the WinoBias (Zhao et al., 2018) dataset which provides opportunities to evaluate models on prostereotypical and anti-stereotypical coreference examples. We report the differences in F1 (F1-Diff) on two subsets of data. On occupation prediction, following Ravfogel"
2021.naacl-main.296,N19-1064,0,0.0355792,"Missing"
2021.naacl-main.296,N18-2003,0,0.147097,"lodgett et al., 2020). We ent classifier heads h) while mitigating multiple include two datasets for study: FDCL (Founta et al., kinds of bias. Subsequently, the resulting upstream 2018) and DWMW (Davidson et al., 2017). In both model is transferred to downstream models as be- datasets, we treat abusive, hateful and spam to3772 gether as harmful outcomes (i.e., false positives for each are harmful) to compute false positive rates. Following Sap et al. (2019), we use an off-the-shelf AAVE dialect predictor (Blodgett et al., 2016) to identify examples written in AAVE. Gender Stereotypical Bias. Zhao et al. (2018) summarize a list of occupations that are prone to be stereotyped in practice, leading to coreference resolutions models and occupation prediction models having biases in performance in pro- and antistereotypical instances when trained on short bios. We train the coreference resolution model on the OntoNotes 5.0 dataset (Weischedel et al., 2013) and the occupation classifier on the BiasBios (DeArteaga et al., 2019) dataset. 2.3 Evaluation Protocol and Metrics We evaluate the overall performance of the models on downstream tasks along with appropriate bias metrics for each bias factor, analyzed"
2021.naacl-main.296,D19-1531,0,0.14817,"rmance in common setups (Pan and Yang, 2010; Dai and Le, 2015), we suggest that LMs that undergo bias mitigation acquire inductive bias that is helpful for reducing harmful biases when fine-tuned on new domains and tasks. In four tasks with known bias factors — hate speech detection, toxicity detection, occupation prediction from short bios, and coreference resolution — we explore whether upstream bias mitigation of a LM followed by downstream fine-tuning reduces bias for the downstream model. Though previous work has addressed biases in frozen PTLM or word embeddings (Bolukbasi et al., 2016; Zhou et al., 2019; Bhardwaj et al., 2020; Liang et al., 2020; Ravfogel et al., 2020), for example by measuring associations between gender and occupations in an embedding space, they do not study their effect on downstream classifiers (Fig. 1 (b)), while some of them study the effects while keeping the embeddings frozen (Zhao et al., 2019; Kurita et al., 2019; Prost et al., 2019). Bias in these frozen representations can also be directly corrected by removing associations between feature and sensitive attributes (Elazar and Goldberg, 2018; Madras et al., 2018) (Fig. 1 (c)), but this does not allow predictions"
2021.naacl-main.335,P19-1590,0,0.0185525,"s. Here, if we are able to pinpoint this document’s most essential classes, crafted cereal and baby cereal, as core classes, we can check their ancestor classes in the taxonomy and recover all the true classes. success, people find that applying these methods to many real-world scenarios remains challenging as the human labeling process is often too timeconsuming and expensive. Recently, more studies have been developed to address text classification using smaller amount of 1 Introduction labeled data. First, several semi-supervised methHierarchical multi-label text classification (HMTC) ods (Gururangan et al., 2019; Berthelot et al., 2019) aims to assign each text document to a set of rel- propose to use abundant unlabeled documents to evant classes from a class taxonomy. As a funda- assist model training on labeled dataset. Although mental task in NLP, HMTC has many applications mitigating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of"
2021.naacl-main.335,D14-1181,0,0.0104706,"d entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 4239 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4239–4249 June 6–11, 2021. ©2021 Association for Computational Linguistics In this paper"
2021.naacl-main.335,2020.emnlp-main.724,1,0.949213,"still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 4239 Proceedings of the 2021 Conference of the North American Chapter of the Association for C"
2021.naacl-main.335,D18-1352,0,0.04719,"Missing"
2021.naacl-main.335,P18-1029,0,0.0654361,"Missing"
2021.naacl-main.335,2020.acl-main.30,1,0.882062,"ating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Nevertheless, these models all assume each docu2019), Attention Network (Huang et al., 2019), and ment has only one class and all class surface names achieved decent performance when trained on mas- (or class indicative keywords) must appear in the sive human-labeled documents. Despite such a corpus, which are too restrictive for HMTC. 42"
2021.naacl-main.335,2021.naacl-main.84,0,0.0935152,"Missing"
2021.naacl-main.335,N18-1002,0,0.0220281,"xt classification (HMTC) ods (Gururangan et al., 2019; Berthelot et al., 2019) aims to assign each text document to a set of rel- propose to use abundant unlabeled documents to evant classes from a class taxonomy. As a funda- assist model training on labeled dataset. Although mental task in NLP, HMTC has many applications mitigating the human annotation burden, these such as product categorization (Goumy and Mejri, methods still require a labeled dataset that covers 2018), semantic indexing (Li et al., 2019), and fine- all classes, which could be too expensive to obtain grained entity typing (Xu and Barbosa, 2018). when we have a large number of classes in HMTC. Most existing methods address HMTC in a super- Second, some weakly-supervised models exploit vised fashion — they first ask humans to provide class indicative keywords (Meng et al., 2018; Zeng many labeled documents and then train a text clas- et al., 2019; Mekala and Shang, 2020) or class sursifier for prediction. Many classifiers have been face names (Meng et al., 2020; Wang et al., 2020) developed with different deep learning architec- to derive pseudo-labeled data for model training. tures such as CNN (Kim, 2014), RNN (You et al., Neverthel"
2021.naacl-main.335,D19-1404,0,0.347132,"lso be tagged. Taking the document in Fig. 1 as an example, humans can quickly identify this review text is clearly about “baby cereal” and “crafted cereal”, which are the core classes. After assigning these two most essential classes to the document, people continue to check the core classes’ ancestor classes and find “feeding” as well as “baby food” should be tagged. Motivated by the above human labeling process, we propose TaxoClass, a weakly-supervised HMTC framework including four major steps. First, we calculate the document-class similarity using a pre-trained textual entailment model (Yin et al., 2019). Second, we identify each document’s core classes by (1) selecting candidate core classes that are most similar to the document at each level in a top-down fashion, and (2) choosing hdocument, candidate core classi pairs that are salient across the whole unlabeled corpus. Third, we derive training data from document core classes and use them to train a text classifier. This classifier includes a document encoder based on pre-trained BERT (Devlin et al., 2019), a class encoder capturing class taxonomy structure, and a text matching network computing the probability of a document being tagged w"
2021.naacl-main.366,P17-1171,0,0.0300472,"se knowledge graphs (CSKGs) such as ConceptNet (Speer et al., 2017). However, the binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or more concepts. Formalized in this way, OpenCSR is a question answering task requiring (possibly) iterative retrieval, similar to other open-domain QA tasks (Chen et al., 2017) such as HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). As noted above, however, the surface of commonsense questions in OpenCSR have fewer hints about kinds of multi-hop reasoning required to answer them than the factoid questions in open-domain QA, resulting in a particularly challenging reasoning problem (see Sec. 3). begins with an entity-linked corpus, and computes both sparse and dense indices of entity mentions (i.e., linked named-entity spans). DrKIT’s fundamental reasoning operation is to “hop” from one weighted set of X entities to another, by 1) findi"
2021.naacl-main.366,P19-1222,0,0.0503849,"Missing"
2021.naacl-main.366,2020.emnlp-main.99,1,0.864124,"Missing"
2021.naacl-main.366,2020.emnlp-main.550,0,0.0527905,"a concept-to-fact sparse matrix E and a fact-to-fact sparse matrix S. The dense fact index D is pre-computed with a pre-trained bi-encoder. A weighed set of facts is represented as a sparse vector F . The workflow (left) of D R FACT starts mapping a question to a set of initial facts that have common concepts with it. Then, it recursively performs Fact-Follow operations (right) for computing Ft and At . Finally, it uses learnable hop-weights αt to aggregate the answers. 2019), which learns to maximize the score of facts that contain correct answers to a given question, following the steps of Karpukhin et al. (2020) (i.e., dense passage retrieval), so that we can use MIPS to do dense retrieval over the facts. After pre-training, we embed each fact in F with a dense vector (using the [CLS] token representation). Hence D is a |F |× d dense matrix. Sparse Fact-to-Fact Index S. We pre-compute the sparse links between facts by a set of connection rules, such as fi → fj when fi and fj have at least one common concept and fj introduces at least two more new concepts that are not in fi (see Appendix B (2) for more). Hence S is a binary sparse tensor with the dense shape |F |× |F|. Sparse Index of Concept-to-Fact"
2021.naacl-main.366,2020.findings-emnlp.171,0,0.046857,"dule, the gap between D R FACT and others is even larger. (Sec. 5) 2 Related Work Commonsense Reasoning. Many recent commonsense-reasoning (CSR) methods focus on multiple-choice QA. For example, KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) use an external commonsense knowledge graph as structural priors to individually score each choice. These methods, though powerful in determining the best choice for a multi-choice question, are less realistic for practical applications where answer candidates are typically not available. To address this question, we extend work by Seo UnifiedQA (Khashabi et al., 2020) and other et al. (2019) and Dhingra et al. (2020), and proclosed-book QA models (Roberts et al., 2020) generate answers to questions by fine-tuning a pose an efficient, differentiable multi-hop reasoning method for OpenCSR, named D R FACT (for text-to-text transformer such as BART (Lewis Differentiable Reasoning over Facts). Specifically, et al., 2020a) or T5 (Raffel et al., 2020), but a 4612 disadvantage of closed-book QA models is that they do not provide intermediate explanations for their answers, i.e., the supporting facts, which makes them less trustworthy in downstream applications. Al"
2021.naacl-main.366,Q19-1026,0,0.0192674,"he binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or more concepts. Formalized in this way, OpenCSR is a question answering task requiring (possibly) iterative retrieval, similar to other open-domain QA tasks (Chen et al., 2017) such as HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). As noted above, however, the surface of commonsense questions in OpenCSR have fewer hints about kinds of multi-hop reasoning required to answer them than the factoid questions in open-domain QA, resulting in a particularly challenging reasoning problem (see Sec. 3). begins with an entity-linked corpus, and computes both sparse and dense indices of entity mentions (i.e., linked named-entity spans). DrKIT’s fundamental reasoning operation is to “hop” from one weighted set of X entities to another, by 1) finding mentions of new entities x0 that are related to some entity in X, guided by the ind"
2021.naacl-main.366,P19-1612,0,0.0444597,"Missing"
2021.naacl-main.366,2020.acl-main.703,0,0.0495943,"s et al., 2020) generate answers to questions by fine-tuning a pose an efficient, differentiable multi-hop reasoning method for OpenCSR, named D R FACT (for text-to-text transformer such as BART (Lewis Differentiable Reasoning over Facts). Specifically, et al., 2020a) or T5 (Raffel et al., 2020), but a 4612 disadvantage of closed-book QA models is that they do not provide intermediate explanations for their answers, i.e., the supporting facts, which makes them less trustworthy in downstream applications. Although closed-book models exist that are augmented with an additional retrieval module (Lewis et al., 2020b), these models mainly work for single-hop reasoning. QA over KGs or Text. A conventional source of commonsense knowledge is triple-based symbolic commonsense knowledge graphs (CSKGs) such as ConceptNet (Speer et al., 2017). However, the binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or"
2021.naacl-main.366,D19-1282,1,0.862088,"ning towards practical applications, we prochoice question answering (QA) — i.e., given pose to study open-ended commonsense reasona question and a small set of pre-defined aning (OpenCSR), where answers are generated efswer choices, models are required to determine ficiently, rather than selected from a small list which of the candidate choices best answers the of candidates (see Figure 1). As a step toquestion. Existing commonsense reasoning modward this, here we explore a setting where the els usually work by scoring a question-candidate model produces a ranked list of answers from a pair (Lin et al., 2019; Lv et al., 2020; Feng et al., large question-independent set of candidate con2020). Hence, even an accurate multiple-choice cepts that are extracted offline from a corpus of ∗ common-sense facts written in natural language. The work was mainly done during Bill Yuchen Lin’s internship at Google Research. The OpenCSR task is inherently challenging. 1 Our code and data are available at the project website — One problem is that for many questions, findhttps://open-csr.github.io/. The human annoing an answer requires reasoning over two or tations were collected by the USC-INK group. 4611 Proceedi"
2021.naacl-main.366,D18-1260,0,0.109204,"Missing"
2021.naacl-main.366,D19-1261,0,0.036518,"Missing"
2021.naacl-main.366,D18-1455,1,0.896027,"Missing"
2021.naacl-main.366,D18-1259,1,0.821544,"mputational Linguistics: Human Language Technologies, pages 4611–4625 June 6–11, 2021. ©2021 Association for Computational Linguistics more natural-language facts from a corpus. In the multiple-choice QA setting, as the set of candidates is small, we can pair a question with an answer, and use the combination to retrieve relevant facts and then reason with them. In the open-ended setting, this is impractical: instead one needs to retrieve facts from the corpus using the question alone. In this respect, OpenCSR is similar to multi-hop factoid QA about named entities, e.g. as done for HotpotQA (Yang et al., 2018). However, the underlying reasoning chains of most multi-hop factoid QA datasets are relatively clear and context-independent, and are thus easier to infer. Commonsense questions, in contrast, exhibit more variable types of reasoning, and the relationship between a question and the reasoning to answer the question is often unclear. (For example, a factoid question like “who starred in a movie directed by Bradley Cooper?” clearly suggests following a directed-by relationship and then a starred-in relationship, while the underlying reasoning chains of a question like “what can help alleviate glo"
2021.naacl-main.366,2020.emnlp-main.437,0,0.0229099,"Missing"
2021.naacl-main.366,P19-1436,0,0.034591,"Missing"
2021.naacl-main.366,D19-1242,1,0.848928,"information from a fact for multi-hop reasoning. 3 Open-Ended Commonsense Reasoning Task Formulation. We denote a corpus of knowledge facts as F, and use V to denote a vocabulary of concepts; both are sets consisting of Multi-Hop Reasoning. Many recent models unique elements. A fact fi ∈ F is a sentence for open-domain QA tackle multi-hop reasoning that describes generic commonsense knowledge, through iterative retrieval, e.g., GRAFT-Net (Sun such as “trees remove carbon dioxide from the et al., 2018), MUPPET (Feldman and El-Yaniv, atmosphere through photosynthesis.” A concept 2019), PullNet (Sun et al., 2019), and GoldEn (Qi cj ∈ V is a noun or base noun phrase mentioned et al., 2019). These models, however, are not endfrequently in these facts (e.g., ‘tree’ and ‘carbon to-end differentiable and thus tend to have slower dioxide’). Concepts are considered identical if inference speed, which is a limitation shared by their surface forms are the same (after lemmamany other works using reading comprehension tization). Given only a question q (e.g., “what for multi-step QA (Das et al., 2019; Lee et al., can help alleviate global warming?”), an open2019). As another approach, Neural Query Lanended commo"
2021.nlp4convai-1.23,2021.deelio-1.2,0,0.0385273,"plicit background knowledge I and then produce a response R given both U and I. Extending most neural RG models that treat this as a conditional language modeling problem, we aim to learn the conditional probability distribution P (I, R|U ) by training on human dialogues. 2.1 Data Preparation Knowledge Schema We consider ConceptNet (Speer et al., 2017) as our knowledge schema, which is a large-scale crowdsourced commonsense knowledge base consisting of triples such as “buy, RelatedTo, money”. We have explored several other sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourc"
2021.nlp4convai-1.23,2020.emnlp-main.373,0,0.0374913,"Missing"
2021.nlp4convai-1.23,2020.acl-main.130,0,0.0242908,"to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , implicit knowledge (empty or non-empty) I, and a response R. We enclose"
2021.nlp4convai-1.23,2020.acl-main.703,0,0.0211956,"Think Before You Speak: Learning to Generate Implicit Knowledge for Response Generation by Self-Talk Pei Zhou1∗ Behnam Hedayatnia2 Karthik Gopalakrishnan2 Seokhwan Kim2 Jay Pujara1 Xiang Ren1 Yang Liu2 Dilek Hakkani-Tur2 1 Department of Computer Science, University of Southern California 2 Amazon Alexa AI {peiz,jpujara,xiangren}@usc.edu, {behnam,karthgop,seokhwk,yangliud,hakkanit}@amazon.com 1 Introduction End-to-end response generation (RG) models based on pre-trained transformer-based (Vaswani et al., 2017) language models (LM) have shown to produce human-like responses (Zhang et al., 2020; Lewis et al., 2020; Roller et al., 2020). Humans, however, not only “just utter the right sentence”, but also contribute to the common ground consisting of mutual beliefs and common knowledge “whose truth is taken for granted as part of the background of the conversation” (Stalnaker, 1978; Clark and Schaefer, 1989; Clark and Brennan, 1991). For example, consider the utterance “I need to buy some flowers for my wife”, a potential appropriate response is “Perhaps you’d be interested in red roses”. To produce this response, the participant needs to understand relevant background knowledge such as “rose is a type o"
2021.nlp4convai-1.23,2020.acl-demos.30,0,0.0959559,"Missing"
2021.nlp4convai-1.23,2021.sigdial-1.13,1,0.882617,"RG models that first generate implicit commonsense inferences before producing a response given previous utterances. Inspired by inquiry-based discovery learning (Bruner, 1961) and the self-talk procedure (Shwartz et al., 2020), we encourage the RG model to talk with itself to elicit implicit knowledge before making a response. Collecting training data is challenging for our purpose since commonsense knowledge is by definition often omitted in conversations. We use ConceptNet triples (Speer et al., 2017) as the knowledge schema to represent knowledge and align common sense-focused dialogues (Zhou et al., 2021) with knowledge to train RG models. We conduct extensive human evaluation on different variants of our training procedure and find that models that generate implicit background knowledge before responding produce more grammatical, coherent, and engaging responses compared to RG models that directly generate responses. Further analysis shows that models can sometimes even learn to distinguish when it is necessary to self-talk to generate implicit knowledge, i.e., be aware of potential knowledge gaps in dialogues. We hope our findings encourage more future studies on making RG models better emul"
2021.nlp4convai-1.23,I17-1099,0,0.0227344,"o, money”. We have explored several other sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , i"
2021.nlp4convai-1.23,P19-1534,0,0.0205292,"er sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , implicit knowledge (empty or non-empty) I, an"
2021.sigdial-1.13,I17-1099,0,0.126315,"Missing"
2021.sigdial-1.13,2020.findings-emnlp.165,1,0.852259,"he best of our knowledge, there is no study or largescale multi-turn data for analyzing whether modelgenerated responses present the ability to communicate with commonsense knowledge or reasoning. Lack of real-life interactive setting for Commonsense Reasoning Benchmarks Current commonsense reasoning (CSR) benchmarks mostly target models’ ability to choose a right answer from several candidates given a question. We argue that this is a highly artificial scenario as models do not get options to choose from in real-life, and often they need to generate utterances. Recent work such as CommonGen (Lin et al., 2020) has started to explore generative settings to examine commonsense in natural language processing (NLP) models. This line of work, however, is still far from real use cases as it does not consider a real-life interaction task setup such as conversations. Thus we argue that existing commonsense benchmarks in NLP are not enough to train a language agent that produces smooth interpersonal communications, nor evaluate whether models have such capabilities. 3 2 2.1 Task Introduction and Motivations Commonsense-Focused Dialogue Response Generation We study commonsense-focused response generation for"
2021.sigdial-1.13,D16-1230,0,0.0300229,"n collected with different focuses such as incorporating knowledge (Gopalakrishnan et al., 2019; Dinan et al., 2018), empathy (Rashkin et al., 2019), task completion (Budzianowski et al., 2018), consistency (Nie et al., 2020), personality (Zhang et al., 2018) and reasoning (Cui et al., 2020) within dialog systems. There has also been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commo"
2021.sigdial-1.13,2020.acl-main.130,0,0.168291,"ed language models, GPT2 (Radford et al., 2019), and further train it on our training data sets. Specifically, the model is trained in a multitask fashion that minimizes the LM loss as well as the multiple choice loss following Wolf et al. (2019), and generates responses for a given dialog history. We consider the follow three types of training data setups. • Existing data sets, including DailyDialog (Li et al., 2017) (DD), EmpatheticDialogues (Rashkin et al., 2019)(ED), and Topical-Chat (Gopalakrishnan et al., 2019), a knowledge-grounded open-domain dataset with around 11k dialogues. MuTual (Cui et al., 2020) is not included since it is designed for response selection. • As described in Section 3.1, we use ConceptNet to search for potential triples in response turns and filter three dialogue datasets, DD, ED, and MuTual. We combine the three filtered dialogues from these datasets to form our training data, named ‘filter existing’ (FE, total around 21K dialogues). • The third category includes our collected dialogues using SocialIQA contexts. This is used along with the FE data above: FE and all of the 25k collected dialogues (FE+new crowdsourced), and FE plus the 11K filtered dialogues of our coll"
2021.sigdial-1.13,P17-1103,0,0.0204431,"so been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commonsense of a response, which is the focus of this work. 7 Conclusion We present our empirical study on commonsense in dialogue response generation. To obtain data for commonsense-focused analysis in open domain response generation, we use two strategies: filtering existing dialogue data using a commonsense knowledge graph Concep"
2021.sigdial-1.13,2020.acl-main.704,0,0.0162514,"and reasoning (Cui et al., 2020) within dialog systems. There has also been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commonsense of a response, which is the focus of this work. 7 Conclusion We present our empirical study on commonsense in dialogue response generation. To obtain data for commonsense-focused analysis in open domain response generation, we use two strategies: filterin"
2021.sigdial-1.13,2020.acl-main.64,0,0.112351,"s that contain ConceptNet triples as discussed earlier. In addition, it shows that though overall increasing training data size benefits model performance, the quality of data plays a more important role. We Proposed Commonsense Automatic Evaluation Results We now examine the correlation of our proposed automatic metric (MLP regressor) with human scores on the testing portion of our annotations. We cross-validate on the collected dialogues with 0.8/0.1/0.1 proportions. For comparison, we consider three baselines: our MLP with only symbolic features, our MLP with only neural features, and FED (Mehri and Eskenazi, 2020a), which uses DialoGPT to score how likely the next turn after the response expresses confusion. It requires no training nor human references, and has been shown to correlate with humans judgements on different criteria (commonsense not included). Table 4 shows the Spearman’s correlation of the system computed scores and human annotation scores using all the annotated data in a cross-validation setup. We can see that our simple MLP-based regressor reaches the highest spearman’s correlation with human scores, outperforming other baselines significantly. However, such a correlation result still"
2021.sigdial-1.13,P19-1534,0,0.429257,"odels, and show reasonable correlation with human evaluation of responses’ commonsense quality. 1 1 Introduction Open-domain dialogue response generation (RG) models aim to provide human-like natural language responses given dialogue histories (Chen et al., 2017). To improve generated response quality, many studies have been conducted to develop knowledge-grounded RG (Ghazvininejad et al., ∗ Work done while Pei Zhou was an intern at Amazon Alexa AI 1 Data and code will be released soon. 2018; Gopalakrishnan et al., 2019), personalized dialogue agents (Zhang et al., 2018), empathetic response (Rashkin et al., 2019), etc. For all the above-mentioned directions for RG, large-scale dialogue data geared towards the specific goals is crucial, since most current state-of-the-art neural RG models require training on appropriate and large data. Therefore several datasets have been collected to support such research efforts such as knowledge-grounded dialogues (Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019), PersonaChat (Zhang et al., 2018), and EmpatheticDialogues (Rashkin et al., 2019). Producing natural and logically-coherent responses given dialogue contexts involves making commonsense inferences d"
2021.sigdial-1.13,2020.tacl-1.37,0,0.380271,"; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved (Richardson and Sabharwal, 2020; Richardson et al., 2020; Zhou et al., 2020). In this study we tackle the response generation problem in dialogues, with a focus on collecting commonsense rich dialog data and evaluating commonsense quality of model responses. 6.2 Open Domain Dialogue Response Generation Recently open domain dialog systems have been modeled using end-to-end approaches, more specifically encoder-decoder architectures (Sordoni et al., 2015; Serban et al., 2017, 2016; Vinyals and Le, 2015). Recent work focused on finetuning large pre-trained transformer models (Radford et al., 2019; Zhang et al., 2020) on dialog"
2021.sigdial-1.13,D19-1454,0,0.0742573,"Missing"
2021.sigdial-1.13,2020.acl-tutorials.7,0,0.340866,"not consider a real-life interaction task setup such as conversations. Thus we argue that existing commonsense benchmarks in NLP are not enough to train a language agent that produces smooth interpersonal communications, nor evaluate whether models have such capabilities. 3 2 2.1 Task Introduction and Motivations Commonsense-Focused Dialogue Response Generation We study commonsense-focused response generation for dialogues. Commonsense can be defined as “the basic level of practical knowledge and reasoning concerning everyday situations and events that are commonly shared among most people” (Sap et al., 2020). Dialogue response generation is the task of generating a response turn r in a conversational setting given previous history turns h. Thus by combining these two together, we want to examine models’ ability to produce responses that make sense or is plausible in terms of commonsense. Motivations Commonsense Focused Dialogue Collection To collect more commonsense focused dialogues for response generation model training and evaluation, our effort is along two directions: filtering existing data to collect dialogues with responses that consist of commonsense (Section 3.1), and curating new data"
2021.sigdial-1.13,N15-1020,0,0.164442,"Missing"
2021.sigdial-1.13,N19-1421,0,0.0459046,"tion drop when considering either symbolic or neural features alone in our model, indicating that they might each capture a different 127 aspect for evaluating commonsense. datasets to exhibit multiple attributes (Roller et al., 2020). Metrics Spearman’s Correlation p-Value FED Symbolic Ours Neural All features -0.00797 0.12336 0.06176 0.20789 0.80569 1.27E-08 0.00450 4.53E-22 6.3 Table 4: Spearman’s correlation and p-values for different automatic metrics with human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved ("
2021.sigdial-1.13,N18-1101,0,0.0298111,"ith human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved (Richardson and Sabharwal, 2020; Richardson et al., 2020; Zhou et al., 2020). In this study we tackle the response generation problem in dialogues, with a focus on collecting commonsense rich dialog data and evaluating commonsense quality of model responses. 6.2 Open Domain Dialogue Response Generation Recently open domain dialog systems have been modeled using end-to-end approaches, more specifically encoder-decoder architectures (Sordoni et al., 2015; Serban e"
2021.sigdial-1.13,D18-1009,0,0.0457767,"ere is a large correlation drop when considering either symbolic or neural features alone in our model, indicating that they might each capture a different 127 aspect for evaluating commonsense. datasets to exhibit multiple attributes (Roller et al., 2020). Metrics Spearman’s Correlation p-Value FED Symbolic Ours Neural All features -0.00797 0.12336 0.06176 0.20789 0.80569 1.27E-08 0.00450 4.53E-22 6.3 Table 4: Spearman’s correlation and p-values for different automatic metrics with human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning i"
2021.sigdial-1.13,P18-1205,0,0.202871,"ptNet and pretrained language and dialog models, and show reasonable correlation with human evaluation of responses’ commonsense quality. 1 1 Introduction Open-domain dialogue response generation (RG) models aim to provide human-like natural language responses given dialogue histories (Chen et al., 2017). To improve generated response quality, many studies have been conducted to develop knowledge-grounded RG (Ghazvininejad et al., ∗ Work done while Pei Zhou was an intern at Amazon Alexa AI 1 Data and code will be released soon. 2018; Gopalakrishnan et al., 2019), personalized dialogue agents (Zhang et al., 2018), empathetic response (Rashkin et al., 2019), etc. For all the above-mentioned directions for RG, large-scale dialogue data geared towards the specific goals is crucial, since most current state-of-the-art neural RG models require training on appropriate and large data. Therefore several datasets have been collected to support such research efforts such as knowledge-grounded dialogues (Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019), PersonaChat (Zhang et al., 2018), and EmpatheticDialogues (Rashkin et al., 2019). Producing natural and logically-coherent responses given dialogue conte"
2021.sigdial-1.13,2020.acl-demos.30,0,0.416946,"rom ConceptNet. The triple identifying process is the same as our filtering process described earlier (Section 3.1). That is, we first identify a set of concepts in the response turn and query ConceptNet for potential triples and match those with the other concepts appearing in the dialogue history. Two-hop triples are searched in a similar manner, with the only difference being that the number of potential triples will be much larger. We also include the length of the response as an additional feature. As for neural features, we use the scores from a dialogue-focused language model DialoGPT (Zhang et al., 2020) on both the response itself and the dialogue history concatenated with the response. The score from DialoGPT can be considered as the plausibility of the sentence. We train this MLP model using the human evaluation scores for different responses. 5 5.1 Results and Analysis Automatic Evaluation Results Table 2 shows results according to automatic metrics on our 4.6K testing dialogues. We find that perplexity scores for the GPT2 model trained on filtered existing dialogue data (FE), or plus new collected data (FE+Crowdsourced), are much lower than that just trained on existing datasets as is. T"
2021.woah-1.10,N19-1423,0,0.0518153,"Missing"
2021.woah-1.10,N18-2003,0,0.0287652,"F in a classification task (Sec. 3.2). Lastly, we introduce our counterfactual generation method for Assessing Counterfactual Likelihoods (ACL, Sec. 3.3), which is driven by linguistic analysis of stereotype language in sentences. Related Work Unintended bias in classification is defined as differing model performance on subsets of datasets that contain particular SGTs (Dixon et al., 2018; Mehrabi et al., 2019). To mitigate this bias, data augmentation approaches are proposed to create balanced labels for each SGT or to prevent biases from propagating to the learned model (Dixon et al., 2018; Zhao et al., 2018; Park et al., 2018). Other approaches apply regularization of post-hoc token importance (Kennedy et al., 2020b), or adversarial learning for generating fair representations (Madras et al., 2018; Zhang et al., 2018) to minimize the importance of protected features. By altering sensitive features of the input and assessing the changes in the model prediction, counterfactual fairness (Kusner et al., 2017) seeks causal associations between sensitive features and other data attributes and outputs. Similarly, counterfactual token fairness applies counterfactual fairness to tokens in textual data (G"
D16-1144,D15-1103,0,0.175276,"em as multi-class classification following the type mutual exclusion assumption (i.e., one type per mention) (Nadeau and Sekine, 2007). Recent work has focused on a much larger set of fine-grained types (Yosef et al., 2012; Ling and Weld, 2012). As the type mutual exclusion assumption no longer holds, they cast the problem as multilabel multi-class (hierarchical) classification problems (Gillick et al., 2014; Yosef et al., 2012; Ling and Weld, 2012). Embedding techniques are also recently applied to jointly learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual m"
D16-1144,E14-4040,0,0.0180677,"Missing"
D16-1144,P05-1045,0,0.0126774,"D consists of a set of extracted entity mentions M = {mi }N i=1 , the context (e.g., sentence, paragraph) of each mention {ci }N i=1 , and the candiN date type sets {Yi }i=1 for each mention. We repre N sent D using a set of triples D = (mi , ci , Yi ) i=1 . Problem Description. For each test mention, we aim to predict the correct type-path in Y based on the mention’s context. More specifically, the test set T is defined as a set of mention-context pairs (m, c), where mentions in T (denoted as Mt ) are extracted from their sentences using existing extractors such as named entity recognizer (Finkel et al., 2005). We denote the gold type-path for a test mention m as Y ∗ . This work focuses on learning a typing model from the noisy training corpus D, and estimating Y ∗ from Y for each test mention m (in set Mt ), based on mention m, its context c, and the learned model. Framework Overview. At a high level, the AFET framework (see also Fig. 2) learns low-dimensional representations for entity types and text features, and infers type-paths for test mentions using the learned embeddings. It consists of the following steps: Knowledge Base 1. Extract text features for entity mentions in training set M and t"
D16-1144,P08-1030,1,0.634711,".. author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) wor"
D16-1144,D12-1082,0,0.0335026,"Missing"
D16-1144,P14-5010,0,0.0110603,"o similar sets of entities should be more related to each other than those assigned to quite different entities (Jiang et al., 2015) (e.g., actor is Feature Head Token POS Character Word Shape Length Context Brown Cluster Dependency Description Syntactic head token of the mention Tokens in the mention Part-of-Speech tag of tokens in the mention All character trigrams in the head of the mention Word shape of the tokens in the mention Number of tokens in the mention Unigrams/bigrams before and after the mention Brown cluster ID for the head token (learned using D) Stanford syntactic dependency (Manning et al., 2014) associated with the head token Example “HEAD Turing” “Turing”, “Machine” “NN” “:tu”, “tur”, ..., “ng:” “Aa” for “Turing” “2” “CXT B:Maserati ,”, “CXT A:and the” “4 1100”, “8 1101111”, “12 111011111111” “GOV:nn”, “GOV:turing” Table 2: Text features used in this paper. “Turing Machine” is used as an example mention from “The band’s former drummer Jerry Fuchs—who was also a member of Maserati, Turing Machine and The Juan MacLean—died after falling down an elevator shaft.”. more related to director than to author in the left column of Fig. 3). Thus, type correlation between yk and yk0 (denoted as"
D16-1144,P09-1113,0,0.0309695,"thete politician artist ... business man ... author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies"
D16-1144,W09-1119,0,0.0432782,"age processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessm"
D16-1144,P12-1076,0,0.047099,"learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual matches of entity pairs. In entity typing, however, label noise comes from the assignment of types to entity mentions without considering their contexts. The forms 1377 of distant supervision are different in these two problems. Recently, (Ren et al., 2016b) has tackled the problem of label noise in fine-grained entity typing, but focused on how to generate a clean training set instead of doing entity typing. Partial label learning (PLL) (Zhang, 2014; Nguyen and Caruana, 2008; Cour et al., 2011) deals with the pr"
D16-1144,P15-2048,0,0.476766,"didate type set of each mention, all KB types of its KB-linked entity. However, existing distant supervision methods encounter the following limitations when doing automatic fine-grained typing. • Noisy Training Labels. Current practice of distant supervision may introduce label noise to training data since it fails to take a mention’s local contexts into account when assigning type labels (e.g., see Fig. 1). Many previous studies ignore the label noises which appear in a majority of training mentions (see Table. 1, row (1)), and assume all types obtained by distant supervision are “correct” (Yogatama et al., 2015; Ling and Weld, 2012). The noisy labels may mislead the trained models and cause negative effect. A few systems try to denoise the training corpora using simple pruning heuristics such as deleting mentions with conflicting types (Gillick et al., 2014). However, such strategies significantly reduce the size of training set (Table 1, rows (2a-c)) and lead to performance degradation (later shown in our experiments). The larger the target type set, the more severe the loss. • Type Correlation. Most existing methods (Yogatama et al., 2015; Ling and Weld, 2012) treat every type label in a training"
D16-1144,C12-2133,0,0.474884,"(Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessman} in S3 (investment). But he could also be labeled as {person, politician} in S1 or {person, artist, actor} in S2. Such fine-grained type represe"
D17-1005,P14-1091,0,0.0127696,"d on reliabilities of labeling functions, which are calculated with their proficient subsets’ representations. Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction. Besides, the context representation bridges relation extraction with true label dis2 Preliminaries In this section, we would formally define relation extraction and heterogeneous supervision, including the format of labeling functions. 47 fc vi zc li 2.1 Relation Extraction Here we conduct relation extraction in sentencelevel (Bao et al., 2014). For a sentence d, an entity mention is a token span in d which represents an entity, and a relation mention is a triple (e1 , e2 , d) which consists of an ordered entity pair (e1 , e2 ) and d. And the relation extraction task is to categorize relation mentions into a given set of relation types R, or Not-Target-Type (None) which means the type of the relation mention does not belong to R. oc,i o∗c ρc,i Si sc,i ti Table 1: Notation Table. into a set of relation types. Intuitively, errors of annotations (O) come from mismatch of contexts, e.g., in Fig. 1, λ1 annotates c1 and c2 with ’true’ lab"
D17-1005,J92-4003,0,0.206259,"scovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our relation extractor predict it as None, or (2) the entropy of p(.|zc ) over R exceeds a pre-defined t"
D17-1005,P07-1073,0,0.0521594,"he discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset. Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is"
D17-1005,P14-5010,0,0.00210619,"f ρc,i = δ(oc,i = o∗c ). Thus, we would first infer o∗c = argmaxo∗c JT , then train the true label discovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our r"
D17-1005,P09-1113,0,0.942466,"belongs to λi ’s proficient subset relation type embedding for ri ∈ R 2. Text feature embeddings are utilized to calculate relation mention embeddings (see Fig. 2); 3. With relation mention embeddings, true labels are inferred by calculating labeling functions’ reliabilities in a context-aware manner (see Fig. 1); 4. Inferred true labels would ‘supervise’ all components to learn model parameters (see Fig. 1). We now proceed by introducing these components of the model in further details. 3.1 Modeling Relation Mention As shown in Table 2, we extract abundant lexical features (Ren et al., 2016; Mintz et al., 2009) to characterize relation mentions. However, this abundance also results in the gigantic dimension of original text features (∼ 107 in our case). In The REH ESSION Framework Here, we present REH ESSION, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances 48 Feature Entity mention (EM) head Entity Mention Token Tokens between two EMs Part-of-speech (POS) tag Collocations Entity mention order Entity mention distance Body entity mentions numbers Entity mention context Brown cluster (learned on D) Description Syntactic head token of"
D17-1005,C10-1099,0,0.0117953,"or machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to only feed the inferred true labels, referred as ‘TD’. For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score (Bunescu and Mooney, 2005; Bach and Badaskar, 2007) are employed. Notice that both relation extraction and relation classificat"
D17-1005,D15-1205,0,0.0199289,"nts, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to"
D17-1005,P11-1055,0,0.170271,"well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively up"
D17-1005,C14-1220,0,0.0507984,"on to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way. Word embedding has demonstrated great potential in capturing semantic meaning (Mikolov et al., 2013), and achieved great success in a wide range of NLP tasks like relation extraction (Zeng et al., 2014; Takase and Inui, 2016; Nguyen and Grishman, 2015). In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better. 6 Conclusion and Future Work In this paper, we propose REH ESSION, an embedding framework to extract relation under heterogeneous supervision. When dealing with heterogeneous supervisions, one unique challenge is how to resolve conflicts generated by different labeling functions. Accordingly, we go beyond the “source consistency assumption” in prior works and leverage con"
D17-1005,N13-1008,0,0.165874,"0.3325 0.3360 0.1964 0.5645 0.2914 0.3499 0.3923 0.3107 0.5368 0.3879 0.5726 0.4792 0.3677 0.4933 0.4208 Relation Classification NYT Wiki-KBP Accuracy Accuracy 0.6598 0.6226 0.6905 0.5000 0.7954 0.6355 0.7059 0.6484 0.7033 0.5419 0.6485 0.6935 0.7059 0.6355 0.6292 0.5032 0.7570 0.6452 0.6061 0.6613 0.6803 0.5645 0.6409 0.6890 0.8381 0.7277 Table 5: Performance comparison of relation extraction and relation classification Dataset Total Number of RM RM annotated as None RM with conflicts Conflicts involving None Wiki-KBP 225977 100521 32008 30559 NYT 530767 356497 58198 38756 Universal Schemas (Riedel et al., 2013) is proposed to unify different information by calculating a low-rank approximation of the annotations O. It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type. But it doesnt explicitly model noise and not fit our scenario very well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation m"
D17-1005,D15-1174,0,0.0208116,"he context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor pot"
D18-1153,Q16-1026,0,0.0229967,"84 after pruning on the CoNLL03 dataset. It 1 outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54. Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs). Its perplexity is 69 and its performance on the CoNLL03 dataset is 91.55 ± 0.19. 5 Related Work Sequence labeling. Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER. Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Language modeling. Language modeling is a core task in NLP. Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language"
D18-1153,W17-2622,0,0.0634627,"Missing"
D18-1153,C16-1087,0,0.0358238,"predicting the next word. the Input of the Recurrent Unit: input input (a) (b) Figure 3: Layer-wise dropout conducted on a 4-layer densely connected RNN. (a) is the remained RNN. (b) is the original densely connected RNN. In other words, this dropout is only applied to the input of recurrent layers, which aims to imitate the pruned input without totally removing any layers. 3 Sequence Labeling In this section, we will introduce our sequence labeling architecture, which is augmented with the contextualized representations. 3.1 Neural Architecture Following the recent studies (Liu et al., 2018; Kuru et al., 2016), we construct the neural architecture as in Fig. 4. Given the input sequence {x1 , x2 , · · · , xT }, for tth token (xt ), we assume its word embedding is wt , its label is yt , and its character-level input is {ci,1 , ci,2 , · · · , ci, }, where ci, is the space character following xt . The character-level representations have become the required components for most of the state-of-the-art. Following the recent study (Liu et al., 2018), we employ LSTMs to take the character-level input in a context-aware manner, and mark its output for xt as ct . Similar to the contextualized representation,"
D18-1153,N16-1030,0,0.597778,"beling Pierre … … concatenate … … … … … … … … [] fixed lstm lstm Vinken w2 … … … … Pierre Pierre , … … embedding Vinken ␣ c1, V i n k e n c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 ␣ c2, Vinken , , Forward LM Figure 4: The proposed sequence labeling architecture with contextualized representations. After projections, these vectors are concatenated as vt = [c∗t ; rt ; wt ], ∀i ∈ [1, T ] and further fed into the word-level LSTMs. We refer to their output as U = {u1 , · · · , uT }. To ensure the model to predict valid label sequences, we append a first-order conditional random field (CRF) layer to the model (Lample et al., 2016). Specifically, the model defines the generation probability of y = {y1 , · · · , yT } as QT t=1 φ(yt−1 , yt , ut ) QT yt−1 , yˆt , ut ) ˆ ∈Y(U) t=1 φ(ˆ y p(y|U) = P (8) ˆ = {ˆ where y y1 , . . . , yˆT } is a generic label sequence, Y(U) is the set of all generic label sequences for U and φ(yt−1 , yt , ut ) is the potential function. In our model, φ(yt−1 , yt , ut ) is defined as exp(Wyt ut + byt−1 ,yt ), where Wyt and byt−1 ,yt are the weight and bias parameters. 3.2 Model Training and Inference We use the following negative log-likelihood as the empirical risk. X L=− log p(y|U) (9) U For tes"
D18-1153,P16-1101,0,0.0755163,"outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54. Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs). Its perplexity is 69 and its performance on the CoNLL03 dataset is 91.55 ± 0.19. 5 Related Work Sequence labeling. Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER. Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Language modeling. Language modeling is a core task in NLP. Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language modeling as an additional supervision, an"
D18-1153,P17-1161,0,0.457217,"o the averaged perplexity of the forward and the backward LMs. We can observe that, for those models taking word embedding as the input, embedding composes the vast majority of model parameters. However, embedding can be embodied as a “sparse” layer which is computationally efficient. Instead, the intense calculations are conducted in 1219 Network 8192-1024 (J´ozefowicz et al., 2016) Ind. # Hid. # 1 8192 Layer # Param.# (·107 ) PPL RNN Others 2 15.1] 163] 30.6 89] 30.0 CNN-8192-1024 (J´ozefowicz et al., 2016) 2 8192 2 15.1] CNN-4096-512 (Peters et al., 2018) 3 4096 2 3.8] 40.6] 39.7 2048-512 (Peters et al., 2017) 4 2048 1 0.9] 40.6] 47.50 2048-Adaptive (Grave et al., 2017) 5 2048 2 5.2† 26.5† 39.8 25.6† vanilla LSTM 6 7 2048 1600 2 2 5.3† 3.2† 24.2† 40.27 48.85 LD-Net without Layer-wise Dropout 8 300 10 2.3† 24.2† 45.14 LD-Net with Layer-wise Dropout 9 300 10 2.3† 24.2† 50.06 Table 1: Performance comparison of language models. Models marked with† adopted adaptive softmax and the vanilla LSTMs, which has less softmax parameters. Models marked with] employed sampled softmax LSTMs w. projection, which results in less RNN parameters w.r.t. the size of hidden states. RNN layers and softmax layer for langua"
D18-1153,N18-1202,0,0.458912,"., 2017), slim word embedding (Li et al., 2018), the sampled softmax and the noise contrastive estimation (J´ozefowicz et al., 2016). Since the major focus of our paper does not lie in the language modeling task, we choose the adaptive softmax because of its practical efficiency when accelerated with GPUs. 2.3 Contextualized Representations As pre-trained LMs can describe the text generation accurately, they can be utilized to extract information and construct features for other tasks. These features, referred as contextualized representations, have been demonstrated to be essentially useful (Peters et al., 2018). To capture information from both directions, we utilized not only forward LMs, but also backward LMs. Backward LMs are based on Eqn. 4 instead of Eqn. 2. Similar to forward LMs, backward LMs approach p(xt |xt+1 , · · · , xT ) with NNs. For reference, the output of the RNN in backward LMs for xt is recorded as hrt . p(x1 , · · · , xn ) = T Y p(xt |xt+1 , · · · , xT ) (4) t=1 Ideally, the final output of LMs (e.g., h∗t ) would be the same as the representation of the target word (e.g., xt+1 ); therefore, it may not contain much context information. Meanwhile, the output of the densely connecte"
D18-1153,W09-1119,0,0.118852,"Following TagLM (Peters et al., 2017), we evaluate our methods in two benchmark datasets, the CoNLL03 NER task (Tjong Kim Sang and De Meulder, 2003) and the CoNLL00 Chunking task (Tjong Kim Sang and Buchholz, 2000). CoNLL03 NER has four entity types and includes the standard training, development and test sets. CoNLL00 chunking defines eleven syntactic chunk types (e.g., NP and VP) in addition to Other. Since it only includes training and test sets, we sampled 1000 sentences from training set as a held-out development set (Peters et al., 2017). In both cases, we use the BIOES labeling scheme (Ratinov and Roth, 2009) and use the micro-averaged F1 as the evaluation metric. Based on the analysis conducted in the development set, we set λ0 = 0.05 for the NER task, and λ0 = 0.5 for the Chunking task. As discussed before, we conduct optimization with the stochastic gradient descent with momentum. We set the batch size, the momentum, and the learning rate to 10, 0.9, η0 and ηt = 1+ρt respectively. Here, η0 = 0.015 is the initial learning rate and ρ = 0.05 is the decay ratio. Dropout is applied in our model, and its ratio is set to 0.5. For a better stability, we use gradient clipping of 5.0. Furthermore, we emp"
D18-1153,P17-1194,0,0.0218824,"een paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017). Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (J´ozefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017). Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language modeling as an additional supervision, and conduct co-training for knowledge transfer (Dai and Le, 2015; Liu et al., 2018; Rei, 2017). Others, including this paper, aim to construct additional features (referred as contextualized representations) with the pre-trained language models (Peters et al., 2017, 2018). Neural Network Acceleration. There are mainly three kinds of NN acceleration methods, i.e., prune network into smaller sizes (Han et al., 2015; Wen et al., 2016), converting float operation into customized low precision arithmetic (Hubara et al., 2018; Courbariaux et al., 2016), and using shallower networks to mimic the output of deeper ones (Hinton et al., 2015; Romero et al., 2014). However, most of them require co"
D18-1153,W00-0726,0,0.662905,"Missing"
D18-1153,W03-0419,0,0.71951,"Missing"
D18-1230,P05-1045,0,0.681709,"ate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictiona"
D18-1230,W17-5224,0,0.308462,"ed models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictionaries) are becoming increasingly popular, such as WikiData and YAGO in the general domain, as well as MeSH and CTD in the biomedical domain. The existence of such dictionaries makes it possible to generate training data for NER at a large scale without additional human effort. Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Gian"
D18-1230,W12-3016,0,0.0556348,"Missing"
D18-1230,P16-1101,0,0.68106,"nal framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including"
D18-1230,P09-1113,0,0.809382,"018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009). Meanwhile, open knowledge bases (or dictionaries) are becoming increasingly popular, such as WikiData and YAGO in the general domain, as well as MeSH and CTD in the biomedical domain. The existence of such dictionaries makes it possible to generate training data for NER at a large scale without additional human effort. Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Giannakopoulos et al., 2017; He, 2017). In these m"
D18-1230,S14-2004,0,0.0744032,"Missing"
D18-1230,W09-1119,0,0.808465,"me surface name in the dictionary. To address these challenges, we propose and compare two neural architectures with customized tagging schemes. We start with adjusting models under the traditional sequence labeling framework. Typically, NER models are built upon conditional random 2054 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2054–2064 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics fields (CRF) with the IOB or IOBES tagging scheme (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Ratinov and Roth, 2009; Finkel et al., 2005). However, such design cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTMCRF (Lample et al., 2016) into a Fuzzy CRF layer, which allows each token to have multiple labels without sacrificing computing efficiency. To adapt to imperfect labels generated by distant supervision, we go beyond the traditional sequence labeling framework and propose a new prediction model. Specifically, instead of predicting the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not (i."
D18-1230,N16-1030,0,0.858046,"ropose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks. 1 Introduction Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016). However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005). This is particularly challenging in specific do∗ Equal contribution. ‡ teng.ren@cootek.cn mains, where domain-expert annotation is expensive and/or slow to obtain. To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang"
D18-1230,P16-1209,0,0.0234109,"leave further improvements of AutoNER to the future work. 6 Related Work The task of supervised named entity recognition (NER) is typically embodied as a sequence labeling problem. Conditional random fields (CRF) models built upon human annotations and handcrafted features are the standard (Finkel et al., 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin, 2006). Unlike these existing approaches, our study focuses on the distantly supervised setting without any expert-curated training data. Distant supervision has attracted many attentions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to supervise relation"
D18-1230,W04-1221,0,0.182675,"o without sufficient human annotations. Still, we observe that, when the supervised benchmark is trained with all annotations, it achieves the performance better than AutoNER. We conjugate that this is because AutoNER lacks more advanced techniques to handle distant supervision, and we leave further improvements of AutoNER to the future work. 6 Related Work The task of supervised named entity recognition (NER) is typically embodied as a sequence labeling problem. Conditional random fields (CRF) models built upon human annotations and handcrafted features are the standard (Finkel et al., 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin,"
D18-1230,W06-3328,0,0.0544292,", 2005; Settles, 2004; Leaman and Gonzalez, 2008). Recent advances in neural models have freed do2061 main experts from handcrafting features for NER tasks. (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018). Such neural models are increasingly common in the domain-specific NER tasks (Sahu and Anand, 2016; Dernoncourt et al., 2017; Wang et al., 2018). Semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition (Kuksa and Qi, 2010; Tang et al., 2014; Vlachos and Gasperin, 2006). Unlike these existing approaches, our study focuses on the distantly supervised setting without any expert-curated training data. Distant supervision has attracted many attentions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to supervise relation extraction tasks (Craven et al., 1999; Mintz et al., 2009). AutoPhrase has demonstrated powers in extracting high-quality phrases from domain-specific corpora like scientific papers and business reviews (Shang et al., 2018) but it cannot categorize phrases into typed entities in a contextaware manner. We incorp"
D19-1026,D16-1245,0,0.0252187,"oven to be able to alleviate such accumulated noises in the decision sequence in many recent works (Narasimhan et al., 2016; Feng et al., 2018). In this work, we propose an RL ranking model for DCA-enhanced entity linking. Gt = ρT −t RT (7) To maximize the expected reward of all trajectories, the Agent utilizes the REINFORCE algorithm (Sutton and Barto, 1998) to compute Monte Carlo policy gradient over all trajectories, and perform gradient ascent on its parameters: X 0 θ ←θ+α Gt ∇θ ln πθ (Ajt |St−1 , St−1 ) (8) Agent: The Agent is a candidate ranking model that has a similar architecture to (Clark and Manning, 2016), aiming to output the action prefer0 , Aj ) of each linking action ence Hθ (St−1 , St−1 t Ajt = (mt → ejt ). It is a 2-layer feedforward neural network with following components: t In following sections, to fully investigate the effectiveness of the proposed method, we report and compare the performances of both the Supervisedlearning model and the Reinforcement-learning model. Input Layer: For each (mt , ejt ) pair, DCARL extracts context-dependent features from 0 , and concatenates them with other St−1 , St−1 context-independent features to produce an Idimensional input vector h~0 (mt , ejt"
D19-1026,D07-1074,0,0.185755,"Missing"
D19-1026,D16-1261,0,0.0312163,"linked mentions. Then the value Gt (expected reward) of each previous state St can be retraced back with a discount factor ρ according to RT : Reinforcement Learning Method Naturally, the usage of dynamic context augmentation forms a sequential decision problem, as each linking step depends on previous linking decisions. Correct linking results provide valuable information for future decisions, while previous mistakes can lead to error accumulation. Reinforcement Learning (RL) algorithms have proven to be able to alleviate such accumulated noises in the decision sequence in many recent works (Narasimhan et al., 2016; Feng et al., 2018). In this work, we propose an RL ranking model for DCA-enhanced entity linking. Gt = ρT −t RT (7) To maximize the expected reward of all trajectories, the Agent utilizes the REINFORCE algorithm (Sutton and Barto, 1998) to compute Monte Carlo policy gradient over all trajectories, and perform gradient ascent on its parameters: X 0 θ ←θ+α Gt ∇θ ln πθ (Ajt |St−1 , St−1 ) (8) Agent: The Agent is a candidate ranking model that has a similar architecture to (Clark and Manning, 2016), aiming to output the action prefer0 , Aj ) of each linking action ence Hθ (St−1 , St−1 t Ajt = (m"
D19-1026,N16-1150,0,0.0167182,"es may be irrelevant to the current mention. Some falsely linked entities may even introduce noise. To alleviate error propagation, we further explore two strategies: (1) soft/hard attention mechanisms that favour the most relevant entities; (2) a reinforcement learning-based ranking model, which proves to be effective as reported in other information extraction tasks. 2.2 Local Base Models for Entity Linking We apply the DCA process in two popular local models with different styles: the first is a neural attention model named ETHZ-Attn (Ganea and Hofmann, 2017), the other is the BerkeleyCNN (Francis-Landau et al., 2016) model which is made up of multiple convolutional neural networks (CNN). ETHZ-Attn. For each mention mt and a candidate ejt ∈ Et , three local features are considered: (1) Mention-entity Prior Pˆ (ejt |mt ) is the empirical distribution estimated from massive corpus (e.g.Wikipedia); (2) Context Similarity ΨC (mt , ejt ) measures the textual similarity between ejt and the local context of mt ; (3) Type Similarity ΨT (mt , ejt ) considers the similarity between the type of ejt and contexts around mt . Pˆ (ejt |mt ) and ΨC (mt , ejt ) are calculated in the same way as (Ganea and Hofmann, 2017). F"
D19-1026,D17-1277,0,0.345507,"Missing"
D19-1026,C16-1218,0,0.0355655,"Missing"
D19-1026,P16-1059,0,0.286506,"fully investigate the effectiveness of the proposed method, we report and compare the performances of both the Supervisedlearning model and the Reinforcement-learning model. Input Layer: For each (mt , ejt ) pair, DCARL extracts context-dependent features from 0 , and concatenates them with other St−1 , St−1 context-independent features to produce an Idimensional input vector h~0 (mt , ejt ). 5 Analysis of Computational Complexity For each document D, the train and inference of the global EL models are heavily relied on the inter-entity coherence graph Φg . Many studies (Ratinov et al., 2011; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) obtain Φg by calculating all Hidden Layers: Let Drop(~x) be the dropout operation (Srivastava et al., 2014) and ReLU (~x) be the rectifier nonlinearity (Nair and Hinton, 2010). So the output ~h1 of the hidden layer is defined as: ~h1 = Drop(ReLU (W ~ 1 · ~h0 + ~b1 )), (6) (5) 274 # mention # doc Mentions per doc Gold recall AIDA-train AIDA-A AIDA-B 18448 4791 4485 946 216 231 19.5 22.1 19.4 97.3 98.3 MSNBC AQUAINT ACE2004 CWEB WIKI 656 727 257 11154 6821 20 50 36 320 320 32.8 14.5 7.1 34.8 21.3 98.5 94.2 90.6 91.1 92.4 Dataset"
D19-1026,D14-1162,0,0.0909287,"Missing"
D19-1026,P13-2006,0,0.0573311,"Missing"
D19-1026,N15-1026,0,0.0379008,"Missing"
D19-1026,D11-1072,0,0.801123,"Missing"
D19-1026,U14-1005,0,0.021506,"a tradeoff between accuracy and efficiency: state-of-theart collective/global models suffer from high time complexity. From the perspective of computational efficiency, optimal global configuration inference is NP-hard. Approximation methods, such as loopy belief propagation (Ganea and Hofmann, 2017) or iterative substitutions (Shen et al., 2015), are still computationally expensive due to the huge hypothesis space, and thus can hardly be scaled to handle large corpus. Many previous works have discussed the urgent needs of more efficient linking system for production, both in time complexity (Hughes et al., 2014) and memory consumption (Blanco et al., 2015). 2 2.1 Background Problem Definition Given a set of entity mentions M = {m1 , ..., mT } in corpus D, Entity Linking aims to link each mention mt to its corresponding gold entity e∗t . Such a process is usually divided into two steps: Candidate generation first collects a set of possible (can|E | didate) entities Et = {e1t , ..., et t } for mt ; Candidate ranking is then applied to rank all candidates by likelihood. The linking system selects the top ranked candidate as the predicted entity eˆt . The key challenge is to capture high-quality features"
D19-1026,P11-1138,0,0.236086,"Missing"
D19-1026,P18-1148,0,0.720317,"pare the performances of both the Supervisedlearning model and the Reinforcement-learning model. Input Layer: For each (mt , ejt ) pair, DCARL extracts context-dependent features from 0 , and concatenates them with other St−1 , St−1 context-independent features to produce an Idimensional input vector h~0 (mt , ejt ). 5 Analysis of Computational Complexity For each document D, the train and inference of the global EL models are heavily relied on the inter-entity coherence graph Φg . Many studies (Ratinov et al., 2011; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) obtain Φg by calculating all Hidden Layers: Let Drop(~x) be the dropout operation (Srivastava et al., 2014) and ReLU (~x) be the rectifier nonlinearity (Nair and Hinton, 2010). So the output ~h1 of the hidden layer is defined as: ~h1 = Drop(ReLU (W ~ 1 · ~h0 + ~b1 )), (6) (5) 274 # mention # doc Mentions per doc Gold recall AIDA-train AIDA-A AIDA-B 18448 4791 4485 946 216 231 19.5 22.1 19.4 97.3 98.3 MSNBC AQUAINT ACE2004 CWEB WIKI 656 727 257 11154 6821 20 50 36 320 320 32.8 14.5 7.1 34.8 21.3 98.5 94.2 90.6 91.1 92.4 Dataset System O(Φg ) = O( |Ei | X |Ej | X Φ(ei x , ej y )) (9) , where is"
D19-1026,K16-1025,0,0.350732,"fectiveness of the proposed method, we report and compare the performances of both the Supervisedlearning model and the Reinforcement-learning model. Input Layer: For each (mt , ejt ) pair, DCARL extracts context-dependent features from 0 , and concatenates them with other St−1 , St−1 context-independent features to produce an Idimensional input vector h~0 (mt , ejt ). 5 Analysis of Computational Complexity For each document D, the train and inference of the global EL models are heavily relied on the inter-entity coherence graph Φg . Many studies (Ratinov et al., 2011; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) obtain Φg by calculating all Hidden Layers: Let Drop(~x) be the dropout operation (Srivastava et al., 2014) and ReLU (~x) be the rectifier nonlinearity (Nair and Hinton, 2010). So the output ~h1 of the hidden layer is defined as: ~h1 = Drop(ReLU (W ~ 1 · ~h0 + ~b1 )), (6) (5) 274 # mention # doc Mentions per doc Gold recall AIDA-train AIDA-A AIDA-B 18448 4791 4485 946 216 231 19.5 22.1 19.4 97.3 98.3 MSNBC AQUAINT ACE2004 CWEB WIKI 656 727 257 11154 6821 20 50 36 320 320 32.8 14.5 7.1 34.8 21.3 98.5 94.2 90.6 91.1 92.4 Dataset System O(Φg ) = O( |"
D19-1026,N18-1071,0,0.0563405,"ambiguation for dynamic web data like Twitter). In contrast, the computational complexity of our model is O(T × |E |× I × K), where K is the key hyper-parameter described in Section 3 and is usually set to a small number. This indicates the response time of our method grow linearly as a function of T × |E|. Φ(ei x , ej y ) 6.1 Prior (p(e|m)) (Ganea and Hofmann, 2017) 71.51 84.21 92.72 ± 0.3 92.37 ± 0.1 90.88 94.64 ± 0.2 93.73 ± 0.2 To evaluate the generalization ability of each model, we apply cross-domain experiments following the same setting in (Ganea and Hofmann, 2017; Le and Titov, 2018; Yang et al., 2018). Models are trained on AIDA-train, and evaluated on five popular public datasets: AQUAINT (Milne and Witten, 2008), MSNBC (Cucerzan, 2007), ACE2004 (Ratinov et al., 2011), CWEB (Guo and Barbosa, 2016) and WIKI (Guo and Barbosa, 2016). The statistics of these datasets are available in Table 1. In the candidate generation step, we directly use the candidates provided by the Ment-Norm system (Le and Titov, 2018)2 , and their quality is also listed in Table 1. i=1 j=1,j6=i ei x ∈Ei ej y ∈Ej 6 84.8 89.0 90.7 91.0 92.22 93.07 Table 2: In-domain Performance Comparison on the AIDA-B Dataset. For our"
D19-1042,N18-1150,0,0.0126189,"l probability distribution pFlat of all the labels on the hierarchy: pFlat = σ(WFlat ed ). The combination of the base model and the flat component functions the same as a flat model and ensures that the object representation ed has the capability of flat classification. We denote the flat loss that measures the binary the labels P entropy over all P cross Flat (l) + (1 − L (l) × logp by Of = − N i i=1 l∈L i (l)). Combining the flat Li (l)) × log(1 − pFlat i and local losses, the supervised loss in HiLAP-SL is defined as OSL = λOf + (1 − λ)Ol , where λ ∈ [0, 1] is the mixing ratio. Similar to Celikyilmaz et al. (2018), we also found that mixing a proportion of the supervised loss is beneficial to the learning process of HiLAP. Further combining the global information Og (i.e., ORL ), the total loss of HiLAP is defined as O = ORL + αOSL , where α is a scaling factor accounting for the difference in magnitude between ORL and OSL . While we do not use the flat component during inference, it helps the representation learning of the base model and improves the performance of both HiLAP-SL and HiLAP (see Sec. 4.5). 4 Dataset Evaluation Metrics. We use standard metrics (Johnson and Zhang, 2014; Meng et al., 2018;"
D19-1042,P18-1229,1,0.848397,"ormulate HTC as Rewards. The agent receives scalar rewards as feedback for its actions. Different from exist447 ing work where each label of one example2 is treated independently, HiLAP measures the quality of all the labels assigned to each example xi by rewarding the agent with the Example-based F1 (see Sec. 4.1 for details of this metric). Intuitively, the agent would realize how similar the assigned and the ground-truth labels of one example are. Instead of waiting until the end of the label assignment process and comparing the predicted labels with the gold labels, we use reward shaping (Mao et al., 2018), i.e., giving intermediate rewards at each time step, to accelerate the learning process. Specifically, we set the reward r of xi at time step t to be the difference of Example-based F1 scores between current and the last time step: i rtxi = F1xt i − F1xt−1 . If current F1 is better than that at the last time step, the reward would be positive, and vice versa. The cumulative reward from current time step to the end of an episode would cancel the intermediate rewards and thus reflect whether the current action improves the holistic label assignment or not. As a result, the learned policy would"
D19-1042,D14-1162,0,0.0817881,"Missing"
D19-1042,P09-2042,0,0.073075,"Missing"
D19-1042,D14-1181,0,0.00689806,"Missing"
D19-1042,D16-1076,0,0.0360859,"Missing"
D19-1042,N16-1174,0,0.496145,"dy shows that HiLAP is especially beneficial to those unpopular labels at the bottom levels. been predicted positive. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale. Global approaches use one single classifier and model the label hierarchy more explicitly. Traditional global approaches (Wang et al., 2001; Silla Jr and Freitas, 2009) are largely based on specific flat models and often make unrealistic assumptions (Cai and Hofmann, 2004) as in flat approaches. Recent neural approaches (Kim, 2014; Yang et al., 2016) mainly focus on flat classification while their performance in HTC is relatively less studied. Even if the classification is supposed to be hierarchical, prior work (Gopal and Yang, 2013; Johnson and Zhang, 2014; Peng et al., 2018) still make flat and independent predictions or utilize simple constraints without considering the holistic quality of label assignment. One recent framework (Wehrmann et al., 2018) attempts to leverage both local and global information but it uses static features as input and its inference process is still flat. In this paper, we formulate HTC as a Markov decision"
D19-1269,D18-1362,0,0.113394,"Task. Given an entity (e.g., Miami) and a query relation (e.g., located in), we learn to infer reasoning paths over the existing graph structure to help predict the answer entity (i.e., USA). Knowledge graph completion or reasoning—i.e., the task of inferring the missing facts (entity relationships) for a given graph—is an important problem in natural language processing and has a wide range of applications (Bordes et al., 2011; Socher et al., 2013; Trouillon et al., 2016). Recent neural graph reasoning methods, such as MINERVA (Das et al., 2017), DeepPath (Xiong et al., 2017) and Multi-Hop (Lin et al., 2018), have achieved impressive results on the task, offering both good prediction accuracy (compared to embedding-based methods (Trouillon et al., Work done while the authors interned at USC. B.M.W. trade_war_with ∧ is_main_producer_ of→ raise_tariff_of Introduction ∗ is_main_producer_ of USA summarize pattern In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open know"
D19-1269,P16-1200,0,0.0248907,"set of sentences labeled with respective entity pairs, namely C = {(si : (ek , ej ))|si ∈ S, ek , ej ∈ E}, where S is the set of sentences, and the corpus shares the same entity set with G. In our problem setting, we assume the entities have already been extracted; thus, extracting facts from the corpus is equivalent to the relation extraction task. We process the corpus by labeling the sentences with subject and object entity pairs through Distant Supervision (Mintz et al., 2009). There may be many sen2673 tences labeled with the same entity pair. Following the formulation of previous work (Lin et al., 2016), we organize the sentences into sentence bags, i.e., A sentence bag contains the sentences which are labeled with the same entity pair. Problem. Formally, Open Knowledge Graph Reasoning (OKGR) aims to perform KGR based on both G and C, where G is dynamically enriched by the facts extracted from C. This paper focuses on OKGR, i.e., empowering KGR with the corpus information and enriching the graph with relevant facts dynamically. Thus, the evaluation of the relation extraction performance are out of the scope of this paper. We leave this as future work. 3 Proposed Framework Overview. To resolv"
D19-1269,P09-1113,0,0.0418873,"paths, while traditional KG completion methods rank all possible answer triples by exhaustively enumerating. A background corpus is a set of sentences labeled with respective entity pairs, namely C = {(si : (ek , ej ))|si ∈ S, ek , ej ∈ E}, where S is the set of sentences, and the corpus shares the same entity set with G. In our problem setting, we assume the entities have already been extracted; thus, extracting facts from the corpus is equivalent to the relation extraction task. We process the corpus by labeling the sentences with subject and object entity pairs through Distant Supervision (Mintz et al., 2009). There may be many sen2673 tences labeled with the same entity pair. Following the formulation of previous work (Lin et al., 2016), we organize the sentences into sentence bags, i.e., A sentence bag contains the sentences which are labeled with the same entity pair. Problem. Formally, Open Knowledge Graph Reasoning (OKGR) aims to perform KGR based on both G and C, where G is dynamically enriched by the facts extracted from C. This paper focuses on OKGR, i.e., empowering KGR with the corpus information and enriching the graph with relevant facts dynamically. Thus, the evaluation of the relatio"
D19-1269,D18-1455,0,0.0243172,"et al., 2016) conducts the same task with the raw corpus text. As a newer joint model developed from (Han et al., 2016), (Han et al., 2018) deals with fact extraction by employing the mutual attention. Open-World KG Completion. There are works focusing on similar topics as ours. (Shi and Weninger, 2018) defines an Open World KG Completion problem, in which they complete the KG with unseen entities. (Friedman and Broeck, 2019) introduces the Open-World Probabilistic Databases, an analogy to KGs. Unlike our setting, they try to complete the KG with logical inferences without extra information. (Sun et al., 2018) proposes an open, incomplete KB environment (or KG) with text corpora, but they focus on extracting answers from question-specific subgraphs. 7 Conclusion In this paper, we focus on a new task named as Open Knowledge Graph Reasoning, which aims at boosting the knowledge graph reasoning with new knowledge extracted from the background corpus. We propose a novel and general framework, namely Collaborative Policy Learning, for this task. CPL trains two collaborative agents, the reasoner and fact extractor, which learns the path-reasoning policy and relevant-fact-extraction policy respectively. C"
D19-1269,D15-1174,0,0.0486967,"tional neural networks (Dettmers et al., 2018). Performances of these methods are promising, but their predictions are barely interpretable. RL plays a crucial role in interpretable KG reasoning. MINERVA (Das et al., 2017) and DeepPath (Xiong et al., 2017) employ policy networks; (Xiong et al., 2017) uses extra rule-based supervision. Multi-hop (Lin et al., 2018) improves MINERVA via reward shaping and action drop-out. Joint Embedding of Text and KG. Joint embedding methods aim to unite text corpus and KG. Contrary to our focus, they mainly utilize KGs for better performances of other tasks. (Toutanova et al., 2015) focuses on fact-extraction on the corpus labeled via dependency parsing with the aids of KG and word embeddings, while (Han et al., 2016) conducts the same task with the raw corpus text. As a newer joint model developed from (Han et al., 2016), (Han et al., 2018) deals with fact extraction by employing the mutual attention. Open-World KG Completion. There are works focusing on similar topics as ours. (Shi and Weninger, 2018) defines an Open World KG Completion problem, in which they complete the KG with unseen entities. (Friedman and Broeck, 2019) introduces the Open-World Probabilistic Datab"
D19-1269,D17-1060,0,0.129607,"on of the Knowledge Graph Reasoning Task. Given an entity (e.g., Miami) and a query relation (e.g., located in), we learn to infer reasoning paths over the existing graph structure to help predict the answer entity (i.e., USA). Knowledge graph completion or reasoning—i.e., the task of inferring the missing facts (entity relationships) for a given graph—is an important problem in natural language processing and has a wide range of applications (Bordes et al., 2011; Socher et al., 2013; Trouillon et al., 2016). Recent neural graph reasoning methods, such as MINERVA (Das et al., 2017), DeepPath (Xiong et al., 2017) and Multi-Hop (Lin et al., 2018), have achieved impressive results on the task, offering both good prediction accuracy (compared to embedding-based methods (Trouillon et al., Work done while the authors interned at USC. B.M.W. trade_war_with ∧ is_main_producer_ of→ raise_tariff_of Introduction ∗ is_main_producer_ of USA summarize pattern In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target"
D19-1282,N19-1423,0,0.0719157,"et being {classroom(7), office (3), desk drawer (7)}, a commonsense reasoner is expected to differentiate the correct choice from other “distractive” candidates. False choices are usually highly related to the question context, but just less possible in realworld scenarios, making the task even more challenging. This paper aims to tackle the research question of how we can teach machines to make such commonsense inferences, particularly in the question-answering setting. It has been shown that simply fine-tuning large, pre-trained language models such as G PT (Radford et al., 2018) and B ERT (Devlin et al., 2019) 2829 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2829–2839, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics can be a very strong baseline method. However, there still exists a large gap between performance of said baselines and human performance. Reasoning with neural models is also lacking in transparency and interpretability. There is no clear way as to how they manage to answer commonsense questions, thus making their inferences"
D19-1282,K17-1010,0,0.0348333,"exactly match n-grams in sentences with the surface tokens of concepts in V . For example, in the question “Sitting too close to watch tv can cause what sort of pain?”, the exact matching result Cq would be {sitting, close, watch tv, watch, tv, sort, pain, etc.}. We are aware of the fact that such retrieved mentioned concepts are not always perfect (e.g. “sort” is not a semantically related concept, “close” is a polysemous concept). How to efficiently retrieve contextually-related knowledge from noisy knowledge resources is still an open research question by itself (Weissenborn et al., 2017; Khashabi et al., 2017), and thus most prior works choose to stop here (Zhong et al., 2018; Wang et al., 2019b). We enhance this straightforward approach with some rules, such as soft matching with lemmatization and filtering of stop words, and further deal with noise by pruning paths (§3.3) and reducing their importance with attention mechanisms (§4.3). 3.2 Schema Graph Construction ConceptNet. Before diving into the construction of schema graphs, we would like to briefly introduce our target knowledge graph ConceptNet. ConceptNet can be seen as a large set of triples of the form (h, r, t), like (ice, HasProperty,"
D19-1282,D17-1082,0,0.113654,"Missing"
D19-1282,P16-1137,0,0.0759244,"fined or reusable structures for explainable commonsense reasoning. We argue that it would be more beneficial to propose reasoners that can exploit commonsense knowledge bases (Speer et al., 2017; Tandon et al., 2017; Sap et al., 2019a). Knowledgeaware models can explicitly incorporate external knowledge as relational inductive biases (Battaglia et al., 2018) to enhance their reasoning capacity, as well as to increase the transparency of model behaviors for more interpretable results. Furthermore, a knowledge-centric approach is extensible through commonsense knowledge acquisition techniques (Li et al., 2016; Xu et al., 2018). We propose a knowledge-aware reasoning framework for learning to answer commonsense questions, which has two major steps: schema graph grounding (§3) and graph modeling for inference (§4). As shown in Fig. 1, for each pair of question and answer candidate, we retrieve a graph from external knowledge graphs (e.g. ConceptNet) in order to capture the relevant knowledge for determining the plausibility of a given answer choice. The graphs are named “schema graphs” inspired by the schema theory proposed by Gestalt psychologists (Axelrod, 1973). The grounded schema graphs are usu"
D19-1282,2021.ccl-1.108,0,0.387327,"Missing"
D19-1282,D17-1159,0,0.0473593,"ir specific schema graphs context. Think of polysemous concepts such as “close” (§3.1), which can either be a verb concept like in “close the door” or an adjective concept meaning “a short distance apart”. Using GCNs to update the concept vector with their neighbors is thus helpful for disambiguation and contextualized concept embedding. Also, the pattern of schema graph structures provides potentially valuable information for reasoning. For instance, shorter and denser connections between question and answer concepts could mean higher plausibility under specific contexts. As many works show (Marcheggiani and Titov, 2017; Zhang et al., 2018), relational GCNs (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information. We thus apply GCNs on the plain version (unlabeled, nondirectional) of schema graphs, ignoring relation types on the edges. Specifically, the vector for concept ci ∈ Vg in the schema graph g is initialized by their pre-trained embeddings at first (0) (hi = Vi ). Then, we update them at the (l + 1)th layer by pooling features of their neighboring nodes (Ni ) and their own at the l-th layer with an non-linear activation function σ"
D19-1282,P18-1076,0,0.281115,"IDAF++ utilizes Google web snippets as context and is further augmented with a self-attention layer while using ELM O as input features. G PT/B ERT-L ARGE are fine-tuning methods with an additional linear layer for classification as the authors suggested. They both add a special token ‘[sep]’ to the input and use the hidden state of the ‘[cls]’ as the input to the linear layer. More details about them can be found in the dataset paper (Talmor et al., 2019). • Knowledge-aware Methods. We also adopt some recently proposed methods of incorporating knowledge graphs for question answering. KVM EM (Mihaylov and Frank, 2018) is a method that incorporates retrieved triples from ConceptNet at the word-level, which uses a key-valued memory module to improve the representation of each token individually by learning an attentive aggregation of related triple vectors. CBPT (Zhong et al., 2018) is a plug-in method of assembling the predictions of any models with a straightforward method of utilizing pre-trained concept embeddings from ConceptNet. T EXT G RAPH C AT (Wang et al., 2019c) concatenates the graphbased and text-based representations of the statement and then feed it into a classifier. We create sentence templa"
D19-1282,D18-1260,0,0.15125,"Missing"
D19-1282,P19-1487,0,0.272313,"improve the representation of each token individually by learning an attentive aggregation of related triple vectors. CBPT (Zhong et al., 2018) is a plug-in method of assembling the predictions of any models with a straightforward method of utilizing pre-trained concept embeddings from ConceptNet. T EXT G RAPH C AT (Wang et al., 2019c) concatenates the graphbased and text-based representations of the statement and then feed it into a classifier. We create sentence template for generating sentences and then feed retrieved triples as additional text inputs as a baseline method T RIPLE S TRING. Rajani et al. (2019) propose to collect human explanations for commonsense reasoning from annotators as additional knowledge (C O S-E), and then train a language model based on such human annotations for improving the model performance. 5.3 Implementation Details of KagNet Our best (tested on OFdev) settings of K A G N E T have two GCN layers (100 dim, 50dim respectively), and one bidirectional LSTMs (128dim) . We pre-train KGE using TransE (100 dimension) initialized with GloVe embeddings. The statement encoder in use is B ERT-L ARGE, which works as a pre-trained sentence encoder to obtain fixed features for eac"
D19-1282,D19-1454,0,0.0692043,"Missing"
D19-1282,N19-1421,0,0.253346,"[Ri,j ; Ti,j ] g= |Cq |× |Ca | α(i,j,k) = Ti,j W1 LSTM(Pi,j [k]), , where [· ; ·] means concatenation of two vectors. The statement vector s in the above equation is obtained from a certain language encoder, which can either be a trainable sequence encoder like LSTM or features extracted from pre-trained universal language encoders like G PT/B ERT). To encode a question-answer pair with universal language encoders, we simply create a sentence combining the question and the answer with a special token (“question+ [sep] + answer”), and then use the vector of ‘[cls]’ as suggested by prior works (Talmor et al., 2019).. We concatenate Ri,j with an additional vector Ti,j before doing average pooling. The Ti,j is inspired from the Relation Network (Santoro et al., 2017), which also encodes the latent relational information yet from the context in the statement s instead of the schema graph g. Simply put, we want to combine the relational representations of a pair of question/answer concepts from both the schema graph side (symbolic space) and the language side (semantic space). Finally, the plausibility score of the answer candidate a to the question q can be computed as score(q, a) = sigmoid(MLP(g)). 4.3 Hi"
D19-1282,P17-4020,0,0.0679586,"Missing"
D19-1282,P19-1393,0,0.36043,"in the question “Sitting too close to watch tv can cause what sort of pain?”, the exact matching result Cq would be {sitting, close, watch tv, watch, tv, sort, pain, etc.}. We are aware of the fact that such retrieved mentioned concepts are not always perfect (e.g. “sort” is not a semantically related concept, “close” is a polysemous concept). How to efficiently retrieve contextually-related knowledge from noisy knowledge resources is still an open research question by itself (Weissenborn et al., 2017; Khashabi et al., 2017), and thus most prior works choose to stop here (Zhong et al., 2018; Wang et al., 2019b). We enhance this straightforward approach with some rules, such as soft matching with lemmatization and filtering of stop words, and further deal with noise by pruning paths (§3.3) and reducing their importance with attention mechanisms (§4.3). 3.2 Schema Graph Construction ConceptNet. Before diving into the construction of schema graphs, we would like to briefly introduce our target knowledge graph ConceptNet. ConceptNet can be seen as a large set of triples of the form (h, r, t), like (ice, HasProperty, cold), where h and t represent head and tail concepts in the concept set V and r is a"
D19-1282,D18-1009,0,0.119974,"Missing"
D19-1282,D18-1244,0,0.0263533,"ext. Think of polysemous concepts such as “close” (§3.1), which can either be a verb concept like in “close the door” or an adjective concept meaning “a short distance apart”. Using GCNs to update the concept vector with their neighbors is thus helpful for disambiguation and contextualized concept embedding. Also, the pattern of schema graph structures provides potentially valuable information for reasoning. For instance, shorter and denser connections between question and answer concepts could mean higher plausibility under specific contexts. As many works show (Marcheggiani and Titov, 2017; Zhang et al., 2018), relational GCNs (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information. We thus apply GCNs on the plain version (unlabeled, nondirectional) of schema graphs, ignoring relation types on the edges. Specifically, the vector for concept ci ∈ Vg in the schema graph g is initialized by their pre-trained embeddings at first (0) (hi = Vi ). Then, we update them at the (l + 1)th layer by pooling features of their neighboring nodes (Ni ) and their own at the l-th layer with an non-linear activation function σ: X 1 (l+1) (l) (l) ("
D19-1282,P18-2016,1,0.857611,"structures for explainable commonsense reasoning. We argue that it would be more beneficial to propose reasoners that can exploit commonsense knowledge bases (Speer et al., 2017; Tandon et al., 2017; Sap et al., 2019a). Knowledgeaware models can explicitly incorporate external knowledge as relational inductive biases (Battaglia et al., 2018) to enhance their reasoning capacity, as well as to increase the transparency of model behaviors for more interpretable results. Furthermore, a knowledge-centric approach is extensible through commonsense knowledge acquisition techniques (Li et al., 2016; Xu et al., 2018). We propose a knowledge-aware reasoning framework for learning to answer commonsense questions, which has two major steps: schema graph grounding (§3) and graph modeling for inference (§4). As shown in Fig. 1, for each pair of question and answer candidate, we retrieve a graph from external knowledge graphs (e.g. ConceptNet) in order to capture the relevant knowledge for determining the plausibility of a given answer choice. The graphs are named “schema graphs” inspired by the schema theory proposed by Gestalt psychologists (Axelrod, 1973). The grounded schema graphs are usually much more com"
D19-1282,P17-1132,0,0.0416887,"at large language models show promising results in WSC resolution task (Levesque, 2011), but this approach can hardly be applied in a more general question answering setting and also not provide explicit knowledge used in inference. A unique merit of our K A G N E T method is that it provides grounded explicit knowledge triples and paths with scores, such that users can better understand and put trust in the behaviors and inferences of the model. Injecting external knowledge for NLU. Our work also lies in the general context of using external knowledge to encode sentences or answer questions. Yang and Mitchell (2017) are the among first ones to propose to encode sentences by keeping retrieving related entities from knowledge bases and then merging their embeddings into LSTM networks computations, to achieve a better performance on entity/event extraction tasks. Weissenborn et al. (2017), Mihaylov and Frank (2018), and Annervaz et al. (2018) follow this line of works to incorporate the embeddings of related knowledge triples at the word-level and improve the performance of natural language understanding tasks. In contrast to our work, they do not explicitly impose graph-structured knowledge into models , b"
D19-1282,N16-1174,0,0.0350053,"bove Ri,j can be viewed as the latent re(q) lation between the question concept ci and the (a) answer concept cj , for which we aggregate the representations of all the paths between them in the schema graph. Now we can finalize the vector representation of a schema graph g by aggregating all vectors in the matrix R using mean pooling: (i) concepts equally contribute to the reasoning. Therefore, we propose a hierarchical path-based attention mechanism to selectively aggregate important path vectors and then more important question-answer concept pairs. This core idea is similar to the work of Yang et al. (2016), which proposes a document encoder that has two levels of attention mechanisms applied at the word- and sentence-level. In our case, we have path-level and concept-pair-level attention for learning to contextually model graph representations. We learn a parameter matrix W1 for path-level attention scores, and the importance of the path Pi,j [k] is denoted as α ˆ (i,j,·) . (j) Ti,j = MLP([s ; cq ; ca ]) P i,j [Ri,j ; Ti,j ] g= |Cq |× |Ca | α(i,j,k) = Ti,j W1 LSTM(Pi,j [k]), , where [· ; ·] means concatenation of two vectors. The statement vector s in the above equation is obtained from a certa"
D19-1282,N18-1029,0,\N,Missing
D19-1397,P11-1055,0,0.906501,"relation mention into a given set of relation types, or a Not-Target-Type (N ONE). 2.2 Datasets We select three popular relation extraction datasets as benchmarks. Specifically, two of them are distantly supervised and one is human-annotated. KBP (Ling and Weld, 2012) uses Wikipedia articles annotated with Freebase entries as train set, and manually-annotated sentences from 2013 KBP slot filling assessment results (Ellis et al., 2012) as test set. NYT (Riedel et al., 2010) contains New York Times news articles and has been already heuristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et"
D19-1397,P16-1200,0,0.725331,", 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.18 47.88 ± 0.85 34.51 ± 0.99 48.15 ± 0.87 37.28 ± 0.81 46.33 ± 0.64 37.50 ± 1.21 49.67 ± 1.06 33.74 ± 2.19 46.82 ± 0.82 Human-annotated TACRED 45.97 ± 0.34 58.06 ± 0.54 51.67 ± 0.03 56.96 ± 0.43 58.39 ± 0.71 65.38 ± 0.60 62.74 ± 0.23 65.69 ± 0.48 - Table 2: Performance Comparison of RE Models. 5-time average and standard deviation of F1 scores are reported. (POS) tags, named-entity recognition"
D19-1397,D17-1005,1,0.947167,"d KBP NYT 7 23,784 289 25 235,982 395 Human-annotated TACRED 42 37,311 6,277 Table 1: Statistics of Datasets Used in Our Study. 2 Experiment Setup In this paper, we conduct extensive empirical analyses on distantly supervised relation extraction (DS-RE). For a meaningful comparison, we ensure the same setup in all experiments. In this section, we provide a brief introduction on the setting, while more details could be found in Appendix A. All implementations are available at https://github.com/INK-USC/ shifted-label-distribution. 2.1 Problem Setting Following previous works (Ren et al., 2017; Liu et al., 2017), we conduct relation extraction at sentence level. Formally speaking, the basic unit is the relation mention, which is composed of one sentence and one ordered entity pair within the sentence. The relation extraction task is to categorize each relation mention into a given set of relation types, or a Not-Target-Type (N ONE). 2.2 Datasets We select three popular relation extraction datasets as benchmarks. Specifically, two of them are distantly supervised and one is human-annotated. KBP (Ling and Weld, 2012) uses Wikipedia articles annotated with Freebase entries as train set, and manually-ann"
D19-1397,P14-5010,0,0.00266614,"a articles annotated with Freebase entries as train set, and manually-annotated sentences from 2013 KBP slot filling assessment results (Ellis et al., 2012) as test set. NYT (Riedel et al., 2010) contains New York Times news articles and has been already heuristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.18 47.88 ± 0.85 34.51 ± 0.99 48.15 ± 0.87 37.28 ± 0.81 46.33 ± 0.64 3"
D19-1397,P09-1113,0,0.901409,"w York Times news articles and has been already heuristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.18 47.88 ± 0.85 34.51 ± 0.99 48.15 ± 0.87 37.28 ± 0.81 46.33 ± 0.64 37.50 ± 1.21 49.67 ± 1.06 33.74 ± 2.19 46.82 ± 0.82 Human-annotated TACRED 45.97 ± 0.34 58.06 ± 0.54 51.67 ± 0.03 56.96 ± 0.43 58.39 ± 0.71 65.38 ± 0.60 62.74 ± 0.23 65.69 ± 0.48 - Table 2: Performance Comp"
D19-1397,D18-1230,1,0.861607,"one longstanding bottleneck is the lack of large-scale labeled training data. In order to alleviate the dependency on human supervision, Mintz et al. (2009) proposed distant supervision, namely constructing large datasets automatically by aligning text to an existing knowledge bases (e.g., Freebase). Also, distantly supervised relation extraction is formulated into a reinforcement learning problem by Feng et al. (2018) for selecting highquality instances. Similar annotation generation strategy using distant supervision has also been used for other NLP tasks, such as named entity recognition (Shang et al., 2018) and sentiment classification (Go et al., 2009). Though this strategy lightens annotation burdens, distant supervision inevitably introduces label noises. As the relation types are annotated merely according to entity mentions in the sentence, the local context may be annotated with labels that are not expressed in the sentence, In recent years, researchers mainly focus on dealing with label noises, and proposed the following methods: Riedel et al. (2010) use multi-instance single-label learning paradigm; Hoffmann et al. (2011); Surdeanu et al. (2012) propose multiinstance multi-label learning"
D19-1397,D12-1042,0,0.296575,"NLP tasks, such as named entity recognition (Shang et al., 2018) and sentiment classification (Go et al., 2009). Though this strategy lightens annotation burdens, distant supervision inevitably introduces label noises. As the relation types are annotated merely according to entity mentions in the sentence, the local context may be annotated with labels that are not expressed in the sentence, In recent years, researchers mainly focus on dealing with label noises, and proposed the following methods: Riedel et al. (2010) use multi-instance single-label learning paradigm; Hoffmann et al. (2011); Surdeanu et al. (2012) propose multiinstance multi-label learning paradigm. Recently, with the advance of neural network techniques, deep learning methods (Zeng et al., 2015; Lin et al., 2016) are applied to distantly supervised datasets, with powerful automatic feature extraction and advanced label noised reducing techniques such as selective attention. Liu et al. (2017) proposed a general framework to consolidate heterogeneous information and refine the true labels from noisy labels. Label noise is certainly an important factor limiting the performance of DS-RE models. Meanwhile, we argue that shifted label distr"
D19-1397,D15-1203,0,0.856952,"uristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.18 47.88 ± 0.85 34.51 ± 0.99 48.15 ± 0.87 37.28 ± 0.81 46.33 ± 0.64 37.50 ± 1.21 49.67 ± 1.06 33.74 ± 2.19 46.82 ± 0.82 Human-annotated TACRED 45.97 ± 0.34 58.06 ± 0.54 51.67 ± 0.03 56.96 ± 0.43 58.39 ± 0.71 65.38 ± 0.60 62.74 ± 0.23 65.69 ± 0.48 - Table 2: Performance Comparison of RE Models. 5-time average and standard"
D19-1397,C14-1220,0,0.492548,"s and has been already heuristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.18 47.88 ± 0.85 34.51 ± 0.99 48.15 ± 0.87 37.28 ± 0.81 46.33 ± 0.64 37.50 ± 1.21 49.67 ± 1.06 33.74 ± 2.19 46.82 ± 0.82 Human-annotated TACRED 45.97 ± 0.34 58.06 ± 0.54 51.67 ± 0.03 56.96 ± 0.43 58.39 ± 0.71 65.38 ± 0.60 62.74 ± 0.23 65.69 ± 0.48 - Table 2: Performance Comparison of RE Models. 5-t"
D19-1397,D17-1004,0,0.387565,"ess, other factors may have been overlooked. Here, we observe model behaviors to be different on DS datasets and clean dataset, which implies existence of other challenges that restrict performance of DS-RE models. In this paper, we conduct thorough analyses over both real-world and synthetic datasets to explore the question — what limits the performance of DS-trained neural models. Our analysis starts with a performance comparison among recent relation extraction methods on both DS datasets (i.e., KBP (Ellis et al., 2012), NYT (Riedel et al., 2010)) and human-annotated dataset (i.e., TACRED (Zhang et al., 2017)), with the goal of seeking models that can consistently yield strong results. We observe that, on human3841 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3841–3850, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics annotated dataset, neural relation extraction models outperform feature-based models by notable gaps, but these gaps diminish when the same models are applied to DS datasets—neural models merely achieve performance comparable"
D19-1397,D14-1162,0,0.0820475,"sed and one is human-annotated. KBP (Ling and Weld, 2012) uses Wikipedia articles annotated with Freebase entries as train set, and manually-annotated sentences from 2013 KBP slot filling assessment results (Ellis et al., 2012) as test set. NYT (Riedel et al., 2010) contains New York Times news articles and has been already heuristically annotated. Test set is constructed manually by (Hoffmann et al., 2011). TACRED (Zhang et al., 2017) is a large-scale crowd-sourced dataset, and is sufficiently larger than previous manually annotated datasets. 2.3 Pre-processing We leverage pre-trained GloVe (Pennington et al., 2014) embedding1 , and use the StanfordNLP toolkit (Manning et al., 2014) to get part of speech 1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 3842 Method / Dataset Feature-based Neural CoType-RM (Ren et al., 2017) ReHession (Liu et al., 2017) Logistic (Mintz et al., 2009) CNN (Zeng et al., 2014) PCNN (Zeng et al., 2015) Bi-GRU Bi-LSTM PA-LSTM (Zhang et al., 2017) Bi-GRU-ATT (Lin et al., 2016) PCNN-ATT (Lin et al., 2016) Distantly-supervised Wiki-KBP NYT 28.98 ± 0.76 40.26 ± 0.51 36.07 ± 1.06 46.79 ± 0.75 37.58 ± 0.27 47.33 ± 0.44 30.53 ± 2.26 46.75 ± 2.79 31.58 ± 0.35 44.63 ± 2.70 37.77 ± 0.1"
D19-1580,P15-1017,0,0.0271459,"Another prominent method which we compared the MIL results with is Hierarchical Attention Networks (HAN; Yang et al., 2016). HANs apply attention first at the level of words, then at the level of sentences, to produce representations of documents subject to local variations in textual importance. We also compare the results of neural network models with TF-IDF as a text classification baseline. Event Extraction The most challenging aspect of extracting events from a sentence is that the context of a document should be considered in order to interpret an entity and the type of triggered event (Chen et al., 2015). Approaches that exclusively use word features for the task usually lack comprehensiveness. The event detection model in the previous section produces, for each positive prediction, a small set of sentences likely to influence the document’s label. In the event extraction step, we use a bidirectional LSTM text classifier to predict the attributes of a crime event. The attributes of a crime event are determined by the taxonomy proposed by Kennedy et al. (2018) for annotating hate rhetoric. In our case (see Section 3), we are predicting two attributes: the target of a crime event, and the type"
D19-1580,D14-1162,0,0.0816591,"Missing"
D19-1580,N16-1174,0,0.017131,"the sentences in an article, the probabilistic score of each sentence in an article is calculated using a fully connected layer with sigmoid activation. This probabilistic score shows the extent to which the sentence contributes to predicting the crime label of the article. The label for a bag of sentences is calculated by averaging the k highest probabilistic scores. We checked the results with k being set to 2 or 3, since a few number of sentences in each article can determine the label. Another prominent method which we compared the MIL results with is Hierarchical Attention Networks (HAN; Yang et al., 2016). HANs apply attention first at the level of words, then at the level of sentences, to produce representations of documents subject to local variations in textual importance. We also compare the results of neural network models with TF-IDF as a text classification baseline. Event Extraction The most challenging aspect of extracting events from a sentence is that the context of a document should be considered in order to interpret an entity and the type of triggered event (Chen et al., 2015). Approaches that exclusively use word features for the task usually lack comprehensiveness. The event de"
D19-1580,P14-5010,0,0.00347698,"nd duplicated articles about an incident. To provide an accurate set of unreported 5755 hate crime incidents we removed duplicated and misclassified articles from the set of 678 unrepresented hate crime incidents. In order to account for the possible duplications, we utilize the event extraction model to capture the event entities, namely target and action type. Running the extraction model with the same hyperparameters yields the results presented in Table 3. We use the entities together with the time (mentioned in the dataset) and location (extracted with named entity recognizer of CoreNLP (Manning et al., 2014)) of the articles to detect duplicated events. Label Target Action Precision 63.9 67.7 Recall 65.3 68.0 1 2 3 False Positive Examples A former Ku Klux Klan leader in Ozark was sentenced Thursday to a decade in prison for sexually abusing a woman in southern Alabama. The FBI is part of an investigation into a suspicious substance delivered to a Council on American-Islamic Relations office in Santa Clara on Thursday. The hate from the violent white nationalist gathering that resulted in the death of an anti-racism protester in Charlottesville, can be found anywhere. Table 4: First sentences of s"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
N16-3015,P15-1056,1,0.400174,"ructured data in different modalities (e.g., texts, images and videos) is posted online for ready viewing. Complex event extraction and recommendation is critical for many information distillation tasks, including tracking current events, providing alerts, and predicting possible changes, as related to topics of ongoing concern. State-of-the-art Information Extraction (IE) technologies focus on extracting events from a single data modality and ignore cross-media fusion. More importantly, users are presented with extracted events in a passive way (e.g., in a temporally ordered event chronicle (Ge et al., 2015)). Such technologies do not leverage user behavior to identify the event 1 The system demo is available at: http://nlp.cs. rpi.edu/multimedia/event/navigation_dark. html properties of interest to them in selecting new scenarios for presentation. In this paper we present a novel event extraction and recommendation system that incorporates advances in extracting events across multiple sources with data in diverse modalities and so yields a more comprehensive understanding of collective events, their importance, and their inter-connections. The novel capabilities of our system include: • Event Ex"
N16-3015,P12-1038,0,0.0307608,"of sentences and apply spectral clustering to find several clustering centers (i.e., representative sentences including the most important phrases) as the summary. The user is also provided two options to show the original documents and the document containing the summary. 4.2 Visual Information Extraction For each event, we retrieve the most representative video/image online using the key-phrases such as date and entities as queries. Videos and images are often more impressive and efficient at conveying information. We first apply a pretrained convolutional neural network (CNN) architecture (Kuznetsova et al., 2012) to extract visual concepts from each video key frame based on the EventNet concept library (Ye et al., 2015). For example, the extracted visual concepts “crowd on street, riot, demonstration or protest, people marching” appear when the user’s mouse is over the video of the primary event (Figure 3). Then we adopt the approach described in (Li et al., 2015) which applies CNN and association rule mining technique to generate visual patterns and extract semantically meaningful relations between visual and textual information to name the patterns. Figure 3: Recommendation Interface. 5 6 Conclusion"
N16-3015,D14-1198,1,0.888744,"Missing"
N18-6003,H05-1066,0,0.174527,"Missing"
N18-6003,W02-2020,0,0.0398699,"Missing"
N18-6003,P09-1113,0,0.38883,"Missing"
N18-6003,W14-1611,0,0.0475727,"Missing"
N18-6003,D15-1064,1,0.829291,"d TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and appli"
N18-6003,P11-1055,0,0.138239,"Missing"
N18-6003,P10-1029,0,0.0521991,"Missing"
N18-6003,Q17-1008,1,0.820066,"ing tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and applications. Topics to be covered in this tutorial. The first 2/3 of this tutorial presents a comprehensive overview of the information extraction techniques developed in recent years for constructing knowledge bases (see also Section 2 for a more"
N18-6003,P08-1068,0,0.0888921,"Missing"
N18-6003,D11-1049,0,0.0401887,"xt corpora; (2) entity recognition and typing: preliminaries, challenges, and methodologies; and (3) relation extraction: previous efforts, limitations, recent progress, and a joint entity and relation extraction method using distant supervision; (4) multi-task and multi-domain learning for lowresource information extraction; (5) distill linguistic knowledge into neural models to help low-resource information extraction. The second half of the tutorial presents a comprehensive overview of KB reasoning techniques. For path-based methods, we will first describe the Path-Ranking Algorithm (PRA) (Lao et al., 2011) and briefly describe extensions such as ProPPR (Wang et al., 2013). Our tutorial will also cover the recent integration of Relevance to ACL. Machine “reading” and “reasoning” of large text corpora have long been the interests to CL and NLP communities, especially when people now are exposed to an explosion of information in the form of free text. Extracting structured information is key to understanding messy and scattered raw data, and effective reasoning tools are critical for the use of KBs in downstream tasks like QA. This tutorial will present an organized picture of recent research on k"
N18-6003,P14-1038,0,0.029821,"Missing"
N18-6003,W09-1119,0,0.10705,"Missing"
N18-6003,D16-1144,1,0.814067,"Missing"
N18-6003,D17-1060,1,0.829166,"and help knowledge extraction in low-resource settings with minimal supervision. In the reasoning part, we aim to leverage the existing background knowledge and design various algorithms to fill in the missing link between entities in the KB, given the extracted KBs are likely incomplete. More specifically, this part will introduce two lines of research for KB reasoning: path-based and embedding-based methods. PRA with recurrent neural networks. For the embedding based method, we will briefly describe RESCAL (Nickel et al., 2011) and TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chines"
N18-6003,P00-1015,0,0.0168191,"Missing"
N18-6003,P10-1040,0,0.141393,"Missing"
N18-6003,P05-1053,0,0.080725,"Missing"
N19-1290,C14-1220,0,0.0201895,"sion. An expressed-at-least-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) adopts REINFORCE in instance selection for distant supervision which is the basis of our work. Posterior regularization (Ganchev, 2010) is a framework to handle the problem that a variety of tasks and domains require the creation of large problem-specific annotated data. This framework incorporates external pr"
N19-1290,P16-2034,0,0.29305,"st-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) adopts REINFORCE in instance selection for distant supervision which is the basis of our work. Posterior regularization (Ganchev, 2010) is a framework to handle the problem that a variety of tasks and domains require the creation of large problem-specific annotated data. This framework incorporates external problem-specific information"
N19-1290,P16-1200,0,0.0164938,"arge amount of annotated data (Bach and Badaskar, 2007). Distant supervision is proposed to alleviate this problem by aligning plain text with Freebase. However, distant supervision inevitably suffers from the wrong label problem. Some previous research has been done in handling noisy data in distant supervision. An expressed-at-least-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) a"
N19-1290,P09-1113,0,0.512698,"domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training. 1 Introduction Relation extraction is a fundamental work in natural language processing. Detecting and classifying the relation between entity pairs from the unstructured document, it can support many other tasks such as question answering. While relation extraction requires lots of labeled data and make methods labor intensive, (Mintz et al., 2009) proposes distant supervision (DS), a widely used automatic annotating way. In distant supervision, knowledge base (KB) , such as Freebase, is aligned with nature documents. In this way, the sentences which contain an entity pair in KB all express the exact relation that the entity pair has in KB. We usually call the set of instances that contain the same entity pair a bag. In this way, the training instances can be divided into N bags B = {B 1 , B 2 , ..., B N }. Each bag B k are corresponding to an unique entity pair ∗ Corresponding author E k = (ek1 , ek2 ) and contains a sequence of instan"
N19-1290,D15-1203,0,0.0622836,"Missing"
N19-1290,D17-1005,1,\N,Missing
N19-1294,E17-1075,0,0.197388,"en proposed to distantly supervised FET. The first kind of work try to filter out noisy labels using heuristic rules (Gillick et al., 2014). However, such heuristic pruning significantly reduces the amount of training data, and thus cannot make full use of distantly annotated data. In contrast, the other thread of works try to incorporate such imperfect annotation by partiallabel loss (PLL). The basic assumption is that, for a noisy mention, the maximum score associated with its candidate types should be greater than the scores associated with any other non-candidate types (Ren et al., 2016a; Abhishek et al., 2017; Xu and Barbosa, 2018). Despite their success, PLLbased models still suffer from Confirmation Bias by taking its own prediction as optimization objective in the next step. Specifically, given an entity mention, if the typing system selected a wrong 2862 Proceedings of NAACL-HLT 2019, pages 2862–2872 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Knowledge Base Unlabeled Corpus athlete author artist director legal music ... actor location root person ... Candidate Typeφ /person/artist/actor /person/political_figure /person/legal S1: by Defense S"
N19-1294,W99-0613,0,0.732707,"Missing"
N19-1294,C18-1024,0,0.03455,"Missing"
N19-1294,P14-5010,0,0.00513362,"Missing"
N19-1294,P09-1113,0,0.0193383,", and then labeled with all possible types of the KB entity as supervision. However, despite its efficiency, distant supervision also brings the challenge of outof-context noise, as it assigns labels in a context agnostic manner. Early works usually ignore such noise in supervision (Ling and Weld, 2012; Shimaoka et al., 2016), which dampens the performance of distantly supervised models. Introduction Recent years have seen a surge of interests in fine-grained entity typing (FET) as it serves as an important cornerstone of several nature language processing tasks including relation extraction (Mintz et al., 2009), entity linking (Raiman and Raiman, 2018), and knowledge base completion (Dong et al., 2014). To reduce manual efforts in labelling training data, distant supervision (Mintz et al., 2009) has been widely adopted by recent FET systems. With the help of an external knowledge base (KB), an entity mention is first ∗ Corresponding Author. Towards overcoming out-of-context noise, two lines of work have been proposed to distantly supervised FET. The first kind of work try to filter out noisy labels using heuristic rules (Gillick et al., 2014). However, such heuristic pruning significantly reduces th"
N19-1294,C18-1196,0,0.019179,"BBN dataset, the CLSC model yield a comparable result with the NFETC model trained on full data. This comparison clearly shows the superiority of our approach in the effectiveness of utilizing noisy data. 4.7 Ablation: Do Markov Chains improve typing performance? Table 3 shows the performance of CLSC with onestep transition (L1−step ) and with Markov Chains (Lclsc ) as described in Section 3.2. Results show Named entity Recognition (NER) has been excavated for a long time (Collins and Singer, 1999; Manning et al., 2014), which classifies coarsegrained types (e.g. person, location). Recently, (Nagesh and Surdeanu, 2018a,b) applied ladder network (Rasmus et al., 2015) to coarse-grained entity classification in a semi-supervised learning fashion. (Ling and Weld, 2012) proposed FineGrained Entity Recognition (FET). They used distant supervision to get training corpus for FET. Embedding techniques was applied to learn feature representations since (Yogatama et al., 2015; Dong et al., 2015). (Shimaoka et al., 2016) introduced attention mechanism for FET to capture informative words. (Xin et al., 2018a) used the TransE entity embeddings (Bordes et al., 2013) as the query vector of attention. Early works ignore th"
N19-1294,N18-2057,0,0.0223049,"BBN dataset, the CLSC model yield a comparable result with the NFETC model trained on full data. This comparison clearly shows the superiority of our approach in the effectiveness of utilizing noisy data. 4.7 Ablation: Do Markov Chains improve typing performance? Table 3 shows the performance of CLSC with onestep transition (L1−step ) and with Markov Chains (Lclsc ) as described in Section 3.2. Results show Named entity Recognition (NER) has been excavated for a long time (Collins and Singer, 1999; Manning et al., 2014), which classifies coarsegrained types (e.g. person, location). Recently, (Nagesh and Surdeanu, 2018a,b) applied ladder network (Rasmus et al., 2015) to coarse-grained entity classification in a semi-supervised learning fashion. (Ling and Weld, 2012) proposed FineGrained Entity Recognition (FET). They used distant supervision to get training corpus for FET. Embedding techniques was applied to learn feature representations since (Yogatama et al., 2015; Dong et al., 2015). (Shimaoka et al., 2016) introduced attention mechanism for FET to capture informative words. (Xin et al., 2018a) used the TransE entity embeddings (Bordes et al., 2013) as the query vector of attention. Early works ignore th"
N19-1294,D14-1162,0,0.0938759,"cilitates classification as is shown in Figure 1. Noisy data enhances the formation of compact clusters with the help of label propagation. 3.1 Feature Extractor Figure 3 illustrates our feature extractor. For fair comparison, we adopt the same feature extraction pipeline as used in (Xu and Barbosa, 2018). The feature extractor is composed of an embedding layer and two encoders which encode mentions and contexts respectively. Embedding Layer: The output of this layer is a concatenation of word embedding and word position embedding. We use the popular 300dimensional word embedding supplied by (Pennington et al., 2014) to capture the semantic information and random initialized position embedding (Zeng et al., 2014) to acquire information about the relation between words and the mentions. Formally, Given a word embedding matrix Wword of shape dw × |V |, where V is the vocabulary and dw is the size of word embedding, each column of Wword represents a specific word w in V . We map each word wj in (mi , ci ) to a word embedding wjd ∈ Rdw . Analogously, we get the word position embedding wjp ∈ Rdp of each word according to the relative distance between the word and the mention, we only use a fixed length context"
N19-1294,D16-1144,1,0.935473,"nes of work have been proposed to distantly supervised FET. The first kind of work try to filter out noisy labels using heuristic rules (Gillick et al., 2014). However, such heuristic pruning significantly reduces the amount of training data, and thus cannot make full use of distantly annotated data. In contrast, the other thread of works try to incorporate such imperfect annotation by partiallabel loss (PLL). The basic assumption is that, for a noisy mention, the maximum score associated with its candidate types should be greater than the scores associated with any other non-candidate types (Ren et al., 2016a; Abhishek et al., 2017; Xu and Barbosa, 2018). Despite their success, PLLbased models still suffer from Confirmation Bias by taking its own prediction as optimization objective in the next step. Specifically, given an entity mention, if the typing system selected a wrong 2862 Proceedings of NAACL-HLT 2019, pages 2862–2872 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Knowledge Base Unlabeled Corpus athlete author artist director legal music ... actor location root person ... Candidate Typeφ /person/artist/actor /person/political_figure /perso"
N19-1294,D18-1121,0,0.0132718,"r, 1999; Manning et al., 2014), which classifies coarsegrained types (e.g. person, location). Recently, (Nagesh and Surdeanu, 2018a,b) applied ladder network (Rasmus et al., 2015) to coarse-grained entity classification in a semi-supervised learning fashion. (Ling and Weld, 2012) proposed FineGrained Entity Recognition (FET). They used distant supervision to get training corpus for FET. Embedding techniques was applied to learn feature representations since (Yogatama et al., 2015; Dong et al., 2015). (Shimaoka et al., 2016) introduced attention mechanism for FET to capture informative words. (Xin et al., 2018a) used the TransE entity embeddings (Bordes et al., 2013) as the query vector of attention. Early works ignore the out-of-context noise, (Gillick et al., 2014) proposed context dependent FET and use three heuristics to clean the noisy labels with the side effect of losing training data. To utilize noisy data, (Ren et al., 2016a) distinguished the loss function of noisy data from clean data via partial label loss (PLL). (Abhishek et al., 2017; Xu and Barbosa, 2018) proposed variants of PLL, which still suffer from confirmation bias. (Xu and Barbosa, 2018) proposed hierarchical loss to handle o"
N19-1294,N18-1002,0,0.170023,"y supervised FET. The first kind of work try to filter out noisy labels using heuristic rules (Gillick et al., 2014). However, such heuristic pruning significantly reduces the amount of training data, and thus cannot make full use of distantly annotated data. In contrast, the other thread of works try to incorporate such imperfect annotation by partiallabel loss (PLL). The basic assumption is that, for a noisy mention, the maximum score associated with its candidate types should be greater than the scores associated with any other non-candidate types (Ren et al., 2016a; Abhishek et al., 2017; Xu and Barbosa, 2018). Despite their success, PLLbased models still suffer from Confirmation Bias by taking its own prediction as optimization objective in the next step. Specifically, given an entity mention, if the typing system selected a wrong 2862 Proceedings of NAACL-HLT 2019, pages 2862–2872 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Knowledge Base Unlabeled Corpus athlete author artist director legal music ... actor location root person ... Candidate Typeφ /person/artist/actor /person/political_figure /person/legal S1: by Defense Secretary William Cohen"
N19-1294,D15-1083,0,0.0605913,"Missing"
N19-1294,P15-2048,0,0.060273,"ion to encourage compact cluttering. 3.3 4.2 We compare the proposed method with several state-of-the-art FET systems3 : Overall Objective • Attentive (Shimaoka et al., 2016) uses an attention based feature extractor and doesn’t distinguish clean from noisy data; • AFET (Ren et al., 2016a) trains label embedding with partial label loss; • AAA (Abhishek et al., 2017) learns joint representation of mentions and type labels; • PLE+HYENA/FIGER (Ren et al., 2016b) proposes heterogeneous partial-label embedding for label noise reduction to boost typing systems. We compare two PLE models with HYENA (Yogatama et al., 2015) and FIGER (Ling and Weld, 2012) as the base typing system respectively; • NFETC (Xu and Barbosa, 2018) trains neural fine-grained typing system with hierarchy-aware loss. We compare the performance of the NFETC model with two different loss functions: partial-label loss and PLL+hierarchical loss. We denote the two variants as NFETC and NFETChier respectively; • NFETC-CLSC is the proposed model in this work. We use the NFETC model as our base model, based on which we apply Compact Latent Space Clustering Regularization as described in Section 3.2; Similarly, we report results produced by using"
N19-1294,C14-1220,0,0.0245173,"with the help of label propagation. 3.1 Feature Extractor Figure 3 illustrates our feature extractor. For fair comparison, we adopt the same feature extraction pipeline as used in (Xu and Barbosa, 2018). The feature extractor is composed of an embedding layer and two encoders which encode mentions and contexts respectively. Embedding Layer: The output of this layer is a concatenation of word embedding and word position embedding. We use the popular 300dimensional word embedding supplied by (Pennington et al., 2014) to capture the semantic information and random initialized position embedding (Zeng et al., 2014) to acquire information about the relation between words and the mentions. Formally, Given a word embedding matrix Wword of shape dw × |V |, where V is the vocabulary and dw is the size of word embedding, each column of Wword represents a specific word w in V . We map each word wj in (mi , ci ) to a word embedding wjd ∈ Rdw . Analogously, we get the word position embedding wjp ∈ Rdp of each word according to the relative distance between the word and the mention, we only use a fixed length context here. The final embedding of the j-th word is wjE = [wjd , wjp ]. Mention Encoder: To capture lex"
P17-4010,P16-4004,0,0.136194,"whether there exist other drugs that can also treat breast cancer. Introduction Scientific literature is an important resource in facilitating life science research, and a primary medium for communicating novel research results. However, even though vast amounts of biomedical textual information are available online (e.g., publications in PubMed, encyclopedic articles in Wikipedia, ontologies on genes, drugs, etc.), there exists only limited support of exploring and analyzing relevant factual knowledge in the massive • Previous Efforts and Limitations. In life sciences domain, recent studies (Ernst et al., 2016; Szklarczyk et al., 2014; Thomas et al., 2012; 55 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 55–60 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4010 Kim et al., 2008) rely on biomedical entity information associated with the documents to support entity-centric literature search. Most existing information retrieval systems exploit either the MeSH terms manually annotated for each PubMed article (Kim et al., 2008) or textual mentions of biome"
P17-4010,P05-1061,0,0.00983957,"given entity types, and generating hypothetical facts to assist literaturebased knowledge discovery (e.g., drug target prediction). 1 CCR4 MDC1 Kawasaki Disease Aspirin May treat Type Path May prevent Coexpression Physical Interaction BRCA1 Drug NSAID Type Path Disease Associated with RAP2A Pathway Breast Cancer Genetic Interaction Neoplasms Breast Neoplasms May treat May treat May treat BRAF Gene Drug Target Tafinlar CBDCA CDDP Disease Figure 1: A snapshot of the structured network in Life-iNet. literature (Tao et al., 2014), or of gaining new insights from the existing factual information (McDonald et al., 2005; Riedel and McCallum, 2011). Users typically search PubMed using keywords and Medical Subject Headings (MeSH) terms, and then rely on Google and external biomedical ontologies for everything else. Such an approach, however, might not work well on capturing different entity relationships (i.e., facts), or identifying publications related to facts of interest. For example, a biologist who is interested in cancer might need to check what specific diseases belong to the category of breast neoplasms (e.g., breast cancer) and what genes (e.g., BRCA1) and drugs (e.g., Aspirin, Tafinlar) are related"
P17-4010,P09-1113,0,0.00513912,"the network con1 https://www.ncbi.nlm.nih.gov/pubmed/ https://www.ncbi.nlm.nih.gov/pmc/ 3 https://en.wikipedia.org/wiki/Portal:Health_and_ fitness 2 57 text segmentation algorithm, SegPhrase (Liu et al., 2015), to extract high-quality words/phrases as entity candidates. SegPhrase uses entity names from KBs as positive examples to train a quality classifier, and then efficiently segments the corpus by maximizing the joint probability based on the trained classifier. Table 1 shows the statistics of detected entity mentions for the corpora. Distant Supervision Generation.. Distant supervisions (Mintz et al., 2009; Ren et al., 2017, 2016a) leverages the information overlap between external KBs and given corpora to automatically generate large amounts of training data. A typical workflow is as follows: (1) map detected entity mentions to entities in KB, (2) assign, to the entity type set of each entity mention, KB types of its KB-mapped entity, and (3) assign, to the relation type set of each entity mention pair, KB relations between their KB-mapped entities. Such a label generation process may introduce noisy type labels (Ren et al., 2017). Our network construction pipeline faithfully incorporates the"
P17-4010,D16-1144,1,0.911494,"answering questions such as “which genes are distinctively related to the given disease type under GeneDiseaseAssociation relation?”). To systematically incorporate these ideas, LifeiNet leverages the novel distantly-supervised information extraction techniques (Ren et al., 2017, 2016a, 2015) to implement an effort-light network construction framework (see Fig. 2). Specially, it relies on distant supervision in conjunction with external knowledge bases to (1) detect quality entity mentions (Ren et al., 2015), (2) label entity mentions with fine-grained entity types in a given type hierarchy (Ren et al., 2016a), and (3) identify relationships of different types between entities (Ren et al., 2017). In particular, we design specialized loss functions to faithfully model “appropriate” labels and remove “false positive” la• Lack of Factual Structures: Most existing entity-centric systems compute the document/corpus-level co-occurrence statistics between two biomedical entities to capture the relations between them, but cannot identify the semantic relation types between two entities based on the textual evidence in a specific sentence. For example, in Fig. 1, relations between gene entities should be"
P17-4010,D11-1001,0,0.0166053,"d generating hypothetical facts to assist literaturebased knowledge discovery (e.g., drug target prediction). 1 CCR4 MDC1 Kawasaki Disease Aspirin May treat Type Path May prevent Coexpression Physical Interaction BRCA1 Drug NSAID Type Path Disease Associated with RAP2A Pathway Breast Cancer Genetic Interaction Neoplasms Breast Neoplasms May treat May treat May treat BRAF Gene Drug Target Tafinlar CBDCA CDDP Disease Figure 1: A snapshot of the structured network in Life-iNet. literature (Tao et al., 2014), or of gaining new insights from the existing factual information (McDonald et al., 2005; Riedel and McCallum, 2011). Users typically search PubMed using keywords and Medical Subject Headings (MeSH) terms, and then rely on Google and external biomedical ontologies for everything else. Such an approach, however, might not work well on capturing different entity relationships (i.e., facts), or identifying publications related to facts of interest. For example, a biologist who is interested in cancer might need to check what specific diseases belong to the category of breast neoplasms (e.g., breast cancer) and what genes (e.g., BRCA1) and drugs (e.g., Aspirin, Tafinlar) are related to breast cancer, and might"
P18-1229,D14-1088,0,0.106185,"-structured data. To deal with the issue of incomplete coverage, some works (Liu et al., 2012; Dong et al., 2014; Panchenko et al., 2016; Kozareva and Hovy, 2010) utilize data from domain-specific resources or the Web. Panchenko et al. (2016) extract hypernyms by patterns from general purpose corpora and domain-specific corpora bootstrapped from the input vocabulary. Kozareva and Hovy (2010) harvest new terms from the Web by employing Hearst-like lexico-syntactic patterns and validate the learned is-a relations by a web-based concept positioning procedure. Many works (Kozareva and Hovy, 2010; Anh et al., 2014; Velardi et al., 2013; Bansal et al., 2014; Zhang et al., 2016; Panchenko et al., 2016; Gupta et al., 2017) cast the task of hypernymy organization as a graph optimization problem. Kozareva and Hovy (2010) begin with a set of root terms and leaf terms and aim to generate intermediate terms by deriving the longest path from the root to leaf in a noisy hypernym graph. Velardi et al. (2013) induct a taxonomy from the hypernym graph via optimal branching and a weighting policy. Bansal et al. (2014) regard the induction of a taxonomy as a structured learning problem by building a factor graph to m"
P18-1229,P14-1113,0,0.322134,"TaxoRL a set of terms into a taxonomy based on relevant resources such as text corpora. Prior studies on automatic taxonomy induction (Gupta et al., 2017; Camacho-Collados, 2017) often divide the problem into two sequential subtasks: (1) hypernymy detection (i.e., extracting term pairs of “is-a” relation); and (2) hypernymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy). Methods developed for hypernymy detection either harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignor"
P18-1229,P14-1098,0,0.189792,"Missing"
P18-1229,W11-2501,0,0.148077,"Missing"
P18-1229,S13-1005,0,0.0516518,"Missing"
P18-1229,S16-1168,0,0.0322842,"Missing"
P18-1229,C92-2082,0,0.294133,"ormation which are complementary to each other. 6 Related Work 6.1 Hypernymy Detection Finding high-quality hypernyms is of great importance since it serves as the first step of taxonomy induction. In previous works, there are mainly two categories of approaches for hypernymy detection, namely pattern-based and distributional methods. Pattern-based methods consider lexicosyntactic patterns between the joint occurrences of term pairs for hypernymy detection. They generally achieve high precision but suffer from low recall. Typical methods that leverage patterns for hypernym extraction include (Hearst, 1992; Snow et al., 2005; Kozareva and Hovy, 2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards t"
P18-1229,D16-1146,0,0.062882,"Missing"
P18-1229,P14-1089,0,0.125954,"terminates at this point, we call it a partial induction. We can also continue the induction by restoring the original action space at this moment so that all the terms in V are eventually attached to the taxonomy. We call this setting a full induction. In this experiment, we use the English environment and science taxonomies in the SemEval-2016 task 13 (TExEval2) (Bordea et al., 2016). Each taxonomy is composed of hundreds of terms, which is much larger than the WordNet taxonomies. The taxonomies are aggregated from existing resources such as WordNet, Eurovoc6 , and the Wikipedia Bitaxonomy (Flati et al., 2014). Since this dataset provides no training data, we train our model using the WordNet dataset in the first experiment. To avoid possible overlap between these two sources, we exclude those taxonomies constructed from WordNet. In both experiments, we combine three public corpora – the latest Wikipedia dump, the UMBC web-based corpus (Han et al., 2013) and the One Billion Word Language Modeling Benchmark (Chelba et al., 2013). Only sentences where term pairs co-occur are reserved, which results in 2467 6 http://eurovoc.europa.eu/drupal/ Model TAXI HypeNET HypeNET+MST TaxoRL (RE) TaxoRL (NR) Bansa"
P18-1229,N15-1169,0,0.0567274,"Missing"
P18-1229,D10-1108,0,0.394473,"Missing"
P18-1229,P16-1200,0,0.0146951,"eral or too specific. The surface, frequency, and generality features are binned and their embeddings are concatenated as a part of the term pair representation. In summary, the final term pair representation Rxy has the following form: Rxy = [PP(x,y) , Vwx , Vwy , VF (x,y) ], where PP(x,y) , Vwx , Vwy , VF (x,y) denote the path representation, the word embedding of x and y, and the feature embeddings, respectively. Our approach is general and can be flexibly extended to incorporate different feature representation components introduced by other relation extraction models (Zhang et al., 2017; Lin et al., 2016; Shwartz et al., 2016). We leave in-depth discussion of the design choice of hypernymy relation representation components as future work. 3 Reinforcement Learning for End-to-End Taxonomy Induction |Vt |+ |Tt |= |V0 |. The episode terminates when all the terms are attached to the taxonomy, which makes the length of one episode equal to |V0 |. A remaining issue is how to select the first term when no terms are on the taxonomy. One approach that we tried is to add a virtual node as root and consider it as if a real node. The root embedding is randomly initialized and updated with other parameter"
P18-1229,D12-1104,0,0.038867,"tion Finding high-quality hypernyms is of great importance since it serves as the first step of taxonomy induction. In previous works, there are mainly two categories of approaches for hypernymy detection, namely pattern-based and distributional methods. Pattern-based methods consider lexicosyntactic patterns between the joint occurrences of term pairs for hypernymy detection. They generally achieve high precision but suffer from low recall. Typical methods that leverage patterns for hypernym extraction include (Hearst, 1992; Snow et al., 2005; Kozareva and Hovy, 2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shw"
P18-1229,S16-1206,0,0.141148,"Missing"
P18-1229,D14-1162,0,0.0811758,"e and give the agent the difference between the real reward and the baseline reward instead of feeding the real reward directly. We use a moving average of the reward as the baseline for simplicity. 5 We tried to encode induction history by feeding representations of previously selected term pairs into an LSTM, and leveraging the output of the LSTM as history representation (concatenating it with current term pair representations or passing it to a feed-forward network). However, we didn’t observe clear performance change. 2466 3.5 Implementation Details We use pre-trained GloVe word vectors (Pennington et al., 2014) with dimensionality 50 as word embeddings. We limit the maximum number of dependency paths between each term pair to be 200 because some term pairs containing general terms may have too many dependency paths. We run with different random seeds and hyperparameters and use the validation set to pick the best model. We use an Adam optimizer with initial learning rate 10−3 . We set the discounting factor γ to 0.4 as it is shown that using a smaller discount factor than defined can be viewed as regularization (Jiang et al., 2015). Since the parameter updates are performed at the end of each episod"
P18-1229,E14-1054,0,0.0188916,"2010; Panchenko et al., 2016; Nakashole et al., 2012). Distributional methods leverage the contexts of each term separately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shwartz et al., 2016). 6.2 Taxonomy Induction There are many lines of work for taxonomy induction in the prior literature. One line of works (Snow et al., 2005; Yang and Callan, 2009; Shen et al., 2012; Jurgens and Pilehvar, 2015) aims to complete existing taxonomies by attaching new terms in an incremental way. Snow et al. (2005) enrich WordNet by maximizing the probability of an extended taxonomy given evidence 2469 of relations from text corpora. Shen et al. (2012) determine whether an entity is on the taxonomy and either attach it to the right category"
P18-1229,P16-1226,0,0.0570691,"Missing"
P18-1229,D16-1039,0,0.324173,"erms into a taxonomy based on relevant resources such as text corpora. Prior studies on automatic taxonomy induction (Gupta et al., 2017; Camacho-Collados, 2017) often divide the problem into two sequential subtasks: (1) hypernymy detection (i.e., extracting term pairs of “is-a” relation); and (2) hypernymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy). Methods developed for hypernymy detection either harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignore the taxonomy stru"
P18-1229,J13-3007,0,0.148813,"r harvest new terms (Yamada et al., 2009; Kozareva and Hovy, 2010) or presume a vocabulary is given and study term semantics (Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016). The hypernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph pruning methods including maximum spanning tree (MST) (Bansal et al., 2014; Zhang et al., 2016), minimum-cost flow (MCF) (Gupta et al., 2017) and other pruning heuristics (Kozareva and Hovy, 2010; Velardi et al., 2013; Faralli et al., 2015; Panchenko et al., 2016). However, these two-phase methods encounter two major limitations. First, most of them ignore the taxonomy structure when estimating the probability that a term pair holds the hypernymy relation. They estimate the probability of different term pairs independently and the learned term pair representations are fixed during hypernymy organization. In consequence, there is no feedback from the second phase to the first phase and possibly wrong representations cannot be rectified based on the results of hypernymy organization, which causes the error p"
P18-1229,C04-1146,0,0.162442,"Missing"
P18-1229,D09-1097,0,0.0796389,"Missing"
P18-1229,P09-1031,0,0.0184214,"developed in an unsupervised manner. Measures such as symmetric similarity (Lin et al., 1998) and those based on distributional inclusion hypothesis (Weeds et al., 2004; Chang et al., 2017) were proposed. Supervised methods, on the other hand, usually have better performance than unsupervised methods for hypernymy detection. Recent works towards this direction include (Fu et al., 2014; Rimell, 2014; Yu et al., 2015; Tuan et al., 2016; Shwartz et al., 2016). 6.2 Taxonomy Induction There are many lines of work for taxonomy induction in the prior literature. One line of works (Snow et al., 2005; Yang and Callan, 2009; Shen et al., 2012; Jurgens and Pilehvar, 2015) aims to complete existing taxonomies by attaching new terms in an incremental way. Snow et al. (2005) enrich WordNet by maximizing the probability of an extended taxonomy given evidence 2469 of relations from text corpora. Shen et al. (2012) determine whether an entity is on the taxonomy and either attach it to the right category or link it to an existing one based on the results. Another line of works (Suchanek et al., 2007; Ponzetto and Strube, 2008; Flati et al., 2014) focuses on the taxonomy induction of existing encyclopedias (e.g., Wikiped"
P18-1229,D17-1004,0,0.0280064,"t are either too general or too specific. The surface, frequency, and generality features are binned and their embeddings are concatenated as a part of the term pair representation. In summary, the final term pair representation Rxy has the following form: Rxy = [PP(x,y) , Vwx , Vwy , VF (x,y) ], where PP(x,y) , Vwx , Vwy , VF (x,y) denote the path representation, the word embedding of x and y, and the feature embeddings, respectively. Our approach is general and can be flexibly extended to incorporate different feature representation components introduced by other relation extraction models (Zhang et al., 2017; Lin et al., 2016; Shwartz et al., 2016). We leave in-depth discussion of the design choice of hypernymy relation representation components as future work. 3 Reinforcement Learning for End-to-End Taxonomy Induction |Vt |+ |Tt |= |V0 |. The episode terminates when all the terms are attached to the taxonomy, which makes the length of one episode equal to |V0 |. A remaining issue is how to select the first term when no terms are on the taxonomy. One approach that we tried is to add a virtual node as root and consider it as if a real node. The root embedding is randomly initialized and updated wi"
P19-1420,C14-1220,0,\N,Missing
P19-1420,D14-1162,0,\N,Missing
P19-1420,D17-1004,0,\N,Missing
P19-1420,N19-1423,0,\N,Missing
P19-1420,P05-1045,0,\N,Missing
P19-1420,D15-1203,0,\N,Missing
P19-3010,P14-5010,0,0.00320516,"model. One can obtain inference results for their developing complicated systems by querying such APIs for downstream applications. 6 Experiments To investigate the performance of our implemented back-end model with incremental active learning and consolidation, we conduct a prelim61 80 few of them enjoys the above-introduced three features of AlpacaTag. It is true that a few existing tools also support tagging suggestions (instancelevel recommendations) as follows: • BRAT (Stenetorp et al., 2012) and GATE (Bontcheva et al., 2013) can offer suggestions with a fixed tagging model like CoreNLP (Manning et al., 2014). However, it is hardly helpful when users need to annotate sentences with customized label set specific to their interested domains, which is the most common motivation for people to use an annotation framework. • YEDDA (Yang et al., 2018a) simply generates suggestions by exact matching against a continuously updated lexicon of already annotated spans. In comparison, this is a subset of AlpacaTag’s recommendations. Note that YEDDA cannot suggest any unseen spans. • WebAnno (Yimam et al., 2013) integrates a learning component for suggestions, which is based on hand-crafted features and generic"
P19-3010,P17-1028,0,0.195064,"estions. We further present our three key features in the next sections. Automatic crowd consolidation (§4) of the annotations from multiple annotators is an underexplored topic in developing crowd-sourcing annotation frameworks. As a crowdsourcing framework, AlpacaTag collects annotations from multiple (non-expert) contributors with lower cost and a higher speed. However, annotators usually have different confidences, preferences, and biases in annotating, which leads to possibly high interannotator disagreement. It is shown very challenging to train models with such noisy crowd annotations (Nguyen et al., 2017; Yang et al., 2018b). We argue that consolidating crowd labels during annotating can lead annotators to achieve real-time consensus, and thus decrease disagreement of annotations instead of exhausting post-processing. 3 Active Intelligent Recommendation This section first introduces the back-end model (§3.1) and then presents how we use the backend model for both instance-level recommendations (tagging suggestions, §3.2) as well as corpuslevel recommendations (active sampling, §3.3). Real-time model deployment (§5) is also a desired feature for users. We sometimes need to deploy a state-of-th"
P19-3010,N06-4006,0,0.145544,"Missing"
P19-3010,W16-4011,0,0.0809916,"Missing"
P19-3010,N13-3004,0,0.0573771,"Missing"
P19-3010,D18-1318,0,0.0677492,"Missing"
P19-3010,P18-4006,0,0.643836,"ommendations powered by active learning and auto-consolidation of crowd annotations to real-time model deployment. 1 Backend Model Instance Sampling via Active Learning Matching Frequent NPs + Dictionary A batch of raw sentences Recommendations Crowd Annotators Crowd Annotations AlpacaTag Figure 1: Overview of the AlpacaTag framework. Therefore, it is still an important research question that how we can develop a better annotation framework to largely reduces human efforts. Existing open-source sequence annotation tools (Stenetorp et al., 2012; Druskat et al., 2014a; de Castilho et al., 2016; Yang et al., 2018a) mainly focus on enhancing the friendliness of user interfaces (UI) such as data management, fast tagging with shortcut keys, supporting more platforms, and multi-annotator analysis. We argue that there are still three important yet underexplored directions of improvements: 1) active intelligent recommendation, 2) automatic crowd consolidation, 3) real-time model deployment. Therefore, we propose a novel web-based annotation tool named AlpacaTag1 to address these three problems. Active intelligent recommendation (§3) aims to reduce human efforts at both instance-level and corpus-level by lea"
P19-3010,N16-1030,0,0.0295431,") as well as corpuslevel recommendations (active sampling, §3.3). Real-time model deployment (§5) is also a desired feature for users. We sometimes need to deploy a state-of-the-art sequence tagging model while the crowdsourcing is still ongoing, such that users can facilitate the developing of their taggingrequired systems with our APIs. 3.1 Back-end Model: BLSTM-CRF The core component of the proposed AlpacaTag framework is the back-end sequence tagging model, which is learned with an incremental active learning scheme. We use the state-ofthe-art sequence tagging model as our back-end model (Lample et al., 2016; Lin et al., 2017; Liu et al., 2018), which is based on bidirectional LSTM networks with a CRF layer (BLSTMCRF). It can capture character-level patterns, and encode token sequences with pre-trained word embeddings, as well as using CRF layers to capture structural dependencies in tagging. In this section, we assume the model is fixed as we are talking about how to use it for infer recommendations. How to update it by consolidating crowd annotations is illustrated in the Section §4. To the best of our knowledge, there is no existing annotation framework enjoying such three features. AlpacaTag"
P19-3010,W17-4421,1,0.758148,"vel recommendations (active sampling, §3.3). Real-time model deployment (§5) is also a desired feature for users. We sometimes need to deploy a state-of-the-art sequence tagging model while the crowdsourcing is still ongoing, such that users can facilitate the developing of their taggingrequired systems with our APIs. 3.1 Back-end Model: BLSTM-CRF The core component of the proposed AlpacaTag framework is the back-end sequence tagging model, which is learned with an incremental active learning scheme. We use the state-ofthe-art sequence tagging model as our back-end model (Lample et al., 2016; Lin et al., 2017; Liu et al., 2018), which is based on bidirectional LSTM networks with a CRF layer (BLSTMCRF). It can capture character-level patterns, and encode token sequences with pre-trained word embeddings, as well as using CRF layers to capture structural dependencies in tagging. In this section, we assume the model is fixed as we are talking about how to use it for infer recommendations. How to update it by consolidating crowd annotations is illustrated in the Section §4. To the best of our knowledge, there is no existing annotation framework enjoying such three features. AlpacaTag is the first unifi"
P19-3010,P13-4001,0,0.124756,"Missing"
P19-3010,D18-1226,1,0.729123,"stions. Apart from that, Introduction Sequence tagging is a major type of tasks in natural language processing (NLP), including namedentity recognition (detecting and typing entity names), keyword extraction (e.g. extracting aspect terms in reviews or essential terms in queries), chunking (extracting phrases), and word segmentation (identifying word boundary in languages like Chinese). State-of-the-art supervised approaches to sequence tagging are highly dependent on numerous annotations. New annotations are usually necessary for a new domain or task, even though transfer learning techniques (Lin and Lu, 2018) can reduce the amount of them by reusing data of other related tasks. However, manually annotating sequences can be timeconsuming, expensive, and thus hard to scale. ∗ Complicated Downstream Systems Unlabeled Instances 1 The source code is publicly available at http:// inklab.usc.edu/AlpacaTag/. Both authors contributed equally. 58 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 58–63 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 we also greedily match frequent noun phrases an"
P19-3017,P18-1148,0,0.039985,"Missing"
P19-3017,P09-1113,0,0.0193718,"eme to revise the entity linking result. Moreover, KCAT provides an efficient Annotator Client to accelerate the annotation process and a comprehensive Manager Module to analyse crowdsourcing annotations. Experiment shows that KCAT can significantly improve annotation efficiency, the time consumption increases slowly as the size of type set expands. 1 Introduction Recent years Natural Language Processing community has seen a surge of interests in fine-grained entity typing (FET) as it serves as an important cornerstone of several nature language processing tasks including relation extraction (Mintz et al., 2009), entity linking (Raiman and Raiman, 2018), and knowledge base completion (Dong et al., 2014). Given an entity mention (i.e. a sequence of token spans representing an entity) in the corpus, FET aims at uncovering its contextdependent type. Table 1 includes Fine-grained ∗ 1 Corresponding Author. Code is available at https://github.com/donnyslin/KCAT 99 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 99–104 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset BBN FIGER TAC 2018 De"
P19-3017,N19-1250,0,0.0143443,"FET and FET Annotation both increase rapidly with the growth of type hierarchy’s depth. Previous research work mainly focus on generating train corpus with distant supervision (Ling and Weld, 2012; Gillick et al., 2014; Ren et al., 2016a; Choi et al., 2018). In spite of its efficiency, distant supervision brings the problem of noisy labels, for example, {Other, brand} are noisy labels for ‘Kobe’ in “Kobe scored 60 points in the final game.”. According to (Choi et al., 2018), 6000 manually labeled samples achieved greater performance than millions of samples generated by distant supervision. (Onoe and Durrett, 2019) observed that noisy samples may even cause damage to the performance of FET model. (Ren et al., 2016b; Onoe and Durrett, 2019) proposed label noise reduction methods, which are pretty complicated and hard to migrate. Thus the annotation corpus for FET is important and necessary. However, it is not easy to annotate a corpus for FET since it’s hard for human beings to differentiate and memorize thousands of types. To solve this extremely hard annotation task, we use Entity Linking (EL) to constrain the candidate types of the entity mention. Entity Linking, which tries to link entity mention to"
P19-3017,E17-1075,0,0.0127639,"and necessary. However, it is not easy to annotate a corpus for FET since it’s hard for human beings to differentiate and memorize thousands of types. To solve this extremely hard annotation task, we use Entity Linking (EL) to constrain the candidate types of the entity mention. Entity Linking, which tries to link entity mention to a unique entity in a specific knowledge base (i.e. Yago or Freebase), has been studied for years. The state-of-the-art EL system yields 0.93 F1 score in Conll2003(Sang and Buchholz, 2000), while the F1 scores of FET vary from 0.40 (Onoe and Durrett, 2019) to 0.79 (Abhishek et al., 2017) on different datasets. With the help of EL, the candidate types of a mention can be greatly reduced as shown in the Table 1. Based on this observation, we develop a Knowledge-Constraint Typing Annotation Tool (KCAT). KCAT uses an external entity linking tool to constrain the candidate types of Fine-grained Entity Typing is a tough task which suffers from noise samples extracted from distant supervision. Thousands of manually annotated samples can achieve greater performance than millions of samples generated by the previous distant supervision method. Whereas, it’s hard for human beings to di"
P19-3017,D16-1144,1,0.920907,"g Zhuang1 , Fei Wu1 , Zhigang Chen2 , Guoping Hu2 & Xiang Ren3 1 Zhejiang University 2 iFLYTEK Research, 3 University of Southern California, {shenglin, antlar, chenbo123}@zju.edu.cn, {siliang, yzhuang, wufei}@zju.edu.cn, {zgchen, gphu}@iflytek.com, xiangren@usc.edu Abstract Entity Typing datasets in recent years, the target types often form a type hierarchy. The difficulty of FET and FET Annotation both increase rapidly with the growth of type hierarchy’s depth. Previous research work mainly focus on generating train corpus with distant supervision (Ling and Weld, 2012; Gillick et al., 2014; Ren et al., 2016a; Choi et al., 2018). In spite of its efficiency, distant supervision brings the problem of noisy labels, for example, {Other, brand} are noisy labels for ‘Kobe’ in “Kobe scored 60 points in the final game.”. According to (Choi et al., 2018), 6000 manually labeled samples achieved greater performance than millions of samples generated by distant supervision. (Onoe and Durrett, 2019) observed that noisy samples may even cause damage to the performance of FET model. (Ren et al., 2016b; Onoe and Durrett, 2019) proposed label noise reduction methods, which are pretty complicated and hard to migra"
P19-3017,P18-1009,0,0.0128338,", Zhigang Chen2 , Guoping Hu2 & Xiang Ren3 1 Zhejiang University 2 iFLYTEK Research, 3 University of Southern California, {shenglin, antlar, chenbo123}@zju.edu.cn, {siliang, yzhuang, wufei}@zju.edu.cn, {zgchen, gphu}@iflytek.com, xiangren@usc.edu Abstract Entity Typing datasets in recent years, the target types often form a type hierarchy. The difficulty of FET and FET Annotation both increase rapidly with the growth of type hierarchy’s depth. Previous research work mainly focus on generating train corpus with distant supervision (Ling and Weld, 2012; Gillick et al., 2014; Ren et al., 2016a; Choi et al., 2018). In spite of its efficiency, distant supervision brings the problem of noisy labels, for example, {Other, brand} are noisy labels for ‘Kobe’ in “Kobe scored 60 points in the final game.”. According to (Choi et al., 2018), 6000 manually labeled samples achieved greater performance than millions of samples generated by distant supervision. (Onoe and Durrett, 2019) observed that noisy samples may even cause damage to the performance of FET model. (Ren et al., 2016b; Onoe and Durrett, 2019) proposed label noise reduction methods, which are pretty complicated and hard to migrate. Thus the annotati"
P19-3017,W00-0726,0,0.216879,"ods, which are pretty complicated and hard to migrate. Thus the annotation corpus for FET is important and necessary. However, it is not easy to annotate a corpus for FET since it’s hard for human beings to differentiate and memorize thousands of types. To solve this extremely hard annotation task, we use Entity Linking (EL) to constrain the candidate types of the entity mention. Entity Linking, which tries to link entity mention to a unique entity in a specific knowledge base (i.e. Yago or Freebase), has been studied for years. The state-of-the-art EL system yields 0.93 F1 score in Conll2003(Sang and Buchholz, 2000), while the F1 scores of FET vary from 0.40 (Onoe and Durrett, 2019) to 0.79 (Abhishek et al., 2017) on different datasets. With the help of EL, the candidate types of a mention can be greatly reduced as shown in the Table 1. Based on this observation, we develop a Knowledge-Constraint Typing Annotation Tool (KCAT). KCAT uses an external entity linking tool to constrain the candidate types of Fine-grained Entity Typing is a tough task which suffers from noise samples extracted from distant supervision. Thousands of manually annotated samples can achieve greater performance than millions of sam"
P19-3017,E12-2021,0,0.122113,"Missing"
P19-3017,D17-1277,0,0.0597897,"Missing"
P19-3017,P18-4006,0,0.0508977,"Missing"
