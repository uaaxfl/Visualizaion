2021.starsem-1.29,Evaluating a Joint Training Approach for Learning Cross-lingual Embeddings with Sub-word Information without Parallel Corpora on Lower-resource Languages,2021,-1,-1,2,1,1012,ali parizi,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Cross-lingual word embeddings provide a way for information to be transferred between languages. In this paper we evaluate an extension of a joint training approach to learning cross-lingual embeddings that incorporates sub-word information during training. This method could be particularly well-suited to lower-resource and morphologically-rich languages because it can be trained on modest size monolingual corpora, and is able to represent out-of-vocabulary words (OOVs). We consider bilingual lexicon induction, including an evaluation focused on OOVs. We find that this method achieves improvements over previous approaches, particularly for OOVs."
2021.semeval-1.83,{UNBNLP} at {S}em{E}val-2021 Task 1: Predicting lexical complexity with masked language models and character-level encoders,2021,-1,-1,4,1,1851,milton king,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this paper, we present three supervised systems for English lexical complexity prediction of single and multiword expressions for SemEval-2021 Task 1. We explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. Our best system combines information from these three sources. The results indicate that information from masked language models and character-level encoders can be combined to improve lexical complexity prediction."
2021.mwe-1.4,Contextualized Embeddings Encode Monolingual and Cross-lingual Knowledge of Idiomaticity,2021,-1,-1,2,0,1852,samin fakharian,Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021),0,"Potentially idiomatic expressions (PIEs) are ambiguous between non-compositional idiomatic interpretations and transparent literal interpretations. For example, {``}hit the road{''} can have an idiomatic meaning corresponding to {`}start a journey{'} or have a literal interpretation. In this paper we propose a supervised model based on contextualized embeddings for predicting whether usages of PIEs are idiomatic or literal. We consider monolingual experiments for English and Russian, and show that the proposed model outperforms previous approaches, including in the case that the model is tested on instances of PIE types that were not observed during training. We then consider cross-lingual experiments in which the model is trained on PIE instances in one language, English or Russian, and tested on the other language. We find that the model outperforms baselines in this setting. These findings suggest that contextualized embeddings are able to learn representations that encode knowledge of idiomaticity that is not restricted to specific expressions, nor to a specific language."
2020.starsem-1.5,Joint Training for Learning Cross-lingual Embeddings with Sub-word Information without Parallel Corpora,2020,-1,-1,2,1,1012,ali parizi,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"In this paper, we propose a novel method for learning cross-lingual word embeddings, that incorporates sub-word information during training, and is able to learn high-quality embeddings from modest amounts of monolingual data and a bilingual lexicon. This method could be particularly well-suited to learning cross-lingual embeddings for lower-resource, morphologically-rich languages, enabling knowledge to be transferred from rich- to lower-resource languages. We evaluate our proposed approach simulating lower-resource languages for bilingual lexicon induction, monolingual word similarity, and document classification. Our results indicate that incorporating sub-word information indeed leads to improvements, and in the case of document classification, performance better than, or on par with, strong benchmark approaches."
2020.lrec-1.299,Evaluating Approaches to Personalizing Language Models,2020,-1,-1,2,1,1851,milton king,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this work, we consider the problem of personalizing language models, that is, building language models that are tailored to the writing style of an individual. Because training language models requires a large amount of text, and individuals do not necessarily possess a large corpus of their writing that could be used for training, approaches to personalizing language models must be able to rely on only a small amount of text from any one user. In this work, we compare three approaches to personalizing a language model that was trained on a large background corpus using a relatively small amount of text from an individual user. We evaluate these approaches using perplexity, as well as two measures based on next word prediction for smartphone soft keyboards. Our results show that when only a small amount of user-specific text is available, an approach based on priming gives the most improvement, while when larger amounts of user-specific text are available, an approach based on language model interpolation performs best. We carry out further experiments to show that these approaches to personalization outperform language model adaptation based on demographic factors."
2020.lrec-1.330,Evaluating Sub-word Embeddings in Cross-lingual Models,2020,-1,-1,2,1,1012,ali parizi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Cross-lingual word embeddings create a shared space for embeddings in two languages, and enable knowledge to be transferred between languages for tasks such as bilingual lexicon induction. One problem, however, is out-of-vocabulary (OOV) words, for which no embeddings are available. This is particularly problematic for low-resource and morphologically-rich languages, which often have relatively high OOV rates. Approaches to learning sub-word embeddings have been proposed to address the problem of OOV words, but most prior work has not considered sub-word embeddings in cross-lingual models. In this paper, we consider whether sub-word embeddings can be leveraged to form cross-lingual embeddings for OOV words. Specifically, we consider a novel bilingual lexicon induction task focused on OOV words, for language pairs covering several language families. Our results indicate that cross-lingual representations for OOV words can indeed be formed from sub-word embeddings, including in the case of a truly low-resource morphologically-rich language."
2020.lrec-1.333,Evaluating the Impact of Sub-word Information and Cross-lingual Word Embeddings on Mi{'}kmaq Language Modelling,2020,-1,-1,4,0,17326,jeremie boudreau,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Mi{'}kmaq is an Indigenous language spoken primarily in Eastern Canada. It is polysynthetic and low-resource. In this paper we consider a range of n-gram and RNN language models for Mi{'}kmaq. We find that an RNN language model, initialized with pre-trained fastText embeddings, performs best, highlighting the importance of sub-word information for Mi{'}kmaq language modelling. We further consider approaches to language modelling that incorporate cross-lingual word embeddings, but do not see improvements with these models. Finally we consider language models that operate over segmentations produced by SentencePiece {---} which include sub-word units as tokens {---} as opposed to word-level models. We see improvements for this approach over word-level language models, again indicating that sub-word modelling is important for Mi{'}kmaq language modelling."
S19-2092,{UNBNLP} at {S}em{E}val-2019 Task 5 and 6: Using Language Models to Detect Hate Speech and Offensive Language,2019,0,0,3,1,1012,ali parizi,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"In this paper we apply a range of approaches to language modeling {--} including word-level n-gram and neural language models, and character-level neural language models {--} to the problem of detecting hate speech and offensive language. Our findings indicate that language models are able to capture knowledge of whether text is hateful or offensive. However, our findings also indicate that more conventional approaches to text classification often perform similarly or better."
W18-4920,Do Character-Level Neural Network Language Models Capture Knowledge of Multiword Expression Compositionality?,2018,0,0,2,1,1012,ali parizi,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"In this paper, we propose the first model for multiword expression (MWE) compositionality prediction based on character-level neural network language models. Experimental results on two kinds of MWEs (noun compounds and verb-particle constructions) and two languages (English and German) suggest that character-level neural network language models capture knowledge of multiword expression compositionality, in particular for English noun compounds and the particle component of English verb-particle constructions. In contrast to many other approaches to MWE compositionality prediction, this character-level approach does not require token-level identification of MWEs in a training corpus, and can potentially predict the compositionality of out-of-vocabulary MWEs."
S18-1168,{UNBNLP} at {S}em{E}val-2018 Task 10: Evaluating unsupervised approaches to capturing discriminative attributes,2018,0,1,3,1,1851,milton king,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present three unsupervised models for capturing discriminative attributes based on information from word embeddings, WordNet, and sentence-level word co-occurrence frequency. We show that, of these approaches, the simple approach based on word co-occurrence performs best. We further consider supervised and unsupervised approaches to combining information from these models, but these approaches do not improve on the word co-occurrence model."
P18-2055,Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of {E}nglish verb-noun combinations,2018,0,1,2,1,1851,milton king,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Verb-noun combinations (VNCs) - e.g., blow the whistle, hit the roof, and see stars - are a common type of English idiom that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations. Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts. Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness. We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations."
L18-1653,Towards Language Technology for Mi{'}kmaq,2018,-1,-1,3,0,30229,anant maheshwari,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-1906,Supervised and unsupervised approaches to measuring usage similarity,2017,11,0,2,1,1851,milton king,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"Usage similarity (USim) is an approach to determining word meaning in context that does not rely on a sense inventory. Instead, pairs of usages of a target lemma are rated on a scale. In this paper we propose unsupervised approaches to USim based on embeddings for words, contexts, and sentences, and achieve state-of-the-art results over two USim datasets. We further consider supervised approaches to USim, and find that although they outperform unsupervised approaches, they are unable to generalize to lemmas that are unseen in the training data."
S17-1006,Deep Learning Models For Multiword Expression Identification,2017,24,2,3,1,32402,waseem gharbieh,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are unpredictable with respect to their component words. In this paper we propose the first deep learning models for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance."
W16-1817,A Word Embedding Approach to Identifying Verb-Noun Idiomatic Combinations,2016,11,8,3,1,32402,waseem gharbieh,Proceedings of the 12th Workshop on Multiword Expressions,0,None
S16-1113,{UNBNLP} at {S}em{E}val-2016 Task 1: Semantic Textual Similarity: A Unified Framework for Semantic Processing and Evaluation,2016,3,0,4,1,1851,milton king,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1042,Evaluating a Topic Modelling Approach to Measuring Corpus Similarity,2016,18,3,2,0,34768,richard fothergill,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Web corpora are often constructed automatically, and their contents are therefore often not well understood. One technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. In this paper we evaluate a number of measures of corpus similarity, including a method based on topic modelling which has not been previously evaluated for this task. To evaluate these methods we use known-similarity corpora that have been previously used for this purpose, as well as a number of newly-constructed known-similarity corpora targeting differences in genre, topic, time, and region. Our findings indicate that, overall, the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity."
L16-1474,Classifying Out-of-vocabulary Terms in a Domain-Specific Social Media Corpus,2016,0,1,6,0,34287,sohyun park,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we consider the problem of out-of-vocabulary term classification in web forum text from the automotive domain. We develop a set of nine domain- and application-specific categories for out-of-vocabulary terms. We then propose a supervised approach to classify out-of-vocabulary terms according to these categories, drawing on features based on word embeddings, and linguistic knowledge of common properties of out-of-vocabulary terms. We show that the features based on word embeddings are particularly informative for this task. The categories that we predict could serve as a preliminary, automatically-generated source of lexical knowledge about out-of-vocabulary terms. Furthermore, we show that this approach can be adapted to give a semi-automated method for identifying out-of-vocabulary terms of a particular category, automotive named entities, that is of particular interest to us."
C16-1046,Determining the Multiword Expression Inventory of a Surprise Language,2016,29,2,2,1,24743,bahar salehi,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Much previous research on multiword expressions (MWEs) has focused on the token- and type-level tasks of MWE identification and extraction, respectively. Such studies typically target known prevalent MWE types in a given language. This paper describes the first attempt to learn the MWE inventory of a {``}surprise{''} language for which we have no explicit prior knowledge of MWE patterns, certainly no annotated MWE data, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types."
W15-0909,The Impact of Multiword Expression Compositionality on Machine Translation Evaluation,2015,26,6,3,1,24743,bahar salehi,Proceedings of the 11th Workshop on Multiword Expressions,0,"In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact."
P15-2139,Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser,2015,20,81,4,1,25446,long duong,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method."
N15-1099,A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions,2015,25,52,2,1,24743,bahar salehi,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents the first attempt to use word embeddings to predict the compositionality of multiword expressions. We consider both single- and multi-prototype word embeddings. Experimental results show that, in combination with a back-off method based on string similarity, word embeddings outperform a method using count-based distributional similarity. Our best results are competitive with, or superior to, state-of-the-art methods over three standard compositionality datasets, which include two types of multiword expressions and two languages."
K15-1012,Cross-lingual Transfer for Unsupervised Dependency Parsing Without Parallel Data,2015,33,14,4,1,25446,long duong,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Cross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement."
D15-1040,A Neural Network Model for Low-Resource {U}niversal {D}ependency Parsing,2015,28,18,4,1,25446,long duong,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Accurate dependency parsing requires large treebanks, which are only available for a few languages. We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared xe2x80x9cuniversalxe2x80x9d parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations."
W14-5315,Exploring Methods and Resources for Discriminating Similar Languages,2014,31,5,5,0.686275,38273,marco lui,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"The Discriminating between Similar Languages (DSL) shared task at VarDial challenged participants to build an automatic language identification system to discriminate between 13 languages in 6 groups of highly-similar languages (or national varieties of the same language). In this paper, we describe the submissions made by team UniMelb-NLP, which took part in both the closed and open categories. We present the text representations and modeling techniques used, including cross-lingual POS tagging as well as fine-grained tags extracted from a deep grammar of English, and discuss additional data we collected for the open submissions, utilizing custombuilt web corpora based on top-level domains as well as existing corpora."
P14-1025,"Learning Word Sense Distributions, Detecting Unattested Senses and Identifying Novel Senses Using Topic Models",2014,55,27,2,0.909091,3097,jey lau,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions. This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which arenxe2x80x99t attested in the corpus, and identifying novel senses in the corpus which arenxe2x80x99t captured in the sense inventory."
E14-4042,One Sense per Tweeter ... and Other Lexical Semantic Tales of {T}witter,2014,20,9,2,0.909091,9729,spandana gella,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"In recent years, microblogs such as Twitter have emerged as a new communication channel. Twitter in particular has become the target of a myriad of content-based applications including trend analysis and event detection, but there has been little fundamental work on the analysis of word usage patterns in this text type. In this paper xe2x80x94 inspired by the one-sense-perdiscourse heuristic of Gale et al. (1992) xe2x80x94 we investigate user-level sense distributions, and detect strong support for xe2x80x9cone sense per tweeterxe2x80x9d. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus."
E14-1050,Using Distributional Similarity of Multi-way Translations to Predict Multiword Expression Compositionality,2014,25,24,2,1,24743,bahar salehi,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression, based on translations into multiple languages. We evaluate the method over English noun compounds, English verb particle constructions and German noun compounds. We show that the estimation of compositionality is improved when using translations into multiple languages, as compared to simply using distributional similarity in the source language. We further find that string similarity complements distributional similarity. 1 Compositionality of MWEs Multiword expressions (hereafter MWEs) are combinations of words which are lexically, syntactically, semantically or statistically idiosyncratic (Sag et al., 2002; Baldwin and Kim, 2009). Much research has been carried out on the extraction and identification of MWEs1 in English (Schone and Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009) and other languages (Dias, 2003; Evert and Krenn, 2005; Salehi et al., 2012). However, considerably less work has addressed the task of predicting the meaning of MWEs, especially in non-English languages. As a step in this direction, the focus of this study is on predicting the compositionality of MWEs. An MWE is fully compositional if its meaning is predictable from its component words, and it is non-compositional (or idiomatic) if not. For example, stand up xe2x80x9crise to onexe2x80x99s feetxe2x80x9d is composiIn this paper, we follow Baldwin and Kim (2009) in considering MWE xe2x80x9cidentificationxe2x80x9d to be a token-level disambiguation task, and MWE xe2x80x9cextractionxe2x80x9d to be a type-level lexicon induction task. tional, because its meaning is clear from the meaning of the components stand and up. However, the meaning of strike up xe2x80x9cto start playingxe2x80x9d is largely unpredictable from the component words strike and up. In this study, following McCarthy et al. (2003) and Reddy et al. (2011), we consider compositionality to be graded, and aim to predict the degree of compositionality. For example, in the dataset of Reddy et al. (2011), climate change is judged to be 99% compositional, while silver screen is 48% compositional and ivory tower is 9% compositional. Formally, we model compositionality prediction as a regression task. An explicit handling of MWEs has been shown to be useful in NLP applications (Ramisch, 2012). As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statistical machine translation. They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositionality of MWEs into their system, they could improve translation quality. Acosta et al. (2011) showed that treating non-compositional MWEs as a single unit in information retrieval improves retrieval effectiveness. For example, while searching for documents related to ivory tower, we are almost certainly not interested in documents relating to elephant tusks. Our approach is to use a large-scale multi-way translation lexicon to source translations of MWEs and their component words, and then model the relative similarity between each of the component words and the MWE, using distributional similarity based on monolingual corpora for the source language and each of the target languages. Our hypothesis is that using distributional similarity in more than one language will improve the prediction of compositionality. Importantly, in order to make the method as language-independent and"
D14-1096,What Can We Get From 1000 Tokens? A Case Study of Multilingual {POS} Tagging For Resource-Poor Languages,2014,30,19,5,1,25446,long duong,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages. We use parallel data to transfer part-of-speech information from resource-rich to resourcepoor languages. Additionally, we use a small amount of annotated data to learn to xe2x80x9ccorrectxe2x80x9d errors from projected approach such as tagset mismatch between languages, achieving state-of-the-art performance (91.3%) across 8 languages. Our approach is based on modest data requirements, and uses minimum divergence classification. For situations where no universal tagset mapping is available, we propose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy."
D14-1189,Detecting Non-compositional {MWE} Components using {W}iktionary,2014,14,7,2,1,24743,bahar salehi,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary. The approach makes use of the definitions, synonyms and translations in Wiktionary, and is applicable to any type of MWE in any language, assuming the MWE is contained in Wiktionary. Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods."
C14-1154,Novel Word-sense Identification,2014,31,12,1,1,1013,paul cook,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Automatic lexical acquisition has been an active area of research in computational linguistics for over two decades, but the automatic identification of new word-senses has received attention only very recently. Previous work on this topic has been limited by the availability of appropriate evaluation resources. In this paper we present the largest corpus-based dataset of diachronic sense differences to date, which we believe will encourage further work in this area. We then describe several extensions to a state-of-the-art topic modelling approach for identifying new word-senses. This adapted method shows superior performance on our dataset of two different corpus pairs to that of the original method for both: (a) types having taken on a novel sense over time; and (b) the token instances of such novel senses."
W13-1008,"Automatically Assessing Whether a Text Is Cliched, with Applications to Literary Analysis",2013,7,3,1,1,1013,paul cook,Proceedings of the 9th Workshop on Multiword Expressions,0,"Cliches, as trite expressions, are predominantly multiword expressions, but not all MWEs are cliches. We conduct a preliminary examination of the problem of determining how cliched a text is, taken as a whole, by comparing it to a reference text with respect to the proportion of more-frequent n-grams, as measured in an external corpus. We find that more-frequent n-grams are over-represented in cliched text. We apply this finding to the xe2x80x9cEumaeusxe2x80x9d episode of James Joycexe2x80x99s novel Ulysses, which literary scholars believe to be written in a deliberately cliched style."
U13-1003,Classifying {E}nglish Documents by National Dialect,2013,-1,-1,2,0.686275,38273,marco lui,Proceedings of the Australasian Language Technology Association Workshop 2013 ({ALTA} 2013),0,None
S13-2039,unimelb: Topic Modelling-based Word Sense Induction for Web Snippet Clustering,2013,10,14,2,1,3097,jey lau,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes our system for Task 11 of SemEval-2013. In the task, participants are provided with a set of ambiguous search queries and the snippets returned by a search engine, and are asked to associate senses with the snippets. The snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters. Our system adopts a preexisting Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model. Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task."
S13-2051,unimelb: Topic Modelling-based Word Sense Induction,2013,7,16,2,1,3097,jey lau,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes our system for shared task 13 xe2x80x9cWord Sense Induction for Graded and Non-Graded Sensesxe2x80x9d of SemEval-2013. The task is on word sense induction (WSI), and builds on earlier SemEval WSI tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance: participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings. The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model. Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task."
S13-1030,{U}ni{M}elb{\\_}{NLP}-{CORE}: Integrating predictions from multiple domains and feature sets for estimating semantic textual similarity,2013,26,3,5,0.909091,9729,spandana gella,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"In this paper we present our systems for calculating the degree of semantic similarity between two texts that we submitted to the Semantic Textual Similarity task at SemEval2013. Our systems predict similarity using a regression over features based on the following sources of information: string similarity, topic distributions of the texts based on latent Dirichlet allocation, and similarity between the documents returned by an information retrieval engine when the target texts are used as queries. We also explore methods for integrating predictions using different training datasets and feature sets. Our best system was ranked 17th out of 89 participating systems. In our post-task analysis, we identify simple changes to our system that further improve our results."
S13-1036,Unsupervised Word Usage Similarity in Social Media Texts,2013,21,4,2,0.909091,9729,spandana gella,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We propose an unsupervised method for automatically calculating word usage similarity in social media data based on topic modelling, which we contrast with a baseline distributional method and Weighted Textual Matrix Factorization. We evaluate these methods against a novel dataset made up of human ratings over 550 Twitter message pairs annotated for usage similarity for a set of 10 nouns. The results show that our topic modelling approach outperforms the other two methods."
S13-1039,Predicting the Compositionality of Multiword Expressions Using Translations in Multiple Languages,2013,23,26,2,1,24743,bahar salehi,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"In this paper, we propose a simple, languageindependent and highly effective method for predicting the degree of compositionality of multiword expressions (MWEs). We compare the translations of an MWE with the translations of its components, using a range of different languages and string similarity measures. We demonstrate the effectiveness of the method on two types of English MWEs: noun compounds and verb particle constructions. The results show that our approach is competitive with or superior to state-of-the-art methods over standard datasets."
P13-4002,A Stacking-based Approach to {T}witter User Geolocation Prediction,2013,26,38,2,1,33670,bo han,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We implement a city-level geolocation prediction system for Twitter users. The system infers a userxe2x80x99s location based on both tweet text and user-declared metadata using a stacking approach. We demonstrate that the stacking method substantially outperforms benchmark methods, achieving 49% accuracy on a benchmark dataset. We further evaluate our method on a recent crawl of Twitter data to investigate the impact of temporal factors on model generalisation. Our results suggest that user-declared location metadata is more sensitive to temporal change than the text of Twitter messages. We also describe two ways of accessing/demoing our system."
P13-2112,Simpler unsupervised {POS} tagging with bilingual projections,2013,10,16,2,1,25446,long duong,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus. In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying xe2x80x9cgoodxe2x80x9d training sentences from the parallel corpus and applying self-training. In experimental results on eight languages, our method achieves state-of-the-art results."
I13-1041,"How Noisy Social Media Text, How Diffrnt Social Media Sources?",2013,33,111,2,0,1468,timothy baldwin,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"While various claims have been made about text in social media text being noisy, there has never been a systematic study to investigate just how linguistically noisy or otherwise it is over a range of social media sources. We explore this question empirically over popular social media text types, in the form of YouTube comments, Twitter posts, web user forum posts, blog posts and Wikipedia, which we compare to a reference corpus of edited English text. We first extract out various descriptive statistics from each data type (including the distribution of languages, average sentence length and proportion of out-ofvocabulary words), and then investigate the proportion of grammatical sentences in each, based on a linguistically-motivated parser. We also investigate the relative similarity between different data types."
I13-1177,Increasing the Quality and Quantity of Source Language Data for Unsupervised Cross-Lingual {POS} Tagging,2013,11,3,2,1,25446,long duong,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Bilingual corpora offer a promising bridge between resource-rich and resource-poor languages, enabling the development of natural language processing systems for the latter. English is often selected as the resource-rich language, but another choice might give better performance. In this paper, we consider the task of unsupervised cross-lingual POS tagging, and construct a model that predicts the best source language for a given target language. In experiments on 9 languages, this model improves on using a single fixed source language. We then show that further improvements can be made by combining information from multiple source languages."
U12-1014,langid.py for better language modelling,2012,-1,-1,1,1,1013,paul cook,Proceedings of the Australasian Language Technology Association Workshop 2012,0,None
E12-2014,A Support Platform for Event Detection using Social Intelligence,2012,5,15,2,0,1468,timothy baldwin,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper describes a system designed to support event detection over Twitter. The system operates by querying the data stream with a user-specified set of keywords, filtering out non-English messages, and probabilistically geolocating each message. The user can dynamically set a probability threshold over the geolocation predictions, and also the time interval to present data for."
E12-1060,Word Sense Induction for Novel Sense Detection,2012,24,97,2,1,3097,jey lau,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre-determined number of topics (=senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task. We go on to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task."
D12-1039,Automatically Constructing a Normalisation Dictionary for Microblogs,2012,34,135,2,1,33670,bo han,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Microblog normalisation methods often utilise complex models and struggle to differentiate between correctly-spelled unknown words and lexical variants of known words. In this paper, we propose a method for constructing a dictionary of lexical variants of known words that facilitates lexical normalisation via simple string substitution (e.g. tomorrow for tmrw). We use context information to generate possible variant and normalisation pairs and then rank these by string similarity. Highly-ranked pairs are selected to populate the dictionary. We show that a dictionary-based approach achieves state-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing."
C12-1064,Geolocation Prediction in Social Media Data by Finding Location Indicative Words,2012,42,92,2,1,33670,bo han,Proceedings of {COLING} 2012,0,"Geolocation prediction is vital to geospatial applications like localised search and local event detection. Predominately, social media geolocation models are based on full text data, including common words with no geospatial dimension (e.g. today) and noisy strings (tmrw), potentially hampering prediction and leading to slower/more memory-intensive models. In this paper, we focus on finding location indicative words (LIWs) via feature selection, and establishing whether the reduced feature set boosts geolocation accuracy. Our results show that an information gain ratiobased approach surpasses other methods at LIW selection, outperforming state-of-the-art geolocation prediction methods by 10.6% in accuracy and reducing the mean and median of prediction error distance by 45km and 209km, respectively, on a public dataset. We further formulate notions of prediction confidence, and demonstrate that performance is even higher in cases where our model is more confident, striking a trade-off between accuracy and coverage. Finally, the identified LIWs reveal regional language differences, which could be potentially useful for lexicographers."
Y11-1028,Automatic identification of words with novel but infrequent senses,2011,16,8,1,1,1013,paul cook,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"We propose a statistical method for identifying words that have a novel sense in one corpus compared to another based on differences in their lexico-syntactic contexts in those corpora. In contrast to previous work on identifying semantic change, we focus specifically on infrequent word senses. Given the challenges of evaluation for this task, we further propose a novel evaluation method based on synthetic examples of semantic change that allows us to simulate differing degrees of sense change. Our proposed method is able to identify rather subtle simulated sense changes, and outperforms both a random baseline and a previously-proposed approach."
J11-2007,Book Review: A Way with Words: Recent Advances in Lexical Theory and Analysis: A Festschrift for Patrick Hanks edited by Gilles-Maurice de Schryver,2011,1,0,1,1,1013,paul cook,Computational Linguistics,0,None
W10-2109,No Sentence Is Too Confusing To Ignore,2010,17,0,1,1,1013,paul cook,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such constructions are ambiguous, with two possible (and opposite!) interpretations, roughly meaning either that Every X Zs, or that No X Zs. The interpretations have been noted to depend on semantic and pragmatic factors. We show here that automatic disambiguation of this pragmatically complex construction can be largely achieved by using features of the lexical semantic properties of the verb (i.e., Z) participating in the construction. We discuss our experimental findings in the context of construction grammar, which suggests a possible account of this phenomenon."
cook-stevenson-2010-automatically,Automatically Identifying Changes in the Semantic Orientation of Words,2010,22,36,1,1,1013,paul cook,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The meanings of words are not fixed but in fact undergo change, with new word senses arising and established senses taking on new aspects of meaning or falling out of usage. Two types of semantic change are amelioration and pejoration; in these processes a word sense changes to become more positive or negative, respectively. In this first computational study of amelioration and pejoration we adapt a web-based method for determining semantic orientation to the task of identifying ameliorations and pejorations in corpora from differing time periods. We evaluate our proposed method on a small dataset of known historical ameliorations and pejorations, and find it to perform better than a random baseline. Since this test dataset is small, we conduct a further evaluation on artificial examples of amelioration and pejoration, and again find evidence that our proposed method is able to identify changes in semantic orientation. Finally, we conduct a preliminary evaluation in which we apply our methods to the task of finding words which have recently undergone amelioration or pejoration."
J10-1005,Automatically Identifying the Source Words of Lexical Blends in {E}nglish,2010,34,18,1,1,1013,paul cook,Computational Linguistics,0,"Newly coined words pose problems for natural language processing systems because they are not in a system's lexicon, and therefore no lexical information is available for such words. A common way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and pharmaceutical. We propose a statistical model for inferring a blend's source words drawing on observed linguistic properties of blends; these properties are largely based on the recognizability of the source words in a blend. We annotate a set of 1,186 recently coined expressions which includes 515 blends, and evaluate our methods on a 324-item subset. In this first study of novel blends we achieve an accuracy of 40% on the task of inferring a blend's source words, which corresponds to a reduction in error rate of 39% over an informed baseline. We also give preliminary results showing that our features for source word identification can be used to distinguish blends from other kinds of novel words."
W09-2010,An Unsupervised Model for Text Message Normalization,2009,13,110,1,1,1013,paul cook,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,"Cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms. We analyze a sample of creative, non-standard text message word forms to determine frequent word formation processes in texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On a test set of 303 text message forms that differ from their standard form, our model achieves 59% accuracy, which is on par with the best supervised results reported on this dataset."
J09-1005,Unsupervised Type and Token Identification of Idiomatic Expressions,2009,60,95,2,0.306836,10736,afsaneh fazly,Computational Linguistics,0,"Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context."
W07-1106,Pulling their Weight: Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expressions in Context,2007,16,70,1,1,1013,paul cook,Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,0,"Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (token-based knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques."
W06-1207,Classifying Particle Semantics in {E}nglish Verb-Particle Constructions,2006,24,27,1,1,1013,paul cook,Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,0,"Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed the issue of which meaning of the component words is being used in a given VPC. We develop a feature space for use in classification of the sense contributed by the particle in a VPC, and test this on VPCs using the particle up. The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs."
