2015.iwslt-evaluation.18,Improvement of word alignment models for {V}ietnamese-to-{E}nglish translation,2015,-1,-1,2,0,38028,takahiro nomura,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
P13-2119,Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation,2013,23,62,4,0.798447,5136,kevin duh,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams."
D13-1139,Shift-Reduce Word Reordering for Machine Translation,2013,15,4,3,0.833333,12296,katsuhiko hayashi,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of 3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.
2013.iwslt-evaluation.12,{NTT}-{NAIST} {SMT} systems for {IWSLT} 2013,2013,25,1,4,0.8125,1440,katsuhito sudoh,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents NTT-NAIST SMT systems for English-German and German-English MT tasks of the IWSLT 2013 evaluation campaign. The systems are based on generalized minimum Bayes risk system combination of three SMT systems: forest-to-string, hierarchical phrase-based, phrasebased with pre-ordering. Individual SMT systems include data selection for domain adaptation, rescoring using recurrent neural net language models, interpolated language models, and compound word splitting (only for German-English)."
W12-4207,Head Finalization Reordering for {C}hinese-to-{J}apanese Machine Translation,2012,31,10,5,0,33211,dan han,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chinese-to-English (Wang et al., 2007) and English-to-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules."
P12-2020,A Comparative Study of Target Dependency Structures for Statistical Machine Translation,2012,21,2,4,1,6319,xianchao wu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these non-isomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser's PASs achieved the best dependency and translation accuracies."
P12-1001,Learning to Translate with Multiple Objectives,2012,37,11,4,1,5136,kevin duh,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality.n n Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization."
Y11-1022,Automatic Error Analysis Based on Grammatical Questions,2011,8,0,2,0,37909,tomoki nagase,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"The present paper proposes automatic error analysis methods that use patterns representing grammatical check points. Our method is comparable to or slightly outperforms conventional methods for automatic evaluation metrics. Different from the conventional methods, our method enables error analysis for each grammatical check point. While our method does not depend on languages, we experimentally show its validity by using a Japanese-to-Chinese test set. Errors in existing Japanese-to-Chinese translation systems are also analyzed."
I11-1004,Extracting Pre-ordering Rules from Predicate-Argument Structures,2011,26,20,4,1,6319,xianchao wu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word-aligned HPSG-tree-tostring pairs and 2) a bottom-up algorithm to apply the extracted rules to HPSG trees to yield target language style source sentences. Experimental results are reported for large-scale English-to-Japanese translation, showing significant improvements of BLEU score compared with the baseline SMT systems."
I11-1153,Generalized Minimum {B}ayes Risk System Combination,2011,19,11,4,1,5136,kevin duh,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Minimum Bayes Risk (MBR) has been used as a decision rule for both singlesystem decoding and system combination in machine translation. For system combination, we argue that common MBR implementations are actually not correct, since probabilities in the hypothesis space cannot be reliably estimated. These implementations achieve the effect of consensus decoding (which may be beneficial in its own right), but does not reduce Bayes Risk in the true Bayesian sense. We introduce Generalized MBR, which parameterizes the loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality."
2011.mtsummit-papers.11,Alignment Inference and {B}ayesian Adaptation for Machine Translation,2011,-1,-1,4,1,5136,kevin duh,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.34,Extracting Pre-ordering Rules from Chunk-based Dependency Trees for {J}apanese-to-{E}nglish Translation,2011,-1,-1,4,1,6319,xianchao wu,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.36,Post-ordering in Statistical Machine Translation,2011,-1,-1,4,1,1440,katsuhito sudoh,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-1736,Head Finalization: A Simple Reordering Rule for {SOV} Languages,2010,18,64,3,0,36870,hideki isozaki,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"English is a typical SVO (Subject-Verb-Object) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rule-based preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al., 2009). These methods rewrite SVO sentences to derive more SOV-like sentences by using a set of handcrafted rules. In this paper, we propose an alternative single reordering rule: Head Finalization. This is a syntax-based preprocessing approach that offers the advantage of simplicity. We do not have to be concerned about part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Our experiments show that its result, Head Final English (HFE), follows almost the same order as Japanese. We also show that this rule improves automatic evaluation scores."
W10-1757,N-Best Reranking by Multitask Learning,2010,37,9,3,1,5136,kevin duh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We propose a new framework for N-best reranking on sparse feature sets. The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.n n This is motivated by the observation that N-best lists often show significant differences in feature distributions. Training a single reranker directly on this heteroge-nous data can be difficult.n n Our proposed meta-algorithm solves this challenge by using multitask learning (such as e1/e2 regularization) to discover common feature representations across N-best lists. This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature. As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features."
W10-1762,Divide and Translate: Improving Long Distance Reordering in Statistical Machine Translation,2010,27,29,3,1,1440,katsuhito sudoh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals. The non-terminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple word-level reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved significant improvements of 1.4% in BLEU and 1.3% in TER by using Moses, and 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of research paper abstracts in the medical domain."
D10-1092,Automatic Evaluation of Translation Quality for Distant Language Pairs,2010,14,155,5,0,36870,hideki isozaki,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Automatic evaluation of Machine Translation (MT) quality is essential to developing high-quality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.' Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics."
C10-1050,Hierarchical Phrase-based Machine Translation with Word-based Reordering Model,2010,23,11,2,1,12296,katsuhiko hayashi,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Hierarchical phrase-based machine translation can capture global reordering with synchronous context-free grammar, but has little ability to evaluate the correctness of word orderings during decoding. We propose a method to integrate word-based reordering model into hierarchical phrase-based machine translation to overcome this weakness. Our approach extends the synchronous context-free grammar rules of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system."
2010.iwslt-papers.5,Analysis of translation model adaptation in statistical machine translation,2010,15,15,3,1,5136,kevin duh,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Numerous empirical results have shown that combining data from multiple domains often improve statistical machine translation (SMT) performance. For example, if we desire to build SMT for the medical domain, it may be beneficial to augment the training data with bitext from another domain, such as parliamentary proceedings. Despite the positive results, it is not clear exactly how and where additional outof-domain data helps in the SMT training pipeline. In this work, we analyze this problem in detail, considering the following hypotheses: out-of-domain data helps by either (a) improving word alignment or (b) improving phrase coverage. Using a multitude of datasets (IWSLT-TED, EMEA, Europarl, OpenSubtitles, KDE), we show that sometimes outof-domain data may help word alignment more than it helps phrase coverage, and more flexible combination of data along different parts of the training pipeline may lead to better results."
2010.iwslt-evaluation.19,{NTT} statistical {MT} system for {IWSLT} 2010,2010,8,0,3,1,1440,katsuhito sudoh,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
P09-2086,A Succinct N-gram Language Model,2009,10,9,2,0.833333,128,taro watanabe,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Efficient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N-gram language models based on LOUDS, a succinct data structure. LOUDS succinctly represents a trie with M nodes as a 2M  1 bit string. We compress it further for the N-gram language model structure. We also use 'variable length coding' and 'block-wise compression' to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss."
2009.iwslt-papers.3,Structural support vector machines for log-linear approach in statistical machine translation,2009,26,7,3,1,12296,katsuhiko hayashi,Proceedings of the 6th International Workshop on Spoken Language Translation: Papers,0,"Minimum error rate training (MERT) is a widely used learning method for statistical machine translation. In this paper, we present a SVM-based training method to enhance generalization ability. We extend MERT optimization by maximizing the margin between the reference and incorrect translations under the L2-norm prior to avoid overfitting problem. Translation accuracy obtained by our proposed methods is more stable in various conditions than that obtained by MERT. Our experimental results on the French-English WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data."
2008.iwslt-evaluation.13,{NTT} statistical machine translation system for {IWSLT} 2008.,2008,18,3,4,1,1440,katsuhito sudoh,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates k-best translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.
D07-1080,Online Large-Margin Training for Statistical Machine Translation,2007,23,176,3,1,128,taro watanabe,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features.
2007.iwslt-1.16,Larger feature set approach for machine translation in {IWSLT} 2007,2007,28,2,4,1,128,taro watanabe,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"The NTT Statistical Machine Translation System employs a large number of feature functions. First, k-best translation candidates are generated by an efficient decoding method of hierarchical phrase-based translation. Second, the k-best translations are reranked. In both steps, sparse binary features {---} of the order of millions {---} are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007."
W06-3115,{NTT} System Description for the {WMT}2006 Shared Task,2006,10,6,2,1,128,taro watanabe,Proceedings on the Workshop on Statistical Machine Translation,0,"We present two translation systems experimented for the shared-task of Workshop on Statistical Machine Translation, a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora."
P06-1078,Incorporating Speech Recognition Confidence into Discriminative Named Entity Recognition of Speech Data,2006,18,16,2,1,1440,katsuhito sudoh,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature. The ASR confidence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision. These results show that the proposed method is effective in NER for noisy inputs."
P06-1098,Left-to-Right Target Generation for Hierarchical Phrase-Based Translation,2006,14,45,2,1,128,taro watanabe,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the target-normalized form is based on an Early-style top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-to-right generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system."
2006.iwslt-evaluation.14,{NTT} statistical machine translation for {IWSLT} 2006,2006,15,15,3,1,128,taro watanabe,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We present the NTT translation system that is experimented for the evaluation campaign of xe2x80x9cInternational Workshop on Spoken Language Translation (IWSLT).xe2x80x9d The system consists of two primary components: a hierarchical phrase-based statistical machine translation system and a reranking sys tem. The former is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using nonterminals. The latter uses a modified voted perceptron approach with large number of features. Experiments showed that our hierarchical phrase-based model outperformed a conventional phrase-based model. In addition, our reranking algorithm further boosted the performance."
I05-1043,Instance-Based Generation for Interactive Restricted Domain Question Answering Systems,2005,12,3,2,0,51062,matthias denecke,Second International Joint Conference on Natural Language Processing: Full Papers,0,"One important component of interactive systems is the generation component. While template-based generation is appropriate in many cases (for example, task oriented spoken dialogue systems), interactive question answering systems require a more sophisticated approach. In this paper, we propose and compare two example-based methods for generation of information seeking questions."
2005.iwslt-1.15,The {NTT} Statistical Machine Translation System for {IWSLT}2005,2005,19,4,1,1,38029,hajime tsukada,Proceedings of the Second International Workshop on Spoken Language Translation,0,"This paper reports the NTT statistical translation system participating in the evaluation campaign of IWSLT 2005. The NTT system is based on a phrase translation model and utilizes a large number of features with a log-linear model. We studied the various features recently developed in this research field and evaluate the system using supplied data as well as publicly available Chinese, Japanese, and English data. Despite domain mismatch, additional data helped improve translation accuracy."
W04-3255,Efficient Decoding for Statistical Machine Translation with a Fully Expanded {WFST} Model,2004,18,8,1,1,38029,hajime tsukada,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding. In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand. Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding. The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches."
P03-2028,Spoken Interactive {ODQA} System: {SPIQA},2003,9,14,3,0,40303,chiori hori,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information. To test the efficiency of additional information requested by the DQs, the system reconstructs the user's initial question by combining the addition information with question. The combination is then used for answer extraction. Experimental results revealed the potential of the generated DQs."
