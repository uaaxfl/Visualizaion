2007.mtsummit-papers.71,J93-2003,0,0.0134483,"etter than Pharaoh, a state-of-the-art phrase-based SMT system, and other syntax-based methods, such as the synchronous CFG-based method on the small dataset. Keywords: statistical machine translation, syntax-based statistical machine translation, tree-to-tree alignment, synchronous tree-substitution grammar, elementary tree Motivation Phrase-based SMT Phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) to statistical machine translation (SMT) has recently achieved significant improvements in translation accuracy over the original IBM word-alignment-based model (Brown et al., 1993). In phrase-based models, a phrase can be any string of adjacent words without constraints imposed by any syntactic theory. These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. These make it a simple and powerful mechanism for machine translation. However, there exist many open issues to be resolved in phrase-based models. For examples, the handling of discontiguous phrases and modeling of global reordering, estimation of phrase translation probabilities and phrase partition probabilities"
2007.mtsummit-papers.71,W06-1628,0,0.0735041,"Missing"
2007.mtsummit-papers.71,P05-1067,0,0.215492,"Missing"
2007.mtsummit-papers.71,W04-3250,0,0.47186,"02). Hence, all knowledge sources, including source and target string and all hidden variables and any additional knowledge source, such as language model or additional dictionaries, are described as feature functions. In our implementation, we further simplify our model as follows: e1I (7) , PET1K Eq. (5) is the simplified model. Eq. (6) formalizes the modeling process based on log-linear framework. Eq. (7) formulizes the decoding, i.e., the translation process. Finally, for our experiments we use the following seven feature functions that are analogous to the default feature set of Pharaoh (Koehn, 2004a). 1) Bidirectional elementary tree mapping probability: φ (e |f ) = log ∏ k =1 K φ ( f |e) = log ∏ k =1 K N (ξ ek , ξ fk ) N (ξ fk ) N (ξ fk , ξ ek ) N (ξ ek ) 2) Bidirectional elementary tree lexical translation probability: lex ( f |e) and lex(e |f ) . Here, we only consider terminal translation probability and set the non-terminal translation probability to 1. 3) Language model (lm): log ∏ I i =1 p (ei |ei − 2 , ei −1 ) . 4) Number of elementary tree pairs used (pp): K. 5) Number of target words (wp): I. Rule Extraction Rules or PETs are extracted from word-aligned, bi-parsed sentence pai"
2007.mtsummit-papers.71,W04-3312,0,0.0310476,"rs while Huang et al (2006) and Liu et al (2006) work on tree-tostring alignment models. Our method, in terms of modeling, training and decoding algorithms are different from theirs at one or more points. In the rest of this paper, we elaborate our modeling, training and decoding methods and report our experimental results in detail. Tree-to-Tree Alignment-based Model In this section, we first introduce what STSG is and then based on which we define our tree-to-tree alignmentbased SMT model. Finally, we present the modeling process based on log-linear framework. Synchronous TSG (STSG) for SMT Shieber (2004) gives a formal and general definition of STSG. Here we give a more concrete definition of STSG with respect to its application in SMT. A STSG is a septet G =< Σ s, Σ t , Ns, Nt , Ss, St , P > , where: • Σ s and Σ t are source and target terminal alphabets (POSs or lexical words), respectively, and • Ns and Nt are source and target non-terminal alphabets (linguistic phrase tag, i.e., NP/VP…), respectively, and examples of elementary trees which belong to the English parse tree Tt shown in Figure 1. Obviously, a normal subtree (whose leaf nodes must be terminal symbols) is an elementary tree bu"
2007.mtsummit-papers.71,P01-1067,0,0.363922,"n probabilities and phrase partition probabilities are not yet effectively addressed in phrase-based models (Quirk and Menezes, 2006). Much research has been carried out to look into the above issues. One natural extension is to utilize syntax-based structure features for SMT. Syntax-based SMT Recent work in SMT has evolved from the word-based and phrase-based models to syntax-based models, that include hierarchical phrase models (Wu, 1997; Chiang, 2007), bilingual synchronous grammars (Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006;) and other syntax-based models (Yamada and Knight, 2001; Gildea, 2003; Och et al, 2004b; Liu et al., 2006). Wu (1997) and Chiang (2007)’s methods are formally syntax-based, i.e., their methods are not informed by any linguistically syntactic theory. Wu (1997) proposes Inversion Transduction Grammars (ITGs, an instance of synchronous CFGs), treating translation as a process of parallel parsing of the source and target languages via ITGs. Chiang (2007) uses a formal binary synchronous CFG to model hierarchical phrase structures. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Och et al (2004)"
C04-1033,P95-1017,0,0.602569,"d coreferential clusters. Compared with individual NPs, coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usua"
C04-1033,P98-1012,0,0.0814212,"9) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the frame"
C04-1033,W99-0611,0,0.0941913,"ll and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Ba"
C04-1033,N04-1002,0,0.0128346,"rvised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the framework of our NP-Cluster"
C04-1033,N01-1008,0,0.0893674,"Missing"
C04-1033,J94-4002,0,0.569485,"t, the Best-First strategy was applied. As illustrated in the table, we could observe that: 1. Without the three features, the system is equivalent to the baseline system in terms of the same recall and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selecti"
C04-1033,W02-1008,0,0.0936705,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,P02-1014,0,0.267893,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,W03-1307,1,0.587032,"Missing"
C04-1033,J01-4004,0,0.870843,", coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its"
C04-1033,W02-1040,0,0.120755,"ion 4 reports and discusses the experimental results. Section 5 describes related research work. Finally, conclusion is given in Section 6. 2 Baseline: the NP-NP based approach 2.1 Framework description We built a baseline coreference resolution system, which adopts the common NP-NP based learning framework as employed in (Soon et al., 2001). Each instance in this approach takes the form of i {NPj , NPi }, which is associated with a feature vector consisting of 18 features (f1 ∼ f18 ) as described in Table 2. Most of the features come from Soon et al. (2001)’s system. Inspired by the work of (Strube et al., 2002) and (Yang et al., 2004), we use two features, StrSim1 (f17 ) and StrSim2 (f18 ), to measure the string-matching degree of NPj and NPi . Given the following similarity function: Str Simlarity(Str1 , Str2 ) = 100 × |Str1 ∩ Str2 | Str1 StrSim1 and StrSim2 are computed using Str Similarity(SN Pj , SN Pi ) and Str Similarity(SN Pi , SN Pj ), respectively. Here SN P is the token list of NP, which is obtained by applying word stemming, stopword removal and acronym expansion to the original string as described in Yang et al. (2004)’s work. During training, for each anaphor NPj in a given text, a posi"
C04-1033,M95-1005,0,0.468812,"Missing"
C04-1033,P02-1060,1,0.804783,"Missing"
C04-1033,C98-1012,0,\N,Missing
C08-1016,W03-1023,0,\N,Missing
C08-1016,J05-3004,0,\N,Missing
C08-1016,J06-1005,0,\N,Missing
C08-1016,P02-1014,0,\N,Missing
C08-1016,J01-4004,0,\N,Missing
C10-1022,J01-4004,0,0.277206,"on training instance, the various features useful for event pronoun resolution and SVM classifier with adjustment of hyper-plane. Twin-candidate model is further introduced to capture the preferences among candidates. Section 3 presents in details the structural syntactic feature and the kernel functions to incorporate such a feature in the resolution. Section 4 presents the experiment results and some discussion. Section 5 concludes the paper. 2 The Resolution Framework Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al., 2001) and (Ng and Cardie, 2002a). 2.1 Training and Testing instance In the learning framework, training or testing instance of the resolution system has a form of where is the ith candidate of the antecedent of anaphor . An instance is labeled as positive if is the antecedent of , or negative if is not the antecedent of . An instance is associated with a feature vector which records different properties and relations between and . The features used in our system will be discussed later in this paper. During training, for each event pronoun, we consider the preceding verbs in its current and previou"
C10-1022,P02-1011,0,0.646355,"Missing"
C10-1022,C02-1139,0,0.176584,"e various features useful for event pronoun resolution and SVM classifier with adjustment of hyper-plane. Twin-candidate model is further introduced to capture the preferences among candidates. Section 3 presents in details the structural syntactic feature and the kernel functions to incorporate such a feature in the resolution. Section 4 presents the experiment results and some discussion. Section 5 concludes the paper. 2 The Resolution Framework Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al., 2001) and (Ng and Cardie, 2002a). 2.1 Training and Testing instance In the learning framework, training or testing instance of the resolution system has a form of where is the ith candidate of the antecedent of anaphor . An instance is labeled as positive if is the antecedent of , or negative if is not the antecedent of . An instance is associated with a feature vector which records different properties and relations between and . The features used in our system will be discussed later in this paper. During training, for each event pronoun, we consider the preceding verbs in its current and previous two sentences as its an"
C10-1022,P03-1054,0,0.0134325,"Missing"
C10-1022,P03-1023,1,0.917703,"Missing"
C10-1022,P04-1043,0,0.30271,"e or after the pronoun. date and the pronoun3. Such a feature keeps the most information related to the pronoun and candidate pair. Figure 3 shows the structure for feature full-expansion for instance {invaded, it}. As illustrated, the “NP” node for “perhaps different groups” is further expanded to the POS level. All its child nodes are included in the full-expansion tree except the surface words. 3.2 Convolution Parse Tree Kernel and Composite Kernel To calculate the similarity between two structured features, we use the convolution tree kernel that is defined by Collins and Duffy (2002) and Moschitti (2004). Given two trees, the kernel will enumerate all their sub-trees and use the number of common sub-trees as the measure of similarity between two trees. The above tree kernel only aims for the structured feature. We also need a composite kernel to combine the structured feature and the flat features from section 2.2. In our study we define the composite kernel as follows: where is the convolution tree kernel defined for the structured feature, and is the kernel applied on the flat features. Both kernels are divided by their respective length4 for normalization. The new composite kernel , define"
C10-1022,P04-1017,1,0.906771,"to an equivalent level with the minority class samples which is described in (Kubat and Matwin, 1997) and (Estabrooks et al, 2004). In (Ng and Cardie, 2002b), they proposed a negative sample selection scheme which included only negative instances found in between an anaphor and its antecedent. However, in our event pronoun resolution, we are distinguishing the event-anaphoric from non-event anaphoric which is different from (Ng and Cardie, 2002b). 2.2 Feature Space In a conventional pronoun resolution, a set of syntactic and semantic knowledge has been reported as in (Strube and Müller, 2003; Yang et al, 2004;2005a;2006). These features include number agreement, gender agreement and many others. However, most of these features are not useful for our task, as our antecedents are inflectional verbs instead of noun phrases. Thus we have conducted a study on effectiveness of potential positional, lexical and syntactic features. The lexical knowledge is mainly collected from corpus statistics. The syntactic features are mainly from intuitions. These features are purposely engineered to be highly correlated with positive instances. Therefore such kind of features will contribute to a high precision clas"
C10-1022,P05-1021,1,0.89395,"Missing"
C10-1022,I05-1063,1,0.905347,"Missing"
C10-1022,N06-2015,0,0.0827532,"Missing"
C10-1022,P06-1006,1,0.877191,"ine (Vapnik, 1995) to allow the use of kernels to incorporate the structure feature. One advantage of SVM is that we can use tree kernel approach to capture syntactic parse tree information in a particular high-dimension space. Suppose a training set consists of labeled vectors , where is the feature vector of a training instance and is its class label. The classifier learned by SVM is: where is the learned parameter for a support vector . An instance is classified as positive if . Otherwise, is negative.  Adjust Hyper-plane with Development Data Previous works on pronoun resolution such as (Yang et al, 2006) used the default setting for hyper-plane which sets . And an instance is positive if and negative otherwise. In our study, we look into a method of adjusting the hyper-plane’s position using development data to improve the classifier’s performance. Considering a default model setting for SVM as shown in Figure 2(for illustration purpose, we use a 2-D example). Figure 2: 2-D SVM Illustration The objective of SVM learning process is to find a set of weight vector which maximizes the margin (defined as ) with constraints defined 191 by support vectors. The separating hyper-plane is given by as b"
C10-1022,E06-1015,0,0.123054,"Missing"
C10-1022,P07-1103,0,0.613617,"Missing"
C10-1022,J08-3002,1,\N,Missing
C10-1022,P03-1022,0,\N,Missing
C10-1022,P08-1096,1,\N,Missing
C10-1022,P02-1034,0,\N,Missing
C10-1022,P02-1014,0,\N,Missing
C10-1118,D08-1092,0,0.0217423,"search helps avoid this asymmetric problem. 4 Feature Functions In this section, we introduce a variety of feature functions to capture the semantically equivalent 1048 counterparts and structural divergence across languages. For the semantic equivalence, we define lexical and word alignment feature functions. Since those feature functions are directional, we describe most of these functions as conditional feature functions based on the conditional lexical probabilities. We also introduce the tree structural features to deal with the structural divergence of bilingual parse trees. Inspired by Burkett and Klein (2008), we introduce the feature functions in an internal-external manner based on the fact that the feature scores for an aligned sub-tree pair tend to be high inside both sub-trees, while they tend to be low inside one sub-tree and outside the other. 4.1 ∑ ( |) ∑ (∏ ) ( Although the word alignment information within bilingual sentence pairs is to some extent not reliable, the links of word alignment account much for the co-occurrence of the aligned terms. We define the internal word alignment features as follows: ( ∑ ) ( ) ( ) ( |) ( ( ) {( ) | ∑ ( ) ) {( | ) ( )|) 4.4 Internal-External Word Align"
C10-1118,P06-1121,0,0.026109,"idely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenom"
C10-1118,N04-1014,0,0.0133631,"show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later st"
C10-1118,C04-1154,0,0.0821934,"agate the word alignment error to the later stage of rule extraction. Alternatively, some initial attempts have been made to directly conduct syntactic structure alignment. As mentioned in Tinsley et al. (2007), the early work usually constructs the structure alignment by hand, which is time-consuming. Recent research tries to automatically align the bilingual syntactic sub-trees. However, most of these works suffer from the following problems. Firstly, the alignment is conducted based on heuristic rules, which may lose extensibility and generality in spite of accommodating some common cases (Groves et al., 2004). Secondly, various similarity computation methods are used based merely on lexical translation probabilities (Tinsley et al., 2007; Imamura, 2001) regardless of structural features. We believe the structure information is an important issue to capture the non-local structural divergence of languages by modeling beyond the plain text. To address the above issues, we present a statistical framework based on Maximum Entropy (MaxEnt) model. Specifically, we consider subtree alignment as a binary classification problem and use Maximum Entropy model to classify each instance as aligned or unaligned"
C10-1118,2006.amta-papers.8,0,0.0488805,"T systems. We then compare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rul"
C10-1118,2007.mtsummit-papers.62,0,0.543965,"trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later stage of rule extraction. Alternatively, some initial attempts have been made to directly conduct syntactic structure alignment. As mentioned in Tinsley et al. (2007), the early work usually constructs the structure alignment by hand, which is time-consuming. Recent research tries to automatically align the bilingual syntactic sub-trees. However, most of these works suffer from the following problems. Firstly, the alignment is conducted based on heuristic rules, which may lose extensibility and generality in spite of accommodating some common cases (Groves et al., 2004). Secondly, various similarity computation methods are used based merely on lexical translation probabilities (Tinsley et al., 2007; Imamura, 2001) regardless of structural features. We beli"
C10-1118,2007.mtsummit-papers.71,1,0.820315,"ment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later stage of rule extractio"
C10-1118,P08-1064,1,0.808428,"we use Moses (Koehn et al, 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in an SMT system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in Pisces decoder with the same features and settings in Sun et al. (2009). The STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its sub-spans has already been translated. The STSG based experiment can be easily achieved by restricting the translation rule set in the STSSG decoder to be elementary tree pairs only. For the alignment setting of the baselines, we use the word alignment trained on the entire FBIS(240k) corpus by GIZA++ with heuristic grow-diag-final for"
C10-1118,P03-1054,0,0.0499734,"/index.php/resources.html . We licensed the corpus from them for research usage. 1050 experiment due to problems of domain divergence, annotation discrepancy (Chinese parse tree adopts a different grammar from Penn Treebank annotations) and degree of tolerance for parsing errors. Due to the above issues, we annotate a new data set to apply the sub-tree alignment in machine translation. We randomly select 300 bilingual sentence pairs from the Chinese-English FBIS corpus with the length in both the source and target sides. The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. We manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 2. To be fully consistent with the definition, we strictly preserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing. Some initial statistic of the automatically parsed corpus is shown in Table 2. Train Test # of Sentence pair Avg. Sentence Length Avg. # of sub-tr"
C10-1118,P07-2045,0,0.00569676,"f data to train word alignment b. Precision/Recall for HIT test set. c. F-score for HIT test set. d. Precision/Recall for FBIS test set. e. F-score for FBIS test set. sentences with less than 50 characters from the NIST MT-2002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. For the phrase based system, we use Moses (Koehn et al, 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in an SMT system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement"
C10-1118,N03-1017,0,0.00642613,"le set in the STSSG decoder to be elementary tree pairs only. For the alignment setting of the baselines, we use the word alignment trained on the entire FBIS(240k) corpus by GIZA++ with heuristic grow-diag-final for Moses and the syntax systems and perform rule extraction constrained on the word alignment. As for the experiments adopting sub-tree alignment, we use the above word alignment to learn lexical/word alignment features, and train the sub-tree alignment model with FBIS training data (200). 7.2 Experimental results Utilizing the syntactic rules only has been argued to be ineffective (Koehn et al., 2003). Therefore, instead of using the sub-tree aligned rules only, we try to improve the word alignment constrained rule set by sub-tree alignment as shown in Table 5. Firstly, we try to Directly Concatenate (DirC) the sub-tree alignment constraint rule set 3 to the original syntax/phrase rule set based on word alignment. Then we re-train the MT model based 3 For syntax based system, it’s just the sub-tree pairs deducted from the sub-tree alignment; for phrase based system, it&apos;s the phrases with context equivalent to the aligned sub-tree pairs. 1053 250 PP3 IP2 P CP VP VP NP DEC AD VV NR 反对 以色列的 非"
C10-1118,P06-1077,0,0.0530366,"ompare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has di"
C10-1118,W06-1606,0,0.0257243,"ment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully pr"
C10-1118,J03-1002,0,0.00907158,"kipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. Baseline approach We implement the work in Tinsley et al. (2007) as our baseline methodology. Given a tree pair , the baseline approach first takes all the links between the sub-tree pairs as alignment hypotheses, i.e., the Cartesian product of the two sub-tree sets: { } { } By using the lexical translation probabilities, each hypoth"
C10-1118,P03-2041,0,\N,Missing
C10-1118,J08-3004,0,\N,Missing
C10-1118,P09-1103,1,\N,Missing
C10-1145,S07-1012,0,0.0325864,"Wen Ting Wang‡ † ‡ School of Computing National University of Singapore {z-wei, tancl} @comp.nus.edu.sg people search, knowledge base population (KBP), and information extraction, because an entity (such as Abbott Laboratories, a diversified pharmaceuticals health care company) can be referred to by multiple mentions (e.g. “ABT” and “Abbott”), and a mention (e.g. “Abbott”) can be shared by different entities (e.g. Abbott Texas: a city in United States; Bud Abbott, an American actor; and Abbott Laboratories, a diversified pharmaceutical health care company). Both Web People Search (WePS) task (Artiles et al. 2007) and Global Entity Detection & Recognition task (GEDR) in Automatic Content Extraction 2008 (ACE08) disambiguate entity mentions by clustering documents with these mentions. Each cluster then represents a unique entity. Recently entity linking has been proposed in this field. However, it is quite different from the previous tasks. Given a knowledge base, a document collection, entity linking task as defined by KBP-091 (McNamee and Dang, 2009) is to determine for each name string and the document it appears, which knowledge base entity is being referred to, or if the entity is a new entity whic"
C10-1145,D09-1056,0,0.00891313,"ular learning algorithm. During disambiguation, (query, entity) is presented to the classifier which then returns a class label. Each (query, entity) pair is represented by the feature vector using different features and similarity metrics. We chose the following three classes of features as they represent a wide range of information - lexical features, word-category pair, NE type - that have been proved to be effective in previous works and tasks. We now discuss the three categories of features used in our framework in details. Lexical features. For Bag of Words feature in Web People Search, Artiles et al. (2009) illustrated that noun phrase and n-grams longer than 2 were not effective in comparison with tokenbased features and using bi-grams gives the best 1294 results only reaching recall 0.7. Thus, we use token-based features. The similarity metric we choose is cosine (using standard tf.idf weighting). Furthermore, we also take into account the co-occurring NEs and represent it in the form of token-based features. Then, the single cosine similarity feature is based on Co-occurring NEs and Bag of Words. Word Category Pair. Bunescu (2007) demonstrated that word-category pairs extracted from the docum"
C10-1145,P07-1033,0,0.00976239,"Missing"
C10-1145,N04-1002,0,0.026506,"h the highest rank is chosen. Bag of words and co-occurring NEs are represented in the form of token-based feature vectors. Then tf.idf is employed to calculate similarity between feature vectors. To make the baseline system with tokenbased features state-of-the-art, we conduct a series of experiments. Table 1 lists the performances of our token-based ranking systems. In our experiment, local tokens are text segments generated by a text window centered on the mention. We set the window size to 55, which is the value that was observed to give optimum performance for the disambiguation problem (Gooi and Allan, 2004). Full tokens and NE are all the tokens and named entities co-occurring in the text respectively. We notice that tokens of the full text as well as the co-occurring named entity produce the best baseline performance, which we use for the further experiment. http://download.wikipedia.org 1295 local tokens local tokens + NE full tokens + NE Micro-averaged Accuracy 60.0 60.6 61.9 Table 1: Results of the ranking methods 4.3 Experiment and Result As discussed in Section 3.1, we exploit two more knowledge sources in Wikipedia: “did you mean” (DYM) and “Wikipedia search engine” (SE) for name variatio"
C10-1145,zesch-etal-2008-extracting,0,0.00970901,"uation, which links an entity mention with the real world entity it refers to. 3.1 Name Variation The aim for Name Variation is to build a Knowledge Repository of entities that contains vast amount of world knowledge of entities like name variations, acronyms, confusable names, spelling variations, nick names etc. We use Wikipedia to build our knowledge repository since Wikipedia is the largest encyclopedia in the world and surpasses other knowledge bases in its coverage of concepts and up-to-date content. We obtain useful information from Wikipedia by the tool named Java Wikipedia Library 2 (Zesch et al. 2008), which allows to access all information contained in Wikipedia. Cucerzan (2007) extracts the name variations of an entity by leveraging four knowledge sources in Wikipedia: “entity pages”, “disambiguation pages” “redirect pages” and “anchor text”. Entity page in Wikipedia is uniquely identified by its title – a sequence of words, with the first word always capitalized. The title of Entity Page represents an unambiguous name variation for the entity. A redirect page in Wikipedia is an aid to navigation. When a page in Wikipedia is redirected, it means that those set of pages are referring to t"
C10-1145,D07-1074,0,\N,Missing
C10-2172,W05-0613,0,0.0392846,"sohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an impli"
C10-2172,W01-1605,0,0.643567,"Missing"
C10-2172,W03-1210,0,0.0480087,"Missing"
C10-2172,N04-1020,0,0.0089935,"rns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. They showed that phrasal patterns extracted from a text span pair provide useful evidence in the relation classiﬁcation. (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. (Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers wit"
C10-2172,P02-1047,0,0.383456,"e a part of many natural language processing systems, e.g., text summarization system, question answering system. If there are discourse connectives between textual units to explicitly mark their relations, the recognition task on these texts is deﬁned as explicit discourse relation recognition. Otherwise it is deﬁned as implicit discourse relation recognition. However, for implicit relations, there are no connectives to explicitly mark the relations, which makes the recognition task quite difﬁcult. Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). They use unambiguous patterns such as [Arg1, but Arg2] to create synthetic examples of implicit relations and then use [Arg1, Arg2] as an training example of an implicit relation. Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al., 2009a) and (Lin et al., 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses"
C10-2172,P09-1077,0,0.648462,"ional features We predict implicit connectives on both training set and test set. Then we can use the predicted implicit connectives as additional features for supervised implicit relation recognition. Previous works exploited various linguistically informed features under the framework of supervised models. In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, ﬁrst-last words of arguments, cross-argument word pairs, ever used in (Pitler et al., 2009a), production rules of parse trees of arguments used in (Lin et al., 2009), and intra-argument word pairs inspired by the work of (Saito et al., 2006). Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al., 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). In addition, the average length of verb phrase and the part of speech tags of main verb are also included as verb features. Context: If the immediately preceding"
C10-2172,P09-2004,0,0.281099,"Missing"
C10-2172,prasad-etal-2008-penn,0,0.418208,"syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an implicit discourse relation classiﬁer in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and crossargument w"
C10-2172,N06-2034,0,0.0171666,"Missing"
C10-2172,W06-1317,0,0.205213,"Missing"
C10-2172,D09-1036,0,\N,Missing
C10-2172,N03-1030,0,\N,Missing
D09-1108,N04-1035,0,0.124929,"to h. A non-terminal node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 2. A packed forest Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure map1038 Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so the algorithm proposed in this paper is also applicable to the tree sequence-based models. Figure 3. Tree 1 (T1) 3 Figure 4. Tree 2 (T2) Matching Methods in Previous Work In this section, we discuss the two typical rule matching algorithms used in previous work. 3.1 For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted"
D09-1108,P01-1044,0,0.145454,"tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its father node, then we say"
D09-1108,J99-4005,0,0.0555456,"anslation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source structure (either a parse tree or a parse forest), a translation rule is applicable if and only if the left hand side of the translation rule exactly matches a tree fragment of the given source structure. an NP-hard search problem (Knight, 1999). In the SMT research community, the second step has been well studied and many methods have been proposed to speed up the decoding process, such as node-based or span-based beam search with different pruning strategies (Liu et al., 2006; Zhang et al., 2008a, 2008b) and cube pruning (Huang and Chiang, 2007; Mi et al., 2008). However, the first step attracts less attention. The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span. This solution becomes computationally infeasible when it is applied to packed forests with"
D09-1108,P07-2045,0,0.0122955,"Missing"
D09-1108,P06-1077,0,0.15997,"node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 2. A packed forest Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure map1038 Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so the algorithm proposed in this paper is also applicable to the tree sequence-based models. Figure 3. Tree 1 (T1) 3 Figure 4. Tree 2 (T2) Matching Methods in Previous Work In this section, we discuss the two typical rule matching algorithms used in previous work. 3.1 For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as"
D09-1108,P07-1089,0,0.149621,"Missing"
D09-1108,P08-1023,0,0.272369,"Missing"
D09-1108,D08-1022,0,0.407315,"d tree-to-string translation model which serves as the translation platform in this paper. 2.1 Tree-to-string model ping process, which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule."
D09-1108,J03-1002,0,0.00845557,"Missing"
D09-1108,J87-1004,0,0.554845,"source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its"
D09-1108,zhang-etal-2004-interpreting,0,0.105268,"Missing"
D09-1108,A00-2018,0,\N,Missing
D09-1108,C08-1138,1,\N,Missing
D09-1108,W05-1506,0,\N,Missing
D09-1108,P02-1040,0,\N,Missing
D09-1108,P09-1020,1,\N,Missing
D09-1108,P08-1064,1,\N,Missing
D09-1108,P07-1019,0,\N,Missing
D09-1108,W01-1812,0,\N,Missing
D09-1108,P03-1021,0,\N,Missing
D09-1161,E03-1005,0,0.0765955,"Missing"
D09-1161,P06-1055,0,0.260545,"a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination meth"
D09-1161,N07-1051,0,0.289935,"lized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have bee"
D09-1161,P02-1035,0,0.0204866,"versity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework for multiple parsers combination. The proposed framework leverages on the strengths of previous system combination and rerank"
D09-1161,P97-1003,0,0.239574,"both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural"
D09-1161,N06-2033,0,0.822122,"ling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework"
D09-1161,W02-1001,0,0.114872,"and English Penn Treebank corpus. Experimental results show that our final results, an F-Score of 92.62 on English and 85.45 on Chinese, outperform the previously best-reported systems by 0.52 point and 1.21 point, respectively. This convincingly demonstrates the effectiveness of our proposed framework. Our study also shows that the simulated-annealing algorithm (Kirkpatrick et al. 1983) is more effective 1552 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552–1560, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP than the perceptron algorithm (Collins 2002) for feature weight tuning. The rest of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 discusses our method while section 4 presents the feature weight tuning algorithm. In Section 5, we report our experimental results and then conclude in Section 6. 2 Related Work As discussed in the previous section, system combination and re-ranking are two techniques to improve parsing performance by postprocessing parsers’ k-best outputs. Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an ent"
D09-1161,P08-1067,0,0.121123,"rom each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework for multiple parsers combination. The proposed framework leverages on the strengths of previous system combination and reranking methods and is open to any type of f"
D09-1161,W01-1812,0,0.0341403,"Briefly speaking, latent-annotation model views each non-terminal in the Treebank as a non-terminal followed by a set of latent variables, and uses EM algorithms to automatically learn the latent variables’ probability functions to maximize the probability of the given training data. Take the following binarized rule as example, could be viewed as the set of rules The process of computing the probability of a normal tree is to first binarized all the rules in it, and then replace each rule to the corresponding set of rules with latent variables. Now the previous tree becomes a packed forest (Klein and Manning 2001; Petrov et al. 2007) in the latentannotation model, and its probability is the inside probability of the root node. This model is quite different from the head-driven model in which 1554 the probability of a tree is just the product all the rules’ probability. 3.3 Constituent Counts Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). A constituent is a non-terminal node covering a special span. For example, “NP[2,4]” means a constituent labelled as “NP” which covers the span from the second w"
D09-1161,P03-1054,0,0.0220626,"combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual p"
D09-1161,W99-0623,0,0.81626,"ized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper,"
D09-1161,J93-2004,0,0.0341485,"Missing"
D09-1161,N06-1020,0,0.0614305,"two parameter estimation algorithms with significant test; “SA.” is simulated annealing, “AP.” is averaged perceptron, “P-value” is the significant test p-value. 5.6 Table 6. F1 score on 50-best combination with different feature configuration. “I” means the constituent count, “B” means Berkeley parser confidence score and “C” means Charniak parser confidence score. 5.5 Algo. Lang Performance-Enhanced Parsers on English Individual For Charniak’s lexicalized parser, there are two techniques to improve its performance. One is reranking as explained in section 2. The other is the self-training (McClosky et al. 2006) which first parses and reranks the NANC corpus, and then use them as additional training data to retrain the model. In this sub-section, we apply our method to combine the Berkeley parser and the enhanced Charniak parser by using the new model confidence score output from the enhanced Charniak parser. Table 9 and Table 10 show that the Charniak parser enhanced by re-ranking and self-training is able to help to further improve the performance of our method. This is because that the enhanced Charniak parser provides more accurate model confidence score. 1558 parser accuracy P <=40 R words F P A"
D09-1161,A00-2018,0,\N,Missing
D09-1161,W05-1506,0,\N,Missing
D09-1161,J03-4003,0,\N,Missing
D09-1161,P05-1022,0,\N,Missing
D09-1161,W05-1518,0,\N,Missing
D09-1161,P05-1010,0,\N,Missing
D09-1161,D08-1092,0,\N,Missing
D10-1085,C10-1022,1,\N,Missing
D10-1085,W02-1008,0,\N,Missing
D10-1085,N06-2015,0,\N,Missing
D10-1085,J08-3002,1,\N,Missing
D10-1085,P03-1054,0,\N,Missing
D10-1085,P04-1043,0,\N,Missing
D10-1085,P06-1006,1,\N,Missing
D10-1085,P02-1034,0,\N,Missing
D10-1085,P04-1018,0,\N,Missing
D10-1085,I05-1063,1,\N,Missing
D10-1085,P07-1103,0,\N,Missing
D10-1085,P02-1014,0,\N,Missing
D10-1085,J01-4004,0,\N,Missing
D10-1085,P02-1011,0,\N,Missing
D10-1085,E06-1015,0,\N,Missing
D10-1085,P03-1023,1,\N,Missing
D12-1026,vilar-etal-2006-error,0,0.0507574,"in a sentence and thus supervise SMT to reduce tense inconsistency errors against Observations (1) and (2) in the sentence-level. In comparison, Observation (3) actually reflects the tense distributions among one document. After extracting each main tense for each sentence, we build another tense ngram model in the document-level. For clarity, this paper denotes document-level tense as “inter-tense” and sentence-level tense as “intra-tense”. After that, we propose to integrate such tense models into SMT systems in a dynamic way. It is well known there are many errors in the current MT output (David et al., 2006). Unlike previously making trouble with reference texts, the BLEU-4 score cannot be influenced obviously by modifying a small part of abnormal sentences in a static way. In our system, both inter-tense and intra-tense model are integrated into a SMT system via additional features and thus can supervise the decoding procedure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the"
D12-1026,P92-1033,0,0.823893,"Missing"
D12-1026,D11-1084,1,0.858213,"ure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the examples in Sec277 tion 6.4) show the effectiveness of this way. Rather than the rule-based model, our models are fully statistical-based. So they can be easily scaled up and integrated into either phrase-based or syntaxbased SMT systems. In this paper, we employ a strong phrase-based SMT baseline system, as proposed in Gong et al. (2011), which uses document as translation unit, for better incorporating documentlevel information. The rest of this paper is organized as follows: Section 2 reviews the related work. Section 3 and 4 are related to tense models. Section 3 describes the preprocessing work for building tense models. Section 4 presents how to build target-side tense models and discuss their characteristics. Section 5 shows our way of integrating such tense models into a SMT system. Session 6 gives the experimental results. Finally, we conclude this paper in Section 7. 2 Related Work In this section, we focus on relate"
D12-1026,N03-1017,0,0.125285,"Missing"
D12-1026,W04-3250,0,0.0420808,"tal Setting for SMT In our experiment, SRI language modeling toolkit was used to train a 5-Gram general language model on the Xinhua portion of the Gigaword corpus. Word alignment was performed on the training parallel corpus using GIZA++ ( Och and Ney, 2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the BLEU score (Papineni et al., 2002), which measures case-insensitive matching of 4-grams. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004). In this paper, “***” and “**” denote p-values equal to 0.05, and bigger than 0.05, which mean significantly better, moderately better respectively. Role Train Dev Test Corpus Name FBIS NIST2003 NIST2005 Sentences Documents 228455 919 1082 10000 100 100 6.3 Experimental Results All the experiment results are showed on the table 3. Our Baseline is a modified Moses. The major modification is input and output module in order to translate using document as unit. The performance of our baseline exceeds the baseline reported by Gong et al. (2011) about 2 percent based on the similar training and te"
D12-1026,I11-1125,0,0.141759,"nd subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chinese verbs via a paral"
D12-1026,P00-1056,0,0.0454932,"s into SMT In this section, we discuss how to integrate the previous tense models into a SMT system. 5.1 Basic phrase-based SMT system It is well known that the translation process of SMT can be modeled as obtaining the best translation e of the source sentence f by maximizing following posterior probability(Brown et al., 1993): ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3)."
D12-1026,2002.tmi-tutorials.2,0,0.0350901,"nse sequence is about 2.5, we mainly consider intra-tense bigram model and thus n equals to 2. 5 5.4 Determining Tense For SMT Output The current SMT systems often produce odd translations partly because of abnormal word ordering and uncompleted text etc. For these abnormal translated texts, the syntactic parser cannot work well in our initial experiments, so the previous method to parse main tense and tense sequence of regular texts cannot be applied here too. Fortunately, the solely utilization of Stanford POS tagger for our SMT output is not bad although it has the same issues described in Och et al. (2002). The reason may be that phrase-based SMT contains short contexts that POS tagger can utilize while the syntax parser fails. Once obtaining a completed hypothesis, the decoder will pass it to the Stanford POS tagger and according to tense verbs to get all tense sequence for this hypothesis. However, since the POS tagger can not return the information about level structures, the decoder cannot recognize the main tense from such tense sequence. Liu et al. (2011) once used target-side verbs to label tense of source-side verbs. It is natural to consider whether Chinese verbs can provide similar cl"
D12-1026,P03-1021,0,0.0376538,": ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3). Next, the decoder uses such two additional feature values to re-score this hypothesis automatically and choose one hypothesis with highest score as the final translation. After translating one sentence, the decoder caches its main tense and pass it to the next sentence. When one document has been processed, the decoder"
D12-1026,2001.mtsummit-papers.47,0,0.174117,"Missing"
D12-1026,P02-1040,0,0.0831374,"Missing"
D12-1026,I05-1077,0,0.0561652,"particular, they trained models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense i"
D12-1026,2007.mtsummit-papers.69,0,0.839163,"ined models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chi"
D12-1026,J93-2003,0,\N,Missing
D12-1026,N04-1021,0,\N,Missing
D12-1026,W06-0107,0,\N,Missing
D13-1163,J08-1001,0,0.0522055,"mong the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choic"
D13-1163,W13-3304,0,0.0103188,"nts our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language top"
D13-1163,2007.tmi-papers.6,0,0.00909198,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,D07-1007,0,0.0489016,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,W09-2404,0,0.168473,"e model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese new"
D13-1163,P07-1005,0,0.0358991,"ith cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining alg"
D13-1163,C10-3004,0,0.0083293,"as only two sub-classes at level i + 1. Actually, they have multiple sub-classes. tended Cilin contains 77,343 Chinese words, which are organized in a hierarchical structure containing 5 levels as shown in Figure 3. In the 5th level, each node represents an atomic concept which consists of a set of synonyms. These atomic concepts are just like synsets in WordNet. We use them to represent senses of words in the disambiguation graph. We select nouns, verbs, abbreviations and idioms as candidate words for the disambiguation graph. These words are identified by a Chinese part-tospeech tagger LTP (Che et al., 2010) in a preprocessing step. In order to build the disambiguation graph, we first build an array indexed by the atomic concepts of Cilin, then insert a copy of each candidate word into its all concept (sense) entries in the array. After that, we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown (2003). In the second step, we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph. We sum the weights of all semantic links under the different senses of the candidate wor"
D13-1163,J07-2003,0,0.0588006,"which show the number of documents (#Doc) and sentences (#Sent), the number of lexical chains extracted from the source documents (#Chain), the average number of lexical chains per document (#AvgC) and the average number of words per lexical chain (#AvgW). Figure 4: Architecture of an SMT system with the lexical chain based cohesion model. 5 Experiments In this section, we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation. We used a hierarchical phrased-based SMT system (Chiang, 2007) trained on large-scale data. In particular, we aim at: • Measuring the impact of the threshold  on the probability cohesion model and selecting the best threshold on a development test set. • Investigating the effect of the two lexical-chain based cohesion models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC20"
D13-1163,P11-2031,0,0.0155574,"C2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 3 Available at: http://homepages.inf.ed.ac.uk/lzhang10/ maxent toolkit.html  0.05 0.1 0.2 0.3 0.4 System Baseline LexChainCount(top 1) LexChainCount LexChainProb MT06 30.53 31.64 31.45 30.73 31.01 Table 2: BLEU scores of the probability cohesion model Mp (TDt , { LCtk }N k=1 ) with different values for the threshold . et al., 2002) as our evaluation metric. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). 5.2 Setting the Threshold  As the two lexical chain based cohesion models are built on the super target lexical chains that are associated with a parameter , we need to tune the threshold parameter  on the development test set NIST MT06. We conducted a group of experiments using the probability cohesion model defined in Eq. (5) to find the best threshold. Experiment results are shown in Table 2. If we set the threshold too small (e.g., 0.05), the super target lexical chains may contain too many noisy words that are not the translations of source lexical chain words, which may jeopardise t"
D13-1163,H92-1045,0,0.467758,"s defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1. Words in these lexical chains have lexica"
D13-1163,D11-1084,1,0.745373,"Missing"
D13-1163,W13-3302,0,0.591796,"models. Section 5 presents our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a to"
D13-1163,D12-1108,0,0.0625948,"l. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document-level machine translation. They define three cohesion models based on lexical cohesion devices: a direct reward model, a conditional probability model and a mutual information trigger model. The latter two models measure the strength of lexical cohesion relation bet"
D13-1163,D12-1106,0,0.0318609,"els proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the"
D13-1163,J91-1002,0,0.949629,"tween text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document. This assumption is reasonable as the target do"
D13-1163,P02-1040,0,0.0861266,"Missing"
D13-1163,W10-2602,0,0.175477,"arch interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently tran"
D13-1163,N12-1046,0,0.287225,"Missing"
D13-1163,D12-1097,0,0.648458,"76). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct corr"
D13-1163,2011.mtsummit-papers.13,0,0.396437,"this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device b"
D13-1163,W97-0703,0,\N,Missing
H05-1114,C02-1039,0,0.0170041,"learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feat"
H05-1114,W04-2405,0,0.0123837,"ace requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over"
H05-1114,J94-4003,0,0.0118157,"ta lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does n"
H05-1114,P04-1080,1,0.843703,"nly unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feature 907 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 907–914, Vancouver, October 2005. 2005 Association for Computational Linguistics clustering technique to WSD task. Feature clustering has been extensively studied for the benefit of text categorization and document clustering. In the context of text categorization, supervised feature clustering algorithms"
H05-1114,J98-1006,0,0.025447,"which is far from optimal for many classification algorithms. Furthermore, processing data lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision"
H05-1114,W02-1006,0,0.487307,"vised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feature 907 Proceedings of Human Language Technology Conference and Conference on Empirica"
H05-1114,J04-1001,0,0.0119965,"sional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM an"
H05-1114,P04-1036,0,0.0890695,"Missing"
H05-1114,P05-1049,1,0.707284,"ge amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SEN"
H05-1114,W04-0849,0,0.0440224,"Missing"
H05-1114,P00-1069,0,0.0198546,"y and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee a"
H05-1114,W97-0322,0,0.05447,"Missing"
H05-1114,J98-1004,0,0.170863,"Missing"
H05-1114,J98-1005,0,0.0258029,"imal for many classification algorithms. Furthermore, processing data lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SEN"
H05-1114,P95-1026,0,0.197927,"Missing"
H05-1114,W04-0807,0,\N,Missing
I05-1034,A00-2030,0,0.0584531,"stness in handling large-scale or new domain data due to two reasons. First, rules have to be rewritten for different tasks or when porting to different domains. Second, generating rules manually is quite labor- and time-consuming. 1 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity --- an entity with land and a government. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 – 389, 2005. © Springer-Verlag Berlin Heidelberg 2005 Discovering Relations Between Named Entities from a Large Raw Corpus 379 Since then, various supervised learning approaches [2,3,4,5] have been explored extensively in relation extraction. These approaches automatically learn relation patterns or models from a large annotated corpus. To decrease the corpus annotation requirement, some researchers turned to weakly supervised learning approaches [6,7], which rely on a small set of initial seeds instead of a large annotated corpus. However, there is no systematic way in selecting initial seeds and deciding an “optimal” number of them. Alternatively, Hasegawa et al. [8] proposed a cosine similarity-based unsupervised learning approach for extracting relations from a large raw c"
I05-1034,P03-1005,0,0.0243155,"Missing"
I05-1034,P04-1043,0,0.255007,"Missing"
I05-1034,sekine-etal-2002-extended,0,0.0148849,"ht, NP], [sold, NP, yesterday]) = 1 + K ( bought, sold ) + K (NP, NP) = 1+0.25+0.25+K c ([a, red, car], [the, flat]) = 1.5 + K (a, the) + K (car, flat ) =2 The above similarity score is more than one. This is because we did not normalize the score using Formula (6). 2.3 Tree Similarity Based Unsupervised Learning Our method consists of five steps: 1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE types provide more effective information for relation discovery. Here we use Sekine’s NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are defined [21]. This NE tagger has also been adopted by Hasegawa et al. [8]. Besides, Collin’s parser [18] is adopted to generate shallow parse trees. 2) Similarity calculation: The similarity between two relation instances is defined between two parse trees. However, the state-of-the-art of parser is always error-prone. Therefore, we only use the minimum span parse tree including the NE pairs when calculating the similarity function [4]. Please note that the two entities may not be the leftmost or rightmost node in the sub-tree. 3) NE pairs clustering: Clustering of NE pairs is based on the similarity scor"
I05-1034,J03-4003,0,\N,Missing
I05-1034,P04-1054,0,\N,Missing
I05-1034,P04-1053,0,\N,Missing
I05-1034,P02-1034,0,\N,Missing
I05-1034,P04-1016,0,\N,Missing
I05-1035,P04-1053,0,\N,Missing
I05-1035,P97-1009,0,\N,Missing
I05-1035,P03-1029,0,\N,Missing
I05-1035,W02-1010,0,\N,Missing
I05-1035,A00-1039,0,\N,Missing
I05-1063,J01-4004,0,0.461959,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,P02-1014,0,0.307638,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,P03-1023,1,0.949019,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,W03-2604,0,0.0524874,"ng approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between candidates and then select the most preferred one as the antecedent. The previous work has reported that such a model can eﬀectively help antecedent determination for anaphors [5,6]. However, one problem exits with the twin-candidate model. For every encountered NP during resolution, the model would always pick out a “best” candidate as the antecedent, even if the current NP is not an anaphor. The twin-candidate R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 719–730, 2005. c Springer-Verl"
I05-1063,C02-1139,0,0.500844,"Missing"
I05-1063,M95-1005,0,0.135699,"tion and Discussion Experiment Setup The experiments were done on the newswire domain, using MUC coreference data set (Wall Street Journal articles). For MUC-6 [8] and MUC-7 [9], 30 “dryrun” documents were used for training as well as 20-30 documents for testing. In addition, another 100 annotated documents from MUC-6 corpus were also prepared for the purpose of deeper system analysis. Throughout the experiments, C5 was used as the learning algorithm [10]. The recall and precision rates of the coreference resolution systems were calculated based on the scoring scheme proposed by Vilain et al. [11]. A Twin-Candidate Model of Coreference Resolution 725 Table 2. Features for coreference resolution using the twin-candidate model Features describing the new markable M : 1 if M is a deﬁnite NP; else 0 1. M DefNP 1 if M is an indeﬁnite NP; else 0 2. M IndefNP 1 if M is a proper noun; else 0 3. M ProperNP 1 if M is a pronoun; else 0 4. M Pronoun Features describing the candidate, C1 or C2 , of M 1 if C1 (C2 ) is a deﬁnite NP; else 0 5. candi DefNp 1(2) 6. candi IndefNp 1(2) 1 if C1 (C2 ) is an indeﬁnite NP; else 0 7. candi ProperNp 1(2) 1 if C1 (C2 ) is a proper noun; else 0 8. candi Pronoun 1"
I05-2045,P03-1029,0,0.0175592,"Missing"
I05-2045,P97-1009,0,0.009886,"ˆ= the mapping procedure can be formulated as:Ω P|T C| arg maxΩ j=1 tΩ(j),j , where Ω(j) is the index of the estimated cluster associated with the j-th class. Given the result of one-to-one mapping, we can define the evaluation measure as follows: P tΩ(j),j ˆ j Accuracy(P ) = P t . Intuitively, it reflects i,j i,j the accuracy of the clustering result. 3.3 Evaluation method for relation labelling For evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names. To do this, we use one information-content based measure (Lin, 1997), which is provided in WordnetSimilarity package (Pedersen et al., 2004) to evaluate the similarity between two concepts in Wordnet. Intuitively, the relatedness between two concepts in Wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: 1 ,c2 )) Relatednesslin (c1 , c2 ) = 2×IC(lcs(c IC(c1 )+IC(c2 ) . This measure depends upon the corpus to estimate information content. We carried out the experiments using the British National Corpus (BNC) as the source of informat"
I05-2045,N04-3012,0,0.0415364,"axΩ j=1 tΩ(j),j , where Ω(j) is the index of the estimated cluster associated with the j-th class. Given the result of one-to-one mapping, we can define the evaluation measure as follows: P tΩ(j),j ˆ j Accuracy(P ) = P t . Intuitively, it reflects i,j i,j the accuracy of the clustering result. 3.3 Evaluation method for relation labelling For evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names. To do this, we use one information-content based measure (Lin, 1997), which is provided in WordnetSimilarity package (Pedersen et al., 2004) to evaluate the similarity between two concepts in Wordnet. Intuitively, the relatedness between two concepts in Wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: 1 ,c2 )) Relatednesslin (c1 , c2 ) = 2×IC(lcs(c IC(c1 )+IC(c2 ) . This measure depends upon the corpus to estimate information content. We carried out the experiments using the British National Corpus (BNC) as the source of information content. 3.4 Experiments and Results For comparison of the effect of"
I05-2045,P04-1053,0,\N,Missing
I08-1008,J96-1002,0,0.14677,"Missing"
I08-1008,P04-1024,0,0.691486,"and name entity recognition. Unfortunately, little attention has been given to name origin recognition (NOR) so far in the literature. In this paper, we are interested in two kinds of name origin recognition: the origin of names written in English (ENOR) and the origin of names written in Chinese (CNOR). For ENOR, the origins include English (Eng), Japanese (Jap), Chinese Mandarin Pinyin (Man) and Chinese Cantonese Jyutping (Can). For CNOR, they include three origins: Chinese (Chi, for both Mandarin and Cantonese), Japanese and English (refer to Latinscripted language). Unlike previous work (Qu and Grefenstette, 2004; Li et al., 2006; Li et al., 2007) where NOR was formulated with a generative model, we regard the NOR task as a classification problem. We further propose using a discriminative learning algorithm (Maximum Entropy model: MaxEnt) to solve the problem. To draw direct comparison, we conduct experiments on the same personal name corpora as that in the previous work by Li et al. (2006). We show that the MaxEnt method effectively incorporates diverse features and outperforms previous methods consistently across all test cases. The rest of the paper is organized as follows: in section 2, we review"
I08-1008,P07-1016,1,\N,Missing
I08-1008,Y04-1029,0,\N,Missing
I08-1008,J98-4003,0,\N,Missing
I08-2109,P06-2010,1,0.900752,"hods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significant"
I08-2109,P02-1031,0,0.0363188,"verbs or nouns and some constituents of the sentence. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure"
I08-2109,P04-1043,0,0.078076,"ce. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate"
I08-2109,N06-2025,0,0.0686209,"= opt([d]) = 1, p = 3. Then according to Eq (14), ∆p (cn1 , cn2 ) can be calculated recursively as Eq. (15) (Please refer to the next page). Finally, we have ∆p (cn1 , cn2 ) = λ1 × ∆0 (a, a) × 0 ∆ (b, b) × ∆0 (c, c) By means of the above algorithm, we can compute the ∆0 (n1 , n2 ) in O(p|cn1 |· |cn2 |2 ) (Lodhi et al., 2002). This means that the worst case complexity of the FGTK-I is O(pρ3 |N1 |· |N2 |2 ), where ρ is the maximum branching factor of the two trees. 3.2 Fast Grammar-driven Convolution Tree Kernel II (FGTK-II) Our FGTK-II algorithm is motivated by the partial trees (PTs) kernel (Moschitti, 2006). The PT kernel algorithm uses the following recursive formulas to evaluate ∆p (cn1 , cn2 ): |cn1 ||cn2 | ∆p (cn1 , cn2 ) = X X ∆0p (cn1 [1 : i], cn2 [1 : j]) (16) i=1 j=1 where cn1 [1 : i] and cn2 [1 : j] are the child subsequences of cn1 and cn2 from 1 to i and from 1 to j, respectively. Given two child node sequences s1 a = cn1 [1 : i] and s2 b = cn2 [1 : j] (a and b are the last children), the PT kernel computes ∆0p (·, ·) as follows:  0 ∆p (s1 a, s2 b) = µ2 ∆0 (a, b)Dp (|s1 |, |s2 |) 0 if a = b else (17) where ∆0 (a, b) is defined in Eq. (7) and Dp is recursively defined as follows: Dp ("
I08-2109,W05-0639,0,0.0418752,"Missing"
I08-2109,P07-1026,1,0.892395,"in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significantly outperforms the TK (Zhang et al., 2007). Theoretically, the GTK method is applicable to any problem that uses syntax structure features and can be solved by the TK methods, such as parsing, relation extraction, and so on. In this paper, we use SRL as an application to test our proposed algorithms. Although the GTK shows promising results"
I08-2109,W05-0620,0,\N,Missing
I11-1012,P02-1011,0,0.0205233,"results with discussions. Last section will wrap up with a conclusion and future research directions. 2 Previous Work Although event coreference resolution is an important task, it has not attracted much attention. There is only a limited number of previous works related to this task. In (Asher, 1993) chapter 6, a method to resolve references to abstract entities using discourse representation theory is discussed. However, no computational system was proposed. Besides linguistic studies, there are only a few previous works attempting to tackle subproblems of the event coreference resolution. (Byron, 2002; Müller, 2007; Chen et al, 2010a) attempted event pronoun resolution. (Chen et al, 2010b) attempted resolving noun phrases to verb mentions. All these works only focused on identifying pairs of coreferent event mentions in their targeted phenomena. The ultimate goal, which is extracting event chain, is lack of attention. (Pradhan, et al, 2007) applied a conventional co-reference resolution system to OntoNotes1.0 corpus using the same set of features for object coreference resolution. However, there is no specific performance reported on event coreference. As (Chen et al, 2010b) pointed out, t"
I11-1012,C10-1017,0,0.0557971,"coreference, we would like to revisit the two-step resolution framework to understand some of its weaknesses. Most of previous coreference resolution system employs a two-steps approach as in (Soon et al, 2001; Nicolae & Nicolae, 2006) and many others. The first step identifies all the pairs of coreferent mentions. The second step forms coreference chains using the coreferent pairs identified from the first step. 1 Event roles refer to the arguments of the event such as actuator, patient, time, location and etc. 103 Although a handful of single-step frameworks were proposed recently such as (Cai & Strube, 2010), two-step framework is still widely in use because it has been well-studied. Conceptually, the two-step framework adopts a divide-andconquer strategy which in turn, allows us to focus on different sub-problems at different stages. The mention-pair detection step allows us to employ many features associated with strong linguistic intuitions which have been proven useful in the previous linguistic study. The chain formation step allows us to leverage on efficient and robust graph partitioning algorithms such spectral partitioning used in this paper. Practically, the two-step framework is also m"
I11-1012,N06-2015,0,0.014611,"Missing"
I11-1012,W02-1008,0,0.0782086,"Missing"
I11-1012,W06-1633,0,0.0301428,"solution. However, there is no specific performance reported on event coreference. As (Chen et al, 2010b) pointed out, the conventional features do not function properly on event coreference problem. Thus, a thorough investigation on event coreference phenomena is required for a better understanding of the problem. 3 Resolution Framework Before we introduce our proposed system to event coreference, we would like to revisit the two-step resolution framework to understand some of its weaknesses. Most of previous coreference resolution system employs a two-steps approach as in (Soon et al, 2001; Nicolae & Nicolae, 2006) and many others. The first step identifies all the pairs of coreferent mentions. The second step forms coreference chains using the coreferent pairs identified from the first step. 1 Event roles refer to the arguments of the event such as actuator, patient, time, location and etc. 103 Although a handful of single-step frameworks were proposed recently such as (Cai & Strube, 2010), two-step framework is still widely in use because it has been well-studied. Conceptually, the two-step framework adopts a divide-andconquer strategy which in turn, allows us to focus on different sub-problems at dif"
I11-1012,D08-1068,0,0.0762522,"Missing"
I11-1012,J01-4004,0,0.932843,"ntroduction Coreference resolution, the task of resolving and linking different mentions of the same object/event in a text, is important for an intelligent text processing system. The resolved coreferent mentions form a coreference chain representing a particular object/event. Following the natural order in the texts, any two consecutive mentions in a coreference chain form an anaphoric pair with the latter mention referring back to the prior one. The latter mention is called the anaphor while the prior one is named as the antecedent. Most of previous works on coreference resolution such as (Soon et al, 2001; Yang et al, 2006), aimed at object coreference which both the anaphor and its antecedent are mentions of the same 4 National University of Singapore 4 tancl@comp.nus.edu.sg real world object such as person, location and organization. In contrast, an event coreference as defined in (Asher, 1993) is an anaphoric reference to an event, fact, and proposition which is representative of eventuality and abstract entities. In the following example: “Israel has [fired] missiles on the offices of the Palestinian Authority. [It] has caused 7 deaths with many injuries… Israel helicopter gunships [fired]"
I11-1012,P10-2029,0,0.00998812,"which in turn, allows us to focus on different sub-problems at different stages. The mention-pair detection step allows us to employ many features associated with strong linguistic intuitions which have been proven useful in the previous linguistic study. The chain formation step allows us to leverage on efficient and robust graph partitioning algorithms such spectral partitioning used in this paper. Practically, the two-step framework is also more mature for practical uses and has been implemented as a number of standard coreference resolution toolkits widely available such as RECONCILE in (Stoyanov et al, 2010) and BART in (Versley et al, 2008). Performance-wise, two-step approaches also show comparable performance to single step approaches on some benchmark datasets2. In this paper, we are exploiting a brand new type of coreference phenomenon with merely previous attempts. Therefore, we employed the much matured two-step framework with innovative extensions to accommodate complicated event coreference phenomena. Such a divideand-conquer strategy will provide us more insight for further advancements as well. 3.1 Mention-Pair Resolution Models Most of mention-pair models adopt the wellknown machine l"
I11-1012,P08-4003,0,0.064892,"Missing"
I11-1012,P06-1006,1,0.645464,"rence resolution, the task of resolving and linking different mentions of the same object/event in a text, is important for an intelligent text processing system. The resolved coreferent mentions form a coreference chain representing a particular object/event. Following the natural order in the texts, any two consecutive mentions in a coreference chain form an anaphoric pair with the latter mention referring back to the prior one. The latter mention is called the anaphor while the prior one is named as the antecedent. Most of previous works on coreference resolution such as (Soon et al, 2001; Yang et al, 2006), aimed at object coreference which both the anaphor and its antecedent are mentions of the same 4 National University of Singapore 4 tancl@comp.nus.edu.sg real world object such as person, location and organization. In contrast, an event coreference as defined in (Asher, 1993) is an anaphoric reference to an event, fact, and proposition which is representative of eventuality and abstract entities. In the following example: “Israel has [fired] missiles on the offices of the Palestinian Authority. [It] has caused 7 deaths with many injuries… Israel helicopter gunships [fired] across the Gaza St"
I11-1012,C10-1022,1,\N,Missing
I11-1012,P07-1103,0,\N,Missing
I11-1063,P98-1012,0,0.0820438,"large collection to populate a knowledge base (KB) (e.g. Wikipedia). This requires either linking entity mentions in the documents with entries in the KB or highlighting these mentions as new entries to current KB. Entity linking (McNamee and Dang, 2009) involves both finding name variants (e.g. both “George H. W. Bush” and “George Bush Senior” refer to the 41st U.S. president) and name disambiguation (e.g. given “George Bush” and 1 http://nlp.cs.qc.cuny.edu/kbp/2010/ its context, we should be able to disambiguate which president it is referring to). Compared with Cross-Document Coreference (Bagga and Baldwin, 1998) which clusters the articles according to the entity mentioned, entity linking has a given entity list (i.e. the reference KB) to which we disambiguate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the co"
I11-1063,P99-1016,0,0.0717705,"upercategories of c, SC={sc1, sc2, …,sck }. If the head lemma of one of cpi matches the head lemma of scj, label the relation between c and scj as is-a. (2) assign is-a label to the link between two categories if a Wikipedia article is redundantly categorized under both of them. For example, “Internet” is categorized under both “Computer networks” and “Computing” and there is a link between “Computer networks” and “Computing”. Then this link is assigned is-a. Next, we use lexical-syntactic patterns in a corpus. This method uses two sets of patterns. One set is used to identify is-a relations (Caraballo, 1999; Hearst, 1992), for example “such NP1 as NP2”, NP1 and NP2 are the values of categories and their subcategories respectively. The second set is used to identify not-is-a relations. For example “NP1 has NP2”, where the link between NP1 and NP2 will be assigned not-is-a. These patterns are used with a corpus built from Wikipedia articles, and separately with the Tipster corpus (Harman and Liberman, 1993). The label is assigned by majority voting between the frequency counts for the two types of patterns. Finally, we assign is-a labels to links based on transitive closures - all categories along"
I11-1063,C10-1032,0,0.0914314,"guate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the comparison of the weighted literal term vectors (Varma et al., 2009; Li et al., 2009; McNamee et al., 2009; Zhang et al., 2010; Zheng et al., 2010; Dredze et al., 2010). Such literal matching suffers from sparseness issue. For example, consider the following four observations of Michael Jordan without term match: 1) Michael Jordan is a leading researcher in machine learning and artificial intelligence. 2) Michael Jordan is currently a full professor at the University of California, Berkeley. 3) Michael Jordan (born February, 1963) is a former American professional basketball player. 4) Michael Jordan wins NBA MVP of 91-92 season. To measure the similarity of these contexts, the semantic knowledge underlying the words is needed. Furthermore, current state-of-"
I11-1063,C92-2082,0,0.0817485,"c, SC={sc1, sc2, …,sck }. If the head lemma of one of cpi matches the head lemma of scj, label the relation between c and scj as is-a. (2) assign is-a label to the link between two categories if a Wikipedia article is redundantly categorized under both of them. For example, “Internet” is categorized under both “Computer networks” and “Computing” and there is a link between “Computer networks” and “Computing”. Then this link is assigned is-a. Next, we use lexical-syntactic patterns in a corpus. This method uses two sets of patterns. One set is used to identify is-a relations (Caraballo, 1999; Hearst, 1992), for example “such NP1 as NP2”, NP1 and NP2 are the values of categories and their subcategories respectively. The second set is used to identify not-is-a relations. For example “NP1 has NP2”, where the link between NP1 and NP2 will be assigned not-is-a. These patterns are used with a corpus built from Wikipedia articles, and separately with the Tipster corpus (Harman and Liberman, 1993). The label is assigned by majority voting between the frequency counts for the two types of patterns. Finally, we assign is-a labels to links based on transitive closures - all categories along an is-a chain"
I11-1063,D09-1026,0,0.052334,"contributors to assign categories to each article, which are defined as “major topics that are likely to be useful to someone reading the article”. Thus, Wikipedia can serve as a document collection with multiple topical labels, where we can learn the posterior distribution over words for each topical label (i.e. Wikipedia category). Then, from the observed words in the mention’s context and KB entry, we can estimate the distribution of the contexts over the Wikipedia categories. To obtain this distribution, we use a supervised Latent Dirichlet Allocation (LDA) model – labeled LDA defined by Ramage et al. (2009), which represents state-of-the-art method for multi-labeled text classification. It performs better on collections with more semantically diverse labels, which we need in order to leverage on the large semantically diverse categories from Wikipedia as the topical labels. Figure 1 shows us a graphical representation of the labeled LDA for the multi-labeled document collection. Labeled LDA is a three level hierarchical Bayesian model. β is the multinomial distribution over words for a Wikipedia category, which has a Dirichlet prior with hyperparameter η. Both the category set Λ as well as the t"
I11-1063,P04-1075,1,0.746954,"o other entries like “Bud Abbott” “Abbott Texas”, etc. Thus, we need an instance selection approach to reduce the effect of this distribution problem. However, the traditional instance selection approaches (Brighton and Mellish, 2002; Liu and Motoda, 2002) only can solve two problems: 1) a large dataset causes response-time to become slow 2) the noisy instances affect accuracy, which are different from our needs here. We thus propose an instance selection approach to select a more balanced subset from the autoannotated instances. This instance selection strategy is similar to active learning (Shen et al., 2004; Brinker, 2003) for reducing the manual annotation effort on training instances through proposing only the useful candidates to annotators. As we already have a large set of autogenerated training instances, the selection here is a fully automatic process to get a useful and more balanced subset instead. We use the SVM classifier mentioned in Section 2.2 to select the instances from the large dataset. The initial classifier can be trained on a set of initial training instances, which can be a small part of the whole auto-generated data, or the limited manual annotated training instances avail"
I11-1063,C10-1145,1,0.862013,"Missing"
I11-1063,N10-1072,0,0.0765975,"to which we disambiguate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the comparison of the weighted literal term vectors (Varma et al., 2009; Li et al., 2009; McNamee et al., 2009; Zhang et al., 2010; Zheng et al., 2010; Dredze et al., 2010). Such literal matching suffers from sparseness issue. For example, consider the following four observations of Michael Jordan without term match: 1) Michael Jordan is a leading researcher in machine learning and artificial intelligence. 2) Michael Jordan is currently a full professor at the University of California, Berkeley. 3) Michael Jordan (born February, 1963) is a former American professional basketball player. 4) Michael Jordan wins NBA MVP of 91-92 season. To measure the similarity of these contexts, the semantic knowledge underlying the words is needed. Furtherm"
I11-1063,C98-1012,0,\N,Missing
J08-3002,P95-1017,0,0.208093,"Missing"
J08-3002,J96-1002,0,0.0574936,"Missing"
J08-3002,W98-1119,0,0.0464981,"Missing"
J08-3002,J95-2003,0,0.0598271,"Missing"
J08-3002,W03-2604,0,0.499617,"Missing"
J08-3002,J00-4006,0,0.109052,"f the twin-candidate model for anaphora resolution. Further, we explore how to deploy the model in the more complicated coreference resolution task. We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets. The experimental results indicate that our twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution. For the task of coreference resolution, it also performs equally well, or better. 1. Introduction Anaphora is reference to an entity that has been previously introduced into the discourse (Jurafsky and Martin 2000). The referring expression used is called the anaphor and the expression being referred to is its antecedent. The anaphor is usually used to refer to the same entity as the antecedent; hence, they are coreferential with each other. The process of determining the antecedent of an anaphor is called anaphora resolution. As a key problem in discourse and language understanding, anaphora resolution is crucial in many natural language applications, such as machine translation, text summarization, question answering, information extraction, and so on. In recent ∗ 21 Heng Mui Keng Terrace, Singapore,"
J08-3002,W97-0319,0,0.0860925,"r.a-star.edu.sg. ∗∗ 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given a"
J08-3002,N04-1037,0,0.0285086,". . . , Cn . The singlecandidate model assumes that the probability that Ck is the antecedent is only dependent on the anaphor ana and Ck , and independent of all the other candidates. That is: p (ante(Ck ) |ana, C1 , C2 , . . . , Cn ) = p (ante(Ck ) |ana, Ck ) (1) Thus, the probability of a candidate Ck being the antecedent can be approximated using the classiﬁcation result on the instance describing the anaphor and Ck alone. The single-candidate model is widely used in most anaphora resolution systems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and Mueller 2003; Kehler et al. 2004; Ng et al. 2005). In our study, we also build as the 329 Computational Linguistics Volume 34, Number 3 Table 1 A sample text for anaphora resolution. [1 Those ﬁgures] are almost exactly what [2 the government] proposed to [3 legislators] in [4 September]. If [5 the government] can stick with [6 them], [7 it] will be able to halve this year’s 120 billion ruble (US $193 billion) deﬁcit. Table 2 Training instances generated under the single-candidate model for anaphora resolution. Anaphor [6 them] [7 it] Training Instance i{[6 i{[6 i{[6 i{[6 i{[6 them] , [1 them] , [2 them] , [3 them] , [4 them]"
J08-3002,P04-1018,0,0.190663,"rive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with features describing"
J08-3002,W97-1310,0,0.0741564,"Missing"
J08-3002,P05-1020,0,0.0356148,"Missing"
J08-3002,W02-1008,0,0.590151,"-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each o"
J08-3002,P02-1014,0,0.910798,"-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each o"
J08-3002,J01-4004,0,0.959003,"Missing"
J08-3002,P03-1022,0,0.112416,"tar.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with fea"
J08-3002,M95-1005,0,0.32181,"Missing"
J08-3002,H89-1033,0,0.0396633,"gher preference to be referred to in later utterances. The forward-looking centers can be ranked based on grammatical roles or other factors. Distance-based factors: Pronouns prefer candidates in the previous sentence compared with those two or more sentences back (Clark and Sengul 1979). As a matter of fact, “eliminating” factors could also be considered “preferential” if we think of the act of eliminating candidates as giving them low preference. Preference-based strategies are also widely seen in earlier manual approaches to pronominal anaphora resolution. For example, the SHRDLU system by Winograd (1972) prefers antecedent candidates in the subject position over those in the object position. The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with the anaphor. Hobbs’s algorithm (Hobbs 1978) prefers candidates that are closer to the anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers candidates that have a high salience value computed by aggregating the weights of different factors. During resolution, the single-candidate model does select an antecedent based on preference by using classiﬁcation conﬁdence for candidates; that is, th"
J08-3002,P05-1021,1,0.894387,"Missing"
J08-3002,W00-1309,1,0.856944,"Missing"
J08-3002,P02-1060,1,0.413866,"Missing"
J08-3002,J94-4002,0,\N,Missing
N06-2007,P04-1054,0,0.0347615,"in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, which can minimize corpus ann"
N06-2007,P04-1053,0,0.0183029,"Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, which can minimize corpus annotation requirement. Current works on semi-supervised resolution for relation extraction task mostly use the bootstrapping algorithm, which is based on a local consi"
N06-2007,A00-2030,0,0.0345428,"for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has"
N06-2007,P95-1026,0,0.225738,"Missing"
N06-2007,W02-1010,0,0.0279954,"on extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, wh"
N06-2007,A00-2018,0,\N,Missing
P03-1023,P95-1017,0,0.890131,"candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, h"
P03-1023,P99-1048,0,0.113874,"Missing"
P03-1023,P87-1022,0,0.240326,"be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition"
P03-1023,P98-2143,0,0.285496,"ce resolution is the process of linking together multiple expressions of a given entity. The key to solve this problem is to determine the antecedent for each referring expression in a document. In coreference resolution, it is common that two or more candidates compete to be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 200"
P03-1023,P02-1014,0,0.894344,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,C02-1139,0,0.844771,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,P98-2204,0,0.0157449,"n anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for"
P03-1023,M95-1005,0,0.609474,"ables). Without candidate filtering, the recall may rise as the correct antecedents would not be eliminated wrongly. Nevertheless, the precision drops largely due to the numerous invalid NPs in the candidate set. As a result, a significantly low Fmeasure is obtained in their approach. Table 4 summarizes the overall performance of different approaches to coreference resolution. Different from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task. Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the whole resolved chains. In line with the previous experiments, we see reasonable improvement in the performance of the coreference resolution: compared with the baseline approach based on the single-candidate model, the F-measure of approach increases from 69.4 to 71.3 for MUC-6, and from 58.7 to 60.2 for MUC-7. 6 Related Work A similar twin-candidate model was adopted in the anaphoric resolution sys"
P03-1023,W00-1309,1,0.849053,"Missing"
P03-1023,P02-1060,1,0.566195,"Missing"
P03-1023,J01-4003,0,\N,Missing
P03-1023,J01-4004,0,\N,Missing
P03-1023,C98-2138,0,\N,Missing
P03-1023,C98-2199,0,\N,Missing
P05-1021,N04-1038,0,0.0950612,"sis that the government said it1 was going to collect it2 . For anaphor it1 , the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well us"
P05-1021,C90-3063,0,0.721766,"y incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns. 1 Introduction Semantic compatibility is an important factor for pronoun resolution. Since pronouns, especially neutral pronouns, carry little semantics of their own, the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphor’s context, based on the statistics that the corresponding predicate-argument tuples occur in a particular large corpus. Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2 . For anaphor it1 , the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It"
P05-1021,N04-1037,0,0.237545,"ed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic comp"
P05-1021,J03-3005,0,0.222477,"nowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics web. It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one can"
P05-1021,P98-2143,0,0.202456,"reference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, p"
P05-1021,W03-1023,0,0.0886949,"Missing"
P05-1021,P02-1014,0,0.225064,"bability (Eq. 2) metric. In such a situation, count(candi, ana) is the hit number of the inflected queries returned by the search engine, while count(candi) is the hit number of the query formed with only the head of the candidate (i.e.,“the + candi”). 3 Applying the Semantic Compatibility In this section, we discuss how to incorporate the statistics-based semantic compatibility for pronoun resolution, in a machine learning framework. 3.1 The Single-Candidate Model One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). In such a learning model, each training or testing instance takes the form of i{C, ana}, where ana is the possible anaphor and C is its antecedent candidate. An instance is associated with a feature vector to describe their relationships. During training, for each anaphor in a given text, a positive instance is created by pairing the anaphor and its closest antecedent. Also a set of negative instances is formed by pairing the anaphor and each of the intervening candidates. Based on the training instances, a binary classifier is generated using a certain learning algorithm, like C5 (Quinlan,"
P05-1021,P04-1019,0,0.0809532,"Missing"
P05-1021,J01-4004,0,0.279715,"ution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann"
P05-1021,P03-1022,0,0.0887334,"he co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann Arbor, June 2005. 2005 Ass"
P05-1021,P98-2204,0,0.0217787,"ility of the candidate the semantic compatibility difference between two competing candidates Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) computational cost but with high reliability. Table 1 summarizes the features with their respective possible values. The first three features represent the lexical properties of a candidate. The POS properties could indicate whether a candidate refers to a hearerold entity that would have a higher preference to be selected as the antecedent (Strube, 1998). SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978). FirstNP aims to capture the salience of the candidate in the local discourse segment. ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section."
P05-1021,P03-1023,1,0.770302,"way to tackle this problem is to normalize the feature by the frequencies of the anaphor’s context, e.g., “count(collected)” and “count(said)”. This, however, would require extra calculation. In fact, as candidates of a specific anaphor share the same anaphor context, we can just normalize the semantic feature of a candidate by that of its competitor: StatSemN (C, ana) = StatSem(C, ana) max StatSem(ci , ana) ci ∈candi set(ana) The value (0 ∼ 1) represents the rank of the semantic compatibility of the candidate C among candi set(ana), the current candidates of ana. 3.3 The Twin-Candidate Model Yang et al. (2003) proposed an alternative twincandidate model for anaphora resolution task. The strength of such a model is that unlike the singlecandidate model, it could capture the preference relationships between competing candidates. In the model, candidates for an anaphor are paired and features from two competing candidates are put together for consideration. This property could nicely deal with the above mentioned training problem of different anaphor contexts, because the semantic feature would be considered under the current candidate set only. In fact, as semantic compatibility is a preference-based"
P05-1021,P02-1060,1,0.513413,"Missing"
P05-1021,W00-0737,1,0.882222,"Missing"
P05-1021,C98-2138,0,\N,Missing
P05-1021,C98-2199,0,\N,Missing
P05-1049,P02-1033,0,0.049407,"Missing"
P05-1049,J98-1002,0,0.0296964,"saurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is als"
P05-1049,J98-1006,0,0.145626,"consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Sch¨ utze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and"
P05-1049,W02-1006,0,0.267713,"Missing"
P05-1049,J04-1001,0,0.163025,"ry for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in thi"
P05-1049,P97-1009,0,0.039132,"National University of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learn"
P05-1049,P04-1036,0,0.0479931,"iversity of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, boo"
P05-1049,W04-2405,0,0.0556268,"sambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption"
P05-1049,P03-1058,0,0.0514187,"ds. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping pr"
P05-1049,J98-1004,0,0.324855,"Missing"
P05-1049,P95-1026,0,0.916316,"r labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Sch¨ utze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and direct use of word Chew Lim Tan Department of Computer"
P05-1049,P91-1034,0,0.242418,"arning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity inf"
P05-1049,J98-1005,0,\N,Missing
P05-1049,W04-0807,0,\N,Missing
P05-1049,W97-0322,0,\N,Missing
P05-1049,P00-1069,0,\N,Missing
P05-1049,C92-2070,0,\N,Missing
P05-1049,J94-4003,0,\N,Missing
P05-1049,P00-1000,0,\N,Missing
P05-1049,A00-2000,0,\N,Missing
P06-1006,J03-3005,0,0.0214287,"Missing"
P06-1006,C96-1021,0,0.41745,"Missing"
P06-1006,J94-4002,0,0.917584,"the sentence distance between the candidate and the pronominal anaphor. Closeness: whether the candidate is the candidate closest to the pronominal anaphor. FirstNP: whether the candidate is the first noun phrase in the current sentence. Parallelism: whether the candidate has an identical collocation pattern with the pronominal anaphor. Table 1: Feature set for the baseline pronoun resolution system salience measures have to be assigned manually. Luo and Zitouni (2005) proposed a coreference resolution approach which also explores the information from the syntactic parse trees. Different from Lappin and Leass (1994)’s algorithm, they employed a maximum entropy based model to automatically compute the importance (in terms of weights) of the features extracted from the trees. In their work, the selection of their features is mainly inspired by the government and binding theory, aiming to capture the c-command relationships between the pronoun and its antecedent candidate. By contrast, our approach simply utilizes the parse trees as a structured feature, and lets the learning algorithm discover all possible embedded information that is necessary for pronoun resolution. Related Work One of the early work on"
P06-1006,H05-1083,0,0.0665107,"ct of a sentence, a subject of a clause, or not. Object: whether the candidate is an object of a verb, an object of a preposition, or not. Distance: the sentence distance between the candidate and the pronominal anaphor. Closeness: whether the candidate is the candidate closest to the pronominal anaphor. FirstNP: whether the candidate is the first noun phrase in the current sentence. Parallelism: whether the candidate has an identical collocation pattern with the pronominal anaphor. Table 1: Feature set for the baseline pronoun resolution system salience measures have to be assigned manually. Luo and Zitouni (2005) proposed a coreference resolution approach which also explores the information from the syntactic parse trees. Different from Lappin and Leass (1994)’s algorithm, they employed a maximum entropy based model to automatically compute the importance (in terms of weights) of the features extracted from the trees. In their work, the selection of their features is mainly inspired by the government and binding theory, aiming to capture the c-command relationships between the pronoun and its antecedent candidate. By contrast, our approach simply utilizes the parse trees as a structured feature, and l"
P06-1006,P98-2143,0,0.186048,"Missing"
P06-1006,P04-1043,0,0.366702,"ilize the syntactic parse trees to help learning-based pronoun resolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of Syntactic knowled"
P06-1006,P02-1014,0,0.144265,"ance of the algorithm would rely heavily on the accuracy of the parsing results. Lappin and Leass (1994) reported a pronoun resolution algorithm which uses the syntactic representation output by McCord’s Slot Grammar parser. A set of salience measures (e.g. Subject, Object or Accusative emphasis) is derived from the syntactic structure. The candidate with the highest salience score would be selected as the antecedent. In their algorithm, the weights of 3 The Resolution Framework Our pronoun resolution system adopts the common learning-based framework similar to those by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance is formed by a pronoun and one of its antecedent candidate. During training, for each pronominal anaphor encountered, a positive instance is created by paring the anaphor and its closest antecedent. Also a set of negative instances is formed by paring the anaphor with each of the 42 will discuss how to use kernels to incorporate the more complex structured feature. non-coreferential candidates. Based on the training instances, a binary classifier is generated using a particular learning algorithm. During resolution, a pronominal anaph"
P06-1006,J01-4004,0,0.75231,"arse trees, the performance of the algorithm would rely heavily on the accuracy of the parsing results. Lappin and Leass (1994) reported a pronoun resolution algorithm which uses the syntactic representation output by McCord’s Slot Grammar parser. A set of salience measures (e.g. Subject, Object or Accusative emphasis) is derived from the syntactic structure. The candidate with the highest salience score would be selected as the antecedent. In their algorithm, the weights of 3 The Resolution Framework Our pronoun resolution system adopts the common learning-based framework similar to those by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance is formed by a pronoun and one of its antecedent candidate. During training, for each pronominal anaphor encountered, a positive instance is created by paring the anaphor and its closest antecedent. Also a set of negative instances is formed by paring the anaphor with each of the 42 will discuss how to use kernels to incorporate the more complex structured feature. non-coreferential candidates. Based on the training instances, a binary classifier is generated using a particular learning algorithm. During resol"
P06-1006,P95-1017,0,0.514578,"Missing"
P06-1006,A00-2018,0,0.00378114,"we focussed on the third-person pronominal anaphora resolution. All the experiments were done on the ACE-2 V1.0 corpus (NIST, 2003), which contain two data sets, training and devtest, used for training and testing respectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). An input raw text was preprocessed automatically by a pipeline of NLP components, including sentence boundary detection, POS-tagging, Text Chunking and Named-Entity Recognition. The texts were parsed using the maximum-entropybased Charniak parser (Charniak, 2000), based on which the structured features were computed automatically. For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). All classifiers were trained with default learning parameters. The performance was evaluated based on the metric success, the ratio of the number of correctly resolved5 anaphor over the number of all anaphors. For each anaphor, the NPs occurring within the current and previous two sentences were taken as the initial antecedent candidates. Those with mismatched number and gender agreements were"
P06-1006,P04-1017,1,0.914439,"Missing"
P06-1006,P02-1034,0,0.575113,"we will explore how to utilize the syntactic parse trees to help learning-based pronoun resolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of"
P06-1006,P05-1052,0,0.021097,"esolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic inf"
P06-1006,J03-4003,0,\N,Missing
P06-1006,C98-2138,0,\N,Missing
P06-1017,P95-1026,0,0.365851,"Missing"
P06-1017,W02-1010,0,0.0138167,"isfy two constraints: 1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstr"
P06-1017,P05-1053,0,0.0899144,"Missing"
P06-1017,P04-1054,0,0.0747713,"1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstrapping techniques for extra"
P06-1017,P04-1053,0,0.0703311,"n only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstrapping techniques for extracting relations from unstructured text. Snowball shares much in common with DIPRE, including the employment of the bootstrapping framework as well as the use of pattern matching to extract ne"
P06-1017,A00-2030,0,0.28514,"Missing"
P06-1017,A00-2018,0,\N,Missing
P06-2012,P04-1054,0,0.12114,"ating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods. 1 Introduction In this paper, we address the task of relation extraction, which is to find relationships between name entities in a given context. Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of labeled training data is available. However, it is time-consuming and labor-intensive to manually tag a large amount of training data. Semi-supervised learning methods have been put forward to minimize the corpus annotation requirement. Most of semi-supervised methods employ the bootstrapping framework, which only ne"
P06-2012,A00-2030,0,0.0643241,"Missing"
P06-2012,W02-1010,0,0.0297307,"ts. It works by calculating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods. 1 Introduction In this paper, we address the task of relation extraction, which is to find relationships between name entities in a given context. Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of labeled training data is available. However, it is time-consuming and labor-intensive to manually tag a large amount of training data. Semi-supervised learning methods have been put forward to minimize the corpus annotation requirement. Most of semi-supervised methods employ the bootstrappi"
P06-2012,P05-1053,0,0.0468321,"Missing"
P06-2012,A00-2018,0,\N,Missing
P06-2012,P04-1053,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P08-1064,2007.mtsummit-papers.8,0,0.196123,"rase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed mod"
P08-1064,P05-1067,0,0.429194,"od statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine th"
P08-1064,P03-2041,0,0.821253,"that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is"
P08-1064,N04-1035,0,0.617925,"Missing"
P08-1064,P06-1121,0,0.542205,"ive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be comb"
P08-1064,N04-1014,0,0.102142,"Missing"
P08-1064,2003.mtsummit-papers.22,0,0.0978139,"Missing"
P08-1064,N03-1017,0,0.108044,"igned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al."
P08-1064,koen-2004-pharaoh,0,0.260163,"he proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized 562 rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly"
P08-1064,P06-1077,0,0.798378,"al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic tran"
P08-1064,P07-1089,0,0.865288,"d Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic translation unit and u"
P08-1064,W06-1606,0,0.778456,"hods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem i"
P08-1064,P04-1083,0,0.0266683,"Missing"
P08-1064,P02-1038,0,0.183089,"Missing"
P08-1064,P03-1021,0,0.0106146,"s (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1 and h = 2 for the SCFG; d = 1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and develo"
P08-1064,J04-4002,0,0.538035,"pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al.,"
P08-1064,P02-1040,0,0.104502,"trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1"
P08-1064,P05-1034,0,0.739531,"icantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phra"
P08-1064,P07-1090,1,0.843636,"07b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the targe"
P08-1064,J97-3002,0,0.763933,"sh translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tr"
P08-1064,P01-1067,0,0.813115,"modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1"
P08-1064,zhang-etal-2004-interpreting,0,0.20435,"Missing"
P08-1064,2006.amta-papers.8,0,\N,Missing
P08-1064,C00-2092,0,\N,Missing
P08-1064,P03-1054,0,\N,Missing
P08-1064,P06-1123,0,\N,Missing
P08-1064,P06-1066,0,\N,Missing
P08-1064,P03-1011,0,\N,Missing
P08-1064,P07-2045,0,\N,Missing
P08-1064,N06-1032,0,\N,Missing
P08-1064,W06-1628,0,\N,Missing
P08-1064,J08-3004,0,\N,Missing
P08-1096,P95-1017,0,0.282338,"plicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et"
P08-1096,N07-1011,0,0.682128,"n (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large. Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms. A solution seen in previous work (Luo et al., 2004; Culotta et al., 2007) is to design a set of first-order features summarizing the information of the mentions in an entity, for example, “whether the entity has any mention that is a name alias of the active mention?” or “whether most of the mentions in the entity have the same head word as the active mention?” These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity. In this paper, we present a more expressive entity843 Proceedings of ACL-08: HLT, pages 843–851, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Ling"
P08-1096,N07-1030,0,0.213821,"Missing"
P08-1096,P04-1018,0,0.879059,". Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all"
P08-1096,P02-1014,0,0.959589,"nd automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, th"
P08-1096,P05-1020,0,0.108939,"Missing"
P08-1096,P07-1068,0,0.0587945,"Missing"
P08-1096,J01-4004,0,0.901524,"ontained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional menti"
P08-1096,P07-1006,0,0.0425872,"ch other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n}, in which mj is the jth mention occurring in the document. Let ei be the ith entity in the document. We define P (L|ei , mj ), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of ei . By assuming that mentions occurring after mj have no influence on the decision of linking mj to an en"
P08-1096,M95-1005,0,0.818946,"Missing"
P08-1096,P07-1067,1,0.315086,"Missing"
P08-1096,P04-1017,1,0.929248,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,C04-1033,1,0.933005,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,W00-1309,1,0.821131,"Missing"
P08-1096,P02-1060,1,0.450274,"Missing"
P09-1020,P03-2041,0,0.260919,"work while section 3 defines our translation model. In section 4 and section 5, the key rule extraction and decoding algorithms are elaborated. Experimental results are reported in section 6 and the paper is concluded in section 7. 2 Related work As discussed in section 1, two of the major challenges to syntax-based SMT are structure divergence and parse errors. Many techniques have been proposed to address the structure divergence issue while only fewer studies are reported in addressing the parse errors in the SMT research community. To address structure divergence issue, many researchers (Eisner, 2003; Zhang et al., 2007) propose using the Synchronous Tree Substitution Grammar (STSG) grammar in syntax-based SMT since the STSG uses larger tree fragment as translation unit. Although promising results have been reported, STSG only uses one single subtree as translation unit which is still committed to the syntax strictly. Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT, the tree sequencebased translation model is proposed (Liu et al., 2007; Zhang et al., 2008a) that uses tree sequence as the basic translation unit, rather than using single su"
P09-1020,N04-1035,0,0.445227,"as translation input, where a forest is a compact representation of exponentially number of n-best parse trees. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. By using forest in rule extraction and decoding, their methods are able to well address the parse error issue. From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang,"
P09-1020,P08-1067,0,0.479802,"ms. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from differ"
P09-1020,P07-1019,0,0.172606,"vel process in a small span. Finally, we re-build the NSS of current span for upper level NSS combination use (line 20-22). In Fig. 8, the hyper-edge “IP=&gt;NP VV+VV NP” is an auxiliary hyper-edge introduced by Algorithm 2. By Algorithm 2, we convert the translation forest into a complete translation forest. We then use a bottom-up node-based search 4 The concept of translation forest is proposed in Mi et al. (2008). It is a forest that consists of only the hyperedges induced from translation rules. algorithm to do decoding on the complete translation forest. We also use Cube Pruning algorithm (Huang and Chiang 2007) to speed up the translation process. Figure 8. Auxiliary hyper-edge in a translation forest Algorithm 2. add auxiliary hyper-edges into mt forest F Input: mt forest F Output: complete forest F with auxiliary hyper-edges 1. for i := 1 to L do 2. for each node n of span [i, i] do 3. add n into NSS(i, i) 4. for length := 1 to L - 1 do 5. for start := 1 to L - length do 6. stop := start + length 7. for pivot := start to stop-1 do 8. for each ns1 in NSS (start, pivot) do for each ns2 in NSS (pivot+1,stop) do 9. 10. create 1 2 11. if ns is not in NSS(start, stop) then 12. add ns into NSS (start, st"
P09-1020,W01-1812,0,0.155074,"y learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given sentence under a context free grammar (CFG). A forest F is defined as a triple , , , where is non-terminal node set, is hyper-edge set and is leaf node set (i.e. all sentence words). A forest F satisfies the following two conditions: 1) Each node in should cover a phrase, which is a continuous word sub-sequence in . 2) Each hyper-edge in is defined as … … , , , where … … covers a sequence of contiis the father nuous and non-overlap phrases, node of the children sequence … … . The is just the su"
P09-1020,N03-1017,0,0.0217877,"Missing"
P09-1020,P07-2045,0,0.0395597,"Missing"
P09-1020,P06-1077,0,0.317991,"t, where a forest is a compact representation of exponentially number of n-best parse trees. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. By using forest in rule extraction and decoding, their methods are able to well address the parse error issue. From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang, 2005), which is us"
P09-1020,P07-1089,0,0.738499,"tical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from different viewpoints. Therefore, one natural way is to co"
P09-1020,P08-1023,0,0.650639,"rimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other"
P09-1020,D08-1022,0,0.756548,"e systems. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from differ"
P09-1020,P02-1038,0,0.0183765,"n model is formulated as: Pr , , ∑ , , , ∏ By the above Eq., translation becomes a tree sequence structure to string mapping issue. Given the F, TS and A, there are multiple derivations that could map F to TS under the constraint A. in our The mapping probability Pr , , study is obtained by summing over the probabilities of all derivations Θ. The probability of each derivation is given as the product of the probabilities of all the rules p (ri ) used in the derivation (here we assume that each rule is applied independently in a derivation). Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2003): 1) bidirectional rule mapping probabilities, 2) bidirectional lexical rule translation probabilities, 3) target language model, 4) number of rules used and 5) number of target words. In addition, we define two new features: 1) number of leaf nodes in auxiliary rules (the auxiliary rule will be explained later in this paper) and 2) product of the probabilities of all hyper-edges of the tree sequences in forest. 4 Training This section discusses how to extract our transla. As we"
P09-1020,P03-1021,0,0.400511,"Missing"
P09-1020,J03-1002,0,0.00270206,"Missing"
P09-1020,P02-1040,0,0.105492,"Missing"
P09-1020,zhang-etal-2004-interpreting,0,0.0526669,"Missing"
P09-1020,2006.amta-papers.8,0,\N,Missing
P09-1020,A00-2018,0,\N,Missing
P09-1020,C08-1138,1,\N,Missing
P09-1020,W05-1506,0,\N,Missing
P09-1020,P08-1064,1,\N,Missing
P09-1103,W04-3312,0,0.181174,"Missing"
P09-1103,P01-1067,0,0.0458264,"5 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et a"
P09-1103,P03-1011,0,0.0228204,"that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them,"
P09-1103,P08-1064,1,0.555183,"; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The above observations are conflicting to each other. In our opinion, the non-contiguous phras"
P09-1103,2003.mtsummit-papers.22,0,0.0301668,"ch in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless"
P09-1103,P07-2045,0,0.00914277,"Missing"
P09-1103,P06-1077,0,0.044472,"oduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence"
P09-1103,W02-1018,0,0.0451173,"contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic"
P09-1103,J04-4002,0,0.0988341,"proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equival"
P09-1103,P05-1034,0,0.0295013,"y significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-conti"
P09-1103,2007.mtsummit-papers.8,0,\N,Missing
P09-1103,C08-1138,1,\N,Missing
P09-1103,P03-1054,0,\N,Missing
P09-1103,P02-1040,0,\N,Missing
P09-1103,P06-1123,0,\N,Missing
P09-1103,P07-1089,0,\N,Missing
P09-1103,P05-1067,0,\N,Missing
P09-1103,P03-2041,0,\N,Missing
P09-1103,W06-1606,0,\N,Missing
P09-1103,N03-1017,0,\N,Missing
P09-1103,P06-1121,0,\N,Missing
P09-1103,zhang-etal-2004-interpreting,0,\N,Missing
P10-1032,D08-1092,0,0.0612318,"Missing"
P10-1032,P06-1104,1,0.833494,"ndering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Bilingual Tree Kernel (iBTK), which individually m"
P10-1032,2007.mtsummit-papers.71,1,0.677302,"the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* DirC EWoS STSG DirC EWoS STSSG DirC EWoS 23.86 23.98 24.48 24.71 25.16 25.38 25.92 25.95 26.45 Syntax STSG Syntax STSSG Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translatin"
P10-1032,C04-1154,0,0.297672,"sub-tree alignment, translational equivalent sub-tree pairs are coupled as aligned counterparts. Each pair consists of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees. Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. 1. Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features. Groves et al. (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. Tinsley et al. (2007) 306 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic str"
P10-1032,P08-1064,1,0.836887,"use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-di"
P10-1032,P03-1054,0,0.0682289,"corpus with parsing errors. In addition, HIT corpus is not applicable for MT experiment due to the problems of domain divergence, annotation discrepancy (Chinese parse tree employs a different grammar from Penn Treebank annotations) and degree of tolerance for parsing errors. Due to the above issues, we annotate a new data set to apply the sub-tree alignment in machine translation. We randomly select 300 bilingual sentence pairs from the Chinese-English FBIS corpus with the length 30 in both the source and target sides. The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. We manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 1. To be fully consistent with the definition, we strictly reserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing as shown in Table 2. # of Sentence pair Avg. Sentence Length Avg. # of sub-tree Avg. # of alignment Chinese English 300 16.94 20.81 28.97 34.39 17.0"
P10-1032,N03-1017,0,0.0113571,"stic grow-diag-final for both Moses and the syntax system. For sub-treealignment, we use the above word alignment to learn lexical/word alignment feature, and train with the FBIS training corpus (200) using the composite kernel of Plain+dBTK-Root+iBTKRdSTT. 7.2 Experimental results Compared with the adoption of word alignment, translational equivalences generated from structural alignment tend to be more grammatically aware and syntactically meaningful. However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (Koehn et al., 2003). Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods. • Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment (Tinsley et al., 2009). As shown in Table 5, we gain minor improvement in the Bleu score for all configurations. • Alternatively, we proposed a new approach to generate the rule set from the scratch. We constrain the bilingual phrases t"
P10-1032,P07-2045,0,0.00705698,"002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* DirC EWoS STSG DirC EWoS STSSG DirC EWoS 23.86 23.98 24.48 24.71 25.16 25.38 25.92 25.95 26.45 Syntax STSG Syntax STSSG Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implem"
P10-1032,J03-1002,0,0.00680383,"kipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. 2 s1 denotes score function 1 in Tinsley et al. (2007), skip2_s1_span1 denotes the utilization of heuristics skip2 and span1 while using score function 1 Feature Space Lex Lex +Online Str Plain +dBTK-STT Plain +dBTK-RdSTT Plain +dBTK-RgSTT Plain +dBTK-Root Plain +iBTK-STT Plain +iBTK-RdSTT Plain +iBTK-RgSTT Plain +iBTK-Root Plain +dB"
P10-1032,P04-1043,0,0.202504,"ail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Biling"
P10-1032,P09-1103,1,0.845996,"nefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-diag-final for both Moses and the syntax system. For sub-treealignment, we use the above word alignment to learn lexical/word a"
P10-1032,2007.mtsummit-papers.62,0,0.47504,"two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1 Introduction Syntax based Statistical Machine Translation (SMT) systems allow the translation process to be more grammatically performed, which provides decent reordering capability. However, most of the syntax based systems construct the syntactic translation rules based on word alignment, which not only suffers from the pipeline errors, but also fails to effectively utilize the syntactic structural features. To address those deficiencies, Tinsley et al. (2007) attempt to directly capture the syntactic translational equivalences by automatically conducting sub-tree alignment, which can be defined as follows: A sub-tree alignment process pairs up sub-tree pairs across bilingual parse trees whose contexts are semantically translational equivalent. According to Tinsley et al. (2007), a sub-tree aligned parse tree pair follows the following criteria: (i) a node can only be linked once; Figure 1: Sub-tree alignment as referred to Node alignment (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) a"
P10-1073,D07-1010,0,0.14342,"classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-leve"
P10-1073,N06-2034,0,0.147621,"ing identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et a"
P10-1073,N06-1037,1,0.528018,"o objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. ?? - ?? ) are covered in production rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especially, when there are enough training data, this is similar to the"
P10-1073,D09-1036,0,0.426661,"important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-level production rules"
P10-1073,P02-1047,0,0.105277,"of recognizing such relations between text units including identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for disc"
P10-1073,P04-1043,0,0.0228654,"ploying a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. ?? - ?? ) are covered in production rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Espe"
P10-1073,prasad-etal-2008-penn,0,0.334151,"from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly. These structural syntactic features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification. The experiment shows that tree kernel is able to effectively incorporate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Discourse Treebank (PDTB; Prasad et al., 2008). We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination. Besides, inspired by the linguistic study on tense and discourse anaphor (Webber, 1988), we further propose to incorporate temporal ordering information to constrain the interpretation of discourse relation, which also demonstrates statistical significant improvements for discourse relation recognition on PDTB v2.0 for both explicit and implicit relations. The organization of the re"
P10-1073,P09-1077,0,\N,Missing
P10-1073,P04-1054,0,\N,Missing
P10-1073,J88-2006,0,\N,Missing
P10-1073,P02-1034,0,\N,Missing
P11-2094,D08-1023,0,0.0252377,"s zero probability. For efficiency reason, we choose the probabilistic SCFG as the proposal distribution. We pre-parse the training instances3 before inference and save the structure of synchronous parsing forests. During the inference, we only change rule probabilities in parsing forests without changing the forest structures. The probability of rule r ∈ RA in Q is estimated by relative frequency θr = P ′ [fr ]−i [f ′ ]−i , where RA is the set of rules r r ∈RA rooted at A, and [fr ]−i is the number of times that rule r is used in the tree set T−i . We use the sampling algorithm described in (Blunsom and Osborne, 2008) to draw a synchronous tree from the parsing forest according to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4 : Name Syl Syl Syl NECs SECs TECs NEC SEC TEC → → → → → → → → → → hSyl / Syli+ hNECs / NECsi hNECs SECs / NECs SECsi hNECs TECs / NECs TECsi hNEC / NECi+ hSEC / SECi+ hTEC / TECi+ hsi / tj i hε / tj i hsi / εi 3.2 We imp"
P11-2094,P09-1088,0,0.030397,"f Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and G"
P11-2094,J07-2003,0,0.139092,"Missing"
P11-2094,W09-3510,0,0.0230283,"a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National Universi"
P11-2094,2010.iwslt-papers.7,0,0.0321043,"hout heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National University of Singapore 13 Comput"
P11-2094,D10-1028,0,0.0471331,"learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the"
P11-2094,N09-1036,0,0.152951,"et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, c"
P11-2094,P08-1046,0,0.356951,"dels, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, c Portland, Oregon"
P11-2094,P10-1117,0,0.012015,"n successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534"
P11-2094,P04-1021,1,0.799914,"ble equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one langu"
P11-2094,W09-3501,1,0.873539,"r of flatten rules. One may verify that the rule probabilities are well normalized. Based on this merged grammar G′ , we parse the training string pairs, then encode the parsed forest into the lattice. Figure 2 show a lattice example for the string pair ha a l t o / C[a] [er] ÷[tuo]i. The transition probabilities in the lattice are the “inside” 537 4.1 Data and Settings We conduct experiments on the English-Chinese data in the ACL Named Entities Workshop (NEWS 2009) 6 . Table 1 gives some statistics of the data. For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009). # Entry # En Char # Ch Char # Ch Type Train 31,961 218,073 101,205 370 Dev 2,896 19,755 9,160 275 Test 2,896 19,864 9,246 283 Table 1: Transliteration data statistics In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect adapted subtree statistics for every 10 iterations, and finally stop after 20 collections. After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003). 4.2 Results We implement the joint source-channel model (Li et al., 2004) as the base"
P11-2094,D09-1136,0,0.0225751,"me approaches restrict that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology an"
P11-2094,W09-3528,0,0.0296661,". This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer S"
P11-2094,W09-3520,0,0.0574031,"Missing"
P11-2094,J97-3002,0,0.10341,"to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4 : Name Syl Syl Syl NECs SECs TECs NEC SEC TEC → → → → → → → → → → hSyl / Syli+ hNECs / NECsi hNECs SECs / NECs SECsi hNECs TECs / NECs TECsi hNEC / NECi+ hSEC / SECi+ hTEC / TECi+ hsi / tj i hε / tj i hsi / εi 3.2 We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997). The complexity is O(|s|3 |t|3 ). 4 Similar to (Johnson, 2008), the adapted nonterminal are underlined. Similarly, we also use rules in the regular expression style X → hA / Ai+ to denote the following three rules: → → → Name → hWord / Wordi+ Word → hSyl / Syli+ We might further add a new adapted nonterminal Col to learn the word collocations. The following rules appear in the collocation grammar: Name → hCol / Coli+ Col → hWord / Wordi+ Word → hSyl / Syli+ Figure 1 gives one synchronous parsing trees under the collocation grammar of the example hm a x / ð[mai] [ke] d[si]i. 3 X As As where t"
P11-2094,W09-3515,0,0.0492781,"Missing"
P11-2094,P08-1012,0,0.0262763,"that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segm"
P11-2094,W10-2417,0,\N,Missing
P13-1097,P10-1041,0,0.0995501,"icitly contain the yes or no keywords, but rather provide context information to infer the yes or no answer (e.g. Q: Was she the best one on that old show? A: She was simply funny). Clearly, the sentiment words in IQAPs are the pivots to infer the yes or no answers. We show that sentiment similarity between such words (e.g., here the adjectives best and Funny) can be used effectively to infer the answers. The second application (SO prediction) aims to determine the sentiment orientation of individual words. Previous research utilized the semantic relations between words obtained from WordNet (Hassan and Radev, 2010) and semantic similarity measures (e.g. Turney and Littman, 2003) for this purpose. In this paper, we show that sentiment similarity between word pairs can be effectively utilized to compute SO of words. The contributions of this paper are follows: • We propose an effective approach to predict the sentiment similarity between word pairs through hidden emotions at the sense level, • We show the utility of sentiment similarity prediction in IQAP inference and SO prediction tasks, and • Our hidden emotional model can infer the type and number of hidden emotions in a corpus. 983 Proceedings of the"
P13-1097,C04-1200,0,0.208477,"Missing"
P13-1097,H05-1044,0,0.224123,"Missing"
P13-1097,P10-1018,0,0.408416,". SO based on the similarity function A(.,.) 6 6.1 Evaluation and Results Data and Settings We used the review dataset employed by Maas et al. (2011) as the development dataset that contains movie reviews with star rating from one star (most negative) to 10 stars (most positive). We exclude the ratings 5 and 6 that are more neutral. We used this dataset to compute all the input matrices in Table 2 as well as the enriched matrix. The development dataset contains 50k movie reviews and 90k vocabulary. We also used two datasets for the evaluation purpose: the MPQA (Wilson et al., 2005) and IQAPs (Marneffe et al., 2010) datasets. The MPQA dataset is used for SO prediction experiments, while the IQAP dataset is used for the IQAP experiments. We ignored the neutral words in MPQA dataset and used the remaining 4k opinion words. Also, the IQAPs dataset (Marneffe et al., 2010) contains 125 IQAPs and their corresponding yes or no labels as the ground truth. 6.2 Experimental Results To evaluate our PSSS model, we perform experiments on the SO prediction and IQAPs inference tasks. Here, we consider six emotions for both bridged and series models. We study the effect of emotion numbers in Section 7.1. Also, we set a"
P13-1097,P11-1015,0,\N,Missing
W06-1649,J98-1006,0,0.112705,"Missing"
W06-1649,W02-1006,0,0.0248906,"cted graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). We removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. If the estimated sense number is more than the sense number in the initial tagged corpus XL , then the results from order identification based methods will consist of the instances from clusters of unknown classes. When assessing the agreement between these classification results and the known results on official test set, we will encounter the problem that there is no sense tag for each instance in unknown classes. Slonim and Tishby"
W06-1649,P05-1049,1,0.741051,"ion problem. Therefore the labeled data of other classes cannot be used when determining the positive labeled data for current class. ELP can use the labeled data of all the known classes to determine the seeds of unknown classes. It may explain why LPU’s performance is worse than ELP based sense disambiguation although LPU can correctly estimate the sense number in XL+U We used Jensen-Shannon (JS) divergence (Lin, 1991) as distance measure for semi-supervised clustering and ELP, since plain LP with JS divergence achieves better performance than that with cosine similarity on SENSEVAL-3 data (Niu et al., 2005). For the LP process in ELP algorithm, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), a"
W06-1649,J98-1004,0,0.254649,"Missing"
W06-1649,P95-1026,0,0.146635,"se disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory. When disambiguating the word “boot” in the phrase “boot se"
W06-1649,P91-1034,0,0.304957,"vised sense disambiguation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sens"
W06-1649,J94-4003,0,0.0276465,"uation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory. When disam"
W06-1649,W04-0807,0,\N,Missing
W06-1667,A00-2030,0,0.046813,"Missing"
W06-1667,P04-1054,0,0.10059,"egawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method. 1 Introduction The task of relation extraction is to identify various semantic relations between name entities from text. Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of laCompared with supervised and semi-supervised methods, Hasegawa et al. (2004)’s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al. (2004)’s method is to use a hierarchical clustering method"
W06-1667,W02-1010,0,0.0158005,"ties encoutered in Hasegawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method. 1 Introduction The task of relation extraction is to identify various semantic relations between name entities from text. Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of laCompared with supervised and semi-supervised methods, Hasegawa et al. (2004)’s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al. (2004)’s method is to use a hier"
W06-1667,P05-1053,0,0.0362499,"Missing"
W06-1667,A00-2018,0,\N,Missing
W06-1667,P04-1053,0,\N,Missing
W09-1123,W05-0601,0,0.0512605,"Missing"
W09-1123,J06-1003,0,0.149504,"tedness between word pairs can be expressed in an ordered form while preserving lexical field structure, and if (3) the uniqueness of entries in such an order can be expressed by functions rather than scalars such as distance. As we will show, this line of thought leads to performance improvement in text classification by using kernel-based feature weighting. Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts (Raghavan and Wong, 1986), arguing if distributional similarity is an adequate proxy for lexical semantic relatedness (Budanitsky and Hirst, 2006). We argue for the need to enrich distributional semantics-based text representation by other components because with the statistical, i.e. devoid of word semantics approaches there is gen184 erally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical approaches have been proliferating, especially with developments in kernel methods in th"
W09-1123,O97-1002,0,0.0583535,"ure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also been developed (Resnik, 1995; Jiang and Conrath, 1997). This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. The Jiang-Conrath metric measures the distance between two senses by using the hierarchy of WordNet. By denoting the lowest superordinate of two senses s1 and s2 in the hierarchy with LSuper(s1 ,s2 ), the metric is calculated as follows: d(s1 , s2 ) = IC(s1 )+IC(s2 )−2IC(LSuper(s1 , s2 )), where IC(s) is the information content of a sense s based on a corpus."
W09-1123,P98-2127,0,0.0065235,") denote a weighted undirected graph, where the weights in the set E are defined by the distances between the terms. Various lexical resource-based (Budanitsky and Hirst, 2006) and distributional measures (Mohammad and Hirst, 2005) have been proposed to measure semantic relatedness and distance between terms. Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). This sense of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration. Composite measures that combine the advantages of both approaches have also been developed (Resnik, 1995; Jiang and Conrath, 1997). This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. The Jiang-Conrath metric measures the distance between two senses by using the hierarchy o"
W09-1123,W09-3721,1,0.505894,"Missing"
W09-1123,C98-2122,0,\N,Missing
W09-3721,J06-1003,0,0.352437,"e range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods. 1 Introduction Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts [23], arguing if distributional similarity is an adequate proxy for lexical semantic relatedness [3]. With the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of 235 Proceedings of the 8th International Conference on Computational Semantics, pages 235–247, c Tilburg, January 2009. 2009 International Conference on Computational Semantics search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical applications in informatio"
W09-3721,O97-1002,0,0.621857,"ly entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in l"
W09-3721,P98-2127,0,0.0546687,"fectiveness decreases after 4 expansions for micro-F1 and after 6 expansions for macro-F1. 5 Conclusions Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular cor243 Number of Expansion Terms 0 2 4 6 8 Micro-F1 Macro-F1 0.900 0.901 0.905 0.898 0.896 0.826 0.826 0.828 0.830 0.827 Table 1: Micro-Average and Macro F1 -measure, Reuters-21578 pus or genre of documents, and therefore of little use. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person [14]. This usage of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration when creating a linear ordering of terms. Integrating lexical resources into an upgraded semantic weighting scheme that could augment statistical term weighting is a prospect that cannot be overlooked in information retrieval and text categorization. Our first results with such a scheme in text categorization. At the same time, the results also raise the question, does assigning specific scalar values to terms in an ontology, this far represented by their"
W09-3721,J91-1002,0,0.430671,"be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical dev"
W09-3721,C98-2122,0,\N,Missing
Y12-1061,N10-1083,0,0.328638,"ined: 3 Feature-based CCM where K(S|w) is independent of B and the following production is taken over tree spans only. One advantage of the locally normalized model is that the EM algorithm could be still used to estimate parameters, which will be described in the next subsection. If we define the same factors of CCM (sequence and context for constituent and distituent) and set weights properly, then the probability of featurebased model is degenerated to the original CCM model. So the original CCM can be treated as a special case of the feature-based model. 3.1 Model Definition Motivated by (Berg-Kirkpatrick et al., 2010), we define factors in the log-linear form with local normalization. Let F1,...,K be K different factors. Each factor Fk corresponds to a nk -dimensional feature vector fk and a nk -dimensional weight vector wk . For the kth factor Fk , the corresponding multinomial parameter in original CCM is now treated as a function of weights wk . Define the factor category function δk to be +1 if Fk is constituent factor, and −1 otherwise. In detail, for span hi, ji in some bracketing B for sentence S, define Fk (Shi,ji |wk ) = Pk (Shi,ji |Bhi,ji = δk , wk ) exp(wk · fk (Shi,ji )) (2) =P v exp(wk · fk (v"
Y12-1061,D10-1117,0,0.540973,"long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all"
Y12-1061,P06-1109,0,0.13074,"significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to est"
Y12-1061,A00-2018,0,0.0251888,"e also used for the proposed FCCM, which we plan to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we"
Y12-1061,N09-1009,0,0.0632485,"strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. T"
Y12-1061,N09-1062,0,0.0965438,"n longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the d"
Y12-1061,D11-1018,0,0.0137879,"amework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency g"
Y12-1061,P11-1042,0,0.0215671,"III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with ℓ1 regularization and show improved performance could be achieved with sparse solution. Many unsupervised approaches aim to learn compact and sparse grammar, including the Bayesian models (Johnson et al., 2007; Cohn et al., 2010; Blunsom and Cohn, 2010) and posterior regularization (Ganchev et al., 2010"
Y12-1061,P12-2004,0,0.315104,"stimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and following improved unsupervised models (Smith and Eisner, 2004; Mirroshandel and Ghassem-Sani, 2008; Golland et al., 2012) is the problematic evaluation framework. The previous approaches train and evaluate models on the same dataset, so there is no reasonable way to choose model parameters unless setting them empirically. In this paper, we focus on CCM and present a general feature-based framework in which various overlapping features could be easily added. Previous dependency induction approach (Cohen and Copyright 2012 by Yun Huang, Min Zhang, and Chew Lim Tan 26th Pacific Asia Conference on Language,Information and Computation pages 564–573 Smith, 2009) demonstrates enabling factored covariance between the pr"
Y12-1061,C08-1042,0,0.0316883,"Missing"
Y12-1061,N09-1012,0,0.401925,"Missing"
Y12-1061,N10-1074,0,0.120137,"est set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but"
Y12-1061,P02-1017,0,0.731217,"ments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the precedin"
Y12-1061,P04-1061,0,0.791288,"task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentenc"
Y12-1061,N03-1017,0,0.0065762,"Missing"
Y12-1061,P06-1077,0,0.010649,"an to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with"
Y12-1061,J93-2004,0,0.0397779,"res and constant feature. 5 Experiments 5.1 5.2 Induction Results EM algorithm is sensitive to the initial condition. We adopt the same uniform-split initialization and the same smoothing values (2 for constituents and 8 for distituents) as described in (Klein, 2005). For feature-based model (F-CCM), we still use uniformsplit strategy to initialize probabilities in the first Estep, and set all weights to zero as the initial point of the gradient-based search algorithm in the M-step. Datasets and Settings We carry out experiments on the Wall Street Journal portion of the Penn English Treebank (Marcus et al., 1993). We report the unlabeled F1 score (the harmonic mean of precision and recall) as evaluation metric. Constituents which could not be gotten wrong (single words and entire sentences) are discarded. These are standard settings used in previous work (Klein, 2005). To perform model selection and parameter tunning, we split the treebank into three parts: section 02-21 as training set, section 00 as development set, and section 23 as test set. As standard machine learning pipeline, we perform EM on training set, tune parameters on development set, and report the result of selected model on test set."
Y12-1061,H05-1011,0,0.0281801,", which we plan to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization"
Y12-1061,P11-1108,0,0.155025,"Missing"
Y12-1061,P07-1049,0,0.292535,"t improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameter"
Y12-1061,P04-1062,0,0.0785454,"CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and following improved unsupervised models (Smith and Eisner, 2004; Mirroshandel and Ghassem-Sani, 2008; Golland et al., 2012) is the problematic evaluation framework. The previous approaches train and evaluate models on the same dataset, so there is no reasonable way to choose model parameters unless setting them empirically. In this paper, we focus on CCM and present a general feature-based framework in which various overlapping features could be easily added. Previous dependency induction approach (Cohen and Copyright 2012 by Yun Huang, Min Zhang, and Chew Lim Tan 26th Pacific Asia Conference on Language,Information and Computation pages 564–573 Smith, 20"
Y12-1061,P05-1044,0,0.0353815,"al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with ℓ1 regularization and show improved performance could be achieved with sparse solution. Many unsupervised approaches aim to learn compact and sparse grammar, including the Bayesian models (Johnson et al., 2007; Cohn et al., 2010; Blunsom and Cohn, 2010) and po"
Y12-1061,N10-1116,0,0.0125173,"research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible rea"
Y12-1061,D11-1117,0,0.176292,"re first increases and then decreases with the increase of λd:s . In contrast, with fixed λd:s , the performance varies little for different λc:s . These results somehow demonstrate the distituents modelled in CCM play a more important role than the constituents. 5.4 Discussion Experiments show that we achieve better performance than original CCM while using compact grammars. There are some issues we want to discuss here. 1. We only test a few feature templates. Other features such as words, stems may improve the results. Moreover, punctuations contain useful information in grammar induction (Spitkovsky et al., 2011b; Ponvert et al., 2011), while currently punctuations are ignored in our model. 2. In previous unsupervised constituency grammar induction, how to choose parameters is an art. While in the proposed model, we use development set to perform model selection. 3. EM algorithm could only find sub-optima. One possible solution is the Lateen EM (Spitkovsky et al., 2011a), in which multiple objective functions are an alternative optimized. Another method is the annealing technique during probability estimation process. We will investigate these in future work. 4. ℓ1 -norm regularization is used to lea"
Y12-1061,W11-0303,0,0.100745,"re first increases and then decreases with the increase of λd:s . In contrast, with fixed λd:s , the performance varies little for different λc:s . These results somehow demonstrate the distituents modelled in CCM play a more important role than the constituents. 5.4 Discussion Experiments show that we achieve better performance than original CCM while using compact grammars. There are some issues we want to discuss here. 1. We only test a few feature templates. Other features such as words, stems may improve the results. Moreover, punctuations contain useful information in grammar induction (Spitkovsky et al., 2011b; Ponvert et al., 2011), while currently punctuations are ignored in our model. 2. In previous unsupervised constituency grammar induction, how to choose parameters is an art. While in the proposed model, we use development set to perform model selection. 3. EM algorithm could only find sub-optima. One possible solution is the Lateen EM (Spitkovsky et al., 2011a), in which multiple objective functions are an alternative optimized. Another method is the annealing technique during probability estimation process. We will investigate these in future work. 4. ℓ1 -norm regularization is used to lea"
Y12-1061,C00-2139,0,0.355606,"Missing"
Y12-1061,N10-1016,1,0.794326,"combinations that we can not try each of them in experiments. In experiments, we use following sets of features. The first feature set includes the sequences with length up to 5: {seq1, seq2, seq3, seq4, seq5}. Note that sequences with arbitrary lengths are modelled in the original CCM, while we restrict the maximal sequence length to be 5. Since most of the longer sequences occurs only once or twice in the training corpus, we discard them to speed up training procedure and reduce memory usage. Boundary words have been proven useful for detecting phrase boundaries in supervised learning task (Xiong et al., 2010). We introduce this idea to unsupervised grammar induction. The features used in experiments are combinations of left boundary and right boundary words with lengths up to 2: {lb1, lb2, rb1, rb2, lb1.rb1, lb1.rb2, lb2.rb1, lb2.rb2}. The original CCM also considers the pair of preceding one word and following one word as contexts. We consider combinations of left context and right context words with lengths up to 2: {lx1, lx2, rx1, rx2, lx1.rx1, lx1.rx2, lx2.rx1, lx2.rx2}. The special token ⋄ is introduced to represent sentence boundaries. The last feature used is the constant feature {const}. T"
