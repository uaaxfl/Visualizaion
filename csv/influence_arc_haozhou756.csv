2020.acl-demos.1,P16-1129,0,0.0348835,"Missing"
2020.acl-demos.1,2020.wmt-1.63,0,0.0143566,"he historical records of each team. • In-match Description. It describes most important events in the game such as “some3 Table 1: Examples of Sports News Generation Time 23’ Category Score Player Didac Team Espanyol Generated Text 第23分 钟 ， 西 班 牙 人 迪达克打入一球。 35’ Yellow Card Mubarak Alav´es 第35分 钟 ， 阿 拉 维 斯 穆巴拉克吃到一张黄 牌。 approach 1 . Furthermore, our machine translation system leverages named-entity (NE) replacement for glossaries including team name, player name and so on to improve the translation accuracy. It can be further improved by recent machine translation techniques (Yang et al., 2020; Zheng et al., 2020). 阿拉维斯 与 ⻄班⽛⼈ 的 ⽐赛 打 成 了 input text sequence (Wang et al., 2017). The architecture is illustrated in Figure 4, we made the following augmentations on the base Tacotron 2 model: • We applied an additional speaker as well as language embedding to support multi-speaker and multilingual input. 平⼿ • We introduced a variational autoencoder-style residual encoder to encode the variational length mel into a fix length latent representation, and then conditioned the representation to the decoder. Transformer Encoder Named Entity Replacement Translated Text In the 23rd minute, Espanyol Didac scored a go"
2020.acl-main.28,D17-1098,0,0.0259676,"y, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) fo"
2020.acl-main.28,P19-1602,1,0.921322,"s a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphrases. SA searches to"
2020.acl-main.28,N03-1003,0,0.228491,"14 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Compu"
2020.acl-main.28,K16-1002,0,0.740759,"(Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphra"
2020.acl-main.28,C04-1051,0,0.266948,"annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the ge"
2020.acl-main.28,D17-1158,0,0.0234271,"ed Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeli"
2020.acl-main.28,P19-1331,0,0.0459871,"asing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-bes"
2020.acl-main.28,W07-1424,0,0.0599355,"ersity between a paraphrase and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample s"
2020.acl-main.28,P13-1158,0,0.0824825,"Missing"
2020.acl-main.28,P16-1154,0,0.0272601,"eplace = top- Kw∗ p− LM i − (w∗ , wt,k+1 , . . . , wt,lt ) . p← LM 4 (9) 4.1 For word insertion, the top-K vocabulary Wt,insert is computed in a similar way (except that the position of w∗ is slightly different). Details are not repeated. In our experiments, K is set to 50. Copy Mechanism. We observe that name entities and rare words are sometimes deleted or replaced during SA stochastic sampling. They are difficult to be recovered because they usually have a low language model-suggested probability. Therefore, we propose a copy mechanism for SA sampling, inspired by that in Seq2Seq learning (Gu et al., 2016). Specifically, we allow the candidate sentence generator to copy the words from the original sentence x0 for word replacement and insertion. This is essentially enlarging the top-K sampling vocabulary with the words in x0 , given by ft,op = Wt,op ∪ {w0,1 , . . . , w0,l } W 0 (10) ft,op is the where op ∈ {replace,insert}. Thus, W actual vocabulary from which SA samples the word w∗ for replacement and insertion operation. While such vocabulary reduces the proposal space, it works well empirically because other low-ranked candidate words are either irrelevant or make the sentence disfluent; they"
2020.acl-main.28,Q18-1031,0,0.0349776,"19) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in f"
2020.acl-main.28,2020.acl-main.707,1,0.78785,"ence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we p"
2020.acl-main.28,D17-1126,0,0.0410463,"Missing"
2020.acl-main.28,D18-1421,0,0.0287354,"an-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 201"
2020.acl-main.28,J83-1001,0,0.644195,"s of editing operations (i.e., insertion, replacement, and deletion). At each step, UPSA proposes a candidate modification of the sentence, which is accepted or rejected according to a certain acceptance rate (only accepted modifications are shown). Although sentences are discrete, we make an analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentenc"
2020.acl-main.28,W16-6625,0,0.0225211,"and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned"
2020.acl-main.28,W14-3316,0,0.0682153,"Missing"
2020.acl-main.28,P02-1040,0,0.114331,"oses a candimultiplicative. date word w∗ for the kth step, the resulting canExpression Diversity. The expression diversity didate sentence becomes x∗ = (wt,1 , . . . , wt,k−1 , scoring function computes the lexical difference of w∗ , wt,k+1 . . . , wt,lt ). The insertion operation two sentences. We adopt a BLEU-induced function works similarly. to penalize the repetition of the words and phrases Here, the candidate word is sampled from a probin the input sentence: abilistic distribution, induced by the objective function (2): fexp (x∗ , x0 ) = (1 − BLEU(x∗ , x0 ))S , (5) where the BLEU score (Papineni et al., 2002) computes a length-penalized geometric mean of n-gram precision (n = 1, · · · , 4). S coordinates the importance of fexp (xt , x0 ) in the objective function (2). Language Fluency. Despite semantic preservation and expression diversity, the candidate paraphrase should be a fluent sentence by itself. We use a separately trained (forward) language model −→ (denoted as LM) to compute the likelihood of the candidate paraphrase as our fluency scoring function: fflu (x∗ ) = k=l Y∗ → (w∗,k |w∗,1 , . . . , w∗,k−1 ), (6) p− LM k=1 where l∗ is the length of x∗ and w∗,1 , . . . , w∗,l are words of x∗ . H"
2020.acl-main.28,D14-1162,0,0.0841998,"cluding semantic preservation fsem , expression diversity fexp , and language fluency fflu . Thus, our searching objective is to maximize f (x) = fsem (x, x0 ) · fexp (x, x0 ) · fflu (x), (2) where x0 is the input sentence. Semantic Preservation. A paraphrase is expected to capture all the key semantics of the original sentence. Thus, we leverage the cosine function of keyword embeddings to measure if the key focus of the candidate paraphrase is the same as the input. Specifically, we extract the keywords of the input sentence x0 by the Rake system (Rose et al., 2010) and embed them by GloVE (Pennington et al., 2014). For each keyword, we find the closest word in the candidate paraphrase x∗ in terms of the cosine similarity. Our keyword-based semantic preservation score is given by the lowest cosine similarity among all the keywords, i.e., the least matched keyword: fsem,key (x∗ , x0 ) = If the proposal is accepted, xt+1 = x∗ , or otherwise, xt+1 = xt . 304 min max{cos(w∗,j , e)}, e∈keywords(x0 ) j (3) where w∗,j is the jth word in the sentence x∗ ; e is an extracted keyword of x0 . Bold letters indicate embedding vectors. In addition to keyword embeddings, we also adopt a sentence-level similarity functi"
2020.acl-main.28,C16-1275,0,0.518552,"analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domai"
2020.acl-main.28,W04-3219,0,0.160841,"actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et a"
2020.acl-main.28,P19-1332,0,0.592499,"lly, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual M"
2020.acl-main.28,W04-1013,0,0.0290802,"-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 30"
2020.acl-main.28,P19-1605,0,0.0197168,"upervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only"
2020.acl-main.28,2020.acl-main.452,1,0.877759,"ased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach"
2020.acl-main.28,P17-1099,0,0.0403138,"eported to be the state-of-the-art VAE. We adopted the published source code and generated paraphrases for comparison. CGMH. Miao et al. (2019) use Metropolis– Hastings sampling in the word space for constrained sentence generation. It is shown to outperform latent space sampling as in VAE, and is the state-of-the-art unsupervised paraphrasing approach. We also adopted the published source code and generated paraphrases for comparison. We further compare UPSA with supervised Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performan"
2020.acl-main.28,N18-3006,0,0.0495463,"Missing"
2020.acl-main.28,P12-2008,0,0.229831,"e Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 300 hidden units and trained specifically on each dataset with non-parallel sente"
2020.acl-main.28,P19-1199,0,0.0870125,"methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work"
2020.acl-main.28,P19-1503,0,0.0213944,"uth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) for paraphrasing. 3.1 Th"
2020.acl-main.314,P19-1285,0,0.0297381,"a sets to verify the effectiveness of the proposed MC-Tailor. Empirical results show that MC-Tailor can generate significantly better samples than finetuning, and the resulting model distributions of our model are closer to real data distributions. 2 Pre-Trained Language Model Language models generally estimate the density of sentences in real context within an autoregressive style: N Y P (x) = P (xi |x[1:i−1] ), (1) i=1 where x is a sentence with length N . Recently, with an extremely large number of parameters, pretrained language models like GPT-2 (Radford et al., 2019) and Transformer-XL (Dai et al., 2019) have shown great promise in text generation. PLMs are first trained on a huge general domain data set and then fine-tuned on specific domain datasets of different downstream tasks. Specifically, given a pre-trained GPT2 model, to generate sentences of email domain, we always need to fine-tune the GPT2 on a small set of email domain corpus. Additionally, PLMs have some other important applications. Miao et al. (2019) use fine-tuned language models for constrained text generation. Wolf et al. (2019) fine-tune GPT-2 on a dialog data set to boost the performance of dialog system. However, as stat"
2020.acl-main.314,I17-1099,0,0.026123,"m. Results of SMC are not reported since it leads to very poor Rev-PPLs because of the lack of sample diversity. 4.1 Experimental Setup We conduct experiments on 9 data sets with different styles and sizes. And we use five different metrics, including human evaluation, to measure the generation performance of each method. Datasets. We use the following data sets for experiments. • Ontonotes (Pradhan et al., 2013) is a multigenre data set for sequence annotation. We use sentences from six genres (bn, bc, mz, nw, tc, wb) for the experiment. • Switchboard (Jurafsky et al., 1997) and DailyDialog (Li et al., 2017) are large and medium scale dialog data sets, of which only responses are used for the experiment. • IWSLT-16 (Cettolo et al., 2016) is a data set of paired conference speeches for machine translation. We use English sentences from De-En pairs to test model performance on the special conference speech domain. Evaluation Metrics. To evaluate the generation quality and diversity, we use the following metrics. • PPL reflects the average density of samples from test set in a generative model. Models with lower PPLs have more similar model distributions with real contexts. Unlike baseline models, M"
2020.acl-main.635,D14-1179,0,0.0186658,"Missing"
2020.acl-main.635,N19-1423,0,0.0203577,"current encoder-decoder model that has a specific context RNN to incorporate historical conversational utterances into a context state, which is used as the initial hidden state of the decoder. The adapted model generates the k-th utterance based on the past k − 1 utterances, where k was also set to 8, for fair comparison with Seq2Seq. All the generative models were trained by optimizing the cross-entropy loss: (g) L0 = − T 1X log P(ˆ xt = xt ), T t=1 where x ˆt denotes the predicted token at the time step t, while xt is the t-th token of the target sentence. 4.1.2 Retrieval-based Model BERT (Devlin et al., 2019): We adapted this deep bidirectional transformers (Vaswani et al., 2017) as a retrieval-based model. For each utterance (except the first one in a dialog), we extracted keywords in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ"
2020.acl-main.635,N16-1014,0,0.64384,"ntroducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al."
2020.acl-main.635,P18-1138,0,0.275527,"tudied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (W"
2020.acl-main.635,W15-4640,0,0.0739565,"nt but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use knowledge graphs as knowledge resources. Nevertheless, the number of topics is limited to one (Moon et al., 2019) or two (Wu et al., 2019), which is not sufficient for diversif"
2020.acl-main.635,D16-1147,0,0.0245084,"rds in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ), where y ∈ {0, 1} is the true label. For the test, we selected the candidate response with the largest probability. 4.1.3 Knowledge-aware Models A key-value memory module (Miller et al., 2016) is introduced to the aforementioned models to utilize the knowledge information. We treated all knowledge triples mentioned in a dialogue as the knowledge information in the memory module. For a triple that is indexed by i, we represented the key memory and the value memory respectively as a key vector ki and a value vector vi , where ki is the average word embeddings of the head entity and the relation, and vi is those of the tail entity. We used a query vector q to attend to the key vectors ki (i = 1, 2, ...): αi = softmaxi (q T ki ), then the weighted P sum of the value vectors vi (i = 1,"
2020.acl-main.635,D18-1255,0,0.335864,"commendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations o"
2020.acl-main.635,P19-1081,0,0.169542,"s been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialK"
2020.acl-main.635,D18-1398,0,0.0179714,"nce/character respectively). collect multi-turn conversations from scratch based on such large-scale knowledge. KdConv is proposed as one small step to achieve this goal, where we narrowed down the scale of background knowledge to several domains (film, music, and travel) and collected conversations based on the domainspecific knowledge. KdConv contains similar domains (film and music) and dissimilar domains (film and travel) so that it offers the possibility to investigate the generalization and transferability of knowledge-driven conversational models with transfer learning or meta learning(Gu et al., 2018; Mi et al., 2019). In the following subsections, we will describe the two steps in data collection: (1) Constructing the domain-specific knowledge graph; (2) Collecting conversation utterances and knowledge interactions by crowdsourcing. 3.1 Knowledge Graph Construction As the sparsity and the large scale of the knowledge were difficult to handle, we reduced the range of the domain-specific knowledge by crawling the most popular films and film stars, music and singers, and attractions as start entities, from several related websites for the film4 /music5 /travel6 domain. The knowledge of thes"
2020.acl-main.635,P02-1040,0,0.106856,"DAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and Wu et al. (2019). 8 We adopted several widely-used metrics to measure the quality of the generated response. We calculated Perplexity (PPL) to evaluate whether the generation result is grammatical and fluent. BLEU1/2/3/4 (Papineni et al., 2002) is a popular metric to compute the k-gram overlap between a generated sentence and a reference (Sordoni et al., 2015; Li et al., 2016b). Distinct-1/2/3/4 (Li et al., 2016b) is also provided to evaluates the diversity of generated responses. 4.3.2 Results The results are shown in Table 5. We analyze the results from the following perspectives: The influence of knowledge: after introducing the knowledge, all the models were improved in terms of all the metrics except PPL in all the domains. First, all the models obtain higher Hits@1 scores (in the music domain, BERT obtains an improvement of 0."
2020.acl-main.635,P19-1539,0,0.25324,", 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use knowledge graphs a"
2020.acl-main.635,N10-1020,0,0.0602713,"n adaptation or transfer learning between similar domains (e.g., from film to music) or dissimilar domains (e.g., from music to travel). • We provide benchmark models on this corpus to facilitate further research, and conduct extensive experiments. Results show that the models can be enhanced by introducing background knowledge, but there is still much room for further research. The corpus and the models are publicly available3 . 2 Related Work Recently, open-domain conversation generation has been largely advanced due to the increase of publicly available dialogue data (Godfrey et al., 1992; Ritter et al., 2010; Shang et al., 2015; Lowe et al., 2015). However, the lack of annotation of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2019). These models produce conversations that are substantially different from those humans make, which largely rely on background knowledge. To facilitate the development of conversational models that mimic human conversations, there have been several knowledge-grounded corpora proposed. Some datasets (Zhou et al., 2018b; Ghazvininejad et al., 2018; Liu et"
2020.acl-main.635,P15-1152,0,0.143131,"can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Gha"
2020.acl-main.635,N18-2028,0,0.0254902,"ighted sum of L0 and Latt : (l) (l) Ltot = L0 + λLatt , l ∈ {g, r}. Note that the knowledge-enhanced BERT was initialized from the fine-tuned BERT discussed in Section 4.1.2, and the parameters of the transformers were frozen during training the knowledge related modules. The purpose was to exclude the impact of the deep transformers but only examine the potential effects introduced by the background knowledge. 4.2 Implementation Details 2017) and CoTK (Huang et al., 2020). The Jieba Chinese word segmenter7 was employed for tokenization. The 200-dimensional word embeddings were initialized by Song et al. (2018), while the unmatched ones were randomly sampled from a standard normal distribution N (0, 1). The type of RNN network units was all GRU (Cho et al., 2014) and the number of hidden units of GRU cells were all set to 200. ADAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and"
2020.acl-main.635,N15-1020,0,0.091094,"Missing"
2020.acl-main.635,D15-1199,0,0.0119797,"ing that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan e"
2020.acl-main.635,P19-1369,0,0.310226,"and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2"
2020.acl-main.635,P17-1046,0,0.01859,"ates the k-th utterance based on the past k − 1 utterances, where k was also set to 8, for fair comparison with Seq2Seq. All the generative models were trained by optimizing the cross-entropy loss: (g) L0 = − T 1X log P(ˆ xt = xt ), T t=1 where x ˆt denotes the predicted token at the time step t, while xt is the t-th token of the target sentence. 4.1.2 Retrieval-based Model BERT (Devlin et al., 2019): We adapted this deep bidirectional transformers (Vaswani et al., 2017) as a retrieval-based model. For each utterance (except the first one in a dialog), we extracted keywords in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ), where y ∈ {0, 1} is the true label. For the test, we selected the candidate response with the largest probability. 4.1.3 Knowledge-aware Models A key-value memory module (Miller et al., 2016) is introduced to t"
2020.acl-main.635,P18-1205,0,0.0565722,"ed by Song et al. (2018), while the unmatched ones were randomly sampled from a standard normal distribution N (0, 1). The type of RNN network units was all GRU (Cho et al., 2014) and the number of hidden units of GRU cells were all set to 200. ADAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and Wu et al. (2019). 8 We adopted several widely-used metrics to measure the quality of the generated response. We calculated Perplexity (PPL) to evaluate whether the generation result is grammatical and fluent. BLEU1/2/3/4 (Papineni et al., 2002) is a popular metric to compute the k-gram overlap between a generated sentence and a reference (Sordoni et al., 2015; Li et al., 2016b). Distinct-1/2/3/4 (Li et al., 2016b) is also provided to evaluates the diversity of generated responses. 4.3.2 Results The results are shown in Table 5. We analyze the results from the following perspectives: The i"
2020.acl-main.635,C16-1191,1,0.906669,"Missing"
2020.acl-main.635,D18-1076,0,0.305676,"of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrat"
2020.acl-main.635,D19-1194,0,0.129115,", 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use"
2020.emnlp-main.210,2020.acl-main.747,0,0.0715994,"Missing"
2020.emnlp-main.210,N19-1423,0,0.245367,". Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several li"
2020.emnlp-main.210,P15-1166,0,0.022539,"tialization fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.210,2020.findings-emnlp.283,0,0.0187955,"013a) first introduces dictionaries to align word representations from different languages. A series of followup studies focus on aligning the word representation across languages (Xing et al., 2015; Ammar et al., 2016; Smith et al., 2017; Lample et al., 2018b). Inspired by the success of BERT, Conneau and Lample (2019) introduced XLM - masked language models trained on multiple languages, as a way to leverage parallel data and obtain impressive empirical results on the cross-lingual natural language inference (XNLI) benchmark and unsupervised NMT(Sennrich et al., 2016a; Lample et al., 2018a; Garcia et al., 2020). Huang et al. (2019) extended XLM with multi-task learning and proposed a universal language encoder. Different from these works, a) mRASP is actually a multilingual sequence to sequence model which is more desirable for NMT pre-training; b) mRASP introduces alignment regularization to bridge the sentence representation across languages. 6 Conclusion In this paper, we propose a multilingual neural machine translation pre-training model (mRASP). To bridge the semantic space between different languages, we incorporate word alignment into the pre-training model. Extensive experiments are conduct"
2020.emnlp-main.210,D19-1252,0,0.34639,"f parallel corpus to simulate different scenarios. Most of the En-X parallel datasets are from the pre-training phase to avoid introducing new information. Most pairs for fine-tuning are from previous years of WMT and IWSLT. Specifically, we use WMT14 for EnDe and En-Fr, WMT16 for En-Ro. For pairs like Nl(Dutch)-Pt(Portuguese) that are not available in WMT or IWSLT, we use news-commentary instead. For a detailed description, please refer to the Appendix. 2652 8 CTNMT only reports the Transformer-base setting. Lang-Pairs Size En→De 4.5M Zh→En 20M En→Fr 40M Direct CTNMT8 (2020) mBART (2020) XLM (2019) MASS (2019) mBERT (2019) 29.3 30.1 28.8 28.9 28.6 24.1 - 43.2 42.3 41.0 - mRASP 30.3 24.7 44.3 Table 2: Fine-tuning performance for popular medium and rich resource MT tasks. For fair comparison, we report detokenized BLEU on WMT newstest18 for Zh→En and tokenized BLEU on WMT newstest14 for En→Fr and En→De. Notice unlike previous methods (except CTNMT) which do not improve in the rich resource settings, mRASP is again able to consistently improve the downstream MT performance. It is the first time to verify that low-resource language pairs can be utilized to improve rich resource MT. Based on"
2020.emnlp-main.210,Q17-1024,0,0.0398988,"Missing"
2020.emnlp-main.210,2020.acl-main.703,0,0.118192,"Missing"
2020.emnlp-main.210,2020.tacl-1.47,0,0.324,"following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several limitations for machine translation tasks. First, pre-trained language models such as BERT are not easy to directly fine-tune unless using some sophisticated techniques (Yang et al., 2020). Second, there is a discrepancy between existing pre-training objective and down-stream ones in MT. Existing pre-training approaches such as MASS (Song et al., 2019) and mBART (Liu et al., 2020) rely on auto-encoding objectives to pre-train the models, which are different from translation. Therefore, their fine-tuned MT models still do not achieve adequate improvement. Third, existing MT pre-training approaches focus on using multilingual models to improve MT for low resource or medium resource languages. There has not been one pre-trained MT model that can improve for any pairs of languages, even for rich resource settings such as English-French. In this paper, we propose multilingual Random Aligned Substitution Pre-training (mRASP), a method to pre-train a MT model for many languag"
2020.emnlp-main.210,2021.ccl-1.108,0,0.128783,"Missing"
2020.emnlp-main.210,W18-6309,0,0.0189222,"ource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually ac"
2020.emnlp-main.210,P19-1015,0,0.0284706,"M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually achieves inferior accura"
2020.emnlp-main.210,P16-1009,0,0.537502,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,W18-6301,0,0.161617,"ts cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the scale of the dataset increasing, the gap between the randomly initialized baseline and pre-training model is becoming closer. It is worth noting that, for En→De benchmark, we obtain 1.0 BLEU points gains9 . To verify mRASP can further boost performance on rich resource datasets, we also conduct experiments on En→Zh and En→Fr. We compare our results with two strong baselines reported by Ott et al. (2018); Li et al. (2019). As shown in Table 2, surprisingly, when large parallel datasets are provided, it still benefits from pre-training models. In En→Fr, we obtain 1.1 BLEU points gains. Comparing to other Pre-training Approaches We compare our mRASP to recently proposed multilingual pre-training models. Following Liu et al. (2020), we conduct experiments on En-Ro, the only pairs with established results. To make a fair comparison, we report de-tokenized BLEU. As illustrated in Table 4 , Our model reaches comparable performance on both En→Ro and Ro→En. We also combine Back Translation (Sennrich"
2020.emnlp-main.210,P16-1162,0,0.792767,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,D14-1162,0,0.0844587,"Missing"
2020.emnlp-main.210,N18-1202,0,0.293103,"exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there"
2020.emnlp-main.210,W18-6319,0,0.0204161,"ly initialized models directly on downstream bilingual parallel corpus as a comparison with pre-training models. Fine-tuning We fine-tune our obtained mRASP model on the target language pairs. We apply a dropout rate of 0.3 for all pairs except for rich resource such as En-Zh and En-Fr with 0.1. We carefully tune the model, setting different learning rates and learning scheduler warm-up steps for different data scale. For inference, we use beam-search with beam size 5 for all directions. For most cases, We measure case-sensitive tokenized BLEU. We also report de-tokenized BLEU with SacreBLEU (Post, 2018) for a fair comparison with previous works. 3.2 Main Results We first conduct experiments on the (extremely) low-resource and medium-resource datasets, where multilingual translation usually obtains significant improvements. As illustrated in Table 1, we obtain significant gains in all datasets. For extremely low resources setting such as En-Be (Belarusian) where the amount of datasets cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the sca"
2020.emnlp-main.210,N18-2084,0,0.109204,"Missing"
2020.emnlp-main.210,N15-1104,0,0.0311338,"fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.733,S14-2010,0,0.269265,"Missing"
2020.emnlp-main.733,S16-1081,0,0.234494,"Missing"
2020.emnlp-main.733,S12-1051,0,0.0820126,"trate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhari"
2020.emnlp-main.733,S13-1004,0,0.067209,"Missing"
2020.emnlp-main.733,D15-1075,0,0.171507,"an latent variable, is then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this proble"
2020.emnlp-main.733,S17-2001,0,0.0610254,"entence),sentence∼D log pZ (fφ−1 (u)) + log |det ∂fφ−1 (u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) fo"
2020.emnlp-main.733,D18-2029,0,0.0389952,".91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible."
2020.emnlp-main.733,marelli-etal-2014-sick,0,0.0539398,"u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formul"
2020.emnlp-main.733,L18-1269,0,0.080249,"or various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhariwal (2018) 9123 Dataset STS-B SICK-R STS-12 Avg. GloVe embeddings Avg. BERT embeddings BERT CLS-vector Pub"
2020.emnlp-main.733,D17-1070,0,0.058629,") 68.95 (↑) 78.48 (↑) 72.15 73.98 75.53 (↑) 77.62 (↑) 77.35 79.15 80.63 (↑) 81.95 (↑) 73.91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpu"
2020.emnlp-main.733,D14-1162,0,0.109243,"://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highlycosting or even intractable. In this paper, we aim to answer two major questions: (1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly? (2) If the BERT embeddings capture enough semantic information that"
2020.emnlp-main.733,N19-1423,0,0.0613683,"ance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence"
2020.emnlp-main.733,D19-1006,0,0.105365,"on that is hard to be directly utilized, how can we make it easier without external supervision? Towards this end, we first study the connection between the BERT pretraining objective and the semantic similarity task. Our analysis reveals that the sentence embeddings of BERT should be able to intuitively reflect the semantic similarity between sentences, which contradicts with experimental observations. Inspired by Gao et al. (2019) who find that the language modeling performance can be limited by the learned anisotropic word embedding space where the word embeddings occupy a narrow cone, and Ethayarajh (2019) who find that BERT word embeddings also suffer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistentl"
2020.emnlp-main.733,P19-1315,0,0.02216,", h&gt; c wx can be approximately decomposed as follows, ∗ h&gt; c wx ≈ log p (x|c) + λc = PMI(x, c) + log p(x) + λc . (2) (3) p(x,c) where PMI(x, c) = log p(x)p(c) denotes the pointwise mutual information between x and c, log p(x) is a word-specific term, and λc is a context-specific term. PMI captures how frequently two events cooccur more than if they independently occur. Note that co-occurrence statistics is a typical tool to deal with “semantics” in a computational way — specifically, PMI is a common mathematical surrogate to approximate word-level semantic similarity (Levy and Goldberg, 2014; Ethayarajh et al., 2019). Therefore, roughly speaking, it is semantically meaningful to compute the dot product between a context embedding and a word embedding. Higher-Order Co-Occurrence Statistics as Context-Context Semantic Similarity. During pretraining, the semantic relationship between two contexts c and c0 could be inferred and reinforced with their connections to words. To be specific, if both the contexts c and c0 co-occur with the same word w, the two contexts are likely to share similar semantic meaning. During the training dynamics, when c and w occur at the same time, the embeddings hc and xw are encour"
2020.emnlp-main.733,D19-1370,1,0.909392,"her to their kNN neighbors compared to the embeddings of high-frequency words. This demonstrates that lowfrequency words tends to disperse sparsely. Due to the sparsity, many “holes” could be formed around the low-frequency word embeddings in the embedding space, where the semantic meaning can be poorly defined. Note that BERT sentence embeddings are produced by averaging the context embeddings, which is a convexitypreserving operation. However, the holes violate the convexity of the embedding space. This is a common problem in the context of representation learining (Rezende and Viola, 2018; Li et al., 2019; Ghosh et al., 2020). Therefore, the resulted sentence embeddings can locate in the poorly-defined areas, and the induced similarity can be problematic. &quot; Invertible mapping The BERT sentence embedding space Standard Gaussian latent space (isotropic) Figure 1: An illustration of our proposed flow-based calibration over the original sentence embedding space of BERT. 3 Proposed Method: BERT-flow To verify the hypotheses proposed in Section 2.2, and to circumvent the incompetence of the BERT sentence embeddings, we proposed a calibration method called BERT-flow in which we take advantage of an i"
2020.emnlp-main.733,2021.ccl-1.108,0,0.220472,"Missing"
2020.emnlp-main.733,D16-1264,0,0.0232629,"T baselines in most cases, and outperforms the state-of-the-art SBERT/SRoBERTa results by a large margin. Robustness analysis with respect to random seeds are provided in Appendix C. 4.2 Unsupervised Question-Answer Entailment In addition to the semantic textual similarity tasks, we examine the effectiveness of our method on unsupervised question-answer entailment. We use Question Natural Language Inference (QNLI, Wang et al. (2019)), a dataset comprising 110K question-answer pairs (with 5K+ for testing). QNLI extracts the questions as well as their corresponding context sentences from SQUAD (Rajpurkar et al., 2016), and annotates each pair as either entailment or no entailment. In this paper, we further adapt QNLI as an unsupervised task. The similarity between a question and an answer can be predicted by computing the cosine similarity of their sentence embeddings. Then we regard entailment as 1 and no entailment as 0, and evaluate the performance of the methods with AUC. As shown in Table 4, our method consistently improves the AUC on the validation set of QNLI. Also, learning flow on the target dataset can produce superior results compared to learning flows on NLI. 9125 Method AUC BERTbase -NLI-last2"
2020.emnlp-main.733,D19-1410,0,0.237997,"fer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistently better than the [CLS] vector as shown in (Reimers and Gurevych, 2019). 9119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9119–9130, c November 16–20, 2020. 2020 Association for Computational Linguistics product or cosine similarity. To address these issues, we propose to transform the BERT sentence embedding distribution into a smooth and isotropic Gaussian distribution through normalizing flows (Dinh et al., 2015), which is an invertible function parameterized by neural networks. Concretely, we learn a flow-based generative model to maximize the likelihood of generating BERT sentence embeddings from a standard G"
2020.emnlp-main.733,N18-1101,0,0.234674,"s then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this problem. 2 Understanding the S"
2020.findings-emnlp.441,J96-1002,0,0.111295,"ing as the evaluation metric. 2. For the other circumstances where no such special token can be used, a mean-pooling operation P is applied to the encoder output, i.e. 1 x = n nt=1 ht , where ht denotes the contextual word representation of the tth token produced by the encoder. The latent space H is spanned by all the latent states. Baseline Approaches. We use two common baseline approaches in NLP active learning to compare with our framework, namely random sampling (RM) and entropy-based uncertainty sampling (US). For sequence classification tasks, we adopt the widely used Max Entropy (ME) (Berger et al., 1996) as uncertainty measurement: H ME (x) = − c X P (y = m|x) log P (y = m|x) (3) m=1 where c is the number of classes. For sequence labeling tasks, we use total token entropy (TTE) (Settles and Craven, 2008) as uncertainty measurement: H T T E (x) = − l N X X P (yi = m|x) log P (yi = m|x) i=1 m=1 (4) where N is the sequence length and l is the number of labels. Latent Space Definition We use the adversarial attack in our AUSDS learning framework to find informative samples, which rely on a well-defined latent space. Two types of latent spaces are defined here based on the encoder architectures an"
2020.findings-emnlp.441,C04-1051,0,0.0605365,"which requires renewal of the sampler mapper M . The algorithm terminates until the unlabeled text corpus Ti is used up. 4 Experiments We evaluate the AUSDS learning framework on sequence classification and sequence labeling tasks. For the oracle labeler O, we directly use the labels provided by the datasets. In all the experiments, we take average results of 5 runs with different random seeds to alleviate the influence of randomness. 4.1 Set-up Dataset. We use five datasets, namely Stanford Sentiment Treebank (SST-2 / SST-5) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), AG’s News Corpus (AG News) (Zhang et al., 2015) and CoNLL 2003 Named Entity Recognition dataset 4912 Dataset SST-2 (Socher et al., 2013) SST-5 (Socher et al., 2013) MRPC (Dolan et al., 2004) AG News (Zhang et al., 2015) CoNLL’03 (Sang and De Meulder, 2003) Task sequence classification sequence classification sequence classification sequence classification sequence labeling Sample Size 11.8k sentences, 215k phrases 11.8k sentences, 215k phrases 5,801 sentence pairs 12k sentences 22k sentences, 300k tokens Table 1: 5 datasets we used for sentence learning experiments, across sequence classific"
2020.findings-emnlp.441,N18-1202,0,0.150104,"dversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popular pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness. 1 Introduction Deep neural models become popular in natural language processing (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Neural models usually consume massive labeled data, which requires a huge quantity of human labors. But data are not born equal, where informative data with high uncertainty are decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs t"
2020.findings-emnlp.441,W03-0419,0,0.140818,"Missing"
2020.findings-emnlp.441,D08-1112,0,0.883398,"decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs to traverse all unlabeled data to find informative unlabeled samples, which are always near the decision boundary with large entropy. However, the traverse process is very time-consuming, thus cannot be executed frequently (Settles and Craven, 2008). A common choice is to perform the sampling process after every specific period, and it samples and labels informative unlabeled data then trains the model until convergence (Deng et al., 2018). We argue that infrequently performing uncertainty sampling may lead to the “ineffective sampling” problem. Because in the early phase of training, the decision boundary changes quickly, which makes previously collected samples less effective after several updates of the model. Ideally, uncertainty sampling should be performed frequently in the early phase of model training. In this paper, we propose t"
2020.findings-emnlp.441,D13-1170,0,\N,Missing
2020.wmt-1.24,W14-3346,0,0.0287428,"mputed by, R(θ) = S X X Q(y|x(s) ; θ, α)∆(y, y (s) ), s=1 y∈S(x(s) ) (1) where x(s) and y (s) are two paired sentences. ∆ denotes a risk function and S(x(s) ) ∈ Y is a sampled subset of full search space. Then, the distribution Q is defined over space S(x(s) ), P (y|x(s) ; θ)α . 0 (s α y 0 ∈S(x(s)) P (y |x ; θ) (2) In practice, we use 4 candidates for each source sentence x(s) . Although the paper claimed that sampling generates better candidates, we find that the beam search performs better in our extremely large Transformer model. The risk function we used is the 4-gram sentence-level BLEU (Chen and Cherry, 2014) and we tune the optimal α via grid search within {0.005, 0.05, 0.5, 1, 1.5, 2}. Each model is fine-tuned for a max of 1000 steps. Q(y|x(s) ; θ, α) = P 3.6 Ensemble We split each training data into three shards among Clean, Noisy and Sample data respectively, which yields a total number of 9 shards. For each shard, we train seven varieties (two Deeper transformers, two Wider transformers, two AANs and one DTMT) with different model architecture. Then we apply four finetuning approaches on each model, thus the total number of models is quadrupled (about 200 models). For ensemble, it is difficul"
2020.wmt-1.24,P19-1425,0,0.037441,"Missing"
2020.wmt-1.24,W18-6408,0,0.0161268,"via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translat"
2020.wmt-1.24,D18-1045,0,0.32227,"c data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model"
2020.wmt-1.24,W18-2703,0,0.0179191,"and in-domain synthetic data. The out-of-domain synthetic corpus is generated via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it"
2020.wmt-1.24,D16-1139,0,0.190281,"rmers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algori"
2020.wmt-1.24,D17-1230,0,0.0274829,"domain knowledge transfer. In our experiments, we find that iteratively performing the in-domain knowledge transfer can further provide improvements (see Table 2). For each iteration, we replace the indomain synthetic data and retrain our models, and it costs about 10 days on 8 NVIDIA V100 GPU machines. For the final submission, the knowledge transfer is conducted twice. 3.4 Data Augmentation Aside from synthetic data generation, we also apply two data augmentation methods over our synthetic corpus. Firstly, adding synthetic/natural noises to training data is widely applied in the NLP fields (Li et al., 2017; Belinkov and Bisk, 2017; Cheng et al., 2019) to improve model robustness and enhance model performance. Therefore, we proposed to add token-level synthetic noises. Concretely, we perform random replace, random delete, and random permutation over our data. The probability of enabling each of the three operations is 0.1. We refer to this corrupted corpus as Noisy data. Secondly, as illustrated in (Edunov et al., 2018), sampling generation over back-translation shows its potential in building robust NMT systems. Consequently, we investigate the performance of sampled synthetic data. For back-tr"
2020.wmt-1.24,D19-1559,1,0.791509,"by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce more diversity in our Transformer models, we use Average"
2020.wmt-1.24,P19-1233,1,0.820814,"a linear transformation enhanced GRU (L-GRU) followed by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce"
2020.wmt-1.24,2015.iwslt-evaluation.11,0,0.0358315,"T systems. Consequently, we investigate the performance of sampled synthetic data. For back-translated data, we replace beam search with sampling in its generation. For in-domain synthetic data, we replace the golden Chinese with the back sampled pseudo Chinese sentences. We refer to the data with sampling generation as Sample data. As a special case, we refer to the without augmentation data as Clean data. 3.5 In-domain Finetuning We train the model on large-scale out-of-domain data until convergence and then finetune it on smallscale in-domain data, which is widely used for domain adaption (Luong and Manning, 2015; Li et al., 2019). Specifically, we take Chinese− →English test sets of WMT 17 and 18 as in-domain data, and filter out documents that are originally created in 242 English (Sun et al., 2019). We name above finetuning approach as normal finetuning. In all our finetuning experiments, we set the batch size to 4096, and finetune the model for around 400 steps1 on the in-domain data. Furthermore, the well-known problem of exposure bias in sequence-to-sequence generation becomes more serious under domain shift (Wang and Sennrich, 2020). To solve this issue, we further explore some advanced finetun"
2020.wmt-1.24,P19-2049,0,0.161662,"back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and"
2020.wmt-1.24,W19-5333,0,0.019301,"and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translated data. 3.2.2 Knowledge Distillati"
2020.wmt-1.24,P16-1009,0,0.18507,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1162,0,0.554201,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1159,0,0.0483208,"knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section,"
2020.wmt-1.24,W19-5341,0,0.381816,"In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encoding representation."
2020.wmt-1.24,2020.acl-main.326,0,0.094302,"ion method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the mode"
2020.wmt-1.24,P19-1176,0,0.0318592,"submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encodi"
2020.wmt-1.24,D19-1430,0,0.0177972,"els in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper,"
2020.wmt-1.24,P18-1166,0,0.124343,"is the highest among all submissions. 1 Introduction Our WeChat AI team participates in the WMT 2020 shared news translation task on Chinese→English. In this year’s translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies. For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the s"
2021.acl-long.155,2021.naacl-main.458,1,0.735104,"Missing"
2021.acl-long.155,D19-1633,0,0.559953,"ngy2 Hamming h22 Hamming Replace h Replace 0.5 0.5 Replace h0.5 h2h2 y2 y2 h2 yy2 2 y2 y2 y2Hamming 2 2 Inputs Distance Inputs Distance Inputs Distance Inputs 0.7 yy33 y3 N(0.7 0.7y 0.7 3   y h3h3 y3 y3 h3 yy3 3 y3 y3 y3N(Y,yN( 3 Y, Y ) = 33 3  Y Y, ) =Y 3) = 3 y y4 0.6 0.6 hh44 0.6h yy4 4 ysuch y4 yword h4h4Notice y4 y4 h4 that h0.6 sentence. interdependency 4 4 4 4 4 yy55 y5 0.9 0.9 0.9y y0.9 h h y y h5 yy 5 y y y y5 5 is crucial, Transformer explicitly captures 5 5 5 5 5 5 5 5 as5 the that via decoding from left to right (Figure 1a). Several remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding. Their common idea is to decode the target tokens iteratively while each pass of decoding is trained using the masked language model (Figure 1c). Since these methods require multiple passes of decoding, its generation speed is measurably slower than the vanilla NAT. With single-pass generation only, these methods still largely lag behind the autoregressive Transformer. 1993 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natur"
2021.acl-long.155,2020.acl-main.36,0,0.423954,"complex methods to better decide the output lengths: noisy parallel decoding (NPD) and connectionist temporal classification (CTC). For NPD (Gu et al., 2018), we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.7"
2021.acl-long.155,D18-1149,0,0.0780326,"Missing"
2021.acl-long.155,D19-1573,0,0.0461076,"Missing"
2021.acl-long.155,D18-1336,0,0.0366605,"Missing"
2021.acl-long.155,D19-1437,0,0.242673,"ate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.70 / 18.6× 1 1 1 1 1 19.17 24.15 25.20 25.31 26.07 23.20 27.28 29.52 30.68 29.68 29.79 31.45 / 32.20 / 31.44 31.81 / 32.84 / 2.4× 9.7× / / 6.1× 1 1 1 1 1 20.36 25.52 25.21 26.39 26.55 24.81 28.73 29.84 29.54 31.02 28.47 32.60 31.19 32.79 32.87 29.43 33.46 32.04 33.84 33.51 15.3×† 14.6"
2021.acl-long.155,P19-2049,0,0.0193196,"ed language model, and the model iteratively replaces masked tokens with new outputs. (Li et al., 2020) first predict the left token and right token for each position, and decode the final token at the current position conditioned on the left-and-right tokens predicted before. Despite the relatively better accuracy, the multiple decoding iterations reduce the inference efficiency of non-autoregressive models. Scheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019). Although scheduled sampling also modifies decoder inputs in training, there are mainly two differences between our work and scheduled sampling. Firstly, scheduled sampling mixes up the predicted sequence and the gold target sequence, and our method does not mix predicted sequences into decoder inputs. Besides, GLAT aims to learn word interdependency for single-pass parallel generation and scheduled sampling is designed for alleviating exposure bias. 6 Conclusion In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel ge"
2021.acl-long.155,2020.emnlp-main.83,0,0.461609,"Missing"
2021.acl-long.155,P16-1162,0,0.0586813,"ated tokens after generation. 4 Experiments In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines. Ablation studies and further analysis are also included to verify the effects of different components used in GLAT. 4.1 Experimental Settings Datasets We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs). These datasets are tokenized and segmented into subword units using BPE encodings (Sennrich et al., 2016). We preprocess WMT14 EN-DE by following the data preprocessing in Vaswani et al. (2017). For WMT16 EN-RO and IWSLT16 DE-EN, we use the processed data provided in Lee et al. (2018). Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets. We employ the transformer with the base setting in Vaswani et al. (2017) as the teacher for knowledge distillation. Then, we train our GLAT on distilled data. Baselines and Setup We compare our method with the base Transformer and strong represent"
2021.acl-long.155,P19-1125,1,0.913324,"t not selected. The training loss above is calculated against these remaining tokens. 1995 GLAT adopts similar encoder-decoder architecture as the Transformer with some modification (Figure 1d). Its encoder fenc is the same multihead attention layers. Its decoder fdec include multiple layers of multi-head attention where each layer attends to the full sequence of both encoder representation and the previous layer of decoder representation. During the initial prediction, the input to the decoder H = {h1 , h2 , ..., hT } are copied from the encoder output using either uniform copy or soft copy (Wei et al., 2019). The initial tokens Yˆ are predicted using argmax decoding with fdec (fenc (X; θ), H; θ). To calculate the loss LGLM , we compare the initial prediction Yˆ against the ground-truth to select tokens within the target sentence, i.e. GS(Y, Yˆ ). We then replace those sampled indices of h’s with corresponding target word embeddings, H 0 = RP(Embyt ∈GS(Y,Yˆ ) (yt ), H), where RP replaces the corresponding indices. Namely, if a token in the target is sampled, its word embedding replaces the corresponding h. Here the word embeddings are obtained from the softmax embedding matrix of the decoder. The"
2021.acl-long.19,N19-1423,0,0.0134957,"16) Katiyar and Cardie (2017) Li et al. (2019) Wang and Lu (2020) Zhong and Chen (2020) Zhong and Chen (2020) Relation R F1 Table 3: Overall evaluation.  means that the model leverages cross-sentence context information. relation type is correct, as well as the boundaries and types of two argument entities are correct. Implementation Details We tune all hyperparameters based on the averaged entity F1 and relation F1 on ACE05 development set, then keep the same settings on ACE04 and SciERC. For fair comparison with previous works, we use three pre-trained language models: bert-base-uncased (Devlin et al., 2019), albert-xxlarge-v1 (Lan et al., 2019) and scibert-scivocab-uncased (Beltagy et al., 2019) as the sentence encoder and fine-tune them in training stage.12 For the MLP layer, we set the hidden size as d = 150 and use GELU as the activation function. We use AdamW optimizer (Loshchilov and Hutter, 2017) with β1 = 0.9 and β2 = 0.9, and observe a phenomenon similar to (Dozat and Manning, 2016) in that setting β2 from 0.9 to 0.999 causes a significant drop on final performance. The batch size is 32, and the learning rate is 5e-5 with weight decay 1e-5. We apply a linear warm-up learning rate schedul"
2021.acl-long.19,doddington-etal-2004-automatic,0,0.124527,"spot that relation label in the same way). In other words, if the adjacent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upe"
2021.acl-long.19,P81-1022,0,0.584483,"Missing"
2021.acl-long.19,C16-1239,0,0.0333364,"Missing"
2021.acl-long.19,P17-1085,0,0.0146782,"can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorith"
2021.acl-long.19,P14-1038,0,0.0845167,"Missing"
2021.acl-long.19,P19-1129,0,0.0134684,"NI RE hard decoding Table 5: Comparison of accuracy and efficiency on ACE05 and SciERC test sets with different context window sizes. † denotes the approximation version with a faster speed and a worse performance. Table 4: Results (F1 score) with different settings on ACE05 and SciERC test sets. Note that we use BERTBASE on ACE05. achieves better performance. Besides, our model can achieve better relation performance even with worse entity results on ACE04. Actually, our base model (BERTBASE ) has achieved competitive relation performance, which even exceeds prior models based on BERTLARGE (Li et al., 2019) and ALBERTXXLARGE (Wang and Lu, 2020). These results confirm the proposed unified label space is effective for exploring the interaction between entities and relations. Note that all subsequent experiment results on ACE04 and ACE05 are based on BERTBASE for efficiency. 4.2 ACE05 Rel Speed (F1) (sent/s) Parameters Ablation Study In this section, we analyze the effects of components in U NI RE with different settings (Table 4). Particularly, we implement a naive decoding algorithm for comparison, namely “hard decoding”, which takes the “intermediate table” as input. The “intermediate table” is"
2021.acl-long.19,D18-1360,0,0.018109,"acent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upenn.edu/LDC2006T06 11 http://nlp.cs.washington.edu/sciIE/ 9 Datas"
2021.acl-long.19,N19-1308,0,0.27982,"4 and +1.7 for relation, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17."
2021.acl-long.19,P16-1105,0,0.140446,"asting research topic in NLP. Typically, it aims to recognize specific entities and relations for profiling the semantic of sentences. An example is shown in Figure 1, where a person entity “David Perkins” and a geography entity “California” have a physical location relation PHYS. Methods for detecting entities and relations can be categorized into pipeline models or joint models. In the pipeline setting, entity models and relation models are independent with disentangled feature spaces and output label spaces. In the joint setting, on the other hand, some parameter sharing of feature spaces (Miwa and Bansal, 2016; Katiyar and and David in Introduction ORG-AFF Figure 1: Example of a table for joint entity relation extraction. Each cell corresponds to a word pair. Entities are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predict"
2021.acl-long.19,D09-1013,0,0.0285477,"ility for each cell). “Decoded Table” presents the final results after decoding. Figure 6: Distribution of five relation extraction errors on ACE05 and SciERC test data. 5 it Related Work Entity relation extraction has been extensively studied over the decades. Existing methods can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et"
2021.acl-long.19,D14-1200,0,0.0952005,"input space is a two-dimensional table with each entry corresponding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural r"
2021.acl-long.19,P19-1131,1,0.956818,"l, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that with the help of mode"
2021.acl-long.19,D18-1249,1,0.848912,", 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem"
2021.acl-long.19,2020.emnlp-main.301,0,0.0348268,"Missing"
2021.acl-long.19,D17-1182,0,0.0334741,"Missing"
2021.acl-long.19,D19-1585,0,0.0713147,"ion, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17.8 113.0 W Z&C(2020) Z&C"
2021.acl-long.19,2020.emnlp-main.133,0,0.354126,"onding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural regularizations. Next, given the table filling with label log"
2021.acl-long.19,2020.emnlp-main.132,1,0.759126,"n between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified labe"
2021.acl-long.19,P13-1161,0,0.167846,"are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that wi"
2021.acl-long.19,P18-1047,0,0.0120585,"g method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified label space. We introduce a 2D table to tackle the overlapping relation problem in (Zheng et al., 2017). Also, our model is more versatile as not relying on complex expertise like (Wang et al., 2018), which requires external expert knowledge to design a complex transition system. 6 Conclusion In this work, we extract entities and relations in a unified label space to better mine the interaction between both sub-tasks. We propose a novel table that presents entities and relations as squares and rectangles. Then this task can"
2021.acl-long.19,P17-1113,0,0.110478,"ty models and relation models share encoders, usually their label spaces are still separate (even in models with joint decoders). Therefore, parallel to (Zhong and Chen, 2020), we would ask whether joint encoders (decoders) deserve joint label spaces? The challenge of developing a unified entityrelation label space is that the two sub-tasks are usually formulated into different learning problems (e.g., entity detection as sequence labeling, relation classification as multi-class classification), and their labels are placed on different things (e.g., words v.s. words pairs). One prior attempt (Zheng et al., 2017) is to handle both sub-tasks with one sequence labeling model. A compound label set was devised to encode both entities and relations. However, the model’s expressiveness is sacrificed: it can detect neither overlapping relations (i.e., entities participating in multiple relation) nor isolated entities (i.e., entities not appearing in any relation). Our key idea of defining a new unified label space is that, if we think Zheng et al. (2017)’s solution is to perform relation classification during entity labeling, we could also consider the reverse direction by seeing entity detection as a specia"
2021.acl-long.571,N19-1352,0,0.0167783,"racter-level vocabularies, it has shorter sentence lengths without rare words. Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on. Despite promising results, most existing subword approaches only consider frequency while the effects of vocabulary size is neglected. Thus, trial training is required to find the optimal size, which brings high computation costs. More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020). 7362 defined by the sum of token entropy. To avoid the effects of token length, here we normalize entropy with the average length of tokens and the final entropy is defined as: 12 10 Count 8 6 4 Hv = − 2 0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Spearman Score Figure 2: MUV and downstream performance are positively correlated on two-thirds of tasks. X-axis classifies Spearman scores into different groups. Y-axis shows the number of tasks in each group. The middle Spearman score is 0.4. 3 Marginal Utility of Vocabularization In this section, we propose to find a good vo"
2021.acl-long.571,D18-1461,0,0.0149653,"ier”. Compared to character-level vocabularies, it has shorter sentence lengths without rare words. Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on. Despite promising results, most existing subword approaches only consider frequency while the effects of vocabulary size is neglected. Thus, trial training is required to find the optimal size, which brings high computation costs. More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020). 7362 defined by the sum of token entropy. To avoid the effects of token length, here we normalize entropy with the average length of tokens and the final entropy is defined as: 12 10 Count 8 6 4 Hv = − 2 0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Spearman Score Figure 2: MUV and downstream performance are positively correlated on two-thirds of tasks. X-axis classifies Spearman scores into different groups. Y-axis shows the number of tasks in each group. The middle Spearman score is 0.4. 3 Marginal Utility of Vocabularization In this section, we propos"
2021.acl-long.571,P16-2058,0,0.0608468,"Missing"
2021.acl-long.571,N19-1423,0,0.0413431,"Missing"
2021.acl-long.571,W19-6620,0,0.324234,"he search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and pred"
2021.acl-long.571,D18-1443,0,0.0611372,"Missing"
2021.acl-long.571,D18-2012,0,0.0244114,"PU hours. 2 Related Work Initially, most neural models were built upon word-level vocabularies (Costa-juss`a and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019). While achieving promising results, it is a common constraint that word-level vocabularies fail on handling rare words under limited vocabulary sizes. Researchers recently have proposed several advanced vocabularization approaches, like bytelevel approaches (Wang et al., 2020), characterlevel approaches (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al., 2016; Kudo and Richardson, 2018). Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies. The general idea is to merge pairs of frequent character sequences to create sub-word units. Sub-word vocabularies can be regarded as a trade-off between character-level vocabularies and word-level vocabularies. Compared to word-level vocabularies, it can decrease the sparsity of tokens and increase the shared features between similar words, which probably have similar semantic meanings, like “happy” and “happier”. Compared to character-level vocabularies, it has shorter sentence lengths without r"
2021.acl-long.571,Q17-1026,0,0.171006,"Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account si"
2021.acl-long.571,W18-6301,0,0.283211,"ch, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus eas"
2021.acl-long.571,2020.acl-main.170,0,0.105947,"these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account since current approaches only consider frequency (or entropy) as the main criteria. Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks. Due to the lack of appropriate inductive bias about size, trial training (namely traversing all possible sizes) is usually required to search for the optimal size, which takes high computation costs. For convenience, most existing studies only adopt the widely-used settings in implementation. For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding et al., 2019). In this paper, we propose t"
2021.acl-long.571,N18-2084,0,0.0673002,"Missing"
2021.acl-long.571,P16-1162,0,0.352719,"are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabu"
2021.acl-long.571,P19-1021,0,0.0216056,"., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account since current approaches only consider frequency (or entropy) as the main criteria. Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks. Due to the lack of appropriate inductive bias about size, trial training (namely traversing all possible sizes) is usually required to search for the optimal size, which takes high computation costs. For convenience, most existing studies only adopt the widely-used settings in implementation. For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding"
2021.acl-long.571,N18-2074,0,0.0430914,"Missing"
2021.deelio-1.4,D18-2029,0,0.0125813,"owledge acquisition. Afterward, the knowledge retriever adopts another attention mechanism to obtain a cumulative knowledge representation K of the decreased knowledge set K. Finally, dialog context D, accumulated knowledge K, topic vector T and graph vector G are concatenated orderly and fed into a transformer decoder. Our transformer decoder will then attentively read the concatenated vector and generates an informative response. 1 33 https://github.com/thunlp/OpenNRE µi = (Wr ri )&gt; tanh(Wh hi + Wt ti ), (1) exp(µi ) ηi = Pm , j=1 exp(µj ) (2) G= m X ηi [hi ; ti ], sentence embedding layer (Cer et al., 2018) will first obtain sentence-level representations of Di and knowledge representation ki as Dis and kis . Next, given the predicted topic Ti+1 , we can pick Ti+1 related knowledge from the original knowledge set to form Ksmall . Our model then orderly attends on each knowledge candidate in Ksmall to generate a knowledge representation for the next turn as below: (3) i=1 where Wh , Wr , Wt are weight matrices for h, r, t, and [hi ; ti ] denotes the concatenated vector of hi and ti . 3.3 Encoder A shared transformer-based (Vaswani et al., 2017) encoder is employed to encode dialog utterances, kno"
2021.deelio-1.4,W19-4103,0,0.0200322,"2019). Although these methods have obtained promising results, they are facing two main issues. First, such models are agnostic to internal topic coherence, which usually leads to less logical conversations. Second, their conversation partners often have pre-defined roles or the external knowledge provided for these partners is usually symmetric, which could not reflect realworld chit-chats. Topic-aware Conversational models A variety of approaches proposed to leverage topic information by recognizing topic words inside or outside the previous utterances (Xing et al., 2017; Wang et al., 2018; Dziri et al., 2019). However, simply fusing topic words into text representations makes these models ignore logical relationships between topics and thus fail to perform smooth topic transitions. Except for applying attention mechanism on topic words, researchers have also investigated proactive conversation, whose topic transition and conversation development are conditioned on pre-scheduled chatting goals (Li et al., 2018; Wu et al., 2019; Xu et al., 2020). Nevertheless, these models are limited by pre-defined topic and goal sequences, hence not applicable to open-domain and open-topic conversations. Graph-enh"
2021.deelio-1.4,N15-1086,0,0.0777163,"Missing"
2021.deelio-1.4,N16-1014,0,0.0407574,"ed under symmetric knowledge settings (Young et al., 2018), or asymmetric settings with pre-defined roles (Dinan et al., 2018). Yet people usually have unequal personal knowledge prior to real-world conversations. Hence, such models cannot reflect the effect of information transferring Introduction Conversational AI, especially the open-domain dialog system, is an essential and challenging problem that leads to a variety of applications (Vinyals and Le, 2015; Serban et al., 2017). Previous works introduce external background knowledge to help their systems generate more informative responses (Li et al., 2016b; Dinan et al., 2018; Ghazvininejad et al., 2018; Young et al., 2018). However, these systems are facing a main issue that they tend to only utilize dialog utterances as queries to match appropriate knowledge sentences. Table 1 shows two responses corresponding to the same post. As can be seen, response1 changes the chatting topic 31 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 31–39 Online, June 10, 2021. ©2021 Association for Computational Linguistics and learning between two strangers, whic"
2021.deelio-1.4,P16-1094,0,0.0293089,"ed under symmetric knowledge settings (Young et al., 2018), or asymmetric settings with pre-defined roles (Dinan et al., 2018). Yet people usually have unequal personal knowledge prior to real-world conversations. Hence, such models cannot reflect the effect of information transferring Introduction Conversational AI, especially the open-domain dialog system, is an essential and challenging problem that leads to a variety of applications (Vinyals and Le, 2015; Serban et al., 2017). Previous works introduce external background knowledge to help their systems generate more informative responses (Li et al., 2016b; Dinan et al., 2018; Ghazvininejad et al., 2018; Young et al., 2018). However, these systems are facing a main issue that they tend to only utilize dialog utterances as queries to match appropriate knowledge sentences. Table 1 shows two responses corresponding to the same post. As can be seen, response1 changes the chatting topic 31 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 31–39 Online, June 10, 2021. ©2021 Association for Computational Linguistics and learning between two strangers, whic"
2021.deelio-1.4,P19-1002,0,0.0183605,"conversation model (DKGT). Given a dialogue context and a corresponding knowledge base, we first extract knowledge triples from each utterance and then jointly combine those triples through a static graph attention mechanism. Such logical information will then be fed into a topic predictor to predict the next chatting topic, which assists background knowledge selection and dialog generation. We further demonstrate the effectiveness of our method on Topical-Chat (Gopalakrishnan et al., 2019), comparing to several baselines. The main contributions of this paper can be wrapped as follows: 2018; Li et al., 2019). Although these methods have obtained promising results, they are facing two main issues. First, such models are agnostic to internal topic coherence, which usually leads to less logical conversations. Second, their conversation partners often have pre-defined roles or the external knowledge provided for these partners is usually symmetric, which could not reflect realworld chit-chats. Topic-aware Conversational models A variety of approaches proposed to leverage topic information by recognizing topic words inside or outside the previous utterances (Xing et al., 2017; Wang et al., 2018; Dziri"
2021.deelio-1.4,P19-1081,0,0.0206359,"smooth topic transitions. Except for applying attention mechanism on topic words, researchers have also investigated proactive conversation, whose topic transition and conversation development are conditioned on pre-scheduled chatting goals (Li et al., 2018; Wu et al., 2019; Xu et al., 2020). Nevertheless, these models are limited by pre-defined topic and goal sequences, hence not applicable to open-domain and open-topic conversations. Graph-enhanced Conversational Models Structured knowledge has been studied to improve dialog generation for a long time (Hixon et al., 2015; Zhou et al., 2018; Moon et al., 2019; Tuan et al., 2019). However, these models are mainly based on pre-defined knowledge graphs, which restrict their ability under symmetric knowledge settings. By contrast, our dynamic knowledge graphs enable our system to learn logical information through the conversation like humans, which facilitates both topic management and dialog generation. • To the best of our knowledge, this is the first attempt to dynamically mine logical relationships between chatting topics during a conversation to assist topic management, in the form of knowledge graphs. • The proposed model has two benefits: 1. Th"
2021.deelio-1.4,D19-1194,0,0.0178328,"tions. Except for applying attention mechanism on topic words, researchers have also investigated proactive conversation, whose topic transition and conversation development are conditioned on pre-scheduled chatting goals (Li et al., 2018; Wu et al., 2019; Xu et al., 2020). Nevertheless, these models are limited by pre-defined topic and goal sequences, hence not applicable to open-domain and open-topic conversations. Graph-enhanced Conversational Models Structured knowledge has been studied to improve dialog generation for a long time (Hixon et al., 2015; Zhou et al., 2018; Moon et al., 2019; Tuan et al., 2019). However, these models are mainly based on pre-defined knowledge graphs, which restrict their ability under symmetric knowledge settings. By contrast, our dynamic knowledge graphs enable our system to learn logical information through the conversation like humans, which facilitates both topic management and dialog generation. • To the best of our knowledge, this is the first attempt to dynamically mine logical relationships between chatting topics during a conversation to assist topic management, in the form of knowledge graphs. • The proposed model has two benefits: 1. The dynamic built KG c"
2021.deelio-1.4,P19-1369,0,0.0307555,"ck suitable knowledge to generate informative responses comparing to several strong baselines. 1 Table 1: Example responses generated by two models. abruptly and thus becomes incoherent. By contrast, response2 first manages to deepen the current topic &quot;William Shakespeare&quot;, then picks a suitable knowledge candidate to generate an engaging response. Therefore, a good topic managing strategy is also very crucial to dialog generation. To solve this problem, some papers propose to plan a set of conversational topics as chatting goals in advance to boost knowledge matching and response generation (Wu et al., 2019; Xu et al., 2020). However, it is difficult to schedule an appropriate topic transition route beforehand since topics are switching dynamically during a chit-chat based on many real-time factors, especially when two partners have different personal knowledge. Hence, these methods could not pre-schedule a topic at each turn properly and thus becoming nonattractive. Another problem these knowledge-grounded or topic-enhanced models might encounter is that they are typically tested under symmetric knowledge settings (Young et al., 2018), or asymmetric settings with pre-defined roles (Dinan et al."
2021.eacl-main.251,P19-1129,0,0.0317583,"Missing"
2021.eacl-main.251,N19-1308,0,0.194291,"epresentations for Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four no"
2021.eacl-main.251,P16-1105,0,0.0798562,"(entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The training set is automatically annotated by distant supervision. While the validation and testing data are manually labeled by (Jia et al., 2019). We choose the latest version of NYT released by (Jia et al., 2019). Evaluation. As previous works (Miwa and Bansal, 2016; Sun et al., 2019a), we evaluate the 6 https://github.com/tticoin/LSTM-ER http://nlp.cs.washington.edu/sciIE/ 8 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-ARNOR/ 2881 7 Model Sun, 2019a Li, 2019 Luan, 2019?, ◦ Wadden, 2019 , ◦ E N PA R  Entity Relation Relation (exactly) 84.2 84.8 88.4 88.6 86.9 – – 63.2 63.4 66.1 59.1 60.2 – – 63.5 Table 1: Results on the ACE05 test data.  means that the model uses BERT. ? means that the model uses ELMo as token embeddings. ◦ stands for training the model with multi-task learning. E N PA R is the proposed model fine-t"
2021.eacl-main.251,P19-1131,1,0.71528,"roduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method customized for entity relation extraction. E N PA R consists of an underlying sentence encoder, an entity encoder, and an entity pair encoder. Compared with BERT"
2021.eacl-main.251,D18-1249,1,0.853386,"1080 Ti GPU. 3 Experiments We conduct experiments on three benchmark entity relation extraction datasets: ACE05, SciERC, and NYT. For space limitation, we will mainly discuss the results on ACE05 and report basic results on the remaining two datasets. ACE05 The ACE05 dataset 6 that is a standard corpus for entity relation extraction task annotates entity and relation labels for a collection of documents. ACE05 contains 7 entity types and 6 relation types. We use the same data split and preprocessing of ACE05 dataset (351 training, 80 validating and 80 testing) as (Miwa and Bansal, 2016) and (Sun et al., 2018). SciERC The SciERC dataset 7 annotates entity, coreference and relation labels for 500 scientific abstracts from 12 AI conference/workshop proceedings. We only use the annotations of entities and relations. SciERC contains 6 scientific term (entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The tra"
2021.eacl-main.251,D19-1585,0,0.273858,"Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e.,"
2021.eacl-main.251,P13-1161,0,0.0234792,"tperforms BERT on entity performance and relation performance. This again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we"
2021.eacl-main.251,P19-1139,0,0.226172,"ntences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method custo"
2021.eacl-main.251,P17-1113,0,0.0199405,"s again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we restrict the joint model of parameters sharing, which can be ea"
2021.emnlp-main.184,P19-1081,0,0.0234423,"dels EARL can generate informative responses with both seen knowledge graphs and unseen Some prior works introduce high-quality structured knowledge graphs in two benchmark datasets. knowledge graph for conversation generation. Zhu Ablation studies demonstrate the influence of et al. (2017) presented an end-to-end knowledge different mechanisms and conversation frame- grounded conversation model using a copy networks. work (Gu et al., 2016). A large-scale commonsense 2384 knowledge graph is introduced to open-domain conversation generation by graph attention mechanisms in (Zhou et al., 2018). Moon et al. (2019) proposed a knowledge graph walker to select relevant entities of the knowledge graph to improve the performance of retrieval-based conversation models. The adjacency matrix (Tuan et al., 2019) is introduced to modeling the dynamic knowledge graph in conversation generation. However, these studies adopt pre-trained knowledge graph embeddings (Zhou et al., 2018), word embeddings (Wu et al., 2019), or the adjacency matrix (Tuan et al., 2019) to represent knowledge triples, making them not applicable for large-scale and unseen knowledge graphs. By contrast, our model addresses this issue by repre"
2021.emnlp-main.184,Y14-1039,0,0.0305778,"ion and the structure informaEARL consists of three modules: an encoder to tion of the knowledge graph. Entity-agnostic repreconvert the context to the hidden representations, sentations are defined as category representations a knowledge interpreter to represent each subject for entities sharing the same context and structure and object entity based on the context and structure information, including two major circumstances. information, and a decoder to generate a token or One is caused by the one-to-many mapping propselect an entity from the knowledge graph deter- erty of knowledge graphs (Fan et al., 2014; Xiao mined by a knowledge selector. The overview of et al., 2016), where a subject has multiple objects EARL is presented in Figure 3. with the same relation. As shown in the left knowlInstead of parameterizing specific representa- edge graph in Figure 1, (Chuck Palahniuk, Write, tions for entities of knowledge graphs as used in Pygmy) and (Chuck Palahniuk, Write, Tell-All) have prior studies (Zhou et al., 2018; Wu et al., 2019; the one-to-many mapping property, and EARL Tuan et al., 2019), EARL learns entity-agnostic rep- learns the same category representation for Pygmy resentations condit"
2021.emnlp-main.184,C16-1316,0,0.0150829,"2 2.1 Related Work Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can ge"
2021.emnlp-main.184,P16-1154,0,0.0249314,"k Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can generate informative"
2021.emnlp-main.184,W15-4616,0,0.0158617,"lity of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can generate informative responses with both seen knowledge graphs and unseen Some prior works introduce high-quality structured knowledge graphs in two benchmark datasets. knowledge graph for conversation generation. Zhu Ablation studies demonstrate the influence of et al. (2017) presented an end-to-end knowledge different mechanisms and conversation frame- grounded conversation model using a copy networks. work (Gu et al., 2016). A large-scale commonsense"
2021.emnlp-main.184,D16-1230,0,0.0290225,"p performances in automatic evaluation. For each response pair, three judges were hired to give a preference between the two responses in terms of the following two metrics. The tie was allowed. Notice that system identifiers were masked during annotation. Metrics: We adopted two widely used metrics, Appropriateness and Informativeness as proposed in (Zhou et al., 2018). Appropriateness measures the quality of the generated response at the 4 BLEU (Papineni et al., 2002) is not adopted due to its low content level (whether the response is appropricorrelation with human judgment, as proposed by Liu et al. (2016). ate in relevance, coherence, and adequacy). In2389 Dataset Model Entity Precision Recall F1 Distinct-3 Distinct-4 PPL DuConv Seen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.068 0.141 0.195 0.131 0.650 0.655 1.269 0.020 0.054 0.084 0.051 0.399 0.376 0.435 0.013 0.036 0.062 0.036 0.396 0.392 0.478 0.015 0.041 0.068 0.040 0.379 0.365 0.422 0.128 0.078 0.179 0.135 0.255 0.239 0.379 0.201 0.125 0.278 0.232 0.378 0.350 0.519 20.54 9.94 19.88 25.30 15.63 20.71 17.00 DuConv Unseen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.062 0.133 0.195 0.110 0.684 0.686 1.310 0.020"
2021.emnlp-main.184,P02-1040,0,0.114287,"e generated by EARL and the one by a baseline for the same context. In total, there are 1,200 pairs since we chose three baselines, which achieve top performances in automatic evaluation. For each response pair, three judges were hired to give a preference between the two responses in terms of the following two metrics. The tie was allowed. Notice that system identifiers were masked during annotation. Metrics: We adopted two widely used metrics, Appropriateness and Informativeness as proposed in (Zhou et al., 2018). Appropriateness measures the quality of the generated response at the 4 BLEU (Papineni et al., 2002) is not adopted due to its low content level (whether the response is appropricorrelation with human judgment, as proposed by Liu et al. (2016). ate in relevance, coherence, and adequacy). In2389 Dataset Model Entity Precision Recall F1 Distinct-3 Distinct-4 PPL DuConv Seen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.068 0.141 0.195 0.131 0.650 0.655 1.269 0.020 0.054 0.084 0.051 0.399 0.376 0.435 0.013 0.036 0.062 0.036 0.396 0.392 0.478 0.015 0.041 0.068 0.040 0.379 0.365 0.422 0.128 0.078 0.179 0.135 0.255 0.239 0.379 0.201 0.125 0.278 0.232 0.378 0.350 0.519 20.54 9.94 19.88"
2021.emnlp-main.184,P15-1152,0,0.0217111,"te The Selfish Gene. Figure 1: Conversation samples generated with welltrained knowledge graphs (left) and unseen knowledge graphs (right). Grey nodes are well-trained entities, and white nodes are unseen nodes in the training data. Entities in the blue rectangle share the same entity-agnostic representation (see Section 3.2 for more details). 2 2.1 Related Work Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative r"
2021.emnlp-main.184,N15-1020,0,0.0775017,"Missing"
2021.emnlp-main.184,P16-1008,0,0.0217122,"object entity, respectively. Although aforementioned methods is able to represent the relevant entities related to the context, it cannot represent entities, which are not mentioned in the context or not connected to the subject entity in the context with any path in the knowledge graph. In this case, we resort to represent the entity i with Nri relations connected to it by graph attention based on the hidden state hX of the context, which is formulated as follows: Nri X is designed to allow the decoder to select object entities from knowledge graphs or words from the vocabulary. Inspired by Tu et al., 2016, we also introduce a coverage mechanism to facilitate the decoder to avoid generating repetitive entities. The decoding process is formulated as follows: (11) where gt ∈ [0, 1] is a scalar to balance the choice between an entity obj i and a generic word wg , Pg /Pe is the distribution over generic words / entities respectively, and P (yt ) is the final word decoding distribution. 3.6 Loss Function The loss function is the cross entropy between the predicted token distribution P (yt ) and the groundtruth distribution pt in the training corpus. Additionally, we apply supervised signals on the k"
2021.emnlp-main.184,D19-1194,0,0.200814,"incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models. KG Freebase Wikidata ConceptNet # Entities # Triples # Relations 40M 18M 8M 637M 66M 21M 35,000 1,632 36 Table 1: Statistics of some widely used knowledge graphs (KG, Knowledge Graph; M, million). Prior studies adopt either pre-trained knowledge graph embeddings (Zhou et al., 2018), e.g. TransE (Bordes et al., 2013), word embeddings (Wu et al., 2019), or adjacency matrix (Tuan et al., 2019) to model entities and relations in knowledge graphs and incorporate them to conversation generation. These models face two major challenges when applied to introduce large-scale knowledge graphs. First, there is a significant gap in representations between knowledge and text (Buitelaar and Cimiano, 2008; Zhou et al., 2018), which requires model training to apply knowledge in conversation generation based on different knowledge representations. However, the training corpus of knowledgegrounded conversations only contains a small subset of entities for applying knowledge, while the 1 Introducti"
2021.emnlp-main.184,P19-1369,0,0.356449,"for entities, which is generalized to incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models. KG Freebase Wikidata ConceptNet # Entities # Triples # Relations 40M 18M 8M 637M 66M 21M 35,000 1,632 36 Table 1: Statistics of some widely used knowledge graphs (KG, Knowledge Graph; M, million). Prior studies adopt either pre-trained knowledge graph embeddings (Zhou et al., 2018), e.g. TransE (Bordes et al., 2013), word embeddings (Wu et al., 2019), or adjacency matrix (Tuan et al., 2019) to model entities and relations in knowledge graphs and incorporate them to conversation generation. These models face two major challenges when applied to introduce large-scale knowledge graphs. First, there is a significant gap in representations between knowledge and text (Buitelaar and Cimiano, 2008; Zhou et al., 2018), which requires model training to apply knowledge in conversation generation based on different knowledge representations. However, the training corpus of knowledgegrounded conversations only contains a small subset of entities for a"
2021.emnlp-main.184,P16-1219,1,0.873385,"Missing"
2021.emnlp-main.184,2020.acl-demos.30,0,0.202847,"ns some noisy data, e.g. empty utterances in the dialogue. After filtering the noisy data, we randomly split the corpus in the same way as DuConv. The statistics is presented in Table 2. Conversations Training 14,845 Validation 1,800 Seen 900 Test Unseen 900 Training 10,583 Validation 1,200 Seen 600 Test Unseen 600 Knowledge Graphs Entity 12,909 Relation 39 Triple 113,959 Entity Relation 100,717 1,380 Triple 1,172,552 Table 2: Statistics of datasets and knowledge graphs. (Mikolov et al., 2010), which is widely used in open-domain conversation systems. • DIALOGPT: a pre-trained dialogue model (Zhang et al., 2020; Wang et al., 2020) based on transformers, which is widely adopted in dialogue generation. • MemNet: a knowledge-grounded model adapted from (Ghazvininejad et al., 2018), of which the memory units store word embeddings of knowledge triples. • PostKS: a knowledge-grounded model selecting knowledge by prior and posterior distributions proposed by Wu et al. (2019), where we adopt word embeddings, instead of the RNN knowledge encoder, to represent knowledge triples. • CopyNet: a copy network model (Zhu et al., 2017), which represents knowledge triples by word embeddings, and can copy words from k"
2021.emnlp-main.95,P16-1231,0,0.0230795,"y carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regulari"
2021.emnlp-main.95,N19-1423,0,0.0500728,"Missing"
2021.emnlp-main.95,P15-1061,0,0.0144566,"o the input structure, we can divide the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power-"
2021.emnlp-main.95,D18-1247,0,0.02449,"n extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., en"
2021.emnlp-main.95,P19-1423,0,0.017814,"the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the"
2021.emnlp-main.95,D12-1110,0,0.0840567,"dependencies. According to the input structure, we can divide the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus ma"
2021.emnlp-main.95,P16-1228,0,0.0103035,"tability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion extraction by learning logic rules. The model learning approaches. These efforts demonstrated can explicitly capture dependencies between entithe effectiveness of integrating neural networks ties and output relations, while enjoy better interwith logical reasoning. Despite doc-RE providing pretation. a suitable scenario for logical reasoning (with rela• We propose an efﬁcient iterative-based method tions serving as predicates and entities a"
2021.emnlp-main.95,P19-1131,1,0.902418,"(i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the identiﬁcation of royalty_of(Kate,UK) requires information in all three sentences. The demonstrated logic rule can be appli"
2021.emnlp-main.95,2021.ccl-1.108,0,0.0374173,"Missing"
2021.emnlp-main.95,D18-1249,1,0.795996,"g graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion"
2021.emnlp-main.95,2020.acl-main.141,0,0.0182973,"powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducte"
2021.emnlp-main.95,W18-2314,0,0.0229799,"works, LogiRE can explicitly capture long-range dependencies between entities and output relations in a document and enjoy better interpretation. Our main contributions are listed below: to optimize LogiRE based on the EM algorithm. • Empirical results show that LogiRE signiﬁcantly outperforms several strong baselines in terms of relation performance (∼1.8 F1 score) and logical consistency (over 3.3 logic score). 2 Related Work For document-level relation extraction, prior efforts on capturing long-range dependencies mainly focused on two directions: pursuing stronger sequence representation (Nguyen and Verspoor, 2018; Verga et al., 2018; Zheng et al., 2018) or including prior for interactions among entities as graphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in"
2021.emnlp-main.95,Q17-1008,0,0.0233666,"g et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent"
2021.emnlp-main.95,N18-1080,0,0.0175289,"y capture long-range dependencies between entities and output relations in a document and enjoy better interpretation. Our main contributions are listed below: to optimize LogiRE based on the EM algorithm. • Empirical results show that LogiRE signiﬁcantly outperforms several strong baselines in terms of relation performance (∼1.8 F1 score) and logical consistency (over 3.3 logic score). 2 Related Work For document-level relation extraction, prior efforts on capturing long-range dependencies mainly focused on two directions: pursuing stronger sequence representation (Nguyen and Verspoor, 2018; Verga et al., 2018; Zheng et al., 2018) or including prior for interactions among entities as graphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (C"
2021.emnlp-main.95,2020.emnlp-main.303,0,0.161834,"raphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is l"
2021.emnlp-main.95,D18-1215,0,0.0247203,"mplicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion extraction by learning logic rules. The model learning approaches. These efforts demonstrated can explicitly capture dependencies between entithe effectiveness of integrating neural networks ties and output relations, while enjoy better interwith logical reasoning. Despite doc-RE providing pretation. a suitable scenario for logical reasoning (with rela• We propose an efﬁcient iterative-based method tions serving as predicates and entities as variables), 1240 Fig"
2021.emnlp-main.95,2021.eacl-main.251,1,0.837568,"Missing"
2021.emnlp-main.95,2021.acl-long.19,1,0.822672,"Missing"
2021.emnlp-main.95,2020.emnlp-main.453,0,0.0620715,"Missing"
2021.emnlp-main.95,P19-1074,0,0.229234,"de the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggreg"
2021.emnlp-main.95,2020.emnlp-main.582,0,0.11188,"ant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference"
2021.emnlp-main.95,2020.emnlp-main.127,1,0.915686,"r (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the identiﬁcation of royalty_of(Kate,UK) requires information in all three"
2021.emnlp-main.95,D18-1244,0,0.0198302,"into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) represent"
2021.findings-acl.242,D19-1252,0,0.0463746,"Missing"
2021.findings-acl.242,2020.inlg-1.14,0,0.0284611,"Missing"
2021.findings-acl.242,D18-1208,0,0.0318748,"Missing"
2021.findings-acl.242,N19-1423,0,0.00584685,". We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising t"
2021.findings-acl.242,D18-2012,0,0.0146274,"tible at home since the start of the season, Manchester City crushed Arsenal (6-3) in the game at the top of the Premier League. [q] The Mancuniens are three points behind the Gunners at the top of the standings.) Table 1: A Fr example of our dataset. The text in brackets is the corresponding English translation. The sentences are separated by ‘[q]’. a Transformer-based architecture (Vaswani et al., 2017) with 12 layers of encoder and 12 layers of the decoder. The hidden size is 1024 with 16 attention heads. mBART covers 25 languages and shares the vocabulary with the sentencepiece tokenizer (Kudo and Richardson, 2018), which includes 250,000 subword tokens. We follow the language indicators with mBART, and change its position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other"
2021.findings-acl.242,2020.acl-main.703,0,0.222467,"also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lack"
2021.findings-acl.242,P18-2027,0,0.0288599,"e multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models focus on training one model for different language or partly share encoder/decoder layers (Wang et al., 2018; Lin et al., 2018; Scialom et al., 2020). Cao et al. (2020) and Lewis et al. (2020a) try to train one model for all languages, but they find that although low-resource languages can benefit from the larger training data, the performance of rich-resource languages has been sacrificed. Thus, we want to investigate the following question: Can we design a unified multilingual summarization model that can benefit both high-resource and low-resource languages? In this paper, we design a neural model with the contrastive aligned joint learning strategy for multilingual summarization (CALMS) with two new training obje"
2021.findings-acl.242,2020.emnlp-main.210,1,0.791666,"tion dataset used in our experiment and the (3) experimental settings. (l ) k where si,pos is the score of the positive candidate of (lk ) si,neg j the i-th example in language lk , and is the j-th negative candidate for i-th example. We use a linear layer with sigmoid function to get the score from the masked hidden state of the last layer of the encoder.  is a hyper-parameter for the margin distance. 3.3 Sentence Aligned Substitution Training with multiple languages makes it possible to share the representative space across languages and obtain a universal representation for summarization. Lin et al. (2020) randomly replaces words with a different language during the pre-training phase for machine translation. However, the input for summarization is longer than sentence-level machine translation and the single word replacement shows little influence (Kedzie et al., 2018). Thus, we propose sentence aligned substitution (SAS) for summarization. We take lead sentences rather than randomly sampling from the document because these sentences are more important in the summarization task. We use an extra translation tool 1 to translate our sentences into another language to get the aligned information."
2021.findings-acl.242,D19-1387,0,0.0806602,"to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both o"
2021.findings-acl.242,2020.tacl-1.47,0,0.275213,"It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models"
2021.findings-acl.242,D18-1206,0,0.016562,"than 40 languages and each article is written by native authors. France24 is an international news website with 4 languages and faz is a German website. All of these websites have a highlight written by the editor at the beginning of the news article to summarize the main idea, which can be viewed as the summary. This information can be easily extracted through the HTML tag (’storybody introduction’ in BBC, ’t-content chapo’ in france24, ’atc-IntroText’ in faz). We collect MLGSum mainly from BBC and use france24 to expand French, English, and Spanish. Faz is used for German. Similar to XSum (Narayan et al., 2018) and Newsroom (Grusky et al., 2018), we provide the Wayback archived URL of each article and the processing script to release MLGSum. The Wayback Machine9 is an initiative of the Internet Archive, building a digital library of Internet sites that archive billions of web pages. We search news articles ranging from 2010 to 2020 for the above websites. We emphasize that the intellectual property and privacy rights of the articles belong to the original authors and the corresponding website. We carefully check the terms of use, privacy policy, and copyright policy10 of the Internet Archive and the"
2021.findings-acl.242,D19-5411,0,0.046841,"Missing"
2021.findings-acl.242,2020.emnlp-main.647,0,0.042479,"Missing"
2021.findings-acl.242,2020.acl-main.553,1,0.784845,"that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-"
2021.findings-acl.242,2020.emnlp-main.294,0,0.0855807,"Missing"
2021.findings-acl.242,2020.acl-main.552,1,0.909487,"tal results indicate that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization task"
2021.findings-acl.242,N19-4009,0,0.0194461,"position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other four languages with equal probability and substitute sentences with a ratio r = 0.2. We use fairseq6 (Ott et al., 2019) to implement the architecture. We limit the max tokens to 2048 for each GPU and set the gradient accumulation to 4. The Adam optimizer (Kingma and Ba, 2015) is 6 https://github.com/pytorch/fairseq Language Size Doc. Summ. Train De En Ru Fr Zh 494,514 191,365 87,125 85,030 65,203 457 476 499 463 799 27 24 24 36 56 445,062 172,228 78,412 76,527 58,682 Hi Es Id Tr Vi Uk Pt 59,145 43,162 35,495 26,539 26,539 33,214 20,945 565 703 360 342 847 444 927 28 30 21 20 34 21 34 53,230 38,845 31,945 33,047 23,885 29,892 18,850 Total 1,168,276 573.5 29.6 1,060,605 Table 2: The dataset statistic. Doc. and S"
2021.findings-acl.277,D19-5808,0,0.0145005,"st partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof gen"
2021.findings-acl.277,2021.ccl-1.108,0,0.021698,"Missing"
2021.findings-acl.277,W09-3714,0,0.0303451,", 2005; Berant et al., 2013; Berant and Liang, 2014), researchers focus on developing theorem provers by combining the symbolic techniques with the differentiable learning from neural networks (Reed and de Freitas, 2016; Abdelaziz et al., 2020; Abboud et al., 2020), such as NLProlog (Weber et al., 2019), SAT solving (Selsam et al., 2019) and Neural programme (Neelakantan et al., 2016). To bypass this expensive and error-prone intermediate logical representation, reasoning over natural language statements in an end-to-end manner is promising. Text Reasoning over Natural Language Natural logic (MacCartney and Manning, 2009) focuses on semantic containment and monotonicity by incorporating semantic exclusion and implicativity. Subsequently, Clark et al. (2020) proposes to use a Transformer-based model to emulate deductive reasoning and achieves high accuracy on synthetically generated data. PROVER (Saha et al., 2020) 7 For more details, please refer to the supplementary materials. 3147 PROBR PROBR+GOLD PROBR+KL PROBR+GOLD+KL Figure 3: QA accuracy compared among PROBR, PROBR + Gold, PROBR + KL, and PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity datase"
2021.findings-acl.277,2020.emnlp-main.9,0,0.466799,"of to prove or disprove the query. For example, in Figure 1, there are two facts, six rules and two queries, each of which is expressed by natural language. To predict the true/false of each query, starting from the facts, we need to reason deductively by applying given rules ∗ Equal contribution. until we can derive the truth value of the query. The process of deduction can be represented as a graph, whose node is either a fact, rule or special NAF node (explained in the Section 2.1). Generating answer and proof together makes a system easier to interpret and diagnose. Recent work by PROVER (Saha et al., 2020) first explored this problem through two modules: question answering and proof generation. It trains these two modules through implicit parameter sharing, and then uses integer linear programming (ILP) to enforce consistency constraints (only test time). It is difficult to ensure that the proof generation module contributes to the question answering module, because the proof is not explicitly involved in the answer prediction. Parameter sharing becomes more limited under few/zero-shot settings, as demonstrated in our experiments. We expect the proof to enhance the capability of question answer"
2021.findings-acl.277,D19-1608,0,0.0134592,"nd PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint questi"
2021.findings-acl.277,P19-1618,0,0.0627666,"Missing"
2021.findings-acl.277,D18-1259,0,0.0249411,"test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof generation. PROBR defines a joint dis"
2021.findings-acl.30,J05-3002,0,0.0708098,"ment summarization research. Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as"
2021.findings-acl.30,P15-1153,0,0.0241395,"y similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extrac"
2021.findings-acl.30,N13-1136,0,0.0830578,"Missing"
2021.findings-acl.30,2020.acl-main.457,0,0.0172432,"ons across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summariz"
2021.findings-acl.30,N19-1423,0,0.0301625,"input embedding matrix and the matrix Wo to reuse linguistic knowledge (Paulus et al., 2018). We further add a copy mechanism as proposed by See et al. (2017). 3.6 Training Our training process follows that of the traditional sequence-to-sequence modeling, with maximum likelihood estimation that minimizes: 1 X Lseq = − log p(y|x; θ) (17) |D| (y,x)∈D where x and y are document-summary pairs from training set D, and θ are parameters to be learned. 3.7 Pre-trained LMs as Document Encoder Our document encoder illustrated in section 3.3 can be replaced by a pre-trained language model such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Pre-trained language models can be more effective on short inputs than training stacked transformer layers from scratch. We feed input tokens to a pre-trained language model and take the last layer output as token embeddings. Then a single-layer bidirectional LSTM is employed over token embeddings, producing token features. Finally, we perform the same multi-head pooling strategy to obtain paragraph representations. 4 4.1 Experimental Setup We conduct experiments on two major datasets used in the literature of multi-document summarization, namely WikiSum (Liu e"
2021.findings-acl.30,P19-1102,0,0.0174963,"19). Pre-trained language models can be more effective on short inputs than training stacked transformer layers from scratch. We feed input tokens to a pre-trained language model and take the last layer output as token embeddings. Then a single-layer bidirectional LSTM is employed over token embeddings, producing token features. Finally, we perform the same multi-head pooling strategy to obtain paragraph representations. 4 4.1 Experimental Setup We conduct experiments on two major datasets used in the literature of multi-document summarization, namely WikiSum (Liu et al., 2018) and MultiNews (Fabbri et al., 2019). WikiSum Dataset Liu et al. (2018) treat the generation of Wikipedia section titles as a supervised multi-document summarization task. Liu and Lapata (2019a) crawled Wikipedia articles and source reference documents through the provided urls. They further split the long and messy source documents into multiple paragraphs by line-breaks and select the top-40 paragraphs as input for summarization systems. However, the top-40 dataset is quite heavy for entity extraction and co-reference resolution. Experiment shows that the ROUGE-L recall of top-20 paragraphs against the gold target text is 53.8"
2021.findings-acl.30,D19-1428,0,0.0135799,"ormation and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extrac"
2021.findings-acl.30,D08-1019,0,0.0553414,"Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language"
2021.findings-acl.30,W18-2501,0,0.0160934,"e words as semantic units in addition to sentence nodes, acting as the intermediary to enrich the relationships between sentences. However, we argue that word-level semantic units are too fine and will bring huge computational costs. For multi-document summarization, models are usually required to process tens of documents. The total number of words will be vast, which further causes a hindrance for the graph construction and message passing process. Therefore, we use entity clusters as more advanced semantic units. We utilize the co-reference resolution tool (Lee et al., 2017) from AllenNLP (Gardner et al., 2018) to extract entity clusters. Note that we perform extraction globally, which means we concatenate all the documents into one long document. We denote the extracted entity clusters as C = {C1 , C2 , . . . , Cm }, where Ci = {mention1 , mention2 , . . . , mentionl }, and l is 353 Figure 2: Overall architecture of our model. output is xlw . the number of entity mentions in cluster Ci . 3.2 l−1 hlw = LayerNorm(xl−1 w + MHAttn(xw )) (1) Graph Construction Given a source document cluster D, we firstly divide them into smaller semantic units P = {P1 , P2 , . . . , Pn }, such as paragraphs and sentenc"
2021.findings-acl.30,D18-1443,0,0.0124164,"arization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). T"
2021.findings-acl.30,P17-1147,0,0.0204335,"ous work (Liu and Lapata, 2019a), the evaluation score takes three aspects into account: (1) Informativeness: does the summary include salient 358 Dataset WikiSum MultiNews Model FT T-DMCA HT EMSum FT T-DMCA HS EMSum sults. Experiments on standard datasets show the effectiveness of our model. In the future, we would like to explore other approaches such as reinforcement learning based methods (Sharma et al., 2019) to further improve the summary quality in the context of multidocument summarization. We would also like to apply our method to other tasks such as multidocument question answering (Joshi et al., 2017). Rating -0.517 -0.117 0.250 0.383 -0.650 -0.033 0.317 0.367 Table 5: Human evaluation results on summary quality rating. FT, T-DMCA, HT, HS are baseline models explained in Section 4.2. parts of the input? (2) Fluency: Is the summary fluent and grammatical? (3) Succinctness: does redundancy occur in the summary? We used BestWorst Scaling (Louviere et al., 2015) because it has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Annotators are presented with the gold summary and summaries generated from 3 out of 4 systems and decide which summary is"
2021.findings-acl.30,P17-2074,0,0.021197,"context of multidocument summarization. We would also like to apply our method to other tasks such as multidocument question answering (Joshi et al., 2017). Rating -0.517 -0.117 0.250 0.383 -0.650 -0.033 0.317 0.367 Table 5: Human evaluation results on summary quality rating. FT, T-DMCA, HT, HS are baseline models explained in Section 4.2. parts of the input? (2) Fluency: Is the summary fluent and grammatical? (3) Succinctness: does redundancy occur in the summary? We used BestWorst Scaling (Louviere et al., 2015) because it has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Annotators are presented with the gold summary and summaries generated from 3 out of 4 systems and decide which summary is the best and which is the worst based on the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from -1 (worst) to 1 (best). On the WikiSum dataset, we choose FT, TDMCA, HT, EMSum and conduct human evaluation to compare their performance. On the MultiNews dataset, we choose FT, T-DMCA, HS, together with EMSum. The results are shown in Table 5. These resu"
2021.findings-acl.30,D18-1446,0,0.03189,"Missing"
2021.findings-acl.30,D17-1018,0,0.0278375,"Extraction Wang et al. (2020a) use words as semantic units in addition to sentence nodes, acting as the intermediary to enrich the relationships between sentences. However, we argue that word-level semantic units are too fine and will bring huge computational costs. For multi-document summarization, models are usually required to process tens of documents. The total number of words will be vast, which further causes a hindrance for the graph construction and message passing process. Therefore, we use entity clusters as more advanced semantic units. We utilize the co-reference resolution tool (Lee et al., 2017) from AllenNLP (Gardner et al., 2018) to extract entity clusters. Note that we perform extraction globally, which means we concatenate all the documents into one long document. We denote the extracted entity clusters as C = {C1 , C2 , . . . , Cm }, where Ci = {mention1 , mention2 , . . . , mentionl }, and l is 353 Figure 2: Overall architecture of our model. output is xlw . the number of entity mentions in cluster Ci . 3.2 l−1 hlw = LayerNorm(xl−1 w + MHAttn(xw )) (1) Graph Construction Given a source document cluster D, we firstly divide them into smaller semantic units P = {P1 , P2 , . . . ,"
2021.findings-acl.30,2020.acl-main.703,0,0.0327088,"Missing"
2021.findings-acl.30,D15-1219,0,0.0204091,"ively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-l"
2021.findings-acl.30,2020.acl-main.555,0,0.177398,"eam sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are playing unique role"
2021.findings-acl.30,D18-1441,0,0.0162434,"equires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further fi"
2021.findings-acl.30,W04-1013,0,0.12547,"undancy explicitly. The mechanism can also reduce the computational cost, making it easier to process long inputs. • Our model achieves state-of-the-art results on WikiSum and MultiNews. Extensive analysis including ablation studies show the effectiveness of our model.1 1 2 2.1 Related Work Abstractive Document Summarization Abstractive summarization is often regarded as the ultimate goal of document summarization research. Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pi"
2021.findings-acl.30,P19-1500,0,0.0892904,"marization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are"
2021.findings-acl.30,D19-1387,0,0.100233,"marization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are"
2021.findings-acl.30,2021.ccl-1.108,0,0.0516067,"Missing"
2021.findings-acl.30,D18-1206,0,0.0226576,"on which is composed of an entity-aware content selection module and a summary generation module. By contrast, our EMSum model is an end-to-end method for multi-document summarization. Gunel et al. (2020) inject structural world knowledge from Wikidata to a transformer-based model, enabling the model to be more fact-aware. Zhu et al. (2020) extract factual relations from the source texts to build a local knowledge graph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that incorporate topic information into summarization model. Narayan et al. (2018) recommend an encoder associating each word with a topic vector capturing whether it is representative of the document’s content, and a decoder where each word prediction is conditioned on a document topic vector. Zheng et al. (2019) propose to mine cross-document subtopics. In their work, sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience. Perez-Beltrachini et al. (2019) explicitly model the topic structure of summaries, and utilize it to guide a structured convolutional decoder. Wang et al. (2020b) rearrange and further explore the sema"
2021.findings-acl.30,P19-1504,0,0.0116176,"aph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that incorporate topic information into summarization model. Narayan et al. (2018) recommend an encoder associating each word with a topic vector capturing whether it is representative of the document’s content, and a decoder where each word prediction is conditioned on a document topic vector. Zheng et al. (2019) propose to mine cross-document subtopics. In their work, sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience. Perez-Beltrachini et al. (2019) explicitly model the topic structure of summaries, and utilize it to guide a structured convolutional decoder. Wang et al. (2020b) rearrange and further explore the semantics of the topic model and develope a friendly topic assistant for transfomer-based abstractive summarization models. 3 Model Our model is illustrated in Figure 2, which follows the transformer-based encoder-decoder architecture (Vaswani et al., 2017). We modify the encoder with graph neural networks, so we can incorporate entity information and graph representations at the same time. We design a novel two-level decoding pro"
2021.findings-acl.30,P14-1084,0,0.0171724,"4). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and do"
2021.findings-acl.30,P17-1099,0,0.297658,"ation is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et"
2021.findings-acl.30,D19-1323,0,0.0661926,"ractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summarization. 2.3 Summarization with Additional Features In addition to the direct application of the general sequence-to-sequence framework, researchers attempted to incorporate various features into summarization. Cao et al. (2018) extract actual fact descriptions from the source text and propose a dual-attention mechanism to force the generation conditioned on both the source text and the extracted fact descriptions. Sharma et al. (2019) take a pipeline method for single-document summarization which is composed of an entity-aware content selection module and a summary generation module. By contrast, our EMSum model is an end-to-end method for multi-document summarization. Gunel et al. (2020) inject structural world knowledge from Wikidata to a transformer-based model, enabling the model to be more fact-aware. Zhu et al. (2020) extract factual relations from the source texts to build a local knowledge graph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that i"
2021.findings-acl.30,P17-1108,0,0.0552391,"Missing"
2021.findings-acl.30,D08-1079,0,0.0497617,"red for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oceandam/EMSum 352 2.2 Graph-based Document Summarization Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convo"
2021.findings-acl.30,2020.acl-main.553,0,0.0258088,"Missing"
2021.findings-acl.30,P13-1137,0,0.054895,"Missing"
2021.findings-acl.30,2020.emnlp-main.35,0,0.152198,"playing unique roles that contribute towards the coherence and conciseness of the text. We believe that entities can be regarded 351 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 351–362 August 1–6, 2021. ©2021 Association for Computational Linguistics as the indicator of saliency and can be used to reduce redundancy. This motivates us to propose an entity-aware abstractive multi-document summarization model that effectively encodes relations across documents with the help of entities, and explicitly solve the issues of saliency and redundancy. Inspired by Wang et al. (2020a), we build a heterogeneous graph that consists of nodes that represent documents and entities. The entity nodes can serve as bridges that connect different documents – we can model the relations across documents through entity clusters. We apply the graph attention network (GAT) (Veliˇckovi´c et al., 2017) to enable information flow between nodes and iteratively update the node representations. In the decoding process, we design a novel two-level attention mechanism. The decoder first attends to the entities. Next, the attention weights of entities are incorporated with graph edge weights to"
2021.findings-acl.30,K17-1045,0,0.0190012,"tes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the su"
2021.findings-acl.30,P19-1499,0,0.0191991,"tion extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further finetuned for the summarization task. Zhang et al. (2020) propose PEGASUS, in which they design a pre-training objective tailored for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oce"
2021.findings-acl.30,D19-1311,0,0.0342774,"Missing"
2021.findings-acl.30,2020.emnlp-main.297,1,0.709786,"ocument summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further finetuned for the summarization task. Zhang et al. (2020) propose PEGASUS, in which they design a pre-training objective tailored for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oceandam/EMSum 352 2.2 Graph-based Document Summarization Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level informa"
2021.findings-emnlp.337,D18-1547,0,0.149469,"To this end, we build a new dataset named NUANCED that focuses on such realistic settings, with 5.1k dialogues, 26k turns of high-quality user responses. We conduct experiments, showing both the usefulness and challenges of our problem setting. We believe NUANCED can serve as a valuable resource to push existing research from the agent-centric system to the user-centric system. The dataset is publicly available1 . 1 Introduction Conversational artificial intelligence is one of the long-standing research problems in natural language processing, such as task-oriented dialogue (Wen et al., 2017; Budzianowski et al., 2018; Hosseini-Asl et al., 2020), conversational recommendation (Sun and Zhang, 2018; Zhang et al., 2018) and chi-chat (Adiwardana et al., 2020; Roller et al., 2020) etc. However, most existing systems are agent-centric. Such systems require the users to unnaturally adapt to and even have a learning curve on the system ontology, which is largely unknown System ontology: category: Japanese, Korean, Chinese, New American, etc. alcohol: full bar, beer and wine, don’t serve attire: casual, dressy, formal wifi: free, paid, no Traditional Dataset Hello, can you help me find some good Chinese restaurants"
2021.findings-emnlp.337,P19-1360,0,0.0168325,"rom large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose utterances that imply the distribution. icy to ask the right questions and make accurate recommendations, such as (Xu et al., 2020; Lei et al., 3.2.1 Dialogue Simulator 2020; Li et al., 2"
2021.findings-emnlp.337,N19-1423,0,0.0205213,"Missing"
2021.findings-emnlp.337,W18-6539,0,0.0153413,"distributions may differ among individuals which causes variances, In this work, we aim to aggregate estimated distributions from large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose utterances that imply the distribution. icy to ask the right q"
2021.findings-emnlp.337,D17-1237,0,0.0161506,"the dialogue up to the current turn. Note that the preference distributions may differ among individuals which causes variances, In this work, we aim to aggregate estimated distributions from large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose"
2021.findings-emnlp.337,D15-1199,0,0.0492943,"Missing"
2021.findings-emnlp.337,2020.coling-main.463,1,0.904397,"ploy professional linguists to annotate the dataset, and end up with 5.1k dialogues and 26k turns of high-quality user utterances. Our dataset captures a wide range of phenomena naturally occurring in realistic user utterances, including specified factoid knowledge, commonsense knowledge and users’ own situations. We conduct comprehensive experiments and analyses to demonstrate the challenges. We hope NUANCED can serve as a valuable resource to bridge the gap between current researches and real-world applications. 2 Related Work et al., 2014), Multi-WOZ (Budzianowski et al., 2018), MGConvRex (Xu et al., 2020), etc, the utterances from the users mostly closely follow the system ontology. While in task-oriented dialogue systems, parsing the user utterances into dialogue states is more on hard matching, in conversational recommendation systems soft matching is more encouraged since the user preferences are more salient and diverse in this type of conversations. 3 3.1 The NUANCED Dataset User Preference Modeling Given a system ontology, denote the set of all slots as {Si }, with the option values for each slot as {Vij }. Denote the current user utterance as T and dialogue context (of past turns) as C."
C16-1191,P06-4018,0,0.00511708,"s. The act types and slot-value pairs are labeled in the dataset. The details about the dialogue act are provided in Table 1. The training, validation and testing set are partitioned in the ratio of 3:1:1. And upsampling w.r.t act type is applied to make the corpus more uniform similar to (Wen et al., 2015). 4.2 Implementation Details We use Theano (Bergstra et al., 2010) to implement the proposed model. For each dialogue act and input question, we generate 20 responses and select the top 5 responses as the output after reranking. The BLEU-4 metric (Papineni et al., 2002) implemented by NLTK (Bird, 2006) is used for quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as"
C16-1191,D16-1127,0,0.0494517,"open domain dialogue. In addition to the input text, the hierarchical model encodes the context information to generate the response. Data-driven statistical approaches have also been studied for the text planning phase. In the text planning phase, NLG chooses the proper information of every sentence to be presented to users. The generation models mentioned above are trained by predicting the system response in a given conversational context using the maximum-likelihood estimation (MLE) objective so that they tend to generate nonsense responses such as “I dont know”. To address this problem, Li et al. (2016) applies deep reinforcement learning to model long-term reward in chatbot dialogue which can plan the information in the response and avoid generating the nonsense responses in the dialogue. 2.2 Task-oriented NLG for Specific Domain The statistical methods mentioned above are designed for open-domain chatbots, which emphasize on generating relevant and fluent responses according to the input text. While these methods are not suitable for task-solving scenarios (for instance, dialogue systems for restaurant and hotel reservation), which aims at providing correct answers to the input questions,"
C16-1191,W00-0306,0,0.133133,"quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as proposed by Oh and Rudnicky (2000), the 2-hidder-layer semantically conditioned LSTM network (SC-LSTM) proposed by Wen et al. (2015). For our own method, we experiment with several settings: the basic setting (denoted by CA-LSTM), the Context-Aware LSTM with attention (CA-LSTM+att) which encodes the question vector with an attention mechanism, and the Context-Aware LSTM with attention and act type embeddings (CALSTM+att+emb). The result is shown in Table 2. As we can see, the performances of our methods have been greatly improved compared to the baselines shown in the first block (hdc,kNN,classlm and SC-LSTM). By combining mor"
C16-1191,P02-1040,0,0.103079,"ponse turns sampled from about 1000 dialogues. The act types and slot-value pairs are labeled in the dataset. The details about the dialogue act are provided in Table 1. The training, validation and testing set are partitioned in the ratio of 3:1:1. And upsampling w.r.t act type is applied to make the corpus more uniform similar to (Wen et al., 2015). 4.2 Implementation Details We use Theano (Bergstra et al., 2010) to implement the proposed model. For each dialogue act and input question, we generate 20 responses and select the top 5 responses as the output after reranking. The BLEU-4 metric (Papineni et al., 2002) implemented by NLTK (Bird, 2006) is used for quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kN"
C16-1191,D14-1162,0,0.0788637,"Missing"
C16-1191,D11-1054,0,0.0382846,"ogue or question answering system. NLG can be treated as a single-turn dialogue generation. Traditional approaches to NLG problem are mostly rule-based or template-based (Bateman and Henschel, 1999; Busemann and Horacek, 2002). However, these methods tend to generate rigid and stylised language without the natural variation of human language. In addition, they need a heavy workload to design the templates or rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answ"
C16-1191,P15-1152,0,0.0253425,"ering system. NLG can be treated as a single-turn dialogue generation. Traditional approaches to NLG problem are mostly rule-based or template-based (Bateman and Henschel, 1999; Busemann and Horacek, 2002). However, these methods tend to generate rigid and stylised language without the natural variation of human language. In addition, they need a heavy workload to design the templates or rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. Howev"
C16-1191,D15-1199,0,0.46474,"rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. However the context information, such as the input question and dialogue act, is ignored. Yin et al. (2016) propose a neural network model that can generate answers to simple factoid questions based on a knowledge base. But a large error rate is observed due to the complex architecture introduced. In this paper, we deal with the NLG problem in this setting: given a question, the correspondin"
C16-1191,W16-0106,0,0.176671,"coder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. However the context information, such as the input question and dialogue act, is ignored. Yin et al. (2016) propose a neural network model that can generate answers to simple factoid questions based on a knowledge base. But a large error rate is observed due to the complex architecture introduced. In this paper, we deal with the NLG problem in this setting: given a question, the corresponding dialogue act, and the semantic slots to be addressed in the response, how to generate a natural language response in a dialogue. We present a statistical task-oriented NLG model based on a Context-Aware Long Short-term Memory network (CA-LSTM), which adopts the general encoder-decoder framework to incorporate"
C16-1191,W98-1425,0,\N,Missing
D17-1079,P14-2131,0,0.0170647,"Missing"
D17-1079,P16-1039,0,0.348057,"ing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarow"
D17-1079,D13-1129,1,0.896381,"Missing"
D17-1079,C14-1078,1,0.843812,"Missing"
D17-1079,P15-1168,0,0.622783,"cter embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra cla"
D17-1079,D14-1093,1,0.462459,"Missing"
D17-1079,D15-1141,0,0.602486,"Missing"
D17-1079,I05-3025,0,0.0362245,"Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sent"
D17-1079,P15-1167,0,0.0923753,"estigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014)"
D17-1079,D16-1257,0,0.0260578,"Missing"
D17-1079,N06-1020,0,0.0153109,"their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empir"
D17-1079,D10-1002,0,0.0159819,"f feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natu"
D17-1079,N16-1118,0,0.0378788,"Missing"
D17-1079,P14-2050,0,0.0361789,"unigram wi and character bigram wi−1 wi , respectively. A forward word representation efi is calculated as follows: ri = concat2 (rilstm−f , rilstm−b ) = tanh(W2 [rilstm−f ; rilstm−b ]) Given the representation ri , we use a scoring unit to score for each potential segment label. Given ri , the score of segment label M is: i fM = WM h, where h = concat3 (ri , eM ), = tanh(W3 [ri ; eM ]) WM is the score matrix for label M, and eM is the label embedding for label M. 3 Word-Context Character Embeddings Our model structure is a derivation from the skipgram model (Mikolov et al., 2013), similar to Levy and Goldberg (2014). Given a sentence with length n: {w1 , w2 , w3 , · · · wn } and its corresponding segment labels: {l1 , l2 , l3 , · · · ln }, the pre-training context of current character wt is the around characters in the windows with size c, together with their corresponding segment labels (Figure 2). Characters wi and labels li in the context are represented by vectors ecwi ∈ Rd and ecli ∈ Rd , respectively, where d is the embedding dimensionality. The word-context embedding of character wt is represented as ewt ∈ Rd , which is trained by predicting the surrounding context representations ecw′ efi = conca"
D17-1079,P14-1043,0,0.01322,"d Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Sco"
D17-1079,D16-1046,0,0.0609597,"Missing"
D17-1079,N15-1142,0,0.0336542,"dicting the surrounding context representations ecw′ efi = concat1 (ewi , ewi−1 wi ), = tanh(W1 [ewi ; ewi−1 wi ]) A backward representation ebi can be obtained in the same way. Then efi and ebi are fed into forward and backward LSTM units at current position, obtaining the corresponding forward and backward LSTM representations rilstm−f and rilstm−b , respectively. In the scoring layer, we first obtain a linear combination of rilstm−f and rilstm−b , which is the final 761 and ecli , parameterizing the labeled segmentation information in the embedding parameters. To capture order information (Ling et al., 2015), we use different embedding matrices for context embedding in different context positions, training different embeddings for the same word when they reside on different locations as the context word. In particular, our context window size is five. As a result, each word has four different versions of ec , namely ec−1 , ec−2 , ec+1 , and ec+2 , each taking a distinct embedding matrix. Given the context window [w−2 , w−1 , w, w+1 , w+2 ], w−1 is the left first context word of the focus word w, ec−1,wi will be selected from embedding matrix E−1 , and w+1 is the right first word of w, ec+1,wi wil"
D17-1079,P14-1028,0,0.295876,"embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li,"
D17-1079,N09-1007,0,0.0471648,"Missing"
D17-1079,I05-3027,0,0.631704,"Missing"
D17-1079,W06-0127,0,0.253287,"in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sentence, and N is the s"
D17-1079,D13-1061,0,0.12946,"endencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-trai"
D17-1079,P13-1043,1,0.546747,"Missing"
D17-1079,P15-1032,0,0.00999897,"Missing"
D17-1079,P16-2092,0,0.103199,"Missing"
D17-1079,O03-4002,0,0.708664,"nference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith cha"
D17-1079,P95-1026,0,0.657468,"2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2"
D17-1079,D13-1031,0,0.0880344,"Missing"
D17-1079,E14-1062,1,0.919559,"Missing"
D17-1079,P16-1040,1,0.632749,"espectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling"
D17-1079,P07-1106,1,0.78255,"Missing"
D18-1492,P18-2003,0,0.0249167,"cially for the balanced-tree modeling, which also automatically select the crucial information from all word representation. Kim et al. (2017) propose a tree structured attention networks, which combine the benefits of tree modeling and attention, and the tree structures in their model are also learned instead of the syntax trees. Although binary parsing trees do not produce better numbers than trivial trees on many downstream tasks, it is still worth noting that we are not claiming the useless of parsing trees, which are intuitively reasonable for human language understanding. A recent work (Blevins et al., 2018) shows that RNN sentence encodings directly learned from downstream tasks can capture implicit syntax information. Their interesting result may explain why explicit syntactic guidance does not work for tree LSTMs. In summary, we still believe in the potential of linguistic features to improve neural sentence modeling, and we hope our investigation could give some sense to afterwards hypothetical exploring of designing more effective tree-based encoders. 6 Conclusions In this work, we propose to empirically investigate what contributes mostly in the tree-based neural sentence encoding. We find"
D18-1492,D15-1075,0,0.023677,"he path between two labels (Li et al., 2015; Socher et al., 2013), we feed the entire sentence together with the nominal indicators (i.e., tags of e1 and e2 ) as words to the framework. We also ignore the order of e1 and e2 in the labels given by the dataset. Thus, this task turns to be a 10-way classification one. 3.2 Sentence Relation Classification To evaluate how well a model can capture semantic relation between sentences, we introduce the second group of tasks: sentence relation classification. 4633 Natural Language Inference (NLI). The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a challenging dataset for sentence-level textual entailment. It has 550K training sentence pairs, as well as 10K for development and 10K for test. Each pair consists of two relative sentences, associated with a label which is one of entailment, contradiction and neutral. Conjunction Prediction (Conj). Information about the coherence relation between two sentences is sometimes apparent in the text explicitly (Miltsakaki et al., 2004): this is the case whenever the second sentence starts with a conjunction phrase. Jernite et al. (2017) propose a method to create conjunction prediction datase"
D18-1492,P16-1139,0,0.315676,"for classification or sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jan"
D18-1492,P17-1177,0,0.210118,"sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Mad"
D18-1492,P17-1152,0,0.35511,"sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Mad"
D18-1492,P18-2116,0,0.028882,"ssions Balanced tree for sentence modeling has been explored by Munkhdalai and Yu (2017) and Williams et al. (2018) in natural language inference (NLI). However, Munkhdalai and Yu (2017) focus on designing inter-attention on trees, instead of comparing balanced tree with other linguistic trees in the same setting. Williams et al. (2018) do compare balanced trees with latent trees, but balanced tree does not outperform the latent one in their experiments, which is consistent with ours. We analyze it in Section 4.2 that sentences in NLI are too short for the balanced tree to show the advantage. Levy et al. (2018) argue that LSTM works for the gates ability to compute an element-wise weighted sum. In such case, tree LSTM can also be regarded as a special case of attention, especially for the balanced-tree modeling, which also automatically select the crucial information from all word representation. Kim et al. (2017) propose a tree structured attention networks, which combine the benefits of tree modeling and attention, and the tree structures in their model are also learned instead of the syntax trees. Although binary parsing trees do not produce better numbers than trivial trees on many downstream ta"
D18-1492,D15-1278,0,0.372242,"n sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (2018) empirically show that, Gumbel softmax produces unstable"
D18-1492,D17-1070,0,0.0214674,"e reward from its shallow and balanced structures. Additionally, Figure 5 demonstrates that binary balanced trees work especially better with relative long sentences. As desired, on shortsentence groups, the performance gap between Bi-LSTM and binary balanced tree LSTM is not obvious, while it grows with the test sentences turning longer. This explains why tree-based encoder gives small improvements on NLI and Para, because sentences on these two tasks are much shorter than others. 4.4 Can Pooling Replace Tree Encoder? Max pooling (Collobert and Weston, 2008; Zhao et al., 2015), mean pooling (Conneau et al., 2017) and self-attentive pooling (also known as selfattention; Santos et al., 2016; Liu et al., 2016; Lin et al., 2017) are three popular and efficient choices to improve sentence encoding. In this part, we will compare the performance of tree LSTMs and biLSTM on the tasks of WSR, MT and AE, with each pooling mechanism respectively, aiming to demonstrate the role that pooling plays in sentence final encoding ?1 ? ?2 3 ?4 ?7 ?5 ?6 hidden states I love cats . leaf states (a) Balanced tree. ?1 ?2 ?3 ?4 final encoding hidden states word embeddings I love cats . (b) Bi-LSTM. Figure 6: An illustration of"
D18-1492,P16-1078,0,0.112039,"In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (20"
D18-1492,P17-1064,0,0.0214871,"tity of instances in train/dev/test set, the average length (by words) of sentences (source sentence only for generation task), as well as the number of classes if applicable. Sentence Generation We also include the sentence generation tasks in our experiments, to investigate the representation ability of different encoders over global (longterm) context features. Note that our framework is based on encoding, which is different from those attention based approaches. Paraphrasing (Para). Quora Question Pair Dataset is a widely applied dataset to evaluate paraphrasing models (Wang et al., 2017; Li et al., 2017b). 1 In this work, we treat the paraphrasing task as a sequence-to-sequence one, and evaluate on it with our sentence generation framework. Machine Translation (MT). Machine translation, especially cross-language-family machine translation, is a complex task, which requires models to capture the semantic meanings of sentences well. We apply a large challenging EnglishChinese sentence translation task for this investigation, which is adopted by a variety of neural translation work (Tu et al., 2016; Li et al., 2017a; Chen et al., 2017a). We extract the parallel data from the LDC corpora,2 selec"
D18-1492,miltsakaki-etal-2004-penn,0,0.011076,"ntroduce the second group of tasks: sentence relation classification. 4633 Natural Language Inference (NLI). The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a challenging dataset for sentence-level textual entailment. It has 550K training sentence pairs, as well as 10K for development and 10K for test. Each pair consists of two relative sentences, associated with a label which is one of entailment, contradiction and neutral. Conjunction Prediction (Conj). Information about the coherence relation between two sentences is sometimes apparent in the text explicitly (Miltsakaki et al., 2004): this is the case whenever the second sentence starts with a conjunction phrase. Jernite et al. (2017) propose a method to create conjunction prediction dataset from unlabeled corpus. They create a list of phrases, which can be classified into nine types, as conjunction indicators. The object of this task is to recover the conjunction type of given two sentences, which can be used to evaluate how well a model captures the semantic meaning of sentences. We apply the method proposed by Jernite et al. (2017) on the Wikipedia corpus to create our conj dataset. 3.3 Dataset #Sentence Train Dev #Cls"
D18-1492,E17-1002,0,0.0402274,"N). The best number(s) for each task are in bold. The top and down arrows indicate the increment or decrement of each pooling mechanism, against the baseline of pure tree based encoder with the same structure. anism has the benefits of the balanced tree modeling, which also fairly treat all words and learn the crucial parts automatically. The path from representation to words in attention are even shorter than the balanced tree. Thus the fact that attentive pooling outperforms balanced trees on WSR is not surprising to us. 5 Discussions Balanced tree for sentence modeling has been explored by Munkhdalai and Yu (2017) and Williams et al. (2018) in natural language inference (NLI). However, Munkhdalai and Yu (2017) focus on designing inter-attention on trees, instead of comparing balanced tree with other linguistic trees in the same setting. Williams et al. (2018) do compare balanced trees with latent trees, but balanced tree does not outperform the latent one in their experiments, which is consistent with ours. We analyze it in Section 4.2 that sentences in NLI are too short for the balanced tree to show the advantage. Levy et al. (2018) argue that LSTM works for the gates ability to compute an element-wis"
D18-1492,P02-1040,0,0.103465,"51.3 53.5 19.7 20.5 19.9 20.6 20.4 20.9 19.0 22.3 19.2 21.6 19.7 23.1 49.4 76.0 48.0 72.9 54.7 80.4 LSTM +bidirectional 91.7 91.7 87.8 87.8 48.8 49.2 98.6 98.7 66.1 67.4 82.6 82.8 52.8 53.3 20.3 20.2 19.1 21.3 46.9 67.0 Avg. Length 31.5 33.7 33.8 20.1 23.1 11.2 23.3 10.2 34.1 34.1 Parsing +bi-leaf-RNN Trivial Trees Balanced +bi-leaf-RNN Left-branching +bi-leaf-RNN Right-branching +bi-leaf-RNN Linear Structures Table 2: Test results for different encoder architectures trained by a unified encoder-classifier/decoder framework. We report accuracy (⇥100) for classification tasks, and BLEU score (Papineni et al., 2002; word-level for English targets and char-level for Chinese targets) for generation tasks. Large is better for both of the metrics. The best number(s) for each task are in bold. In addition, average sentence length (in words) of each dataset is attached in the last row with underline. Section 4.3. Finally, we compare the performance of linear and tree LSTMs with three widely applied pooling mechanisms in Section 4.4. 4.1 Set-up In experiments, we fix the structure of the classifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recurrent neural networks (Cho"
D18-1492,J11-1005,0,0.0372549,"a, 2015) optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. In the 3 We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI. 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of"
D18-1492,D14-1162,0,0.0957312,"verage sentence length (in words) of each dataset is attached in the last row with underline. Section 4.3. Finally, we compare the performance of linear and tree LSTMs with three widely applied pooling mechanisms in Section 4.4. 4.1 Set-up In experiments, we fix the structure of the classifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recurrent neural networks (Cho et al., 2014). 3 The hidden-layer size of MLP is fixed to 1024, while that of GRU is adapted from the size of sentence encoding. We initialize the word embeddings with 300-dimensional GloVe (Pennington et al., 2014) vectors.4 We apply 300-dimensional bidirectional (600-dimensional in total) LSTM as leaf RNN when necessary. We use Adam (Kingma and Ba, 2015) optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. In the 3 We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI. 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dr"
D18-1492,P17-2092,1,0.852791,"units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (2018) empirically show that, Gumbel softmax"
D18-1492,C18-1315,1,0.699624,"akes words in the left of the source sentence more important (Sutskever et al., 2014). If the encoder fails to memorize the left words, the information about right words would not help due to the error propagation. In right-branching trees, left words of the sentence are closer to the final representation, which makes the left words are more easy to be memorized, and we call this structure prior. Oppositely, in the case of left-branching trees, right words of the sentence are closer to the representation. To validate our hypothesis, we propose to visualize the Jacobian as word-level saliency (Shi et al., 2018), which can be viewed as the contribution of each word to the sentence encoding: J(s, w) = krs(w)k1 = X @si | | @wj i,j where s = (s1 , s2 , · · · , sp )T denotes the embedding of a sentence, and w = (w1 , w2 , · · · , wq )T denotes embedding of a word. We can compute the saliency score using backward propagation. For a word in a sentence, higher saliency score means more contribution to sentence encoding. We present the visualization in Figure 3 using the visualization tool from Lin et al. (2017). It shows that right-branching tree LSTM encoders tend to look at the left part of the sentence,"
D18-1492,P15-1117,1,0.771324,"ork has two main components: the encoder part and the classifier/decoder part. In general, models encode a sentence to a length-fixed vector, and then applies the vector as the feature for classification and generation. We fix the structure of the classifier/decoder, and propose to use five different types of tree structures for the encoder part including: • Parsing tree. We apply binary constituency tree as the representative, which is widely used in natural language inference (Bowman et al., 2016) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a). Dependency parsing trees (Zhou et al., 2015, 2016a) are not considered in this paper. I love my pet cat . I love my pet cat . LSTM/Tree LSTM LSTM/Tree LSTM <S&gt; I love cats . Multi-Layer Perceptron Softmax I love cats . </S&gt; (a) Encoder-decoder framework for sentence generation. (b) Encoder-classifier framework for sentence classification. I love my pet cat . I love my pet dog . LSTM/Tree LSTM LSTM/Tree LSTM Multi-Layer Perceptron Softmax (c) Siamese encoder-classifier framework for sentence relation classification. Figure 1: The encoder-classifier/decoder framework for three different groups of tasks. We apply multi-layer perceptron (M"
D18-1492,P16-1132,1,0.82387,"rce sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of downstream tasks, which lead to the following observations: this section, we analyze why these trees achieve high scores in deep. Tree encoders are useful on some tasks. We get the same conclusion with Li et al. (2015) that tree-based encoders perform better on tasks requiring long-term context fea"
D18-1492,D13-1170,0,0.0682442,"s://github. com/ExplorerFreda/TreeEnc. 1 Introduction Sentence modeling is a crucial problem in natural language processing (NLP). Recurrent neural networks with long short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Cho et al., 2014) are commonly used sentence modeling approaches. These models embed sentences into a vector space and the resulting vectors can be used for classification or sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effec"
D18-1492,P15-1150,0,0.275618,"Missing"
D18-1492,P16-1008,0,0.0225749,"tion Pair Dataset is a widely applied dataset to evaluate paraphrasing models (Wang et al., 2017; Li et al., 2017b). 1 In this work, we treat the paraphrasing task as a sequence-to-sequence one, and evaluate on it with our sentence generation framework. Machine Translation (MT). Machine translation, especially cross-language-family machine translation, is a complex task, which requires models to capture the semantic meanings of sentences well. We apply a large challenging EnglishChinese sentence translation task for this investigation, which is adopted by a variety of neural translation work (Tu et al., 2016; Li et al., 2017a; Chen et al., 2017a). We extract the parallel data from the LDC corpora,2 selecting 1.2M from them as our training set, 20K and 80K of them as our development set and test set, respectively. Auto-Encoding (AE). We extract the English part of the machine translation dataset to form a auto-encoding task, which is also compatible with our encoder-decoder framework. 4 Experiments In this section, we present our experimental results and analysis. Section 4.1 introduces our setup for all the experiments. Section 4.2 shows the main results and analysis on ten downstream tasks group"
D18-1492,D16-1058,0,0.0269958,"rd.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of downstream tasks, which lead to the following observations: this section, we analyze why these trees achieve high scores in deep. Tree encoders are useful on some tasks. We get the same conclusion with"
D18-1492,Q18-1019,0,0.158922,"Missing"
D18-1492,P16-2022,0,\N,Missing
L16-1104,C12-1018,0,0.0325284,"Missing"
L16-1104,D12-1133,0,0.0180185,"inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentence"
L16-1104,W08-2102,0,0.068385,"Missing"
L16-1104,P05-1022,0,0.122212,"Missing"
L16-1104,A00-2018,0,0.56498,"Missing"
L16-1104,D14-1082,0,0.266842,"al Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural dependency parsing (Table 1). First,"
L16-1104,Q13-1033,0,0.0129105,"). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural d"
L16-1104,P04-1013,0,0.114156,"Missing"
L16-1104,P08-1067,0,0.0719478,"Missing"
L16-1104,J93-2004,0,0.0583985,"on in the action layer, and lj is the jth label in the label layer. We adopt a greedy decoding strategy in the hierarchical parsing process. In each parsing step, the action type ai with the highest probability is first selected, and then the constituent label lj with the highest probability is selected given the optimal action type ai . As in the baseline parser, we adopt the cross-entropy loss as our training objective: L(θ) = − X log p(yi,j |x, Acts) + yi,j ∈A 5. 5.1. λ k θ k2 2 (10) Experiments Set-up We conduct our experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). Following the standard splits of WSJ, sections 2–21 are used as the labeled training data, section 24 is used as the development data and section 23 is used as the evaluation data. Ten-fold jackknifing (Collins, 2000) is used to automatically assign POS tags to the training data. The SVMTool is used as the POS-tagger1 . 5.2. Parameters We carry out a development experiment to measure the correlation between hidden layer size and constituent parsing accuracies. From Table 3, we can see that both the baseline neural parser and the hierarchical neural parser achieve higher parsing accuracies wi"
L16-1104,N06-1020,0,0.105368,"Missing"
L16-1104,J08-4003,0,0.0283774,"89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging"
L16-1104,N07-1051,0,0.151924,"Missing"
L16-1104,W05-1513,0,0.402545,"parser by using a hierarchical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of"
L16-1104,P12-1046,0,0.0380018,"Missing"
L16-1104,P13-1045,0,0.0608923,"Missing"
L16-1104,P07-1080,0,0.167269,"mapping matrix from hidden layer to the action layer and da is the number of action types. W3i ∈ Rdlabel ×dh is the mapping matrix from the hidden layer to the label layer. The probability of a labeled action yi,j , given its history Acts and input x, is computed as: p(yi,j |x, Acts) = p(ai |x, Acts) × p(lj |x, Acts, ai ) (7) Neural where Model Collins (1999) Charniak (2000) Charniak and Johnson (2005)‡ Huang (2008)‡ McClosky et al. (2006)‡ Shindo et al. (2012) Sagae and Lavie (2005) Petrov and Klein (2007) Carreras et al. (2008) Zhu et al. (2013) Zhu et al. (2013) + padding Henderson (2004)‡ Titov and Henderson (2007) Collobert (2011) Billingsley and Curran (2012) Socher et al. (2013)‡ Legrand and Collobert (2014) Watanabe and Sumita (2015) F1 88.2 89.6 91.1 91.7 92.1 92.4 86.0 90.1 91.1 89.9 90.4 90.1 90.0 87.9 84.9 90.4 88.3 90.7 Speed 3.5 5.7 This Work This Work + hierarchical 89.13 89.06 133.6 320.2 3.7 6.2 100.7 89.5 31.7 6.1 22.0 1.8 i p(ai |x, Acts) = eoact P k eoact (8) ak ∈GEN(Acts) j p(lj |x, Acts, ai ) = eolabel (ai ) P k eolabel (ai ) (9) Table 4: Comparisons with previous work. ‡: reranking model. Speed: sentences per second. lk ∈GEN(Acts) Here ai is the ith action in the action layer, and lj"
L16-1104,W09-3825,1,0.909876,"ing node with constituent label X whose child is s0 ; push the new node back onto the stack. • LEFT/RIGHT-X: pop the top two nodes s1 , s0 off the stack; generate a binary-branching node with constituent label X whose left child is s1 and right child is s0 , with the left (LEFT)/right (RIGHT) child as its head; push the new node back to the stack. The shift-reduce actions only build binarized trees. As a result, a binarization process is necessary to convert the Penn Treebank into binarized trees. During this process, temporary nodes are constructed. To accommodate for binarization, we follow Zhang and Clark (2009), adding counterparts to LEFT/RIGHT-X for temporary nodes, namely LEFT/RIGHT-TEMP-X. 3. h = (W1 x + b1 )3 (3) a∈A Here A is the set of all gold labeled actions in the training data. Mini-bached AdaGrad (Duchi et al., 2011) and dropout (Srivastava et al., 2014) are used for optimization. Hierarchical Output Neural Network Due to its large hidden and output layer sizes, the vanilla neural constituent parser is much slower than the dependency counterpart. The main computation cost is the mapping from hidden layer to output layer. Motivated by the hierarchical neural language model (Mnih and Hinto"
L16-1104,P11-2033,1,0.811029,"archical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency par"
L16-1104,P15-1117,1,0.88504,"Missing"
L16-1104,P13-1043,1,0.834198,"Missing"
L16-1104,P15-1113,0,\N,Missing
L18-1145,D14-1179,0,0.03576,"Missing"
L18-1145,C12-1059,0,0.0335631,"Missing"
L18-1145,Q13-1033,0,0.0129566,"the experimental results indicate that our method can achieve +1.06 BLEU improvements. 2. Related Work To the best of our knowledge, Goldberg et. al. (2012) first define the concept of dynamic oracle and propose an online algorithm for parsing problems, , which provides a set of optimal transitions for every valid parser configuration. For configurations which are not part of a gold derivation, their dynamic oracle permits all transitions that can lead to a tree with minimum loss compared to the gold tree. Based on their approach, several other methods using dynamic oracle have been proposed (Goldberg and Nivre, 2013) (G´omezRodrıguez et al., 2014). However, their work in the field of parsing cannot be directly applied in neural machine translation. To mitigate the discrepancy between training and inference, Daume et al. (2009) introduce SEARN, which aims to tackle the problems that training examples might be different from actual test examples. They show that structured prediction can be mapped into a search setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two"
L18-1145,D14-1099,0,0.0274225,"Missing"
L18-1145,D13-1176,0,0.0203397,"setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two methods respectively, namely language model guided scheduled sampling and pre-trained model guided scheduled sampling. 3.1. Neural Machine Translation Neural machine translation aims to directly model the conditional probability p(Y |X) of translating a source sentence, x1 , ..., xn , to a target sentence, y1 , ..., ym . Generally, it accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013). Basically, the encoder generates a context vector for each source sentence and then the decoder outputs a translation, one target word at a time. During training when we are decoding, we always provide the model with true previous token at every time step. Minibatch stochastic gradient descent is applied to look for a set of parameters θ∗ that maximizes the log likelihood of producing the correct target sentence. Specifically, given a batch of training pairs {(X i , Y i )}, we aim to find θ∗ which satisfies: X log p(Y i |X i ; θ) (1) θ∗ = arg max θ (X i ,Y i ) Whereas during inference the mo"
L18-1145,P02-1040,0,0.102773,"Missing"
L18-1145,C14-1008,0,0.0422661,"ever, there are certain limitations in Scheduled Sampling and we propose two dynamic oracle-based methods to improve it. We manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle’s demonstrations. Experimental results show that the proposed approaches improve translation quality over standard NMT system. Keywords: machine translation, dynamic oracle, language model 1. Introduction Neural networks have been widely used contemporarily and have achieved great performance on a variety of fields like sentiment analysis (Santos and Gattit, 2014) and visual object recognition (Ciregan et al., 2012). For sequential problems, recurrent neural networks can be applied to process sequences. To address issues like long term dependencies in the data (Bengio et al., 1994), the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) can be used to tackle the problem (Cho et al., 2014). A straightforward application of the LSTM and GRU architecture have already shown impressive performance in several difficult tasks, including machine translation (Sutskever et al., 2014), and image captioning (Vinyals et a"
N19-1411,C18-1142,1,0.903629,"ork. Bowman et al. (2016) propose to use the variational autoencoder (VAE, Kingma and Welling, 2014) to map an input sentence to a probabilistic continuous latent space. VAE makes it possible to generate sentences from a distribution, which is desired in various applications. For example, in an open-domain dialog system, the information of an utterance and its response is not necessarily a one-to-one mapping, and multiple plausible responses could be suitable for a given input. Probabilistic sentence generation makes the dialog system more diversified and more meaningful (Serban et al., 2017; Bahuleyan et al., 2018). Besides, probabilistic modeling of the hidden representations serves as a way of posterior regularization (Zhang et al., 2016), facilitating interpolation (Bowman et al., 2016) and manipulation of the latent representation (Hu et al., 2017). However, training VAEs in NLP is more difficult than the image domain (Kingma and Welling, 2014). The VAE training involves a reconstruction loss and a Kullback–Leibler (KL) divergence between the posterior and prior of the latent space. In NLP, the KL term tends to vanish to zero during training, leading to an ineffective latent space. Previous work has"
N19-1411,D15-1075,0,0.0233778,"8.7 Table 1: Results of SNLI-style sentence generation, where WAE is compared with DAE and VAE. D and S refer to the deterministic and stochastic encoders, respectively. ↑/↓ The larger/lower, the better. For Entropy and AvgLen, the closer to corpus statistics, the better (indicated by the → arrow). As a result, this KL term does not force the model to learn the same posterior for all data samples (as in VAE), and thus, the decoder does not degrade to an unconditioned language model. 3 Experiments We evaluate WAE in sentence generation on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) as well as dialog response generation. All models use single-layer RNN with long short term memory (LSTM) units for both the encoder and decoder. Appendix C details our experimental settings. VAE training. VAE is notoriously difficult to train in the RNN setting. While different researchers have their own practice of training VAE, we follow our previous experience (Bahuleyan et al., 2018) and adopt the following tricks to stabilize the training: (1) λVAE was annealed in a sigmoid manner. We monitored the value of λ · KL and stop annealing once it reached its peak value, known as peaking annea"
N19-1411,K16-1002,0,0.584705,"chitecture can be further conditioned on some source information, for example, an input sentence, resulting in a sequence-to-sequence (Seq2Seq) model. Traditionally, sentence generation is accomplished in a deterministic fashion, i.e., the model uses a deterministic neural network to encode an 1 Our code is availabe at https://github.com/ HareeshBahuleyan/probabilistic_nlg A preliminary version of this paper was preprinted at https://arxiv.org/abs/1806.08462 input sentence to some hidden representations, from which it then decodes an output sentence using another deterministic neural network. Bowman et al. (2016) propose to use the variational autoencoder (VAE, Kingma and Welling, 2014) to map an input sentence to a probabilistic continuous latent space. VAE makes it possible to generate sentences from a distribution, which is desired in various applications. For example, in an open-domain dialog system, the information of an utterance and its response is not necessarily a one-to-one mapping, and multiple plausible responses could be suitable for a given input. Probabilistic sentence generation makes the dialog system more diversified and more meaningful (Serban et al., 2017; Bahuleyan et al., 2018)."
N19-1411,N16-1014,0,0.0598322,"VED/WED hyperparameters for each model were chosen by Table 1. 3.2 Dialog Generation We extend WAE to an encoder-decoder framework (denoted by WED) and evaluate it on the DailyDialog corpus (Li et al., 2017).2 We follow Bahuleyan et al. (2018), using the encoder to capture an utterance and the decoder to generate a reply. Table 2 shows that WED with a deterministic encoder (WED-D) is better than the variational encoder-decoder (VED) in BLEU scores, but the generated sentences lack variety, which is measured by output entropy and the percentage of distinct unigrams and bigrams (Dist-1/Dist-2, Li et al., 2016), evaluated on the generated test set responses. We then applied our stochastic encoder for WED and see that, equipped with our KLpenalized stochastic encoder, WED-S outperforms DED, VED, and WED-D in all diversity measures. WED-S also outperforms VED in generation quality, consistent with the results in Table 1. 4 Conclusion In this paper, we address the difficulty of training VAE by using a Wasserstein autoencoder (WAE) for probabilistic sentence generation. WAE implementation can be carried out with either a deterministic encoder or a stochastic one. The deterministic version achieves high"
N19-1411,D16-1050,0,0.0198757,"robabilistic continuous latent space. VAE makes it possible to generate sentences from a distribution, which is desired in various applications. For example, in an open-domain dialog system, the information of an utterance and its response is not necessarily a one-to-one mapping, and multiple plausible responses could be suitable for a given input. Probabilistic sentence generation makes the dialog system more diversified and more meaningful (Serban et al., 2017; Bahuleyan et al., 2018). Besides, probabilistic modeling of the hidden representations serves as a way of posterior regularization (Zhang et al., 2016), facilitating interpolation (Bowman et al., 2016) and manipulation of the latent representation (Hu et al., 2017). However, training VAEs in NLP is more difficult than the image domain (Kingma and Welling, 2014). The VAE training involves a reconstruction loss and a Kullback–Leibler (KL) divergence between the posterior and prior of the latent space. In NLP, the KL term tends to vanish to zero during training, leading to an ineffective latent space. Previous work has proposed various engineering tricks to alleviate this problem, including KL annealing and word dropout (Bowman et al., 2016). I"
P15-1117,D12-1133,0,0.0490078,"h size n, parsing stops after performing exactly 2n − 1 actions. MaltParser uses an SVM classifier for deterministic arc-standard parsing. At each step, MaltParser generates a set of successor states according to the current state, and deterministically selects the highest-scored one as the next state. 2.2 Global Learning and Beam Search The drawback of deterministic parsing is error propagation. An incorrect action will have a negative influence to its subsequent actions, leading to an incorrect output parse tree. To address this issue, global learning and beam search (Zhang and Clark, 2011; Bohnet and Nivre, 2012; Choi and McCallum, 2013) are used. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globa"
P15-1117,D14-1082,0,0.123397,"ompetitive greedy neural parser baseline, giving performance comparable to the best linear parser. 1 Introduction Transition-based methods have given competitive accuracies and efficiencies for dependency parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2013). These parsers construct dependency trees by using a sequence of transition actions, such as SHIFT and REDUCE, over input sentences. High accuracies are achieved by using a linear model and millions of binary indicator features. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference"
P15-1117,C14-1078,1,0.675229,"Missing"
P15-1117,P13-1104,0,0.0175535,"Missing"
P15-1117,P04-1015,0,0.959028,"12). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the li"
P15-1117,W02-1001,0,0.857367,"lark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the linear models. As a result, the log probability of an action sequence can not be modeled just as the sum of log probabilities of each action in the sequence, which is the case of structured linear model. We address the challenge by using a softmax function to directly model the distribution of action sequences. Second, for the structured model above, maximum-likelihood training is computationally intractable, requiring summing over all possible action sequen"
P15-1117,P15-1033,0,0.561306,"Missing"
P15-1117,Q13-1033,0,0.0538833,"Missing"
P15-1117,P04-1013,0,0.426907,"Missing"
P15-1117,P10-1110,0,0.0250292,"word embeddings in supervised training, the embeddings of in-vocabulary words become systematically different from these of out-of-vocabulary words after training, and the effect of pre-trained out-ofvocabulary embeddings become uncertain. In this sense, our model can also be regarded as an almost fully supervised model. The same applies to the models of Chen and Manning (2014). We also compare the speed of the structured neural parser on an Intel Core i7 3.40GHz CPU with 16GB RAM. The structured neural parser runs about as fast as Zhang and Nivre (Zhang and Nivre, 2011) and Huang and Sagae (Huang and Sagae, 2010). The results show that our parser combines the benefits of structured models and neural probabilistic models, offering high accuracies, fast speed and slim model size. 5 Related Work Parsing with neural networks. A line of work has been proposed to explore the effect of neural network models for constituent parsing (Henderson, 2004; Mayberry III and Miikkulainen, 2005; Collobert, 2011; Socher et al., 2013; Legrand and Collobert, 2014). Performances of most of these methods are still well below the state-of-the-art, except for Socher et al.(2013), who propose a neural reranker based on a PCFG"
P15-1117,P14-2128,1,0.864478,"Missing"
P15-1117,J93-2004,0,0.0521026,"sequence as a positive example for parameter update, using the training algorithm of Section 3.3. AdaGrad algorithm (Duchi et al., 2011) with mini-batch is adopted for optimization. In this way, the distribution of ot only full action sequences (i.e. complete parse trees), but also partial action sequences (i.e. partial outputs) are modeled, which makes training more challenging. The advantage of early update is that training is used to guide search, minimizing search errors. 1217 4 Description Baseline Experiments 4.1 Set-up Our experiments are performed using the English Penn Treebank (PTB; Marcus et al., (1993)). We follow the standard splits of PTB3, using sections 2-21 for training, section 22 for development testing and section 23 for final testing. For comparison with previous work, we use Penn2Malt1 to convert constituent trees to dependency trees. We use the POS-tagger of Collins (2002) to assign POS automatically. 10-fold jackknifing is performed for tagging the training data. We follow Chen and Manning (2014), and use the set of pre-trained word embeddings2 from Collobert et al. (2011) with a dictionary size of 13,000. The word embeddings were trained on the entire English Wikipedia, which c"
P15-1117,C04-1010,0,0.155583,"Missing"
P15-1117,J08-4003,0,0.367493,"Missing"
P15-1117,P13-1045,0,0.110444,"Missing"
P15-1117,D09-1058,0,0.019485,"Missing"
P15-1117,P15-1032,0,0.244197,"Missing"
P15-1117,W03-3023,0,0.372233,"Missing"
P15-1117,D08-1059,1,0.847959,"Missing"
P15-1117,J11-1005,1,0.774205,"he-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 20"
P15-1117,P11-2033,1,0.638376,"sed. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globally learned. 1214 The parser of Zhang and Nivre (2011) is developed using this framework. The structured perceptron (Collins, 2002) with early update (Collins and Roark, 2004) is applied for training. By utilizing rich manual features, it gives state-of-the-art accuracies in standard Penn Treebank evaluation. We take this method as one baseline. Fw Ft Fl 2.3 Greedy Neural Network Model Chen and Manning (2014) build a greedy neural arc-standard parser. The model can be regarded as an alternative implementation of MaltParser, using a feedforward neural network to replace the SVM classifier for deterministic parsing. 2.3.1 Model The greedy neural mo"
P15-1117,C12-2136,1,0.792212,"eatures. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Col"
P15-1117,P08-1068,0,\N,Missing
P16-1132,D15-1041,0,0.0171462,"speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been use"
P16-1132,P05-1022,0,0.125056,"of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the"
P16-1132,D14-1082,0,0.694626,"s0 ], j is the head of the queue (i.e. [ q0 = wj , q1 = wj+1 · · · ]), and L is a set of dependency arcs that has been built. At each step, the parser chooses one of the following actions: • S HIFT (S): move the front word wj from the queue onto the stacks. • L EFT-l (L): add an arc with label l between the top two trees on the stack (s1 ← s0 ), and remove s1 from the stack. • R IGHT-l (R): add an arc with label l between the top two trees on the stack (s1 → s0 ), and remove s0 from the stack. Given the sentence “John loves Mary”, the gold standard action sequence is S, S, L, S, R. 2.1 Model Chen and Manning (2014) proposed a deterministic neural dependency parser, which rely on dense embeddings to predict the optimal actions at each step. We propose a variation of Chen and Manning 1394 (2014), which splits the output layer into two hierarchical layers: the action layer and dependency label layer. The hierarchical parser determines a action in two steps, first deciding the action type, and then the dependency label (Figure 2). At each step of deterministic parsing, the neural model extracts n atomic features from the parsing state. We adopt the feature templates of Chen and Manning (2014). Every atomic"
P16-1132,D15-1215,0,0.0311275,"Missing"
P16-1132,W02-1001,0,0.0460083,"ee with the best heuristic reranking score yˆi0 . J(Θh ) = 1 |Dh | X (xi ,yi0 ,ˆ yi0 )∈Dh ri (Θh ) = ri (Θh ) + λ ||Θh || 2 max(0, st (xi , yˆi0 , Θh )) − st (xi , yi0 , Θh ) (20) (21) The detailed training algorithm is given by Algorithm 1. AdaGrad (Duchi et al., 2011) updating with subgradient (Ratliff et al., 2007) and minibatch is adopted for optimization. 5 5.1 section 23 for final testing. Following prior work on reranking, we use Penn2Malt1 to convert constituent trees to dependency trees. Ten-fold POS jackknifing is used in the training of the baseline parser. We use the POS-tagger of Collins (2002) to assign POS automatically. Because our reranking model is a dynamic reranking model, which generates training instances during search, we train 10 baseline parsing models on the 10-fold jackknifing data, and load the baseline parser model dynamically for reranking training . We follow Chen and Manning (2014), using the set of pre-trained word embeddings with a dictionary size of 13,0002 from Collobert et al. (2011). The word embeddings were trained on the entire English Wikipedia, which contains about 631 million words. 5.2 There are two different networks in our system, namely a hierarchic"
P16-1132,P15-1033,0,0.0578806,"curacies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural n"
P16-1132,P13-2111,0,0.0549975,"Missing"
P16-1132,J08-4003,0,0.0982509,"Missing"
P16-1132,D11-1137,0,0.0521305,"Missing"
P16-1132,Q13-1012,0,0.383871,"the best tree output. Denote b(i) as the beam at i-th step of search, k-best candidates in the beam of i + 1 step is: b(i + 1) = arg K (st (x, c, Θh ) + sb (x, c)), (14) c∈c(i) where c(i) denotes the set of newly constructed trees by revising trees in b(i), sb (x, c) is the baseline model score and arg K leaves the k best candidate trees to the next beam. Finally, the output tree yi of reranking is selected from all searched trees C in the revising process yi = arg max(st (x, c, Θc ) + sb (x, c)) c∈C (15) Interpolated Reranker In testing, we also adopt the popular mixture reranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which obtains better reranking performance by a linear combination of the reranking score and the baseline model score. yi = arg max (β(st (xi , y, Θc ) + st (x, y, Θh )) y∈τ (xi ) + (1 − β)sb (xi , y)) (16) Here yi is the final output tree for a sentence xi ; τ (xi ) returns all the trees candidates of the dynamic reranking; β ∈[0, 1] is a hyper-parameter. 4.4 Training As k-best neural rerankers (Socher et al., 2013; Zhu et al., 2015), we use the max-margin criterion to train our model in a stage-wise manner (Doppa et al., 2013). Given training data Dc = (xi , yi , yˆ"
P16-1132,W05-1506,0,0.0964815,"rerankers are capable of capturing global syntax features across the tree. In contrast, the most non-local neural parser with LSTM (Dyer et al., 2015) cannot exploit global features. Different to previous neural rerankers, our work in this paper contributes on integrating search and learning for reranking, instead of proposing a new neural model. Forest Reranking Forest reranking (Huang, 2008; Hayashi et al., 2013) offers a different way to extend the coverage of reranking candidates, with computing the reranking score in the trees forests by decomposing non-local features with cube-pruning (Huang and Chiang, 2005). In contrast, the neural reranking score encodes the whole dependency tree, which cannot be decomposed for forest reranking efficiently and accurately. HC-Search Doppa et al. (2013) proposed a structured prediction model with HC-Search strategy and imitation learning, which is closely related to our work in spirit. They used the complete space search (Doppa et al., 2012) for sequence labeling tasks, and the whole search process halts after a specific time bound. Different from them, we propose a dynamic parsing reranking model based on the action revising process, which is a multi-step proces"
P16-1132,P08-1067,0,0.601351,"local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association fo"
P16-1132,D14-1081,0,0.0457333,"literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rat"
P16-1132,P15-1031,0,0.0450277,"Missing"
P16-1132,P13-1045,0,0.206611,"ve accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016."
P16-1132,P15-1032,0,0.0169178,"by replacing the SVM classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively"
P16-1132,P15-1117,1,0.854947,"classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another"
P16-1132,P15-1112,0,0.250311,"S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rather weak feature lo"
P16-1132,J93-2004,0,0.0549301,"Missing"
P17-2092,P07-2045,0,0.006204,"on in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated withIn typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granular"
P17-2092,P05-1033,0,0.138079,"raged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguisti"
P17-2092,N03-1017,0,0.0576836,"ularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computat"
P17-2092,D14-1179,0,0.106361,"Missing"
P17-2092,P16-1160,0,0.309404,"ut the phrasal component may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, o"
P17-2092,P06-1077,0,0.211161,"Missing"
P17-2092,P16-2058,0,0.0225803,"Missing"
P17-2092,P16-1078,0,0.311835,"h global reordering of phrases and local translation inside phrases. Our model has following benefits: 1. The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible. 2. Our model recognizes phrase structures explicitly. Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words. 3. Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars. 4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart. Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously. 2 bush s"
P17-2092,D15-1166,0,0.0422838,"ine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2092 out attention. The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary"
P17-2092,P08-1114,0,0.19138,"Missing"
P17-2092,P17-1174,0,0.0518004,"f previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results. Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages. Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate. Table 4: Subjective evaluation results. System dl4mt This Work DE-14 16.53 17.40 DE-1213 16.78 17.45 Table 5: Results on German-English the translation is translated by. The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, wh"
P17-2092,P02-1040,0,0.102378,"from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the"
P17-2092,D13-1176,0,0.0742459,"updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work"
P17-2092,W16-2209,0,0.184245,"ponent may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a ch"
P17-2092,Q17-1007,1,0.89772,"Missing"
P17-2092,P16-1162,0,0.0186724,". The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk"
P17-2092,P16-1008,1,0.910907,"Missing"
P17-2092,P08-1066,0,0.0644196,"Missing"
P17-2092,P16-1218,0,0.0232133,"to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the same as the previous step. In the U PDATE operation, ept−1 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016): Ts ! log P (yn |xn ) + log P (ln |xn ) + log P (bn |xn ) bt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not. Two different operations would be executed: # pt−1 , bt = 0 (C OPY ) pt = g(pt−1 , ept−1 , pct ), bt = 1 (U PDATE ) ept−1 = m(st−1 , eyt−1 ) − m(st′ , eyt′ ) N & ! (10) The chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt−1 rather than word state st−1 , and 2) it is only executed at boundary of each chunk rather than at each decoding step. In this way, our model only extracts"
P17-2092,D16-1159,0,0.0691766,"Missing"
P17-2092,1983.tc-1.13,0,0.186514,"Missing"
P17-2092,P16-2049,0,0.0402049,"Missing"
P17-2092,P15-1117,1,0.838643,"Chinese-English translation task. Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag seq"
P17-2092,Q17-1026,0,\N,Missing
P19-1125,W17-4123,0,0.186965,"al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables"
P19-1125,D16-1009,0,0.0211846,"(T 0 t/T ) (Gu et al., 2017; Lee et al., 2018). As the source and target sentences are often of different lengths, AT model need to predict the target length T 0 during inference stage. The length prediction problem can be viewed as a typical classification problem based on the output of the encoder. we follow Lee et al. (2018) to predict the length of the target sequence. The proposed Round function is unstable and non-differentiable, which make the decoding task difficult. We therefore propose a differentiable and robust method named SoftCopy following the spirit of the attention mechanism (Hahn and Keller, 2016; Bengio, 2009). The weight wi,j depends on the distance relationship between the source position i and the target position j. wij = softmax(−|j − i|/τ ) (10) where xi is usually the source embedding at position i. It is also worth mentioning that we take the top-most hidden states instead of the word embedding as xi in order to cache the global context information. 3.3.2 Learning from AT Experts The conditional independence assumption prevents NAT model from properly capturing the highly multimodal distribution of target translations. AT models takes already generated target tokens as inputs,"
P19-1125,P82-1020,0,0.816089,"Missing"
P19-1125,P07-2045,0,0.0125476,"or method in (Gu et al., 2017), (Lee et al., 2018) and (Kaiser et al., 2018) respectively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We rep"
P19-1125,N03-1017,0,0.0808619,"the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De. 1 (a) Autoregressive NMT (b) Non-Autoregressive NMT Figure 1: Neural architectures for Autoregressive NMT and Non-Autoregressive NMT. Introduction Neural machine translation (NMT) with encoderdecoder architectures (Sutskever et al., 2014; Cho et al., 2014) achieve significantly improved performance compared with traditional statistical methods(Koehn et al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al."
P19-1125,D18-1149,0,0.388239,"2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computat"
P19-1125,D18-1336,0,0.243494,"ss, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving sig"
P19-1125,P16-1162,0,0.149108,"tively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We replace the reference target sentence of each pair of training example (X, Y ) with a new"
P19-1125,D18-1044,0,0.0568991,"perty of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving significantly fast tran"
P19-1559,D18-1316,0,0.593166,"d and Dotted lines represent decision boundaries before and after adversarial training, respectively. As unfluent adversarial examples are not in the manifold of real sentences, the victim model only needs to adjust its decision boundary out of the sentence manifold to fit them. As a result, fluent adversarial examples may be more effective than unfluent ones. propose to perturb a sentence by flipping one of the characters, and use the gradient of each perturbation to guide sample selection. But simple character flipping often leads to meaningless words (eg. “mood” to “mooP”). Genetic attack (Alzantot et al., 2018) is a population-based word replacing attacker, which aims to generate fluent sentences by filtering out the unreasonable sentences with a language model. But the fluency of examples generated by genetic attack is still not satisfactory and it is inefficient as the gradient is discarded. To address the aforementioned problems, we propose the Metropolis-Hastings attack (MHA) algorithm in this short paper. MHA is an adversarial example generator based on Metropolis-Hastings (M-H) sampling (Metropolis et al., 1953; HASTINGS, 1970; Chib and Greenberg, 1995). MH sampling is a classical MCMC samplin"
P19-1559,baccianella-etal-2010-sentiwordnet,0,0.140035,"Missing"
P19-1559,D15-1075,0,0.0365575,"-box MHA (bMHA) and a white-box MHA (w-MHA). Specifically, in contrast to previous language generation models using M-H, b-MHA’s stationary distribution is equipped with a language model term and an adversarial attacking term. The two terms make the generation of adversarial examples fluent and effective. w-MHA even incorporates adversarial gradients into proposal distributions to speed up the generation of adversarial examples. Our contributions include that we propose an efficient approach for generating fluent adversarial examples. Experimental results on IMDB (Maas et al., 2011) and SNLI (Bowman et al., 2015) show that, compared with the state-of-the-art genetic model, MHA generates examples faster, achieving higher success rates with much fewer invocations. Meanwhile, adversarial samples from MHA are not only more fluent but also more effective to improve the adversarial robustness and classification accuracy after adversarial training. 2 Preliminary Generally, adversarial attacks aim to mislead the neural models by feeding adversarial examples with perturbations, while adversarial training aims to improve the models by utilizing the perturbed examples. Adversarial examples fool the model into pr"
P19-1559,P18-2006,0,0.131084,"ls (such as a text classifier) is extremely challenging. Firstly, it is difficult to perform gradientbased perturbations since the sentence space is discrete. However, gradient information is critical – it leads to the steepest direction to more effective examples. Secondly, adversarial examples are usually not fluent sentences. Unfluent examples are less effective in attacking, as victim models can easily learn to recognize them. Meanwhile, adversarial training on them usually does not perform well (see Figure 1 for detailed analysis). Current methods cannot properly handle the two problems. Ebrahimi et al. (2018) (HotFlip) ∗ Work done while Huangzhao Zhang was a research intern in ByteDance AI Lab, Beijing, China. Figure 1: Effect of adversarial training on (a) fluent and (b) unfluent adversarial examples. ◦ and • represent positive and negative samples in the training set, while M and N are the corresponding adversarial examples. Solid and Dotted lines represent decision boundaries before and after adversarial training, respectively. As unfluent adversarial examples are not in the manifold of real sentences, the victim model only needs to adjust its decision boundary out of the sentence manifold to f"
P19-1559,esuli-sebastiani-2006-sentiwordnet,0,0.00946615,"Missing"
P19-1559,W16-5502,0,0.028268,"Missing"
P19-1559,P11-1015,0,0.106593,"riants of MHA, namely a black-box MHA (bMHA) and a white-box MHA (w-MHA). Specifically, in contrast to previous language generation models using M-H, b-MHA’s stationary distribution is equipped with a language model term and an adversarial attacking term. The two terms make the generation of adversarial examples fluent and effective. w-MHA even incorporates adversarial gradients into proposal distributions to speed up the generation of adversarial examples. Our contributions include that we propose an efficient approach for generating fluent adversarial examples. Experimental results on IMDB (Maas et al., 2011) and SNLI (Bowman et al., 2015) show that, compared with the state-of-the-art genetic model, MHA generates examples faster, achieving higher success rates with much fewer invocations. Meanwhile, adversarial samples from MHA are not only more fluent but also more effective to improve the adversarial robustness and classification accuracy after adversarial training. 2 Preliminary Generally, adversarial attacks aim to mislead the neural models by feeding adversarial examples with perturbations, while adversarial training aims to improve the models by utilizing the perturbed examples. Adversarial"
P19-1602,P97-1064,0,0.0341154,"onal auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et a"
P19-1602,P17-1177,1,0.860672,"om/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards the syntax of a sen"
P19-1602,W01-0713,0,0.0302084,"ed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John e"
P19-1602,W18-6503,0,0.0200293,"AE could graft the designed syntax to another sentence under certain circumstances. 2 Related Work The variational auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating samplin"
P19-1602,N16-1024,0,0.03024,"lly applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such"
P19-1602,P16-1078,0,0.0219963,"release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely"
P19-1602,P02-1040,0,0.104047,"9.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Weight 1.3 1.2 1.0 0.7 0.5 0.3 0.1 10 9 8 Table 1: BLEU and Forward PPL of VAE with varying KL weights on the PTB test set. The larger↑ (or lower↓ ), the better. 7 31 36 41 46 51 Forward-PPL 1. Reconstruction BLEU. The reconstruction task aims to generate the input sentence itself. In the task, both syntactic and semantic vectors are chosen as the predicted mean of the encoded distribution. We evaluate the reconstruction performance by the BLEU score (Papineni et al., 2002) with input as the reference.3 It reflects how well the model could preserve input information, and is crucial for representation learning and “goal-oriented” text generation. 2. Forward PPL. We then perform unconditioned generation, where both syntactic and semantic vectors are sampled from prior. Forward perplexity (PPL) (Zhao et al., 2018) is the generated sentences’ perplexity score predicted by a pertained language model.4 It shows the fluency of generated sentences from VAE’s prior. We computed Forward PPL based on 100K sampled sentences. 3. Reverse PPL. Unconditioned generation is furth"
P19-1602,D17-1066,0,0.0159685,"ee sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually"
P19-1602,E17-1117,0,0.0226876,"NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such as sentiment. We move one"
P19-1602,N18-1169,0,0.105168,"Missing"
P19-1602,P17-1064,0,0.0150872,"https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards"
P19-1602,D18-1423,0,0.108519,"better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the l"
P19-1602,D17-1013,1,0.827385,"uth training signals; in testing, we do not need external syntactic trees. We build an RNN 1 https://www.sutd.edu.sg/cmsresource/faculty/yuezhang/ zpar.html 6010 with softmax, whose objective is the cross-entropy loss against the groundtruth distribution t, given by: X tw log p(w|zsem ) (3) L(mul) sem = − This is ... S NP .. . /S This is This is ... w∈V ... S NP .. . /S This is ... Figure 2: Overview of our DSS-VAE. Forward dashed arrows are multi-task losses; backward dashed arrows are adversarial losses. where p(w|zsyn ) is the predicted distribution. BoW has been explored by previous work (Weng et al., 2017; John et al., 2018), showing good ability of preserving semantics. For the syntactic space, the multi-task loss trains a model to predict syntax on zsyn . Due to our proposal in §3.2.1, we could build a dedicated RNN, predicting the tokens in the linearized parse tree sequence, whose loss is: Xn log p(si |s1 · · · si−1 , zsyn ) (4) L(mul) syn = − i=1 (independent of the VAE’s decoder) to predict such linearized parse trees, where each parsing token is represented by an embedding (similar to a traditional RNN decoder). Notice that, a node and its backtracking, e.g., NP and /NP, have different"
P19-1602,P18-2053,0,0.0136272,"ters are not updated. 3.2.3 Adversarial Reconstruction Loss Our next intuition is that syntax and semantics are more interwoven to each other than other information such as style and content. Suppose, for example, the syntax and semantics have been perfectly separated by the losses in 6011 §3.2.2, where zsem could predict BoW well, but does not contain any information about the syntactic tree. Even in this ideal case, the decoder can reconstruct the original sentence from zsem by simply learning to re-order words (as zsem does contain BoW). Such word re-ordering knowledge is indeed learnable (Ma et al., 2018), and does not necessarily contain the syntactic information. Therefore, the multi-task and adversarial losses for syntax and semantics do not suffice to regularize DSS-VAE. We now propose an adversarial reconstruction loss to discourage the sentence being predicted by a single subspace zsyn or zsem . When combined, however, they should provide a holistic view of the entire sentence. Formally, let zs be a latent variable (zs = zsyn or zsem ). A decoding adversary is trained to predict the sentence based on zs , denoted by prec (xi |x1 · · · xi−1 , zs ). Then, the adversarial reconstruction los"
P19-1602,J93-2004,0,0.0651646,"er during training. 4 Experiments We evaluate our method on reconstruction and unconditional language generation (§4.1). Then, we apply it two applications, namely, unsupervised paraphrase generation (§4.2) and syntax-transfer generation (§4.3). 4.1 Reconstruction and Unconditional Language Generation First, we compare our model in reconstruction and unconditional language generation with a traditional VAE and a syntactic language model (PRPN, Shen et al., 2017). Dataset We followed previous work (Bowman et al., 2016) and used a standard benchmark, the WSJ sections in the Penn Treebank (PTB) (Marcus et al., 1993). We also followed the standard split: Sections 2–21 for training, Section 24 for validation, and Section 23 for test. Settings We trained VAE and DSS-VAE, both with 100-dimensional RNN states. For the vocabulary, we chose 30k most frequent words. We trained PRPN with the default parameter in the code base.2 Evaluation We evaluate model performance with the following metrics: 6012 2 https://github.com/yikangshen/PRPN 47.33 8.98 45.6 9.6 49.73 9.36 49.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Wei"
P19-1602,P17-2092,1,0.84797,"tion and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and mani"
P19-1602,P01-1017,0,\N,Missing
P19-1602,D14-1179,0,\N,Missing
P19-1602,K16-1002,0,\N,Missing
P19-1617,N19-1240,0,0.0561575,"Missing"
P19-1617,N18-2007,0,0.217335,". This setup is versatile and does not rely on any additional predefined knowledge base. Therefore the models are expected to generalize well and to answer questions in open domains. There are two main challenges to answer questions of this kind. Firstly, since not every document contain relevant information, multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information. To address this, recent studies propose to build entity graphs from input paragraphs and apply graph neural networks (GNNs) to aggregate the information through entity graphs (Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a). However, all of the existing work apply GNNs based on a static global entity graph of each QA pair, which can be considered as performing implicit reasoning. Instead of them, we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query. a fusion process in DFGN to solve the unrestricted QA challenge. We not only aggregate information from documents to the entity graph (doc2graph), but also propagate the information of the entity graph back to document representations"
P19-1617,P14-5010,0,0.00431779,"cores greater than η (= 0.1 in experiments) are selected and concateInput Documents Input Query Paragraph Selector Graph Constructor Context Entity Graph Encoder BERT Bi-attention Fusion Block multi-hop LSTM Prediction Layer Supporting Sentences Answer Span Answer Type Figure 3: Overview of DFGN. nated together as the context C. η is properly chosen to ensure the selector reaches a significantly high recall of relevant paragraphs. Q and C are further processed by upper layers. 3.2 Constructing Entity Graph We do not assume a global knowledge base. Instead, we use the Stanford corenlp toolkit (Manning et al., 2014) to recognize named entities from the context C. The number of extracted entities is denoted as N . The entity graph is constructed with the entities as nodes and edges built as follows. The edges are added 1. for every pair of entities appear in the same sentence in C (sentencelevel links); 2. for every pair of entities with the same mention text in C (context-level links); and 3. between a central entity node and other entities within the same paragraph (paragraph-level links). The central entities are extracted from the title sentence for each paragraph. Notice the context-level links ensur"
P19-1617,P18-1160,0,0.0306546,"Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three document paragraphs are given. Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraphs, predicting a dynamic mask to select a subgraph, propagating information along the graph, and f"
P19-1617,D16-1241,0,0.0283079,"onvolutional neural network (GCN) (Kipf and Welling, 2017). Coref-GRN, MHQA-GRN and Entity-GCN explore the graph construction problem in answering real-world questions. However, it is yet to investigate how to effectively reason about the constructed graphs, which is the main problem studied in this work. Another group of sequential models deals with multi-hop reasoning following Memory Networks (Sukhbaatar et al., 2015). Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner. Munkhdalai and Yu (2017) 6142 and Onishi et al. (2016) incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step. IRNet (Zhou et al., 2018) generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB. The ones with the highest score at each time step are linked together to form an interpretable reasoning chain. However, these models perform reasoning on simple synthetic datasets with a limited number of entities and relations, which are quite d"
P19-1617,P18-2124,0,0.0962946,"Missing"
P19-1617,D16-1264,0,0.774841,"racter created by Tom Clancy who appears in many of his novels and their respective ﬁlm adaptations ... Net Force Explorers is a series of young adult novels created by Tom Clancy and Steve Pieczenik as a spin-off of the military ﬁction series ... Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three docu"
P19-1617,W03-0419,0,0.159076,"Missing"
P19-1617,P18-1150,0,0.176394,"on any additional predefined knowledge base. Therefore the models are expected to generalize well and to answer questions in open domains. There are two main challenges to answer questions of this kind. Firstly, since not every document contain relevant information, multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information. To address this, recent studies propose to build entity graphs from input paragraphs and apply graph neural networks (GNNs) to aggregate the information through entity graphs (Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a). However, all of the existing work apply GNNs based on a static global entity graph of each QA pair, which can be considered as performing implicit reasoning. Instead of them, we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query. a fusion process in DFGN to solve the unrestricted QA challenge. We not only aggregate information from documents to the entity graph (doc2graph), but also propagate the information of the entity graph back to document representations (graph2doc). The fusion process is itera"
P19-1617,N18-1059,0,0.176897,"ic mask to select a subgraph, propagating information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting two or more evidence from the documents and infe"
P19-1617,P17-1018,0,0.0465391,"t novels created by Tom Clancy and Steve Pieczenik as a spin-off of the military ﬁction series ... Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three document paragraphs are given. Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraph"
P19-1617,Q18-1021,0,0.0618714,"om multiple paragraphs, predicting a dynamic mask to select a subgraph, propagating information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting t"
P19-1617,D18-1259,0,0.703628,"g information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting two or more evidence from the documents and inferring among them (see Figure 1 for"
P19-1617,C18-1171,0,0.0194782,"gate how to effectively reason about the constructed graphs, which is the main problem studied in this work. Another group of sequential models deals with multi-hop reasoning following Memory Networks (Sukhbaatar et al., 2015). Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner. Munkhdalai and Yu (2017) 6142 and Onishi et al. (2016) incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step. IRNet (Zhou et al., 2018) generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB. The ones with the highest score at each time step are linked together to form an interpretable reasoning chain. However, these models perform reasoning on simple synthetic datasets with a limited number of entities and relations, which are quite different with largescale QA dataset with complex questions. Also, the supervision of entity-level reasoning chains in synthetic datasets can be easily given following some patterns while they are"
Q18-1011,D17-1151,0,0.0132202,"aseline. Comparison with Other Work. (Rows 9-11). We also conduct experiments with multi-layer decoders (Wu et al., 2016) to see whether the NMT system can automatically model the translated and untranslated contents with additional decoder lay152 ers (Rows 9-10). However, we find that the performance is not improved using a two-layer decoder (Row 9), until a deeper version (three-layer decoder, Row 10) is used. This indicates that enhancing performance by simply adding more RNN layers into the decoder without any explicit instruction is nontrivial, which is consistent with the observation of Britz et al. (2017). Our model also outperforms the word-level C OVERAGE (Tu et al., 2016), which considers the coverage information of the source words independently. Our proposed model can be regarded as a high-level coverage model, which captures higher level coverage information, and gives more specific signals for the decision of attention and target prediction. Our model is more deeply involved in generating target words, by being fed not only to the attention model as in Tu et al. (2016), but also to the decoder state. 5.1.2 Subjective Evaluation Following Tu et al. (2016), we conduct subjective evaluatio"
Q18-1011,W17-3203,0,0.0258306,",000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate"
Q18-1011,W17-4725,0,0.0491104,"Missing"
Q18-1011,D13-1176,0,0.0776419,"TURE contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source. During translation, the decoder implicitly serves several functionalities at the same time: * Equal contributions. Our code can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT)."
Q18-1011,D15-1166,0,0.161208,"de can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT). 3. Maintaining what parts in the source have been translated (PAST) and what parts have not (F UTURE). However, it may be difficult for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation"
Q18-1011,C16-1205,0,0.043448,"del both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work. In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach. Future Modeling. Standard neural sequence decoders"
Q18-1011,D16-1096,0,0.0876841,"ly helps the prediction at the beginning of the sentence. We attribute the vanishing of such signals to the overloaded use of decoder states (e.g., L M, PAST, and F UTURE functionalities), and hence we propose to explicitly model the holistic source summarization by PAST and F UTURE contents at each decoding step. 3 Related Work Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. Coverage Modeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones. Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further. We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) ins"
Q18-1011,D16-1147,0,0.0206683,"o predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rockt¨aschel et al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rockt¨aschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism. In this work, we further separate PAST and F UTURE functionalities from the decoder’s hidden representations. 4 Modeling PAST and F UTURE fo"
Q18-1011,J03-1002,0,0.0224634,") . | {z } PAST loss Dataset. We conduct experiments on ChineseEnglish (Zh-En), German-English (De-En), and English-German (En-De) translation tasks. For Zh-En, the training set consists of 1.6m sentence pairs, which are extracted from the LDC corpora3 . The NIST 2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards port"
Q18-1011,P02-1040,0,0.100819,"rformance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of join"
Q18-1011,E17-2025,0,0.0188295,"14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does"
Q18-1011,W17-4738,0,0.0132824,"uishes the PAST and F UTURE directly, which is a higher level coverage mechanism than the word coverage model. 153 ∆ – -1.00 -3.83 Table 4: Evaluation of the alignment quality. The lower the score, the better the alignment quality. 5.2 Results on German-English We also evaluate our model on the WMT17 benchmarks for both De-En and En-De. As shown in Table 5, our baseline gives comparable BLEU scores to the state-of-the-art NMT systems of WMT17. Our proposed model improves the strong baseline on both De-En and En-De. This shows that our proposed model works well across different language pairs. Rikters et al. (2017) and Sennrich et al. (2017a) obtain higher BLEU scores than our model, because they use additional large scale synthetic data (about 10M) for training. It maybe unfair to compare our model to theirs directly. 5.3 Alignment Quality AER 39.73 38.73 35.90 Analysis We conduct analyses on Zh-En, to better understand our model from different perspectives. Parameters and Speeds. As shown in Table 6, the baseline model (BASE) has 80M parameters. A single F UTURE or PAST layer introduces 15M to 17M parameters, and the corresponding objective introduces 18M parameters. In this work, the most complex mod"
Q18-1011,P16-1162,0,0.121344,"2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For"
Q18-1011,E17-3017,0,0.0569003,"Missing"
Q18-1011,P16-1159,0,0.0842396,"4 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate the proposed model on the Chinese-English translation and alignment tasks. 5.1.1 Translation Quality Table 2 shows the translation performances on Chinese-English. Clearly the proposed approach significantly improves the translation quality in all cases, although there are still considerable differences among different variants. F UTURE Layer. (Rows 1-4). All the activation functions for the F UTURE layer obtain BLEU score im"
Q18-1011,P16-1008,1,0.87499,"election over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (Tu et al., 2016). Ideally, PAST grows and F UTURE declines during the translation process. However, it may be difficult for a single RNN to explicitly model the above processes. In this paper, we propose a novel neural machine translation system that explicitly models PAST and F UTURE contents with two additional RNN layers. The RNN modeling the PAST contents (called PAST layer) starts from scratch and accumulates the in145 Transactions of the Association for Computational Linguistics, vol. 6, pp. 145–157, 2018. Action Editor: Philipp Koehn. Submission batch: 6/2017; Revision batch: 9/2017; Published 3/2018."
Q18-1011,D16-1027,0,0.0173536,"nslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated). In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients"
Q18-1011,D17-1013,1,0.785512,"g feature of the proposed approach. Future Modeling. Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017), in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder’s hidden states to not only generate the current target word, but also predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be allevi"
W12-6312,C10-2139,0,0.0228617,"if ci is numeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under differe"
W12-6312,D11-1090,0,0.0502279,"umeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic conte"
W12-6312,J04-1004,0,0.0396815,", a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic contexts, it may carry a meaning. The more contexts a string appears in, the more likely it is a independent word. Given a string s, we define the left accessor variety of s as the number of distinct characters that precede s in the corpus, denoted by LAV (s). The higher value LAV (s) is, the more likely that s can be separated at its start position. Similarly, right accessor variety of s is defined as the number of distinct charact"
W12-6312,J09-4006,0,0.027947,"Missing"
W12-6312,O03-4002,0,0.619544,"exist in micro-blog text, and we call it rule-based adaptation. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score, compared with 88.73 points of F-score of the unadapted CRF word segmenter on the pre-released development data. Our system achieved 92.51 points of F-score on the final test data. 1 Introduction Recent years have witnessed the great development of Chinese word segmentation (CWS) techniques. Among various approaches, character labelling via Conditional Random Field (CRF) modelling has become a prevailing technique (Lafferty et al., 2001; Xue, 2003; Zhao et al., 2006), due to its good performance in OOV words recognition and low development cost. Given a large-scale corpus with human annotation, the only issue the developer need to focus on is to design an expressive set of feature templates which 63 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 63–68, Tianjin, China, 20-21 DEC. 2012 characters in emoticons, name entities and special punctuation patterns which extensively exist in micro-blog text. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score,"
W12-6312,W06-0127,0,\N,Missing
W12-6312,W10-4132,0,\N,Missing
W19-8604,N19-1320,0,0.0122192,"ings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tence unchanged (Fu et al., 2018; Ficler and Goldberg, 2017; Hu et al., 2017). Because of the lack of parallel datasets, most models focus on the unpaired transfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts"
W19-8604,D14-1181,0,0.00903277,"Missing"
W19-8604,N18-1169,0,0.137121,"ntence structure unchanged. (SOTA) models have achieved inspiring transfer success rates (Zhao et al., 2018; Zhang et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018). However, it is still unclear in current literature about what is transferred and what remains to be unchanged during the transfer process. To answer this question, we perform an in-depth investigation of the linguistic attribute transfer datasets and models. Our investigation starts from a simple observation: in many transfer datasets and models, certain class-related words play very important roles in attribute transfer (Li et al., 2018; Prabhumoye et al., 2018). Figure 1 gives a sentiment transfer example from the controllable generation (CG) model (Hu et al., 2017) on the Yelp dataset. In this example, rude is strongly related to the negative sentiment and good is strongly related to the positive sentiment, thus simply substituting rude with good will transfer the sentence from negative to positive. In this work, We name these words the pivot words for a class. We use the term the pivot effect to refer the effect that certain strong words may be able to determine the class of a sentence. Based on the observation of the piv"
W19-8604,D18-1420,0,0.0139908,"ext style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts from the classification task and mines the words with strong predictive performance. While many previous works focus on one-toone transfer, many recent works extend this task to one-to-many transfer (Logeswaran et al., 2018; Liao et al., 2018; Subramanian et al., 2019). For simplicity, we focus on the one-to-one setting. But it is also easy to extend the pivot analysis into oneto-many transfer settings. form the attribute transfer or it may change higherlevel sentence composationality like syntax? To answer question (1), we propose the pivot analysis, a series of simple yet effective text mining algorithms, to quantitatively examine the pivot effects in different datasets. The basics of the datasets we investigate are listed in Table 1. We first give the algorithm to extract pivot words (Sec 3). We statistically show the stronger"
W19-8604,P19-1601,0,0.0575752,"ttributes, and use the term style or attribute according to the context. 24 Proceedings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tence unchanged (Fu et al., 2018; Ficler and Goldberg, 2017; Hu et al., 2017). Because of the lack of parallel datasets, most models focus on the unpaired transfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classi"
W19-8604,N19-1049,0,0.0670766,"sfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts from the classification task and mines the words with strong predictive performance. While many previous works focus on one-toone transfer, many recent works extend this task to one-to-many transfer (Logeswaran et al., 2018; Liao et al., 2018; Subramanian et al., 2019). For simplicity, we focus on the one-to-one setting. But it i"
W19-8604,W17-4912,0,0.0311698,"s for a class. We use the term the pivot effect to refer the effect that certain strong words may be able to determine the class of a sentence. Based on the observation of the pivot effect, our research questions are: (1) which words are pivot words and how do they influence the attribute class of a sentence in different datasets? (2) does the model only need to modify the pivot words to perIntroduction The task of text attribute transfer (or text style transfer 2 ) is to transform certain linguistic attributes (sentiment, style, authorship, rhetorical devices, etc.) from one type to another (Ficler and Goldberg, 2017; Fu et al., 2018; Hu et al., 2017; Li et al., 2018; Shen et al., 2017). The state-of-the-art ∗ Work done when Yao was an intern at Bytedance AI Lab. 1 Our code can be found at https://github.com/FranxYao/pivot analysis 2 Many existing works also call this task style transfer(Fu et al., 2018), our work view style as one of the linguistic attributes, and use the term style or attribute according to the context. 24 Proceedings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tenc"
W19-8604,P18-1080,0,0.0999749,"Missing"
W19-8604,P18-2031,0,0.0975152,"Missing"
W19-8604,P18-1108,0,0.0384371,"Missing"
W19-8604,D18-1488,0,0.0670857,"Missing"
W19-8604,N18-1138,0,0.0638871,"rform the lexical-level modification, while leaving higher-level sentence structures unchanged. Our work provides an in-depth understanding of linguistic attribute transfer and further identifies the future requirements and challenges of this task1 . 1 Figure 1: Examples of pivot words in sentiment transfer. Certain words are strongly correlated with the sentiment such that a transfer model only need to modify these words to accomplish the transfer task while leaving the higher level sentence structure unchanged. (SOTA) models have achieved inspiring transfer success rates (Zhao et al., 2018; Zhang et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018). However, it is still unclear in current literature about what is transferred and what remains to be unchanged during the transfer process. To answer this question, we perform an in-depth investigation of the linguistic attribute transfer datasets and models. Our investigation starts from a simple observation: in many transfer datasets and models, certain class-related words play very important roles in attribute transfer (Li et al., 2018; Prabhumoye et al., 2018). Figure 1 gives a sentiment transfer example from the controllable generation (CG) mo"
W19-8604,P18-1090,0,\N,Missing
