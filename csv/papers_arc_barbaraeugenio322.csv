2021.sigdial-1.31,Summarizing Behavioral Change Goals from {SMS} Exchanges to Support Health Coaches,2021,-1,-1,2,1,1523,itika gupta,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved mental well-being. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting health coaches by extracting the physical activity goal the user and coach negotiate via text messages. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle."
2020.sigdial-1.30,"Human-Human Health Coaching via Text Messages: Corpus, Annotation, and Analysis",2020,-1,-1,2,1,1523,itika gupta,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Our goal is to develop and deploy a virtual assistant health coach that can help patients set realistic physical activity goals and live a more active lifestyle. Since there is no publicly shared dataset of health coaching dialogues, the first phase of our research focused on data collection. We hired a certified health coach and 28 patients to collect the first round of human-human health coaching interaction which took place via text messages. This resulted in 2853 messages. The data collection phase was followed by conversation analysis to gain insight into the way information exchange takes place between a health coach and a patient. This was formalized using two annotation schemas: one that focuses on the goals the patient is setting and another that models the higher-level structure of the interactions. In this paper, we discuss these schemas and briefly talk about their application for automatically extracting activity goals and annotating the second round of data, collected with different health coaches and patients. Given the resource-intensive nature of data annotation, successfully annotating a new dataset automatically is key to answer the need for high quality, large datasets."
2020.nuse-1.15,Detecting and understanding moral biases in news,2020,-1,-1,2,0,15983,usman shahid,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events",0,"We describe work in progress on detecting and understanding the moral biases of news sources by combining framing theory with natural language processing. First we draw connections between issue-specific frames and moral frames that apply to all issues. Then we analyze the connection between moral frame presence and news source political leaning. We develop and test a simple classification model for detecting the presence of a moral frame, highlighting the need for more sophisticated models. We also discuss some of the annotation and frame detection challenges that can inform future research in this area."
2020.nlpmc-1.6,Heart Failure Education of {A}frican {A}merican and {H}ispanic/{L}atino Patients: Data Collection and Analysis,2020,-1,-1,2,1,1523,itika gupta,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,0,"Heart failure is a global epidemic with debilitating effects. People with heart failure need to actively participate in home self-care regimens to maintain good health. However, these regimens are not as effective as they could be and are influenced by a variety of factors. Patients from minority communities like African American (AA) and Hispanic/Latino (H/L), often have poor outcomes compared to the average Caucasian population. In this paper, we lay the groundwork to develop an interactive dialogue agent that can assist AA and H/L patients in a culturally sensitive and linguistically accurate manner with their heart health care needs. This will be achieved by extracting relevant educational concepts from the interactions between health educators and patients. Thus far we have recorded and transcribed 20 such interactions. In this paper, we describe our data collection process, thematic and initiative analysis of the interactions, and outline our future steps."
2020.lrec-1.74,Augmenting Small Data to Classify Contextualized Dialogue Acts for Exploratory Visualization,2020,-1,-1,2,1,9819,abhinav kumar,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Our goal is to develop an intelligent assistant to support users explore data via visualizations. We have collected a new corpus of conversations, CHICAGO-CRIME-VIS, geared towards supporting data visualization exploration, and we have annotated it for a variety of features, including contextualized dialogue acts. In this paper, we describe our strategies and their evaluation for dialogue act classification. We highlight how thinking aloud affects interpretation of dialogue acts in our setting and how to best capture that information. A key component of our strategy is data augmentation as applied to the training data, since our corpus is inherently small. We ran experiments with the Balanced Bagging Classifier (BAGC), Condiontal Random Field (CRF), and several Long Short Term Memory (LSTM) networks, and found that all of them improved compared to the baseline (e.g., without the data augmentation pipeline). CRF outperformed the other classification algorithms, with the LSTM networks showing modest improvement, even after obtaining a performance boost from domain-trained word embeddings. This result is of note because training a CRF is far less resource-intensive than training deep learning models, hence given a similar if not better performance, traditional methods may still be preferable in order to lower resource consumption."
2020.lrec-1.678,A Corpus for Visual Question Answering Annotated with Frame Semantic Information,2020,-1,-1,2,0,18001,mehrdad alizadeh,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Visual Question Answering (VQA) has been widely explored as a computer vision problem, however enhancing VQA systems with linguistic information is necessary for tackling the complexity of the task. The language understanding part can play a major role especially for questions asking about events or actions expressed via verbs. We hypothesize that if the question focuses on events described by verbs, then the model should be aware of or trained with verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. We created a new VQA dataset annotated with verb semantic information called imSituVQA. imSituVQA is built by taking advantage of the imSitu dataset annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet."
W19-5928,A Quantitative Analysis of Patients{'} Narratives of Heart Failure,2019,0,0,2,1,23758,sabita acharya,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Patients with chronic conditions like heart failure are the most likely to be re-hospitalized. One step towards avoiding re-hospitalization is to devise strategies for motivating patients to take care of their own health. In this paper, we perform a quantitative analysis of patients{'} narratives of their experience with heart failure and explore the different topics that patients talk about. We compare two different groups of patients- those unable to take charge of their illness, and those who make efforts to improve their health. We will use the findings from our analysis to refine and personalize the summaries of hospitalizations that our system automatically generates."
N18-4011,Towards Generating Personalized Hospitalization Summaries,2018,0,0,2,1,23758,sabita acharya,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Most of the health documents, including patient education materials and discharge notes, are usually flooded with medical jargons and contain a lot of generic information about the health issue. In addition, patients are only provided with the doctor{'}s perspective of what happened to them in the hospital while the care procedure performed by nurses during their entire hospital stay is nowhere included. The main focus of this research is to generate personalized hospital-stay summaries for patients by combining information from physician discharge notes and nursing plan of care. It uses a metric to identify medical concepts that are Complex, extracts definitions for the concept from three external knowledge sources, and provides the simplest definition to the patient. It also takes various features of the patient into account, like their concerns and strengths, ability to understand basic health information, level of engagement in taking care of their health, and familiarity with the health issue and personalizes the content of the summaries accordingly. Our evaluation showed that the summaries contain 80{\%} of the medical concepts that are considered as being important by both doctor and nurses. Three patient advisors (i.e. individuals who are trained in understanding patient experience extensively) verified the usability of our summaries and mentioned that they would like to get such summaries when they are discharged from hospital."
W16-6604,Generating summaries of hospitalizations: A new metric to assess the complexity of medical terms and their definitions,2016,9,2,2,1,23758,sabita acharya,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-3639,Towards a dialogue system that supports rich visualizations of data,2016,22,6,3,1,9819,abhinav kumar,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-0406,Hit Songs{'} Sentiments Harness Public Mood {\\&} Predict Stock Market,2016,11,0,3,0,34088,rachel harsley,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,None
W14-4402,{P}atient{N}arr: Towards generating patient-centric summaries of hospital stays,2014,16,3,1,1,1524,barbara eugenio,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"PatientNarr summarizes information taken from textual discharge notes written by physicians, and structured nursing documentation. It builds a graph that highlights the relationships between the two types of documentation; and extracts information from the graph for content planning. SimpleNLG is used for surface realization."
W13-4031,Multimodality and Dialogue Act Classification in the {R}obo{H}elper Project,2013,30,7,2,1,19418,lin chen,Proceedings of the {SIGDIAL} 2013 Conference,0,"We describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions (force exchanges). Haptic actions are rarely analyzed as fullfledged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction. We report our experiments on recognizing Dialogue Acts in both offline and online modes. Our results show that multimodal features and the dialogue game aid in DA classification."
W13-2134,{UIC}-{CSC}: The Content Selection Challenge Entry from the {U}niversity of {I}llinois at {C}hicago,2013,0,4,2,0,40985,hareen venigalla,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"This paper described UIC-CSC, the entry we submitted for the Content Selection Challenge 2013. Our model consists of heuristic rules based on co-occurrences of predicates in the training data."
P13-1027,Translating {I}talian connectives into {I}talian {S}ign {L}anguage,2013,27,2,2,0,38403,camillo lugaresi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This thesis is concerned with the problem of translating Italian connectives into LIS, the Italian Sign Language. We explain the interest and the unique challenges of this problem, and we introduce the ATLAS project for automatic translation from Italian to LIS, which forms the backdrop of our work.n We then detail the analysis we performed on the small bilingual corpus of Italian and LIS sentences that was provided by the ATLAS project. We propose an alignment method between the Italian and LIS syntax trees that helps identify the effect that an Italian connective has on the LIS translation of the sentence. This results in four possible ways that a connective can be translated: with a corresponding sign, by affecting the location or shape of other signs, by affecting the LIS syntax tree, or by being omitted altogether. A clustering process applied to these result groups produces a categorization of connectives that is linguistically meaningful. By training a decision tree based classifier, we are able to extract rules to determine how to translate a given connective, with the aim of integrating these rules into the ATLAS translation pipeline. Finally, we evaluate the performance of our approach in comparison with the ATLAS pipeline."
W12-1640,Improving Sentence Completion in Dialogues with Multi-Modal Features,2012,12,0,2,0,42428,anruo wang,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"With the aim of investigating how humans understand each other through language and gestures, this paper focuses on how people understand incomplete sentences. We trained a system based on interrupted but resumed sentences, in order to find plausible completions for incomplete sentences. Our promising results are based on multi-modal features."
N12-1058,Co-reference via Pointing and Haptics in Multi-Modal Dialogues,2012,14,5,2,1,19418,lin chen,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus. We show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions (actions that involve manipulating an object). After describing our annotation scheme, we discuss the co-reference models we learn from multi-modal features. The usage of haptic-ostensive actions in a co-reference model is a novel contribution of our work."
W11-2035,Improving Pronominal and Deictic Co-Reference Resolution with Multi-Modal Features,2011,13,4,3,1,19418,lin chen,Proceedings of the {SIGDIAL} 2011 Conference,0,"Within our ongoing effort to develop a computational model to understand multi-modal human dialogue in the field of elderly care, this paper focuses on pronominal and deictic co-reference resolution. After describing our data collection effort, we discuss our annotation scheme. We developed a co-reference model that employs both a simple notion of markable type, and multiple statistical models. Our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns."
W11-1408,Exploring Effective Dialogue Act Sequences in One-on-one Computer Science Tutoring Dialogues,2011,27,9,2,1,19418,lin chen,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present an empirical study of one-on-one human tutoring dialogues in the domain of Computer Science data structures. We are interested in discovering effective tutoring strategies, that we frame as discovering which Dialogue Act (DA) sequences correlate with learning. We employ multiple linear regression, to discover the strongest models that explain why students learn during one-on-one tutoring. Importantly, we define flexible DA sequence, in which extraneous DAs can easily be discounted. Our experiments reveal several cognitively plausible DA sequences which significantly correlate with learning outcomes."
W10-3016,A Lucene and Maximum Entropy Model Based Hedge Detection System,2010,6,4,2,1,19418,lin chen,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"This paper describes the approach to hedge detection we developed, in order to participate in the shared task at CoNLL-2010. A supervised learning approach is employed in our implementation. Hedge cue annotations in the training data are used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to determine uncertainty. By making use of Apache Lucene, we are able to do fuzzy string match to extract hedge cues, and to incorporate part-of-speech (POS) tags in hedge cues. Not only can our system determine the certainty of the sentence, but is also able to find all the contained hedges. Our system was ranked third on the Wikipedia dataset. In later experiments with different parameters, we further improved our results, with a 0.612 F-score on the Wikipedia dataset, and a 0.802 F-score on the biological dataset."
P10-1140,Generating Fine-Grained Reviews of Songs from Album Reviews,2010,22,12,2,0,45711,swati tata,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Music Recommendation Systems often recommend individual songs, as opposed to entire albums. The challenge is to generate reviews for each song, since only full album reviews are available on-line. We developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs. We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries; and a user study of the impact of the song review summaries on users' decision making processes. Users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review."
N10-2005,{KSC}-{P}a{L}: A Peer Learning Agent that Encourages Students to take the Initiative,2010,31,4,2,1,45785,cynthia kersey,Proceedings of the {NAACL} {HLT} 2010 Demonstration Session,0,"We present an innovative application of dialogue processing concepts to educational technology. In a previous corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We have incorporated this finding in KSC-PaL, a peer learning agent. KSC-PaL promotes learning by encouraging shifts in task initiative."
tretti-di-eugenio-2010-analysis,Analysis and Presentation of Results for Mobile Local Search,2010,6,0,2,0,46218,alberto tretti,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Aggregation of long lists of concepts is important to avoid overwhelming a small display. Focusing on the domain of mobile local search, this paper presents the development of an application to perform filtering and aggregation of results obtained through the Yahoo! Local web service. First, we performed an analysis of the data available through Yahoo! Local by crawling its database with over 170 thousand local listings located in Chicago. Then, we compiled resources and developed algorithms to filter and aggregate local search results. The methods developed exploit Yahoo!Âs listings categorization to reduce the result space and pinpoint the category containing the most relevant results. Finally, we evaluated a prototype through a user study, which pitted our system against Yahoo! Local and against a plain list of search results. The results obtained from the study show that our aggregation methods are quite effective, cutting down the number of entries returned to the user by 43{\%} on average, but leaving search efficiency and user satisfaction unaffected."
W09-2109,{KSC}-{P}a{L}: A Peer Learning Agent that Encourages Students to take the Initiative,2009,-1,-1,2,1,45785,cynthia kersey,Proceedings of the Fourth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
N09-1064,An effective Discourse Parser that uses Rich Linguistic Information,2009,25,72,2,1,1466,rajen subba,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper presents a first-order logic learning approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIPPER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline."
W08-1114,Simple but effective feedback generation to tutor abstract problem solving,2008,22,2,2,0,6853,xin lu,Proceedings of the Fifth International Natural Language Generation Conference,0,"To generate natural language feedback for an intelligent tutoring system, we developed a simple planning model with a distinguishing feature: its plan operators are derived automatically, on the basis of the association rules mined from our tutorial dialog corpus. Automatically mined rules are also used for realization. We evaluated 5 different versions of a system that tutors on an abstract sequence learning task. The version that uses our planning framework is significantly more effective than the other four versions. We compared this version to the human tutors we employed in our tutorial dialogs, with intriguing results."
xie-etal-2008-extracting,From Extracting to Abstracting: Generating Quasi-abstractive Summaries,2008,26,5,2,0.952381,48045,zhuli xie,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we investigate quasi-abstractive summaries, a new type of machine-generated summaries that do not use whole sentences, but only fragments from the source. Quasi-abstractive summaries aim at bridging the gap between human-written abstracts and extractive summaries. We present an approach that learns how to identify sets of sentences, where each set contains fragments that can be used to produce one sentence in the abstract; and then uses these sets to produce the abstract itself. Our experiments show very promising results. Importantly, we obtain our best results when the summary generation is anchored by the most salient Noun Phrases predicted from the text to be summarized."
fossati-di-eugenio-2008-saw,{I} saw {TREE} trees in the park: How to Correct Real-Word Spelling Mistakes,2008,24,21,2,1,44341,davide fossati,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents a context sensitive spell checking system that uses mixed trigram models, and introduces a new empirically grounded method for building confusion sets. The proposed method has been implemented, tested, and evaluated in terms of coverage, precision, and recall. The results show that the method is effective."
W06-2605,Discourse Parsing: Learning {FOL} Rules based on Rich Verb Semantic Representations to automatically label Rhetorical Relations,2006,16,4,2,1,1466,rajen subba,Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,0,We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences. We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing. We demonstrate that ILP can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers.
W06-1708,The problem of ontology alignment on the Web: A first report,2006,20,13,3,1,44341,davide fossati,Proceedings of the 2nd International Workshop on Web as Corpus,0,"This paper presents a general architecture and four algorithms that use Natural Language Processing for automatic ontology matching. The proposed approach is purely instance based, i.e., only the instance documents associated with the nodes of ontologies are taken into account. The four algorithms have been evaluated using real world test data, taken from the Google and LookSmart online directories. The results show that NLP techniques applied to instance documents help the system achieve higher performance."
subba-etal-2006-building,"Building lexical resources for {P}rinc{P}ar, a large coverage parser that generates principled semantic representations",2006,16,3,2,1,1466,rajen subba,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Parsing, one of the more successful areas of Natural Language Processing has mostly been concerned with syntactic structure. Though uncovering the syntactic structure of sentences is very important, in many applications a meaningrepresentation for the input must be derived as well. We report on PrincPar, a parser that builds full meaning representations. It integrates LCFLEX, a robust parser, with alexicon and ontology derived from two lexical resources, VerbNet and CoreLex that represent the semantics of verbs and nouns respectively. We show that these two different lexical resources that focus on verbs and nouns can be successfully integrated. We report parsing results on a corpus of instructional text and assess the coverage of those lexical resources. Our evaluation metric is the number of verb frames that are assigned a correct semantics: 72.2{\%} verb frames are assigned a perfect semantics, and another 10.9{\%} are assigned a partially correctsemantics. Our ultimate goal is to develop a (semi)automatic method to derive domain knowledge from instructional text, in the form of linguistically motivated action schemes."
P05-1007,Aggregation Improves Learning: Experiments in Natural Language Generation for Intelligent Tutoring Systems,2005,20,28,1,1,1524,barbara eugenio,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"To improve the interaction between students and an intelligent tutoring system, we developed two Natural Language generators, that we systematically evaluated in a three way comparison that included the original system as well. We found that the generator which intuitively produces the best language does engender the most learning. Specifically, it appears that functional aggregation is responsible for the improvement."
P04-1088,{FLSA}: Extending Latent Semantic Analysis with Features for Dialogue Act Classification,2004,19,54,2,0,51773,riccardo serafin,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We discuss Feature Latent Semantic Analysis (FLSA), an extension to Latent Semantic Analysis (LSA). LSA is a statistical method that is ordinarily trained on words only; FLSA adds to LSA the richness of the many other linguistic features that a corpus may be labeled with. We applied FLSA to dialogue act classification with excellent results. We report results on three corpora: CallHome Spanish, MapTask, and our own corpus of tutoring dialogues."
J04-3003,{C}entering: A Parametric Theory and Its Instantiations,2004,112,141,3,0,1743,massimo poesio,Computational Linguistics,0,"Centering theory is the best-known framework for theorizing about local coherence and salience; however, its claims are articulated in terms of notions which are only partially specified, such as utterance, realization, or ranking. A great deal of research has attempted to arrive at more detailed specifications of these parameters of the theory; as a result, the claims of centering can be instantiated in many different ways. We investigated in a systematic fashion the effect on the theory's claims of these different ways of setting the parameters. Doing this required, first of all, clarifying what the theory's claims are (one of our conclusions being that what has become known as Constraint 1 is actually a central claim of the theory). Secondly, we had to clearly identify these parametric aspects: For example, we argue that the notion of pronoun used in Rule 1 should be considered a parameter. Thirdly, we had to find appropriate methods for evaluating these claims. We found that while the theory's main claim about salience and pronominalization, Rule 1xe2x80x94a preference for pronominalizing the backward-looking center (CB)xe2x80x94is verified with most instantiations, Constraint 1xe2x80x93a claim about (entity) coherence and CB uniquenessxe2x80x94is much more instantiation-dependent: It is not verified if the parameters are instantiated according to very mainstream views (vanilla instantiation), it holds only if indirect realization is allowed, and is violated by between 20% and 25% of utterances in our corpus even with the most favorable instantiations. We also found a trade-off between Rule 1, on the one hand, and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of local coherence leads to increased violations of salience, and vice versa. Our results suggest that entity coherencexe2x80x94continuous reference to the same entitiesxe2x80x94must be supplemented at least by an account of relational coherence."
J04-1005,Squibs and Discussions: The Kappa Statistic: A Second Look,2004,0,8,1,1,1524,barbara eugenio,Computational Linguistics,0,None
C04-1202,Using Gene Expression Programming to Construct Sentence Ranking Functions for Text Summarization,2004,15,23,3,0.952381,48045,zhuli xie,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we consider the automatic text summarization as a challenging task of machine learning. We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism. The preliminary experimental results have shown that our prototype system outperforms the baseline systems."
N03-2032,Latent Semantic Analysis for Dialogue Act Classification,2003,12,15,2,0,51773,riccardo serafin,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"This paper presents our experiments in applying Latent Semantic Analysis (LSA) to dialogue act classification. We employ both LSA proper and LSA augmented in two ways. We report results on DIAG, our own corpus of tutoring dialogues, and on the CallHome Spanish corpus. Our work has the theoretical goal of assessing whether LSA, an approach based only on raw text, can be improved by using additional features of the text."
N03-2034,Building lexical semantic representations for Natural Language instructions,2003,7,3,2,0,50371,elena terenzi,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"We report on our work to automatically build a corpus of instructional text annotated with lexical semantics information. We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, Verb-Net for verbs and CoreLex for nouns. We discuss how we built our lexicon and ontology, and the parsing results we obtained."
W02-2116,The {DIAG} experiments: Natural Language Generation for Intelligent Tutoring Systems,2002,20,18,1,1,1524,barbara eugenio,Proceedings of the International Natural Language Generation Conference,0,None
W02-0205,{MUP} - The {UIC} Standoff Markup Tool,2002,11,3,2,0,3530,michael glass,Proceedings of the Third {SIG}dial Workshop on Discourse and Dialogue,0,"Recently developed markup tools for dialogue work are quite sophisticated and require considerable knowledge and overhead, but older tools do not support XML standoff markup, the current annotation style of choice. For the DIAG-NLP project we have created a lightweight but modern markup tool that can be configured and used by the working NLP researcher."
di-eugenio-etal-2002-binomial,"The binomial cumulative distribution function, or, is my system better than yours?",2002,9,2,1,1,1524,barbara eugenio,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In human language technology, it is becoming more and more common to run systematic evaluations in which two or more systems, or two or more versions of the same system, are pitted one against the other. We propose the binomial cumulative distribution function as a way to assess the cumulative effect of the measures collected in such evaluations. We present an application of this measure to the evaluation of the NL interface to an Intelligent Tutoring System. We conclude by discussing a few issues pertaining to this statistical measure."
di-eugenio-2000-usage,On the Usage of Kappa to Evaluate Agreement on Coding Tasks,2000,15,46,1,1,1524,barbara eugenio,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In recent years, the Kappa coefficient of agreement has become the de facto standard to evaluate intercoder agreement in the discourse and dialogue processing community. Together with the adoption of this standard, researchers have adopted one specific scale to evaluate Kappa values, the one proposed in (Krippendorff, 1980). In this position paper, I highlight some issues that should be taken into account when evaluating Kappa values. Finally, I speculate on whether Kappa could be used as a measure to evaluate a systemxe2x80x99s performance."
J00-2009,Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation,2000,0,0,1,1,1524,barbara eugenio,Computational Linguistics,0,None
P98-1052,An Empirical Investigation of Proposals in Collaborative Dialogues,1998,9,12,1,1,1524,barbara eugenio,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We describe a corpus-based investigation of proposals in dialogue. First, we describe our DRI compliant coding scheme and report our inter-coder reliability results. Next, we test several hypotheses about what constitutes a well-formed proposal."
J98-3001,Introduction to the Special Issue on Natural Language Generation,1998,31,19,2,0,42377,robert dale,Computational Linguistics,0,"There are two sides to natural language processing. On the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text--to an underlying representation of the meaning carried by that surface representation. But there is also the question of how one maps from some underlying representation of meaning into text or speech: this is the domain of natural language generation. Whether our end-goal is the construction of artifacts that use natural languages intelligently, the formal characterization of phenomena in human languages, or the computational modeling of the human language processing mechanism, we cannot ignore the fact that language is both spoken (or written) and heard (or read). Both are equally large and important problems, but the literature contains much less work on natural language generation (NLG) than it does on natural language understanding (NLU). There are many reasons why this might be so, although clearly an important one is that researchers in natural language understanding in some sense start out with a more well-defined task: the input is known, and there is a lot of it around. This is not the case in natural language generation: there, it is the desired output that is known, but the input is an unknown; and while the world is awash with text waiting to be processed, there are fewer instances of what we might consider appropriate inputs for the process of natural language generation. For researchers in the field, this highlights the fundamental question that always has to be asked: What do we generate from? Despite this problem, the natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman's work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and"
C98-1051,An Empirical Investigation of Proposals in Collaborative Dialogues,1998,9,12,1,1,1524,barbara eugenio,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We describe a corpus-based investigation of proposals in dialogue. First, we describe our DRI compliant coding scheme and report our inter-coder reliability results. Next, we test several hypotheses about what constitutes a well-formed proposal."
P97-1011,Learning Features that Predict Cue Usage,1997,25,42,1,1,1524,barbara eugenio,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous attempts to devise rules for text generation were based on intuition or small numbers of constructed examples. We apply a machine learning program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation."
W96-0509,Generating {`}Distributed{'} Referring Expressions: an Initial Report,1996,11,1,1,1,1524,barbara eugenio,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,"xe2x80xa2 Di Eugenio and Penstein Ros6 were partially supported by Carnegie Mellon Faculty Development Fund # 1-13709. LIn the appropriate context, (la) could also be interpreted as asking H to bring the ironing board into the basement, where S is (or will be, at the time H executes the action). We currently neglect this possibility. down to the basement is a substep in the plan that achieves bring S the ironing board. In these examples, the referring expression in the NP is not sufficient to uniquely identify the intended referent, but it is its linguistic context that adds other necessary constraints. This is the reason why we call these referring expressions distributed. To our knowledge, while many researchers have worked on generating referring expressions, e.g. [Appelt, 1985], [Kronfeld, 1990], [Dale, 1992], [Pattabhiraman and Cercone, 1990], 2 distributed referring expressions have not been addressed yet. Note that the whole linguistic context must be taken into account while generating (lc): this is shown by the redundant and infelicitous"
W96-0402,Learning Micro-Planning Rules for Preventive Expressions,1996,11,9,2,0,47769,keith linden,Eighth International Natural Language Generation Workshop,0,"Building text planning resources by hand is timeconsuming and difficult. Certainly, a number of planning architectures and their accompanying plan libraries have been implemented, but while the architectures themselves may be reused in a new domain, the library of plans typically cannot. One way to address this problem is to use machine learning techniques to automate the derivation of planning resources for new domains. In this paper, we apply this technique to build microplanning rules for preventative expressions in instructional text. 1 I n t r o d u c t i o n Building text planning resources by hand is timeconsuming and difficult. Certainly, much work has been done in this regard; there are a number of freely available text planning architectures (e.g., Moore and Paris, 1993). It is frequently the case, however, that while the architecture itself can be reused in a new domain, the library of text plans developed for it cannot. In particular, micro-planning rules, those rules that specify the low-level grammatical details of expression, are highly sensitive to variations between sublanguages, and are therefore difficult to reuse. When faced with a new domain in which to generate text, the typical scenario is to perform a * This work is partially supported by the Engineering and Physical Sciences Research Council (EPSRC) Grant J19221, by BC/DAA9 ARC Project 293, and by the Commission of the European Union Grant LRE-62009. t After September 1, Dr. Vander Linden's address will be Department of Mathematics and Computer Science, Calvin College, Grand Rapids, MI 49546, USA. corpus analysis on a representative collection of the text produced by human authors in that domain and to induce a set of micro-planning rules guiding the generation process in accordance with the results. Some fairly simple rules usually jump out of the analysis quickly, mostly based on the analyst's intuitions. For example, in written instructions, user actions are typically expressed as imperatives. Such observations, however, tend to be gross characterisations. More accurate microplanning requires painstaking analysis. In this paper, for example, the micro-planner must distinguish between phrasing such as Don't do action,V' and Take care not to do action-X. Without analysis, it is far from clear how this decision can best be made. Some form of automation would clearly be desirable. Unfortunately, corpus analysis techniques are not yet capable of automating the initial phases of the corpus study (nor will they be for the foreseeable future). There are, however, techniques for rule induction which are useful for the later stages of corpus analysis and for implementation. In this paper, we focus on the use of such rule induction techniques in the context of the microplanning of preventative expressions in instructional text. We define what we mean by a preventative expression, and go on to describe a corpus analysis in which we derive three features that predict the grammatical form of such expressions. We then use the C4.5 learning algorithm to construct a micro-planning sub-network appropriate for these expressions. We conclude with an implemented example in which the technical author is allowed to set the relevant features, and the system generates the appropriate expressions in English and in French."
C96-1059,A Corpus Study of Negative Imperatives in Natural Language Instructions,1996,17,10,2,0,47769,keith linden,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"In this paper, we define the notion of a preventative expression and discuss a corpus study of such expressions in instructional text. We discuss our coding schema, which takes into account both form and function features, and present measures of inter-coder reliability for those features. We then discuss the correlations that exist between the function and the form features."
C96-1060,The discourse functions of {I}talian subjects: a centering approach,1996,12,13,1,1,1524,barbara eugenio,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper examines the discourse functions that different types of subjects perform in Italian within the centering framework (Grosz et al., 1995). I build on my previous work (Di Eugenio, 1990) that accounted for the alternation of null and strong pronouns in subject position. I extend my previous analysis in several ways: for example, I refine the notion of CONTINUE and discuss the centering functions of full NPs."
C96-1061,Using Discourse Predictions for Ambiguity Resolution,1996,15,8,3,0,49828,yan qu,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"In this paper we discuss how we apply discourse predictions along with non context-based predictions to the problem of parse disambiguation in Enthusiast, a Spanish-to-English translation system (Woszcyna et al., 1993; Suhm et al., 1994; Levin et al., 1995). We discuss extensions to our plan-based discourse processor in order to make this possible. We evaluate those extensions and demonstrate the advantage of exploiting context-based predictions over a purely non context-based approach."
P95-1005,Discourse Processing of Dialogues with Multiple Threads,1995,13,52,2,1,2584,carolyn rose,33rd Annual Meeting of the Association for Computational Linguistics,1,In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-to-speech translation system. We will demonstrate that theories of discourse which postulate a strict tree structure of discourse on either the intentional or attentional level are not totally adequate for handling spontaneous dialogues. We will present our extension to this approach along with its implementation in our plan-based discourse processor. We will demonstrate that the implementation of our approach outperforms an implementation based on the strict tree structure approach.
W93-0204,Speaker{'}s Intentions and Beliefs in Negative Imperatives,1993,-1,-1,1,1,1524,barbara eugenio,Intentionality and Structure in Discourse Relations,0,None
P92-1016,Understanding Natural Language Instructions: The Case of Purpose Clauses,1992,15,33,1,1,1524,barbara eugenio,30th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents an analysis of purpose clauses in the context of instruction understanding. Such analysis shows that goals affect the interpretation and / or execution of actions, lends support to the proposal of using generation and enablement to model relations between actions, and sheds light on some inference processes necessary to interpret purpose clauses."
C92-4181,On the Interpretation of Natural Language Instructions,1992,9,14,1,1,1524,barbara eugenio,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we discuss the approach we take to the interpretation of instructions. Instructions describe actions related to each other and to other goals the agent may have; our claim is that the agent must actively compute the actions that s/he has to perform, not simply extract their descriptions from the input.We will start by discussing some inferences that are necessary to understand instructions, and we will draw some conclusions about action representation formalisms and inference processes. We will discuss our approach, which includes an action representation formalism based on Conceptual Structures [Jac90], and the construction of the structure of the agent's intentions. We will conclude with an example that shows why such representations help us in analyzing instructions."
P91-1044,Action representation for {NL} instructions,1991,14,2,1,1,1524,barbara eugenio,29th Annual Meeting of the Association for Computational Linguistics,1,None
C90-2047,Centering theory and the {I}talian pronominal system,1990,5,35,1,1,1524,barbara eugenio,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper, I give an account, in terms of centering theory [GJW86], of some phenomena of pronominalization in Italian, in particular the use of the null or the overt pronoun in subject position. After a general introduction to the Italian pronominal system, I will review centering, and then show how the original rules given in [GJW86] have to be extended or modified. Finally, I will show that centering does not account for two phenomena: first, the functional role of an utterance may override the predictions of centering; second, a null subject can be used to refer to a whole discourse segment. This later phenomenon should ideally be explained in the same terms that the other phenomena in volving null subject are."
C90-2068,Free Adjuncts in Natural Language Instructions,1990,7,21,2,0,9578,bonnie webber,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper, we give a brief account of our project Animation from Instructions, the view of instructions it reflects, and the semantics of one construction - the free adjunct - that is common in Natural Language instructions."
C86-1081,A Logical Formalism for the Representation of Determiners,1986,7,3,1,1,1524,barbara eugenio,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Determiners play an important role in conveying the meaning of an utterance, but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning of a sentence, even if not in a precise way. Another problem with determiners is their inherent ambiguity.In this paper we propose a logical formalism, which, among other things, is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear."
