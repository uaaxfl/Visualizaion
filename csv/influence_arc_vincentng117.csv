2020.aacl-main.66,W06-0901,0,0.0603751,"he number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sent"
2020.aacl-main.66,araki-etal-2014-detecting,0,0.0794582,"ns are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to"
2020.aacl-main.66,D17-1226,0,0.222871,"Missing"
2020.aacl-main.66,R13-1021,0,0.572345,"nts of the two event mentions are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang"
2020.aacl-main.66,W15-0801,0,0.0177846,"at no cross-document coreference links can be established between documents in different clusters (Lee et al., 2012; Choubey and Huang, 2017). These resolvers, unlike ours, are pipelined systems, meaning that topic detection can influence event coreference resolution but not the other way round. As for discourse salience, we are not aware of any event coreference work that attempts to explicitly model it, although one can argue that existing systems may have implicitly encoded it in a shallow manner via exploiting features that encode the distance between two event mentions (Liu et al., 2014; Cybulska and Vossen, 2015). Computing argument compatibility. In addition to discourse-based pruning, candidate antecedents can be pruned based on how compatible the arguments of the two event mentions are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been consi"
2020.aacl-main.66,J14-2004,0,0.0187188,"ting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sentence as the trigger. Our discourse-based pruning method can be thought of as a way of approximating argume"
2020.aacl-main.66,chen-ng-2014-sinocoreferencer,1,0.809783,"f unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sentence as the trigger"
2020.aacl-main.66,N15-1116,1,0.824675,"“step down”). 3 Resolving a mention to the N ULL antecedent is the same as having the mention starts a N EW cluster. 656 preprocessing step seeks to simplify the job of the event coreference model by reducing the number of candidate antecedents it has to consider for a given event mention. Since we aim to encode the discourse context of each event mention using the entities that are salient at the point of the discourse in which the event mention appears, we need to compute the salience score of each entity E w.r.t. each event mention m. We employ the following formula, which was proposed by Chen and Ng (2015b): X g(e) × decay(e) Figure 1: Unary factors for the three tasks, the variables they are connected to, and the possible values of the variables. e∈E In this formula, e is a mention of entity E that appears in either the same sentence as m or one of its preceding sentences. g(e) is a score that is computed based on the grammatical role of e in the sentence: 4 if e is a subject, 2 if it is an object, and 1 otherwise. decay(e) is a decay factor that is set to 0.5dis , where dis is the sentence distance between e and m. We compute discourse entities using Stanford CoreNLP’s neural entity corefere"
2020.aacl-main.66,P15-2053,1,0.692744,"“step down”). 3 Resolving a mention to the N ULL antecedent is the same as having the mention starts a N EW cluster. 656 preprocessing step seeks to simplify the job of the event coreference model by reducing the number of candidate antecedents it has to consider for a given event mention. Since we aim to encode the discourse context of each event mention using the entities that are salient at the point of the discourse in which the event mention appears, we need to compute the salience score of each entity E w.r.t. each event mention m. We employ the following formula, which was proposed by Chen and Ng (2015b): X g(e) × decay(e) Figure 1: Unary factors for the three tasks, the variables they are connected to, and the possible values of the variables. e∈E In this formula, e is a mention of entity E that appears in either the same sentence as m or one of its preceding sentences. g(e) is a score that is computed based on the grammatical role of e in the sentence: 4 if e is a subject, 2 if it is an object, and 1 otherwise. decay(e) is a decay factor that is set to 0.5dis , where dis is the sentence distance between e and m. We compute discourse entities using Stanford CoreNLP’s neural entity corefere"
2020.aacl-main.66,W09-3208,0,0.0269337,"hallow manner via exploiting features that encode the distance between two event mentions (Liu et al., 2014; Cybulska and Vossen, 2015). Computing argument compatibility. In addition to discourse-based pruning, candidate antecedents can be pruned based on how compatible the arguments of the two event mentions are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 20"
2020.aacl-main.66,W09-4303,0,0.272911,"y encoded it in a shallow manner via exploiting features that encode the distance between two event mentions (Liu et al., 2014; Cybulska and Vossen, 2015). Computing argument compatibility. In addition to discourse-based pruning, candidate antecedents can be pruned based on how compatible the arguments of the two event mentions are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 20"
2020.aacl-main.66,D13-1203,0,0.0986986,"t∗i inferred from the LabeledLDA model using Gibbs sampling, and 4.1 Evaluation Experimental Setup We perform training and evaluation on the KBP 2017 English and Chinese corpora. For English, 5 In the conditional log likelihood function, A(Ci∗ ) is the set of antecedent structures that are consistent with Ci∗ . Since our model needs to be trained on antecedent vectors c∗ but the gold coreference annotation for each document i is provided in the form of a clustering Ci∗ , we need to sum over all consistent antecedent structures. 6 The loss function for event coreference, which is introduced by Durrett and Klein (2013) for entity coreference resolution, is a weighted sum of (1) the number of anaphoric mentions misclassified as non-anaphoric, (2) the number of non-anaphoric mentions misclassified as anaphoric, and (3) the number of incorrectly resolved mentions. The loss function for trigger detection is parameterized in a similar way, having three parameters associated with (1) the number of nontriggers misclassified as triggers, (2) the number of triggers misclassified as non-triggers, and (3) the number of triggers labeled with the wrong subtype. The loss function for topic detection is defined in a simil"
2020.aacl-main.66,N19-1085,1,0.681372,"Missing"
2020.aacl-main.66,K16-1024,0,0.195831,"rgument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue."
2020.aacl-main.66,D12-1045,0,0.17576,"eing that m4 , m5 , and m8 share the same entity (realized as “a uniformed soldier”, “The soldier”, and “the wounded soldier”) in their contexts. Since the model retains only the top two candidate antecedents for English, m6 and m7 are being pruned, and the model successfully resolves m8 to m5 . 5 Related Work Using topics and salience. For event coreference, the notion of “topics” has thus far been exploited only for cross-document event coreference, where documents are clustered by topics so that no cross-document coreference links can be established between documents in different clusters (Lee et al., 2012; Choubey and Huang, 2017). These resolvers, unlike ours, are pipelined systems, meaning that topic detection can influence event coreference resolution but not the other way round. As for discourse salience, we are not aware of any event coreference work that attempts to explicitly model it, although one can argue that existing systems may have implicitly encoded it in a shallow manner via exploiting features that encode the distance between two event mentions (Liu et al., 2014; Cybulska and Vossen, 2015). Computing argument compatibility. In addition to discourse-based pruning, candidate ant"
2020.aacl-main.66,liu-etal-2014-supervised,0,0.520005,"ed by topics so that no cross-document coreference links can be established between documents in different clusters (Lee et al., 2012; Choubey and Huang, 2017). These resolvers, unlike ours, are pipelined systems, meaning that topic detection can influence event coreference resolution but not the other way round. As for discourse salience, we are not aware of any event coreference work that attempts to explicitly model it, although one can argue that existing systems may have implicitly encoded it in a shallow manner via exploiting features that encode the distance between two event mentions (Liu et al., 2014; Cybulska and Vossen, 2015). Computing argument compatibility. In addition to discourse-based pruning, candidate antecedents can be pruned based on how compatible the arguments of the two event mentions are. To capture argument compatibility, argument features have been extensively exploited. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity m"
2020.aacl-main.66,L16-1631,1,0.882327,"binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sentence as the trigger. Our discourse-based pru"
2020.aacl-main.66,P17-1009,1,0.873144,"use LDC2015E29, E68, E73, E94, and LDC2016E64 for training. Together they contain 817 documents with 22894 event mentions distributed over 13146 coreference chains. For Chinese, we use LDC2015E78, E105, E112, and LDC2016E64 for training. Together they contain 548 documents with 7388 event mentions distributed over 5526 coreference chains. The KBP 2017 English test set consists of 167 documents with 4375 event mentions distributed over 2963 coreference chains. The Chinese test set consists of 167 documents with 3884 event mentions distributed over 2558 coreference chains. 654 3 Model Following Lu and Ng (2017a), we employ a structured conditional random field, which operates at the document level. Specifically, given a test document, we first extract from it all single- and multiword nouns and verbs that have appeared at least once as a trigger in the training data. We treat each of these extracted nouns and verbs as a candidate event mention. The goal of the model is to make joint predictions for the candidate event mentions in a document. Three predictions will be made for each candidate event mention that correspond to the three tasks in the model: its trigger subtype, its induced topic, and it"
2020.aacl-main.66,C16-1308,1,0.87123,"on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sentence as the trigger. Our discourse-based pruning method can be thought of as a way of approximating argument compatibility without performing argument extraction. 6 Conclusion We incorporated non-local information into a stateof-the-art joint model for event coreference resolution via topic modeling and discourse-base"
2020.aacl-main.66,H05-1004,0,0.0707991,"ng documents, tune parameters on 171 training documents, and report results on the official KBP 2017 English test set. For Chinese, we train models on 438 of the training documents, tune parameters on 110 training documents, and report results on the official KBP 2017 Chinese test set. Results of event coreference and trigger detection are obtained using version 1.8 of the official scorer provided by the KBP 2017 organizers. To evaluate event coreference performance, the scorer employs four commonly-used scoring measures, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). The scorer reports event mention detection performance in terms of Precision (P), Recall (R) and F-score, considering a mention correctly detected if it has an exact match with a gold mention in terms of boundary and event subtype. 4.2 Results Results on the English test set are shown in the top half of Table 2. Specifically, row 1 shows the results of Huang et al.’s (2019) resolver, which has produced best results to date on this test set. Row 2 shows the results of our full model, which substan"
2020.aacl-main.66,P14-5010,0,0.00290134,"a window size of three from m; feature conjunctions created by pairing m’s lemma with each of the following features: the head word of the entity syntactically closest to m, the head word of the entity textually closest to m, the entity type of the entity that is syntactically closest to m, and the entity type of the entity that is textually closest to m.1 In addition, for event mentions with verb triggers, we use the head words and the entity types of their subjects and objects as features, where the subjects and objects are extracted from the dependency parses produced by Stanford CoreNLP (Manning et al., 2014). For event mentions with noun triggers, we create the same features except that we replace the subjects and verbs with heuristically extracted agents and patients. 3.1.2 Topic Model Our first extension to Lu and Ng’s (2017a) model seeks to improve trigger detection using topic information. We train a supervised topic model to infer the topic of each word in a test document, with the goal of understanding each candidate trigger using its global in addition to local context. Like the trigger detection model, each training instance corresponds to a candidate trigger. The class label is the topic"
2020.aacl-main.66,N16-1034,0,0.0153222,"ence model has not been fully exploited and seek to extend it in this paper. Our extensions are based on the observation that the strength of a joint model stems from its ability to facilitate cross-task knowledge transfer. In other words, the better we can model each task involved, the more we can potentially get out of joint modeling. Given this observation, we seek to improve the modeling of these tasks in this joint model as follows. First, we improve trigger detection by exploiting topic information. State-of-the-art trigger detectors, including those based on deep neural networks (e.g., Nguyen et al. (2016)), classify each candidate trigger using local information and largely ignore the fact that the topic of the document in which a trigger appears plays an important role in determining its event subtype. To understand the usefulness 653 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 653–663 c December 4 - 7, 2020. 2020 Association for Computational Linguistics Three journalists at The New York Times on Tuesday announced plans to {leave}ev1 the newspap"
2020.aacl-main.66,D16-1038,0,0.220244,"Missing"
2020.aacl-main.66,D09-1026,0,0.0169206,"e trigger. The class label is the topic label of the candidate trigger. We have 19 topic labels in total: there is a one-to-one correspondence between the 18 subtype labels and 18 of the topic labels. The remaining topic label is OTHER, which is reserved for those words that do not belong to any of the 18 topics. Topic labels can be derived directly from subtype labels given the one-to-one correspondence between them. Each candidate trigger is represented using 19 features, which correspond to the 19 topic labels. The value of a feature, which is derived from the output of a LabeledLDA model (Ramage et al., 2009), encodes the probability that the candidate trigger belongs to the corresponding topic. To train the LabeledLDA model, we first apply LabeledLDA using the Mallet toolkit (McCallum, 1 We use an in-house CRF-based entity extraction model to jointly identify the entity mentions and their types. 655 2002) to the training documents, which learns a distribution over words for each topic, β. We represent each training document using the candidate triggers as well as the context words that are useful for distinguishing the topics.2 To get the useful context words, we rank the words in the training do"
2020.aacl-main.66,M95-1005,0,0.463127,"om the original papers. we train models on 646 of the training documents, tune parameters on 171 training documents, and report results on the official KBP 2017 English test set. For Chinese, we train models on 438 of the training documents, tune parameters on 110 training documents, and report results on the official KBP 2017 Chinese test set. Results of event coreference and trigger detection are obtained using version 1.8 of the official scorer provided by the KBP 2017 organizers. To evaluate event coreference performance, the scorer employs four commonly-used scoring measures, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). The scorer reports event mention detection performance in terms of Precision (P), Recall (R) and F-score, considering a mention correctly detected if it has an exact match with a gold mention in terms of boundary and event subtype. 4.2 Results Results on the English test set are shown in the top half of Table 2. Specifically, row 1 shows the results of Huang et al.’s (2019) resolver, which has produced best results to date on this test set. Row 2"
2020.aacl-main.66,Q15-1037,0,0.017167,"en et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Krause et al., 2016). These features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015a; Lu and Ng, 2016) or the outputs of semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016), and therefore suffer from error propagation (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. Nevertheless, argument extraction remains a very challenging task, especially when the arguments do not appear in the same sentence as the trigger. Our discourse-based pruning method can be thought of as a way of approximating argument compatibility wi"
2020.coling-main.331,P99-1008,0,0.535952,"c linguistic insights. Corpora that contain other discourse-level annotations (e.g., the discourse relations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example,"
2020.coling-main.331,bjorkelund-etal-2014-extended,0,0.0617837,"Missing"
2020.coling-main.331,W03-2607,0,0.160358,"relations should bridging cover? As a linguistic phenomenon, bridging has been studied extensively by linguists (e.g., Clark (1975), Prince (1981), Gundel et al. (1993)). Clark (1975), who started this area of research, introduced a broad concept of bridging that includes coreference (i.e., the identity relation). Coreference, however, is gradually being excluded from bridging over time. For instance, while some early studies still included the difficult cases of coreference where two coreferent mentions do not share the same head as bridging (Poesio and Vieira, 1998; Vieira and Poesio, 2000; Bunescu, 2003)1 , most of the recent studies focus on non-identity cases of bridging, which is the closest to Hawkins’s (1978) concept of associative anaphora. Among the non-identity relations, bridging covers various types of semantic relations. While early studies typically restrict themselves to predefined relations such as part-of, subset, set membership, and possession relations (Poesio and Vieira, 1998; Poesio et al., 2004b), recent studies claim that bridging is a diverse phenomenon that cannot be simply captured with a limited set of predefined relations (Markert et al., 2012; R¨osiger, 2018a). Seco"
2020.coling-main.331,W12-1632,0,0.0252538,"es, BASHI, and ARRAU RST respectively. to identify the bridging anaphors from the gold mentions and then pass the resulting anaphors to the mention-pair model for resolution. While in principle a binary classifier can be trained to determine whether a gold mention is an anaphor or not, previous work has trained classifiers for determining the IS of a mention and assumed that those mentions that are classified as “bridging” are bridging anaphors. Table 4 enumerates the features that have been proposed to train a classifier for determining the IS of a mention (Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012; Markert et al., 2012; Rahman and Ng, 2012a; Hou et al., 2013a; Hou, 2016; Hou et al., 2018). 3 4 A building part is a noun that denotes a part of a building (e.g., “window”, “door”). Argument ratio is a measure designed by Hou et al. (2014) to quantify how likely a NP is to take arguments. 3713 Feature Head match Compound premodification Co-argument Parallel structure Closest modifier WordNet query Google distance WordNet distance Verb pattern (relative) Verb pattern (top) Preposition pattern (relative) Preposition pattern (top) GPE role person match OTHER role person match Relative person m"
2020.coling-main.331,caselli-prodanof-2006-annotating,0,0.066906,"r that is compared to another mention (Modjeska, 2003). In ISNotes, comparative anaphors are excluded from the bridging category because such anaphors often have surface indicators, containing modifiers such as “other” and “another” (Markert et al., 2012). In contrast, BASHI and ARRAU consider them as a subcategory of bridging. Several non-English bridging corpora exist. DIRNDL (Bj¨orkelund et al., 2014), SemDok (B¨arenf¨anger et al., 2008), and GRAIN (Schweitzer et al., 2018) are in German. DEDE (Gardent and Manu´elian, 2005) and PAROLE (Gardent et al., 2003) are in French. Caselli/Prodanof (Caselli and Prodanof, 2006) and Italian Live Memories Corpus (Rodr´ıguez et al., 2010) are in Italian. Prague Dependency Treebank (Hajiˇc et al., 2018) is in Czech. CESS-ECE (Recasens et al., 2007) is in Spanish. COREA (Hendrickx et al., 2008) is in Dutch. The Sasano-Kurohashi corpus (Sasano and Kurohashi, 2009) is in Japanese. RuGenBridge (Roitberg and Nedoluzhko, 2016) is in Russian. Parallel bridging corpora are also available. For instance, Copenhagen Dependency Treebank is a parallel corpus involving Danish, English, Italian, German, and Spanish (Korzen and Buch-kromann, 2011), and CorefPro is a parallel corpus inv"
2020.coling-main.331,W03-0415,0,0.131379,"ions (e.g., the discourse relations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example, Girju et al.’s (2003) decision tree approach to part-whole relation extra"
2020.coling-main.331,D08-1069,0,0.0259907,"lect an antecedent for a bridging anaphor by calculating the vector similarity between the anaphor’s head and a candidate antecedent’s head. Moreover, she combines embeddings PP, which covers only a subset of nouns, and the GloVe embeddings so that both non-nouns and additional nouns are covered (Hou, 2018a). Neural models. Yu and Poesio (2020) propose the first neural model for full bridging resolution, leveraging a span-based neural model originally developed for entity coreference resolution by Kantor and Globerson (2019). Kantor and Globerson’s span-based model is a mention-ranking model (Denis and Baldridge, 2008), meaning that it is trained to rank the candidate antecedents of an anaphor so that the correct antecedent has the highest rank. Key to the success of this and other span-based coreference models is their ability to learn text spans corresponding to entity mentions as well as their representations so 3715 System Approach Gold coref? Hou et al. (2018) Hou (2018a) Yu and Poesio (2020) Hou (2020) Feature based Embedding MTL QA Feature extraction No No No ISNotes 50.7 39.5 40.7 50.1 Dataset BASHI 27.4 34 38.7 ARRAU 32.4 49.3 - Table 5: Accuracies of state-of-the-art bridging resolvers. System App"
2020.coling-main.331,W03-2410,0,0.212218,"Missing"
2020.coling-main.331,N03-1011,0,0.32922,"Missing"
2020.coling-main.331,J06-1005,0,0.0400389,"ra that contain other discourse-level annotations (e.g., the discourse relations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example, Girju et al.’s (2003"
2020.coling-main.331,W15-3403,0,0.055113,"Missing"
2020.coling-main.331,W16-0702,0,0.0173609,"dr´ıguez et al., 2010) are in Italian. Prague Dependency Treebank (Hajiˇc et al., 2018) is in Czech. CESS-ECE (Recasens et al., 2007) is in Spanish. COREA (Hendrickx et al., 2008) is in Dutch. The Sasano-Kurohashi corpus (Sasano and Kurohashi, 2009) is in Japanese. RuGenBridge (Roitberg and Nedoluzhko, 2016) is in Russian. Parallel bridging corpora are also available. For instance, Copenhagen Dependency Treebank is a parallel corpus involving Danish, English, Italian, German, and Spanish (Korzen and Buch-kromann, 2011), and CorefPro is a parallel corpus involving German, English, and Russian (Grishina, 2016). While not widely used, GUM is an ever-expanding English corpus annotated with bridging links by students at Georgetown University (Zeldes, 2017). 4 Evaluation Issues As mentioned before, an issue surrounding bridging resolution research concerns the lack of a standardized evaluation protocol. In this section, we take a look at current evaluation practices. Evaluation settings. Bridging resolvers operate in one of three settings. In end-to-end bridging resolution, a system is given a raw document as input. The goal is to identify the bridging anaphors (a subtask known as bridging recognition)"
2020.coling-main.331,C92-2082,0,0.130099,"-level annotations (e.g., the discourse relations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example, Girju et al.’s (2003) decision tree approach"
2020.coling-main.331,hendrickx-etal-2008-coreference,0,0.11077,"Missing"
2020.coling-main.331,D13-1077,0,0.181703,"anaphor to an antecedent chosen from a set of candidate antecedents. For (full) bridging resolution, the candidate antecedents can simply be taken to be the set of gold mentions that appear in all of the previous sentences or a fixed sentence window (Poesio et al., 2004a). Slightly more sophisticated candidate selection strategies have been employed. For instance, the window size can be tuned for each rule in rule-based systems (Hou et al., 2014; R¨osiger, 2018b; R¨osiger et al., 2018a; R¨osiger et al., 2018b). The top k salient mentions can be used in addition to those from the fixed window (Hou et al., 2013b; Hou, 2018b; Hou, 2018a). Moreover, Hou et al. (2013b) and Hou et al. (2018) have proposed an entity-based evaluation method where an anaphor is resolved to a preceding entity rather than a preceding mention. The idea is to first use gold coreference information to group the candidate antecedents of an anaphor into coreference clusters, and then extract cluster/entity-level features for encoding each of the resulting clusters/entities. The goal of the resolver is to resolve the anaphor to one of these clusters/entities based on the extracted features. Note that the resolution task is simplif"
2020.coling-main.331,N13-1111,0,0.26721,"anaphor to an antecedent chosen from a set of candidate antecedents. For (full) bridging resolution, the candidate antecedents can simply be taken to be the set of gold mentions that appear in all of the previous sentences or a fixed sentence window (Poesio et al., 2004a). Slightly more sophisticated candidate selection strategies have been employed. For instance, the window size can be tuned for each rule in rule-based systems (Hou et al., 2014; R¨osiger, 2018b; R¨osiger et al., 2018a; R¨osiger et al., 2018b). The top k salient mentions can be used in addition to those from the fixed window (Hou et al., 2013b; Hou, 2018b; Hou, 2018a). Moreover, Hou et al. (2013b) and Hou et al. (2018) have proposed an entity-based evaluation method where an anaphor is resolved to a preceding entity rather than a preceding mention. The idea is to first use gold coreference information to group the candidate antecedents of an anaphor into coreference clusters, and then extract cluster/entity-level features for encoding each of the resulting clusters/entities. The goal of the resolver is to resolve the anaphor to one of these clusters/entities based on the extracted features. Note that the resolution task is simplif"
2020.coling-main.331,D14-1222,0,0.137719,"terms of resolution accuracy, which is the fraction of gold anaphors that are correctly resolved. Entity- vs. mention-based evaluation. A resolver needs to resolve an anaphor to an antecedent chosen from a set of candidate antecedents. For (full) bridging resolution, the candidate antecedents can simply be taken to be the set of gold mentions that appear in all of the previous sentences or a fixed sentence window (Poesio et al., 2004a). Slightly more sophisticated candidate selection strategies have been employed. For instance, the window size can be tuned for each rule in rule-based systems (Hou et al., 2014; R¨osiger, 2018b; R¨osiger et al., 2018a; R¨osiger et al., 2018b). The top k salient mentions can be used in addition to those from the fixed window (Hou et al., 2013b; Hou, 2018b; Hou, 2018a). Moreover, Hou et al. (2013b) and Hou et al. (2018) have proposed an entity-based evaluation method where an anaphor is resolved to a preceding entity rather than a preceding mention. The idea is to first use gold coreference information to group the candidate antecedents of an anaphor into coreference clusters, and then extract cluster/entity-level features for encoding each of the resulting clusters/e"
2020.coling-main.331,J18-2002,0,0.327749,"50 WSJ articles in OntoNotes) (R¨osiger, 2018a), ARRAU (composed of articles from four domains, RST, GNOME, PEAR, and TRAINS) (Poesio and Artstein, 2008; Uryupina et al., 2020), and SciCorp (The Scientific Corpus, composed of scientific articles from computational linguistics and genetics) (R¨osiger, 2016). Table 1 compares these corpora along five dimensions: (1) the domain type, (2) the size (in terms of the number of documents, tokens, and mentions), (3) the number of bridging anaphors, (4) the types of 1 These difficult cases of coreference are later being referred to as lenient bridging (Hou et al., 2018). 3709 anaphor, and (5) the types of antecedent. While early corpora limited anaphors to definite NPs and predefined relations (Poesio and Vieira, 1998; Poesio et al., 2004b), many of these newer corpora do not. For instance, ISNotes and BASHI include both definite and indefinite expressions as anaphors and both entity and event mentions as antecedents; moreover, they do not restrict bridging relations to predefined relations. Also, all of these corpora contain coreference in addition to bridging annotations. In addition to the differences shown in Table 1, there are several notable difference"
2020.coling-main.331,C16-1177,0,0.0196375,"ntions and then pass the resulting anaphors to the mention-pair model for resolution. While in principle a binary classifier can be trained to determine whether a gold mention is an anaphor or not, previous work has trained classifiers for determining the IS of a mention and assumed that those mentions that are classified as “bridging” are bridging anaphors. Table 4 enumerates the features that have been proposed to train a classifier for determining the IS of a mention (Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012; Markert et al., 2012; Rahman and Ng, 2012a; Hou et al., 2013a; Hou, 2016; Hou et al., 2018). 3 4 A building part is a noun that denotes a part of a building (e.g., “window”, “door”). Argument ratio is a measure designed by Hou et al. (2014) to quantify how likely a NP is to take arguments. 3713 Feature Head match Compound premodification Co-argument Parallel structure Closest modifier WordNet query Google distance WordNet distance Verb pattern (relative) Verb pattern (top) Preposition pattern (relative) Preposition pattern (top) GPE role person match OTHER role person match Relative person match Both spatial/temporal Utterance distance First mention (local) First"
2020.coling-main.331,D18-1219,0,0.334138,"edent chosen from a set of candidate antecedents. For (full) bridging resolution, the candidate antecedents can simply be taken to be the set of gold mentions that appear in all of the previous sentences or a fixed sentence window (Poesio et al., 2004a). Slightly more sophisticated candidate selection strategies have been employed. For instance, the window size can be tuned for each rule in rule-based systems (Hou et al., 2014; R¨osiger, 2018b; R¨osiger et al., 2018a; R¨osiger et al., 2018b). The top k salient mentions can be used in addition to those from the fixed window (Hou et al., 2013b; Hou, 2018b; Hou, 2018a). Moreover, Hou et al. (2013b) and Hou et al. (2018) have proposed an entity-based evaluation method where an anaphor is resolved to a preceding entity rather than a preceding mention. The idea is to first use gold coreference information to group the candidate antecedents of an anaphor into coreference clusters, and then extract cluster/entity-level features for encoding each of the resulting clusters/entities. The goal of the resolver is to resolve the anaphor to one of these clusters/entities based on the extracted features. Note that the resolution task is simplified when an"
2020.coling-main.331,N18-2001,0,0.221101,"edent chosen from a set of candidate antecedents. For (full) bridging resolution, the candidate antecedents can simply be taken to be the set of gold mentions that appear in all of the previous sentences or a fixed sentence window (Poesio et al., 2004a). Slightly more sophisticated candidate selection strategies have been employed. For instance, the window size can be tuned for each rule in rule-based systems (Hou et al., 2014; R¨osiger, 2018b; R¨osiger et al., 2018a; R¨osiger et al., 2018b). The top k salient mentions can be used in addition to those from the fixed window (Hou et al., 2013b; Hou, 2018b; Hou, 2018a). Moreover, Hou et al. (2013b) and Hou et al. (2018) have proposed an entity-based evaluation method where an anaphor is resolved to a preceding entity rather than a preceding mention. The idea is to first use gold coreference information to group the candidate antecedents of an anaphor into coreference clusters, and then extract cluster/entity-level features for encoding each of the resulting clusters/entities. The goal of the resolver is to resolve the anaphor to one of these clusters/entities based on the extracted features. Note that the resolution task is simplified when an"
2020.coling-main.331,2020.acl-main.132,0,0.45188,"aging a span-based neural model originally developed for entity coreference resolution by Kantor and Globerson (2019). Kantor and Globerson’s span-based model is a mention-ranking model (Denis and Baldridge, 2008), meaning that it is trained to rank the candidate antecedents of an anaphor so that the correct antecedent has the highest rank. Key to the success of this and other span-based coreference models is their ability to learn text spans corresponding to entity mentions as well as their representations so 3715 System Approach Gold coref? Hou et al. (2018) Hou (2018a) Yu and Poesio (2020) Hou (2020) Feature based Embedding MTL QA Feature extraction No No No ISNotes 50.7 39.5 40.7 50.1 Dataset BASHI 27.4 34 38.7 ARRAU 32.4 49.3 - Table 5: Accuracies of state-of-the-art bridging resolvers. System Approach Gold coref? R¨osiger et al. (2018b) Hou et al. (2018) Yu and Poesio (2020) Rule based Feature based MTL Anaphor filtering Feature extraction Anaphor filtering ISNotes Rec Res 29.3 20.4 46.1 21.6 43.6 23.2 Dataset BASHI Rec Res 28.7 14.1 27.2 14.4 ARRAU Rec Res 30.8 19.5 36.7 24.0 Table 6: Recognition (Rec) and resolution (Res) F-scores of state-of-the-art full bridging resolvers. that the"
2020.coling-main.331,2020.tacl-1.5,0,0.0256938,"ased approaches, where feature engineering plays a critical role in performance, this neural model employs only two features, the length of a mention and mention-pair distance. More recently, Hou (2020) has proposed a neural approach to bridging resolution based on question answering (QA). Given a gold anaphor, the idea is to (1) create a question from the anaphor in the form of “anaphor of what?”, (2) create candidate answers from the candidate antecedents (i.e., the preceding mentions that appear in a fixed sentence window), and (3) use a BERT-based QA system pretrained on the SQuAD corpus (Joshi et al., 2020) to choose the most probable answer (i.e., the antecedent). An appealing aspect of this approach is that it does not require any gold or system mention information as the antecedent candidates. Hou further hypothesizes that the results could be improved if the model were pretrained on a bridging corpus rather than a QA corpus. However, as mentioned before, all existing bridging corpora are too small to train an effective neural model. To address this problem, Hou employs a distant supervision method (see the embedding approaches above) to generate an automatically labeled bridging corpus, and"
2020.coling-main.331,P19-1066,0,0.0463453,"esponding features. an embedding model, embeddings PP. The resulting embeddings can be used to select an antecedent for a bridging anaphor by calculating the vector similarity between the anaphor’s head and a candidate antecedent’s head. Moreover, she combines embeddings PP, which covers only a subset of nouns, and the GloVe embeddings so that both non-nouns and additional nouns are covered (Hou, 2018a). Neural models. Yu and Poesio (2020) propose the first neural model for full bridging resolution, leveraging a span-based neural model originally developed for entity coreference resolution by Kantor and Globerson (2019). Kantor and Globerson’s span-based model is a mention-ranking model (Denis and Baldridge, 2008), meaning that it is trained to rank the candidate antecedents of an anaphor so that the correct antecedent has the highest rank. Key to the success of this and other span-based coreference models is their ability to learn text spans corresponding to entity mentions as well as their representations so 3715 System Approach Gold coref? Hou et al. (2018) Hou (2018a) Yu and Poesio (2020) Hou (2020) Feature based Embedding MTL QA Feature extraction No No No ISNotes 50.7 39.5 40.7 50.1 Dataset BASHI 27.4"
2020.coling-main.331,W03-2606,0,0.128056,"on. Vieira and Teufel (1997) use a heuristic to resolve bridging anaphors based on synonymy, hyponymy, and meronymy relations from WordNet 1.6. Poesio et al. (1997) improve this system by limiting the use of some WordNet relations and improving the antecedent search strategy. For further improvement of this system, Poesio et al. (2002) complement WordNet coverage with another lexical resource of meronymy relations, which is acquired by querying syntactic patterns such as NP of NP and NP’s NP in the British National Corpus. To have a large corpus as a resource for acquiring semantic relations, Markert et al. (2003) use the Web to extract meronymy and hyponymy relations. Following an early rule-based bridging system (Vieira and Poesio, 2000)2 , all recently-developed rulebased bridging systems are composed of rules that perform recognition and resolution at the same time. For instance, Hou et al. (2014) propose a system consisting of eight rules for ISNotes. While R¨osiger et al. (2018b) manage to apply Hou et al.’s system to BASHI with just one additional rule, R¨osiger (2018b) finds that her system is not directly applicable to ARRAU: ARRAU consists mostly of lexical bridging while ISNotes, for which H"
2020.coling-main.331,P12-1084,0,0.804292,"998; Vieira and Poesio, 2000; Bunescu, 2003)1 , most of the recent studies focus on non-identity cases of bridging, which is the closest to Hawkins’s (1978) concept of associative anaphora. Among the non-identity relations, bridging covers various types of semantic relations. While early studies typically restrict themselves to predefined relations such as part-of, subset, set membership, and possession relations (Poesio and Vieira, 1998; Poesio et al., 2004b), recent studies claim that bridging is a diverse phenomenon that cannot be simply captured with a limited set of predefined relations (Markert et al., 2012; R¨osiger, 2018a). Second, what types of linguistic expressions can serve as bridging anaphors? Many traditional studies (Hawkins, 1978; Poesio and Vieira, 1998; Lassalle and Denis, 2011; R¨osiger, 2016) limited bridging anaphors to definite expressions, excluding indefinite expressions since they generally introduce new information that can be interpreted without the discourse context. However, L¨obner (1998) claimed that bridging anaphors can also be indefinite because these indefinite expressions can have semantic relations with preceding expressions. Recent studies therefore allow both de"
2020.coling-main.331,P15-1138,0,0.0498542,"Missing"
2020.coling-main.331,W03-1023,0,0.128698,"s so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example, Girju et al.’s (2003) decision tree approach to part-whole relation extraction has employed many of the features that are also used in feature-based approaches to bridging resolution. Rather than reinventing the wheels, we encourage researchers in these different areas work together on bridging resolution. In particular, it is conceivable that the best approach to bridging resolution may involve first decomposing the task into different"
2020.coling-main.331,P02-1014,1,0.602346,"RAU RST (A). As we can see, these are mostly lowrecall rules: many bridging anaphors cannot be recognized or resolved using these rules. Moreover, each rule has different performances (in terms of recognition and resolution) on different corpora, meaning that these rules, which are designed for ISNotes, do not generalize across corpora. 5.2 Learning-based Approaches We divide existing learning-based approaches into three categories. Feature-based approaches. In these approaches, a pairwise classifier, known as the mention-pair model in the coreference resolution literature (Soon et al., 2001; Ng and Cardie, 2002), is trained to determine whether two mentions has a bridging relation. Each training instance therefore corresponds to two mentions, one of which is a bridging anaphor and the other is its candidate antecedent. If the candidate antecedent is its correct antecedent, the instance is labeled as P OSITIVE; otherwise, it is labeled as N EGATIVE. Table 3 shows the list of features that have been used to train the mention-pair model. The mention-pair model works well if a resolver is given gold anaphors as input. To perform full bridging resolution, in which gold mentions are given, we need to first"
2020.coling-main.331,W06-1612,0,0.0695464,"). ’I’, ’B’, and ’A’ refer to ISNotes, BASHI, and ARRAU RST respectively. to identify the bridging anaphors from the gold mentions and then pass the resulting anaphors to the mention-pair model for resolution. While in principle a binary classifier can be trained to determine whether a gold mention is an anaphor or not, previous work has trained classifiers for determining the IS of a mention and assumed that those mentions that are classified as “bridging” are bridging anaphors. Table 4 enumerates the features that have been proposed to train a classifier for determining the IS of a mention (Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012; Markert et al., 2012; Rahman and Ng, 2012a; Hou et al., 2013a; Hou, 2016; Hou et al., 2018). 3 4 A building part is a noun that denotes a part of a building (e.g., “window”, “door”). Argument ratio is a measure designed by Hou et al. (2014) to quantify how likely a NP is to take arguments. 3713 Feature Head match Compound premodification Co-argument Parallel structure Closest modifier WordNet query Google distance WordNet distance Verb pattern (relative) Verb pattern (top) Preposition pattern (relative) Preposition pattern (top) GPE role person"
2020.coling-main.331,P06-1015,0,0.0278597,"ations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While space limitations have precluded a detailed discussion of this line of related work, it is important to note that some of the ideas we have seen in this paper have also been explored in research on extracting these specific relations. For instance, like Hou (2018b), Hearst (1992) and Modjeska et al. (2003) have explored the use of lexico-syntactic patterns for automatically harvesting hyponyms and antecedents of other-anaphora, respectively. As another example, Girju et al.’s (2003) decision tree approach to part-whole relation extraction has employed many of the fe"
2020.coling-main.331,D14-1162,0,0.0836223,". (2013b) Hou et al. (2013b) Hou et al. (2013b) Hou et al. (2018) Hou et al. (2018) Hou et al. (2013b) Hou et al. (2013b) Poesio et al. (2004a) Poesio et al. (2004a) Poesio et al. (2004a) Hou et al. (2018) Hou et al. (2013b) Hou et al. (2013b) Hou et al. (2013b) Hou et al. (2018) Hou et al. (2018) Table 3: Features for bridging resolution. In this table, mj is a bridging anaphor, and mi is a candidate antecedent of mj . The “Paper” column shows the papers that proposed the corresponding features. Embedding approaches. Hou (2018b) observes that commonly used word representations such as GloVe (Pennington et al., 2014) capture “genuine” similarity and relatedness, but bridging resolution requires lexical association knowledge instead of semantic similarity information between synonyms or hypernyms. This motivates her to train task-specific embeddings for bridging resolution. To do so, she first observes that the prepositional (i.e., X of Y) and possessive structures (i.e., Y’s X) of NPs encode a variety of bridging relations between anaphors and their antecedents. For example, the window of the room implies a part-of relation between the window and the room, and in Japan’s prime minister, there is a bridgin"
2020.coling-main.331,poesio-artstein-2008-anaphoric,0,0.164382,"can serve as bridging anaphors? Many traditional studies (Hawkins, 1978; Poesio and Vieira, 1998; Lassalle and Denis, 2011; R¨osiger, 2016) limited bridging anaphors to definite expressions, excluding indefinite expressions since they generally introduce new information that can be interpreted without the discourse context. However, L¨obner (1998) claimed that bridging anaphors can also be indefinite because these indefinite expressions can have semantic relations with preceding expressions. Recent studies therefore allow both definite and indefinite expressions to serve as bridging anaphors (Poesio and Artstein, 2008; Markert et al., 2012; R¨osiger, 2018a). 3 Corpora This section provides an overview of existing corpora used for bridging research, with a focus on four widely-used English corpora, namely ISNotes (composed of 50 WSJ articles in OntoNotes) (Markert et al., 2012) , BASHI (The Bridging Anaphors Hand-annotated Inventory, composed of another 50 WSJ articles in OntoNotes) (R¨osiger, 2018a), ARRAU (composed of articles from four domains, RST, GNOME, PEAR, and TRAINS) (Poesio and Artstein, 2008; Uryupina et al., 2020), and SciCorp (The Scientific Corpus, composed of scientific articles from computa"
2020.coling-main.331,J98-2001,0,0.843423,"er look at these two issues. First, what types of relations should bridging cover? As a linguistic phenomenon, bridging has been studied extensively by linguists (e.g., Clark (1975), Prince (1981), Gundel et al. (1993)). Clark (1975), who started this area of research, introduced a broad concept of bridging that includes coreference (i.e., the identity relation). Coreference, however, is gradually being excluded from bridging over time. For instance, while some early studies still included the difficult cases of coreference where two coreferent mentions do not share the same head as bridging (Poesio and Vieira, 1998; Vieira and Poesio, 2000; Bunescu, 2003)1 , most of the recent studies focus on non-identity cases of bridging, which is the closest to Hawkins’s (1978) concept of associative anaphora. Among the non-identity relations, bridging covers various types of semantic relations. While early studies typically restrict themselves to predefined relations such as part-of, subset, set membership, and possession relations (Poesio and Vieira, 1998; Poesio et al., 2004b), recent studies claim that bridging is a diverse phenomenon that cannot be simply captured with a limited set of predefined relations (Mar"
2020.coling-main.331,W97-1301,0,0.488429,"should be used in the filtering process. Data splits. While ARRAU has a standard train-dev-test split, the other corpora do not. In the absence of a standard data split, resolvers are evaluated via k-fold cross validation, which makes a head-to-head comparison of their results difficult. 5 5.1 Bridging Resolution Approaches Rule-based Approaches Virtually all early ruled-based resolvers operate in the least challenging setting, i.e., bridging resolution. Vieira and Teufel (1997) use a heuristic to resolve bridging anaphors based on synonymy, hyponymy, and meronymy relations from WordNet 1.6. Poesio et al. (1997) improve this system by limiting the use of some WordNet relations and improving the antecedent search strategy. For further improvement of this system, Poesio et al. (2002) complement WordNet coverage with another lexical resource of meronymy relations, which is acquired by querying syntactic patterns such as NP of NP and NP’s NP in the British National Corpus. To have a large corpus as a resource for acquiring semantic relations, Markert et al. (2003) use the Web to extract meronymy and hyponymy relations. Following an early rule-based bridging system (Vieira and Poesio, 2000)2 , all recentl"
2020.coling-main.331,poesio-etal-2002-acquiring,0,0.153844,"solvers are evaluated via k-fold cross validation, which makes a head-to-head comparison of their results difficult. 5 5.1 Bridging Resolution Approaches Rule-based Approaches Virtually all early ruled-based resolvers operate in the least challenging setting, i.e., bridging resolution. Vieira and Teufel (1997) use a heuristic to resolve bridging anaphors based on synonymy, hyponymy, and meronymy relations from WordNet 1.6. Poesio et al. (1997) improve this system by limiting the use of some WordNet relations and improving the antecedent search strategy. For further improvement of this system, Poesio et al. (2002) complement WordNet coverage with another lexical resource of meronymy relations, which is acquired by querying syntactic patterns such as NP of NP and NP’s NP in the British National Corpus. To have a large corpus as a resource for acquiring semantic relations, Markert et al. (2003) use the Web to extract meronymy and hyponymy relations. Following an early rule-based bridging system (Vieira and Poesio, 2000)2 , all recently-developed rulebased bridging systems are composed of rules that perform recognition and resolution at the same time. For instance, Hou et al. (2014) propose a system consi"
2020.coling-main.331,P04-1019,0,0.532734,"me early studies still included the difficult cases of coreference where two coreferent mentions do not share the same head as bridging (Poesio and Vieira, 1998; Vieira and Poesio, 2000; Bunescu, 2003)1 , most of the recent studies focus on non-identity cases of bridging, which is the closest to Hawkins’s (1978) concept of associative anaphora. Among the non-identity relations, bridging covers various types of semantic relations. While early studies typically restrict themselves to predefined relations such as part-of, subset, set membership, and possession relations (Poesio and Vieira, 1998; Poesio et al., 2004b), recent studies claim that bridging is a diverse phenomenon that cannot be simply captured with a limited set of predefined relations (Markert et al., 2012; R¨osiger, 2018a). Second, what types of linguistic expressions can serve as bridging anaphors? Many traditional studies (Hawkins, 1978; Poesio and Vieira, 1998; Lassalle and Denis, 2011; R¨osiger, 2016) limited bridging anaphors to definite expressions, excluding indefinite expressions since they generally introduce new information that can be interpreted without the discourse context. However, L¨obner (1998) claimed that bridging anaph"
2020.coling-main.331,J04-3003,0,0.179784,"Missing"
2020.coling-main.331,postolache-etal-2006-transferring,0,0.159572,"Missing"
2020.coling-main.331,W11-1901,0,0.110707,"Missing"
2020.coling-main.331,W12-4501,0,0.0772573,"resolution and full bridging resolution are far from being solved. 3716 7 Concluding Remarks We have presented a survey of the current state of research on bridging resolution, a task that is far from being solved. Given that significant advances have been made on coreference resolution recently, we believe that bridging resolution will be the anaphora resolution task that will receive increasing attention in the near future. We conclude this paper with several recommendations on future research directions. Resources and evaluation. The CoNLL 2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) have played a crucial role in the accelerated progress on entity coreference resolution in the past few years by providing a standardized evaluation protocol (e.g., standard evaluation corpus and metrics) that facilitates performance comparisons of different resolvers. Progress on current bridging resolution research, which is reminiscent of that of entity coreference research in the pre-CoNLL era, is hindered in part by the lack of such standardization. As we move forward, it is imperative that a common evaluation framework be established for bridging research. Part of this effort should inc"
2020.coling-main.331,prasad-etal-2008-penn,0,0.0245411,"much larger than those currently available. While the use of distant supervision (to produce automatically labeled data) and pretrained models (to transfer knowledge from other tasks) may have reduced the amount of annotated data needed for model training, we believe having a large annotated corpus will not only stimulate interest in the task among researchers in other areas (e.g., by allowing them to develop complex models) but also provide task-specific linguistic insights. Corpora that contain other discourse-level annotations (e.g., the discourse relations in the Penn Discourse Treebank (Prasad et al., 2008)) would be ideal choices, as they can facilitate the development of joint models that enable the study of the potential interactions between bridging and other discourse phenomena. Cross-area collaboration. While bridging has been studied primarily by discourse researchers, the task is so broad that it covers many semantic relations studied by researchers in lexical semantics and information extraction, such as meronymy (Hearst, 1998; Berland and Charniak, 1999; Girju et al., 2006), hyponymy (Hearst, 1992; Cederberg and Widdows, 2003; Pantel and Pennacchiotti, 2006), and set-membership. While"
2020.coling-main.331,E12-1081,1,0.797642,"fy the bridging anaphors from the gold mentions and then pass the resulting anaphors to the mention-pair model for resolution. While in principle a binary classifier can be trained to determine whether a gold mention is an anaphor or not, previous work has trained classifiers for determining the IS of a mention and assumed that those mentions that are classified as “bridging” are bridging anaphors. Table 4 enumerates the features that have been proposed to train a classifier for determining the IS of a mention (Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012; Markert et al., 2012; Rahman and Ng, 2012a; Hou et al., 2013a; Hou, 2016; Hou et al., 2018). 3 4 A building part is a noun that denotes a part of a building (e.g., “window”, “door”). Argument ratio is a measure designed by Hou et al. (2014) to quantify how likely a NP is to take arguments. 3713 Feature Head match Compound premodification Co-argument Parallel structure Closest modifier WordNet query Google distance WordNet distance Verb pattern (relative) Verb pattern (top) Preposition pattern (relative) Preposition pattern (top) GPE role person match OTHER role person match Relative person match Both spatial/temporal Utterance distan"
2020.coling-main.331,N12-1090,1,0.81252,"fy the bridging anaphors from the gold mentions and then pass the resulting anaphors to the mention-pair model for resolution. While in principle a binary classifier can be trained to determine whether a gold mention is an anaphor or not, previous work has trained classifiers for determining the IS of a mention and assumed that those mentions that are classified as “bridging” are bridging anaphors. Table 4 enumerates the features that have been proposed to train a classifier for determining the IS of a mention (Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012; Markert et al., 2012; Rahman and Ng, 2012a; Hou et al., 2013a; Hou, 2016; Hou et al., 2018). 3 4 A building part is a noun that denotes a part of a building (e.g., “window”, “door”). Argument ratio is a measure designed by Hou et al. (2014) to quantify how likely a NP is to take arguments. 3713 Feature Head match Compound premodification Co-argument Parallel structure Closest modifier WordNet query Google distance WordNet distance Verb pattern (relative) Verb pattern (top) Preposition pattern (relative) Preposition pattern (top) GPE role person match OTHER role person match Relative person match Both spatial/temporal Utterance distan"
2020.coling-main.331,rodriguez-etal-2010-anaphoric,0,0.0548312,"Missing"
2020.coling-main.331,W16-0709,0,0.0160027,"nglish bridging corpora exist. DIRNDL (Bj¨orkelund et al., 2014), SemDok (B¨arenf¨anger et al., 2008), and GRAIN (Schweitzer et al., 2018) are in German. DEDE (Gardent and Manu´elian, 2005) and PAROLE (Gardent et al., 2003) are in French. Caselli/Prodanof (Caselli and Prodanof, 2006) and Italian Live Memories Corpus (Rodr´ıguez et al., 2010) are in Italian. Prague Dependency Treebank (Hajiˇc et al., 2018) is in Czech. CESS-ECE (Recasens et al., 2007) is in Spanish. COREA (Hendrickx et al., 2008) is in Dutch. The Sasano-Kurohashi corpus (Sasano and Kurohashi, 2009) is in Japanese. RuGenBridge (Roitberg and Nedoluzhko, 2016) is in Russian. Parallel bridging corpora are also available. For instance, Copenhagen Dependency Treebank is a parallel corpus involving Danish, English, Italian, German, and Spanish (Korzen and Buch-kromann, 2011), and CorefPro is a parallel corpus involving German, English, and Russian (Grishina, 2016). While not widely used, GUM is an ever-expanding English corpus annotated with bridging links by students at Georgetown University (Zeldes, 2017). 4 Evaluation Issues As mentioned before, an issue surrounding bridging resolution research concerns the lack of a standardized evaluation protocol"
2020.coling-main.331,W18-0705,0,0.0386429,"Missing"
2020.coling-main.331,C18-1298,0,0.259643,"Missing"
2020.coling-main.331,L16-1275,0,0.0602671,"Missing"
2020.coling-main.331,L18-1058,0,0.135306,"Missing"
2020.coling-main.331,W18-0703,0,0.0293088,"Missing"
2020.coling-main.331,D09-1151,0,0.0258877,"consider them as a subcategory of bridging. Several non-English bridging corpora exist. DIRNDL (Bj¨orkelund et al., 2014), SemDok (B¨arenf¨anger et al., 2008), and GRAIN (Schweitzer et al., 2018) are in German. DEDE (Gardent and Manu´elian, 2005) and PAROLE (Gardent et al., 2003) are in French. Caselli/Prodanof (Caselli and Prodanof, 2006) and Italian Live Memories Corpus (Rodr´ıguez et al., 2010) are in Italian. Prague Dependency Treebank (Hajiˇc et al., 2018) is in Czech. CESS-ECE (Recasens et al., 2007) is in Spanish. COREA (Hendrickx et al., 2008) is in Dutch. The Sasano-Kurohashi corpus (Sasano and Kurohashi, 2009) is in Japanese. RuGenBridge (Roitberg and Nedoluzhko, 2016) is in Russian. Parallel bridging corpora are also available. For instance, Copenhagen Dependency Treebank is a parallel corpus involving Danish, English, Italian, German, and Spanish (Korzen and Buch-kromann, 2011), and CorefPro is a parallel corpus involving German, English, and Russian (Grishina, 2016). While not widely used, GUM is an ever-expanding English corpus annotated with bridging links by students at Georgetown University (Zeldes, 2017). 4 Evaluation Issues As mentioned before, an issue surrounding bridging resolution rese"
2020.coling-main.331,L18-1457,0,0.0604195,"Missing"
2020.coling-main.331,J01-4004,0,0.40689,", BASHI (B), and ARRAU RST (A). As we can see, these are mostly lowrecall rules: many bridging anaphors cannot be recognized or resolved using these rules. Moreover, each rule has different performances (in terms of recognition and resolution) on different corpora, meaning that these rules, which are designed for ISNotes, do not generalize across corpora. 5.2 Learning-based Approaches We divide existing learning-based approaches into three categories. Feature-based approaches. In these approaches, a pairwise classifier, known as the mention-pair model in the coreference resolution literature (Soon et al., 2001; Ng and Cardie, 2002), is trained to determine whether two mentions has a bridging relation. Each training instance therefore corresponds to two mentions, one of which is a bridging anaphor and the other is its candidate antecedent. If the candidate antecedent is its correct antecedent, the instance is labeled as P OSITIVE; otherwise, it is labeled as N EGATIVE. Table 3 shows the list of features that have been used to train the mention-pair model. The mention-pair model works well if a resolver is given gold anaphors as input. To perform full bridging resolution, in which gold mentions are g"
2020.coling-main.331,J00-4003,0,0.360727,"es. First, what types of relations should bridging cover? As a linguistic phenomenon, bridging has been studied extensively by linguists (e.g., Clark (1975), Prince (1981), Gundel et al. (1993)). Clark (1975), who started this area of research, introduced a broad concept of bridging that includes coreference (i.e., the identity relation). Coreference, however, is gradually being excluded from bridging over time. For instance, while some early studies still included the difficult cases of coreference where two coreferent mentions do not share the same head as bridging (Poesio and Vieira, 1998; Vieira and Poesio, 2000; Bunescu, 2003)1 , most of the recent studies focus on non-identity cases of bridging, which is the closest to Hawkins’s (1978) concept of associative anaphora. Among the non-identity relations, bridging covers various types of semantic relations. While early studies typically restrict themselves to predefined relations such as part-of, subset, set membership, and possession relations (Poesio and Vieira, 1998; Poesio et al., 2004b), recent studies claim that bridging is a diverse phenomenon that cannot be simply captured with a limited set of predefined relations (Markert et al., 2012; R¨osig"
2020.coling-main.331,P97-1072,0,0.499653,"comparison between systems that employ filtering and those that do not, we believe that predicted, rather than gold, coreference information should be used in the filtering process. Data splits. While ARRAU has a standard train-dev-test split, the other corpora do not. In the absence of a standard data split, resolvers are evaluated via k-fold cross validation, which makes a head-to-head comparison of their results difficult. 5 5.1 Bridging Resolution Approaches Rule-based Approaches Virtually all early ruled-based resolvers operate in the least challenging setting, i.e., bridging resolution. Vieira and Teufel (1997) use a heuristic to resolve bridging anaphors based on synonymy, hyponymy, and meronymy relations from WordNet 1.6. Poesio et al. (1997) improve this system by limiting the use of some WordNet relations and improving the antecedent search strategy. For further improvement of this system, Poesio et al. (2002) complement WordNet coverage with another lexical resource of meronymy relations, which is acquired by querying syntactic patterns such as NP of NP and NP’s NP in the British National Corpus. To have a large corpus as a resource for acquiring semantic relations, Markert et al. (2003) use th"
2020.coling-main.331,H01-1035,0,0.106123,"Missing"
2020.coling-main.331,2020.coling-main.315,0,0.202469,"Missing"
2020.emnlp-main.536,K15-1002,0,0.0516165,"Missing"
2020.emnlp-main.536,D14-1162,0,0.0889177,"respond to the sentences in the given document, whereas in Joshi et al. the input instances correspond to fixed-length non-overlapping segments of the input document6 ; (2) Lee et al. use a LSTM, whereas Joshi et al. use a transformer; and (3) the pretrained embeddings are different. For each of these resolvers, we derive two variants. Specifically, Lee et al. (2018) employ GloVe+ELMo embeddings, but to better understand the effect of the contextual information provided by ELMo embeddings (Peters et al., 2018) on CR performance, we evaluate a version of Lee et al. using only GloVe embeddings (Pennington et al., 2014). We will henceforth refer to these two versions of Lee et al. as ELMo (i.e., ELMo+GloVe) and GloVe, respectively. Joshi 2 LEA (Moosavi and Strube, 2016) is a coreference evaluation metric recently designed to address the shortcomings associated with B3 and CEAFe , but we found no difference in the performance trends in our experiments according to CoNLL and LEA. See the Appendix for the LEA results. 6621 3 https://github.com/clarkkev/deep-coref https://github.com/kentonl/e2e-coref 5 https://github.com/mandarjoshi90/coref 6 This is the independent version in Joshi et al. (2019). 4 ACE Hyperpar"
2020.emnlp-main.571,D18-1199,0,0.129407,"ning insights into the strategies humans employ when creating hyperboles, and (3) addressing the automatic hyperbole detection task with deep learning techniques. 1 Introduction Recent years have seen a surge of interest in the automatic processing of figurative language in the NLP community, as evidenced by the successful organization of the NAACL 2018 Workshop on Figurative Language Processing. Much of the work on figurative language processing conducted so far, however, has focused on metaphor and metonymy (Tsvetkov et al., 2014), and more recently, sarcasm (Hazarika et al., 2018), idioms (Liu and Hwa, 2018), and puns (He et al., 2019). In particular, hyperbole, also known as exaggeration, is a relatively under-studied phenomenon in the community. This is somewhat surprising, especially given that the prevalence of hyperbole as a rhetorical device is only second to metaphor (Kreuz et al., 1996). Humans exaggerate in different situations for various purposes, such as creating amusement, expressing emotion and drawing attention (Li, 2013). The vast amount of work on metaphor detection in the past few years was stimulated in large part by the availability of standard evaluation corpora. Progress on"
2020.emnlp-main.571,N19-1423,0,0.0166184,"ve criteria for hyperboles are encoded partially by Unexpectedness, whereas the qualitative criteria are encoded by Polarity, Subjectivity, and Emotional Intensity. We will henceforth refer to this feature set as TF. Embedding features are features derived from word embeddings. These features have recently been used extensively in various NLP tasks. We experiment with three types of pre-trained word embeddings: (1) the 300-dimensional Skip-gram representations, (2) the 200-dimensional Directional Skip-gram embeddings, and (3) the 768dimensional contextualized embeddings trained based on BERT (Devlin et al., 2019), which we obtain by feeding the input sentence into Cui et al.’s (2019) implementation of BERT for Chinese. We generate the embedding features for a given sentence by averaging the embeddings of its constituent words. We will refer to the resulting feature sets produced via Skip-gram, Directional Skipgram, and BERT as SG, DS, and BE, respectively. 5.2 Neural Network Setting Troiano et al. (2018) employ only traditional learners in their experiments. A natural question is: will deep learners offer better performance on the hyperbole detection task? To answer this question, we employ the two co"
2020.emnlp-main.571,C18-1156,0,0.151422,"our corpus, with the goal of gaining insights into the strategies humans employ when creating hyperboles, and (3) addressing the automatic hyperbole detection task with deep learning techniques. 1 Introduction Recent years have seen a surge of interest in the automatic processing of figurative language in the NLP community, as evidenced by the successful organization of the NAACL 2018 Workshop on Figurative Language Processing. Much of the work on figurative language processing conducted so far, however, has focused on metaphor and metonymy (Tsvetkov et al., 2014), and more recently, sarcasm (Hazarika et al., 2018), idioms (Liu and Hwa, 2018), and puns (He et al., 2019). In particular, hyperbole, also known as exaggeration, is a relatively under-studied phenomenon in the community. This is somewhat surprising, especially given that the prevalence of hyperbole as a rhetorical device is only second to metaphor (Kreuz et al., 1996). Humans exaggerate in different situations for various purposes, such as creating amusement, expressing emotion and drawing attention (Li, 2013). The vast amount of work on metaphor detection in the past few years was stimulated in large part by the availability of standard eval"
2020.emnlp-main.571,N19-1172,0,0.118773,"ies humans employ when creating hyperboles, and (3) addressing the automatic hyperbole detection task with deep learning techniques. 1 Introduction Recent years have seen a surge of interest in the automatic processing of figurative language in the NLP community, as evidenced by the successful organization of the NAACL 2018 Workshop on Figurative Language Processing. Much of the work on figurative language processing conducted so far, however, has focused on metaphor and metonymy (Tsvetkov et al., 2014), and more recently, sarcasm (Hazarika et al., 2018), idioms (Liu and Hwa, 2018), and puns (He et al., 2019). In particular, hyperbole, also known as exaggeration, is a relatively under-studied phenomenon in the community. This is somewhat surprising, especially given that the prevalence of hyperbole as a rhetorical device is only second to metaphor (Kreuz et al., 1996). Humans exaggerate in different situations for various purposes, such as creating amusement, expressing emotion and drawing attention (Li, 2013). The vast amount of work on metaphor detection in the past few years was stimulated in large part by the availability of standard evaluation corpora. Progress on the computational study of e"
2020.emnlp-main.571,N18-2028,0,0.0217781,". The hand-crafted features are taken from those described in Troiano et al. (2018). More specifically, we reimplement four of the five hand-crafted features used by Troiano et al., namely Unexpectedness (how coherent a word is with the rest of the discourse), Polarity (the sentiment of the sentence), Subjectivity (whether the sentence is objective or not), and Emotional Intensity (the sentiment strength of the sentence). To be specific, we compute Unexpectedness with the pretrained Skipgram vectors provided by Google (Mikolov et al., 2013) and the Directional Skip-gram embeddings provided by Song et al. (2018), and Polarity with the SnowNLP library3 and HowNet4 . However, we are unable to implement the Imageability feature, which encodes the degree to which a word evokes a mental image. The reason is that Troiano et al. computed this feature based on the imageability ratings of the MRC psycholinguistic database, but such a resource is absent for Chinese. According to Troiano et al., the quantitative criteria for hyperboles are encoded partially by Unexpectedness, whereas the qualitative criteria are encoded by Polarity, Subjectivity, and Emotional Intensity. We will henceforth refer to this feature"
2020.emnlp-main.571,D18-1367,0,0.383343,"hyperbole as a rhetorical device is only second to metaphor (Kreuz et al., 1996). Humans exaggerate in different situations for various purposes, such as creating amusement, expressing emotion and drawing attention (Li, 2013). The vast amount of work on metaphor detection in the past few years was stimulated in large part by the availability of standard evaluation corpora. Progress on the computational study of exaggeration, on the other hand, is hindered by the lack of annotated resources. To our knowledge, HYPO, the first dataset that focuses on exaggeration, was only released in late 2018 (Troiano et al., 2018). HYPO consists of 709 hyperbolic sentences, each of which has a non-hyperbolic version created by manually paraphrasing its hyperbolic counterpart. Given the dataset, Troiano et al. introduced the task of automatic hyperbole detection, where the goal is to determine whether a sentence is a hyperbole. Given the status quo, our goal is to further the computational study of exaggeration. Specifically, our contributions in this work are three-fold. First, we create HYPO-cn, the first Chinese dataset on exaggeration. HYPO-cn consists of 4762 sentences, of which 2680 are hyperbolic and 2082 are non"
2020.emnlp-main.571,P14-1024,0,0.0208006,"(2) performing a statistical and manual analysis of our corpus, with the goal of gaining insights into the strategies humans employ when creating hyperboles, and (3) addressing the automatic hyperbole detection task with deep learning techniques. 1 Introduction Recent years have seen a surge of interest in the automatic processing of figurative language in the NLP community, as evidenced by the successful organization of the NAACL 2018 Workshop on Figurative Language Processing. Much of the work on figurative language processing conducted so far, however, has focused on metaphor and metonymy (Tsvetkov et al., 2014), and more recently, sarcasm (Hazarika et al., 2018), idioms (Liu and Hwa, 2018), and puns (He et al., 2019). In particular, hyperbole, also known as exaggeration, is a relatively under-studied phenomenon in the community. This is somewhat surprising, especially given that the prevalence of hyperbole as a rhetorical device is only second to metaphor (Kreuz et al., 1996). Humans exaggerate in different situations for various purposes, such as creating amusement, expressing emotion and drawing attention (Li, 2013). The vast amount of work on metaphor detection in the past few years was stimulate"
2020.lrec-1.839,W06-1651,0,0.0454185,"they share over half their tokens. We perform most of our analysis on approximate match results rather than exact match results as it can be difficult even for human annotators to identify exactly the same boundaries for an argument component.4 We use the same formulas for calculating these numbers for RI except that j and i represent a true relation and an RC respectively, two relations approximately (exactly) match if both their source and target ACCs approximately (exactly) match, and n is the no-relation class. 4 Approximate match has been used in evaluating opinion mining systems (e.g., Choi et al. (2006), Yang and Cardie (2013)), where researchers have also reported difficulties in having human annotators identify exactly the same boundaries for an opinion expression and its sources and targets. 6800 Metric Approx Exact System PIPE ILP U Eger et al. (2017) PIPE ILP U Eger et al. (2017) MC-F 44.0 50.7 24.9 – 39.6 41.7 19.0 – C-F 36.1 42.6 42.2 – 33.9 34.9 36.4 – ACI P-F P 69.8 75.2 76.7 61.0 77.2 73.5 – – 61.7 64.0 63.5 50.6 59.6 57.7 – – R 49.3 66.9 57.6 – 45.8 55.1 45.2 – F 59.6 63.8 64.6 77.2 53.4 52.7 50.7 70.8 S-F 20.1 32.5 37.9 – 17.2 23.0 26.0 – A-F 00.0 01.4 00.0 – 00.0 00.0 00.0 – RI"
2020.lrec-1.839,P17-1002,0,0.033238,"Missing"
2020.lrec-1.839,W13-2707,0,0.036077,"xactly where the S node ends, or if S ends in a punctuation, immediately before the punctuation. If the S node ends in a (possibly nested) SBAR node, immediately before the nth shallowest SBAR. If the S node ends in a (possibly nested) PP node, immediately before the nth shallowest PP. Related Work Recall that identifying argumentative structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardia"
2020.lrec-1.839,J19-4006,0,0.0508084,"Missing"
2020.lrec-1.839,C14-1141,0,0.0201956,"g the locations and types of the argument components, and (2) identifying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the tas"
2020.lrec-1.839,P14-5010,0,0.00512097,"Missing"
2020.lrec-1.839,P16-1107,0,0.0170134,"ponent typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the task by assuming as input gold argument components. Finally, some work addressed all argument mining subtasks (e.g., Persing and Ng (2016)). Unlike our system, however, virtually all of the aforementioned systems are supervised. 3. Corpus Our corpus consists of 402 persuasive student essays collected and annotated by S&G. Some relevant statistics"
2020.lrec-1.839,W14-2104,0,0.0287473,"ative structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and rel"
2020.lrec-1.839,W14-2105,0,0.0322792,"sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the task by assuming as input gold argument components. Finally, some work addressed all argument mining subtasks (e.g., Persing and Ng (2016)). Unlike our system, however, virtually all of the aforementio"
2020.lrec-1.839,D15-1110,0,0.135274,"rivaled by an unsupervised system. For instance, is it the features that fail to adequately capture the information provided by the annotated data or the learner that fails to effectively exploit the annotated data? Or is the current amount of annotated training data insufficient for effective learning? Second, our promising results could spark interest in the development of unsupervised and semi-supervised approaches to argument mining, which could reduce a system’s reliance on 1 Many existing argument mining systems were evaluated in a non-end-to-end setting (e.g., Stab and Gurevych (2014), Peldszus and Stede (2015), Wei et al. (2017)). For instance, Stab and Gurevych (2014) trained an ACI classifier and applied it to classify only gold argument components (i.e., text spans corresponding to a major claim, claim, or premise in the gold standard) or sentences that contain no gold argument components (as nonargumentative). Similarly, they applied their learned RI classifier to classify only the relation between two gold argument components. In other words, they simplified both tasks by avoiding the challenging task of identifying the locations of argument components. Consequently, their approach cannot be a"
2020.lrec-1.839,N16-1164,1,0.537542,"ficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the task by assuming as input gold argument components. Finally, some work addressed all argument mining subtasks (e.g., Persing and Ng (2016)). Unlike our system, however, virtually all of the aforementioned systems are supervised. 3. Corpus Our corpus consists of 402 persuasive student essays collected and annotated by S&G. Some relevant statistics are shown in Table 1. Each essay is an average of 4.6 paragraphs (16.8 sentences) in length and is written in response to a topic such as “competition or co-operation-which is better?”. This corpus is ideal for argumentation mining because (1) student essays are more simply structured than professional writing, making them appropriate for early work on the task, (2) a major application"
2020.lrec-1.839,D15-1050,0,0.0171731,"ying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the task by assuming as input gold argument components. Finally, some work addr"
2020.lrec-1.839,W15-0508,0,0.0245356,", 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identification instead. For instance, Nguyen and Litman (2016) showed how context can be exploited to identify relations between argument components. Stab and Gurevych (2014) and Peldszus and Stede (2015), on the other hand, addressed both argument component typing and relation identification, but simplified the task by assuming as input gold argument components. Finally, some work addressed all argument mining subtasks (e.g., Persing and Ng (2016)). Unlike our system, however, virtually all of the aforementioned systems are supervised. 3. Corpus Our corp"
2020.lrec-1.839,W14-2110,0,0.0188218,"s in a punctuation, immediately before the punctuation. If the S node ends in a (possibly nested) SBAR node, immediately before the nth shallowest SBAR. If the S node ends in a (possibly nested) PP node, immediately before the nth shallowest PP. Related Work Recall that identifying argumentative structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers foc"
2020.lrec-1.839,D14-1006,0,0.0629802,"their performances to be rivaled by an unsupervised system. For instance, is it the features that fail to adequately capture the information provided by the annotated data or the learner that fails to effectively exploit the annotated data? Or is the current amount of annotated training data insufficient for effective learning? Second, our promising results could spark interest in the development of unsupervised and semi-supervised approaches to argument mining, which could reduce a system’s reliance on 1 Many existing argument mining systems were evaluated in a non-end-to-end setting (e.g., Stab and Gurevych (2014), Peldszus and Stede (2015), Wei et al. (2017)). For instance, Stab and Gurevych (2014) trained an ACI classifier and applied it to classify only gold argument components (i.e., text spans corresponding to a major claim, claim, or premise in the gold standard) or sentences that contain no gold argument components (as nonargumentative). Similarly, they applied their learned RI classifier to classify only the relation between two gold argument components. In other words, they simplified both tasks by avoiding the challenging task of identifying the locations of argument components. Consequently,"
2020.lrec-1.839,J17-3005,0,0.0176618,"f research). Argumentation mining typically involves addressing two subtasks: (1) argument component identification (ACI), which consists of identifying the locations and types of the components that make up the arguments (i.e., major claims, claims, and premises), and (2) relation identification (RI), which involves identifying the type of relation that holds between two argument components (i.e., support, attack, none). As an example, consider the following text segment taken from a corpus of student essays that Stab and Gurevych (S&G) annotated with argument components and their relations (Stab and Gurevych, 2017): In my view point, (1) I would agree to idea of taking a break before starting higher education. (2) Students who take break benefit by traveling around the places or by working. (3) They tend to contribute more due to their real world experiences which makes them more mature. In this example, premise (3) supports claim (2), and (1) is a major claim. State-of-the-art argument mining systems are supervised, adopting a feature-rich approach that typically include large variety of structural, lexical, and syntactic features. The plethora of features seem to suggest that argumentannotated trainin"
2020.lrec-1.839,W15-4631,0,0.0273868,"immediately before the punctuation. If the S node ends in a (possibly nested) SBAR node, immediately before the nth shallowest SBAR. If the S node ends in a (possibly nested) PP node, immediately before the nth shallowest PP. Related Work Recall that identifying argumentative structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Some researchers focused on argument location identification, classifying whether a sentence contains an argument (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). Others focused on argument component typing, determining the type of an argument component. While the vast majority of previous works perform argument component typing at the sentence level (Rooney et al., 2012; Teufel, 1999; Burstein et al., 2003; Ong et al., 2014; Falakmasir et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Lippi and Torroni, 2016; Rinott et al., 2015), some recent work focused on the more difficult task of typing argument components at the clause level (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015). Some researchers focused on relation identi"
2020.lrec-1.839,P13-1161,0,0.0267327,"f their tokens. We perform most of our analysis on approximate match results rather than exact match results as it can be difficult even for human annotators to identify exactly the same boundaries for an argument component.4 We use the same formulas for calculating these numbers for RI except that j and i represent a true relation and an RC respectively, two relations approximately (exactly) match if both their source and target ACCs approximately (exactly) match, and n is the no-relation class. 4 Approximate match has been used in evaluating opinion mining systems (e.g., Choi et al. (2006), Yang and Cardie (2013)), where researchers have also reported difficulties in having human annotators identify exactly the same boundaries for an opinion expression and its sources and targets. 6800 Metric Approx Exact System PIPE ILP U Eger et al. (2017) PIPE ILP U Eger et al. (2017) MC-F 44.0 50.7 24.9 – 39.6 41.7 19.0 – C-F 36.1 42.6 42.2 – 33.9 34.9 36.4 – ACI P-F P 69.8 75.2 76.7 61.0 77.2 73.5 – – 61.7 64.0 63.5 50.6 59.6 57.7 – – R 49.3 66.9 57.6 – 45.8 55.1 45.2 – F 59.6 63.8 64.6 77.2 53.4 52.7 50.7 70.8 S-F 20.1 32.5 37.9 – 17.2 23.0 26.0 – A-F 00.0 01.4 00.0 – 00.0 00.0 00.0 – RI P 22.3 23.9 38.2 – 18.6"
2020.lrec-1.840,M98-1001,0,0.609109,"Missing"
2020.lrec-1.840,N15-1146,0,0.0183769,"and the corresponding OTE is waiter, with opinion polarity negative. However, in reality, this statement directly expresses an opinion on the waiter’s behavior, specifically, its appropriateness. Because of this, we can interpret this 6807 as an opinion on the waiter, and as a consequence, as an opinion on customer service. This information is lost under the SemEval annotation scheme due to the fact that the lexical target and opinion expression are not annotated. Another relevant well-known resource is the MultiPerspective Question Answering (MPQA) dataset (Wiebe et al., 2005; Wilson, 2008; Deng and Wiebe, 2015), which is composed of 535 political news articles spanning 10 different topics and annotated with opinion expressions, opinion holders, and lexical opinion targets. The MPQA dataset introduces a general framework for fine-grained sentiment analysis, and contains annotations for opinions in terms of lexical targets, opinion expressions, and opinion holders. Given that it is composed of political news articles, there is no notion of targets as entity, aspect pairs, and the language, topics, and opinion holders can be quite different from those found in user-generated reviews. 3.2. Annotation Sc"
2020.lrec-1.840,M95-1001,0,0.652717,"best in town!, it is annotated as the lexical target, but crab cake is also labeled as the resolution of it. As far as we know, none of the existing ABSA datasets has pronominal targets resolved. In addition, for each lexical target we annotate its semantic head: • The views around the mountain are beautiful → The opinion target is The views around the mountain and its head is views. • I love swimming in the ocean → The opinion target is swimming in the ocean and its head is swimming Our annotation of semantic heads is motivated by the scheme used for annotating the MUC-6 and MUC-7 datasets (Grishman and Sundheim, 1995; Chinchor, 1998), which were designed for information extraction tasks such as named entity recognition and entity coreference resolution; Complex phrases are typically hard for a system to extract, so in order not to penalize a system for its failure to extract a phrase because of its complexity, one can consider that it extracts the phrase correctly as long as it extracts the simpler (and thus arguably easier to extract) phrase denoted by the semantic head. As far as we know, none of the existing fine-grained sentiment analysis corpora has semantic heads annotated. 3.2.2. Opinion Expression"
2020.lrec-1.840,S14-2004,0,0.0648834,"Missing"
2020.lrec-1.840,S15-2082,0,0.117713,"example, the previous sentence expresses a positive polarity towards (food, price). The entity and the aspect are both chosen from predefined sets. So, if an opinion is expressed on an entity that does not appear in the predefined set of entities, then an ABSA system is not expected to output any opinion for that entity. The important role this task can play in decision making for governments, companies, and individuals has resulted in a large body of research on the subject being developed over the years, along with the organization of multiple shared tasks in SemEval (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016), TASS (Mart´ınez C´amara et al., 2018), and GermanEval (Wojatzki et al., 2017). The datasets published in the SemEval ABSA tasks (Pontiki et al., 2016), which are composed of user-generated reviews of products and services such as those found on Amazon and Yelp, have heavily influenced work in the area, as researchers frequently rely on them for system training and testing. However, these datasets follow a task-specific annotation scheme, under which opinion annotations do not always capture the full meaning of the sentiment expressed in text. Consider the following pai"
2020.lrec-1.840,S16-1002,0,0.0537864,"Missing"
2020.lrec-1.840,P13-1161,0,0.0326898,"the semantic target can not be described in a straightforward manner, such as in The way the waiter looked at me was totally inappropriate, where the lexical target is The way the waiter looked at me and the opinion expression is totally inappropriate. To better understand the significance of our observation, consider its implication for opinion model M 2. Recall that The ink of the printer is too expensive. has three valid targets that correspond to different levels of granularity under M 2: (ink, price), (printer, cost of operation), 4 In the general fine-grained opinion mining literature (Yang and Cardie, 2013), lexical targets are simply referred to as opinion targets, while the notion of semantic targets is absent. and (printer, ink). Simply put, the combination of lexical target and opinion expression sufficiently encodes the original meaning of the sentiment intended by the author, and can therefore be used to infer all valid (entity, attribute) pairs in M 2. Specifically, from the lexical target ink of the printer and opinion expression too expensive, it is easy to see that among the three valid targets, (ink, price) matches most closely the meaning of the original text. In addition, it can be"
2021.codi-sharedtask.1,D19-1422,0,0.0559867,"variant that is only trained on bridging annotations. We evaluated their best-performing model, which was trained on the RST sub-corpus of ARRAU, on CODI - CRAC 2021 data.19 The baseline for Task 3 leverages a simple heuristic that only considers demonstrative pronouns (this, that) as anaphors and considers the immediately preceding clause/utterance in the conversation to be their antecedent. Although simplistic, the algorithm achieves respectable scores on the CODI CRAC 2021 development corpus. The performance KU_NLP submitted results for tasks 1 and 2. For identity anaphora, they leveraged Cui and Zhang (2019)’s model with an ELECTRA-large backbone 18 20 The necessary scripts are available from https:// github.com/sopankhosla/codi2021_scripts 21 Participants were allowed to create teams. https://github.com/lxucs/coref-hoi/ https://github.com/juntaoy/ dali-bridging 19 8 9 Discourse Deixis Resolution Bridging Resolution Anaphora Resolution Track - Joshi et al. Leverage baseline’s architecture to find the correct (2019) bridging antecedent in the gold setting. KU_NLP INRIA - Yu and Poe- A multi-pass sieve approach which used the baseline sio (2020) as one of the sieves and consists of a set of learnin"
2021.codi-sharedtask.1,E89-1022,0,0.449685,"v., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect"
2021.codi-sharedtask.1,J18-3007,0,0.521559,"chael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialog"
2021.codi-sharedtask.1,D17-1018,0,0.0879633,"iple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, h"
2021.codi-sharedtask.1,N18-2108,0,0.205145,"Missing"
2021.codi-sharedtask.1,2020.acl-main.132,0,0.135367,"urse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on an"
2021.codi-sharedtask.1,J18-2002,1,0.943726,"ridging, and Discourse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the liter"
2021.codi-sharedtask.1,L16-1145,0,0.382038,"Missing"
2021.codi-sharedtask.1,2020.tacl-1.5,0,0.344926,"ons. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addit"
2021.codi-sharedtask.1,D19-1588,0,0.0733603,"om the end-to-end neural coreference resolution model of Joshi et al. (2020). They recognized singletons, encoded speakers for all turns, and leveraged other out-of-domain datasets during training. Eval DD (Pred) UTD_NLP DFKI 42.70 20.97 35.35 17.43 39.64 23.76 35.43 23.86 38.3 21.5 Baseline 12.12 15.75 18.27 13.55 14.9 Table 5: Performance on Task 3 (Evaluation Phase) – Discourse Deixis (CoNLL Avg. F1) INRIA submitted an end-to-end transformerbased model fine-tuned for the bridging resolution task. They formulated the bridging problem as antecedent selection, and leveraged Lee et al. (2018); Joshi et al. (2019)’s architecture to find the correct antecedent. for mention detection. The resulting mention representation, created from the constituent token representations, is then fed to a pointer-network (Vinyals et al., 2015) based coreference resolution model for clustering. They solved the bridging resolution problem using a machine reading comprehension framework, where they constructed a query for each entity of the form – ""What is related of ENTITY?"". The input of their model is the query and the document (i.e., all utterances of dialogue), and the output is the entity span that is the answer for"
2021.codi-sharedtask.1,2021.crac-1.2,1,0.739024,".edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves much more deictic reference, vaguer anaphoric and discourse deictic reference, speaker grounding of pronouns and long-distance conversation structure. These are complexities that are often missing in news or Wikipedia articles,"
2021.codi-sharedtask.1,P19-1066,0,0.0336391,"vidually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity"
2021.codi-sharedtask.1,D17-1021,0,0.24606,"Missing"
2021.codi-sharedtask.1,2020.coling-main.331,1,0.631559,"atasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spo"
2021.codi-sharedtask.1,J93-2004,0,0.0756707,"ther markable and thus form a singleton coreference chain. Moreover, in ARRAU non-referring markables are manually sub-classified into expletives, predicative, and quantifiers. In addition, all generic references are marked, including premodifiers when the entity referred to is mentioned again, e.g., in the case of ARRAU: Corpus and Annotation Scheme Genres The ARRAU corpus4 (Poesio and Artstein, 2008; Uryupina et al., 2020) was designed to cover a variety of genres. It includes a substantial amount of news text in a sub-corpus called RST, consisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). In addition to the news data, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus5 as well as the pilot dialogues in the socalled TRAINS-91 corpus. The PEAR sub-corpus consists of the complete collection of spoken nar6 The original intention had been to use the soon-to-bereleased ARRAU 3, but as the work on this version was still under way by the time the training data had to be released, ARRAU 2 was used instead–i.e., the exact same version used for the CRA"
2021.codi-sharedtask.1,2021.naacl-main.131,1,0.669111,"osla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important"
2021.codi-sharedtask.1,D14-1056,0,0.107024,"2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no longer to be seen: she found herself in [a long, low hall, which was lit up by a row of lamps hanging from [the roof]]. There were doors all round the hall, but they were all locked; and when Alice had been all the way down one side and up the other, trying every door, she walked sadly down [the mi"
2021.codi-sharedtask.1,P12-1084,1,0.830857,"es of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is rel"
2021.codi-sharedtask.1,J98-2001,1,0.56338,"CHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers q"
2021.codi-sharedtask.1,W00-1007,0,0.805051,"nt. . . The statement was the [US]1 ’s government first acknowledgment of what other groups, such as the International Monetary Fund, have been predicting for months. • an undersp-rel relation for ‘obvious cases of bridging that didn’t fit any other category’. Discourse deixis Discourse deixis in its full form is a very complex form of reference, both to annotate (Kolhatkar et al., 2018) and to resolve. Very few anaphoric annotation projects have attempted to annotate discourse deixis in its entirety (Kolhatkar et al., 2018). More typical is a partial annotation, as in (Byron and Allen, 1998; Navarretta, 2000), who annotated pronominal reference to abstract objects; in O NTO N OTES, where event anaphora was marked (Pradhan et al., 2007); and in the work of Kolhatkar and Hirst (2014), which focused on so-called shell nouns. In ARRAU, A coder specifying that a referring expression is discourseold is asked whether its antecedent was introduced using a phrase (markable) or a segment (discourse segment). Coders who choose segment have to mark a sequence of predefined clauses. (9) The Treasury report, which is required annually by a provision of the 1988 trade act, again took South Korea to task for its"
2021.codi-sharedtask.1,P14-2006,1,0.93955,"association such as identity of sense anaphora, etc. (Poesio, 2016). Some of these resources are of a sufficient size to support shared tasks. In particular, the AR RAU corpus was used as the dataset for the Shared Task on Anaphora Resolution with ARRAU in the CRAC 2018 Workshop (Poesio et al., 2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no"
2021.codi-sharedtask.1,W13-2313,0,0.0303269,"f anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is related to / associated with, but not identical to, th"
2021.codi-sharedtask.1,W12-4501,0,0.677797,"ric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for wh"
2021.codi-sharedtask.1,nissim-etal-2004-annotation,0,0.152087,"2) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers question receivers on provided topics, such as child care, recycling, and news media. 440 speakers participate in these 1,155 conversations, producing 221,616 utterances. It was 7 Identity anaphora also includes split antecedent plural anaphoric reference. 8 https://catalog.ldc.upenn.edu/ LDC97S62 5 annotated for dialogue acts by Stolcke et al. (1997)9 and for information status by Nissim et al. (2004). exactly the same MMAX style – and by the same two annotators from the DALI team at Queen Mary University and University of Essex, Dr. Maris Camilleri and Dr. Paloma Carretero Garcia, who annotated and checked ARRAU Release 3, which is currently being prepared for release. However, due to time constraints, each document was only annotated by a single annotator, with spot checks carried out by the other annotator and Massimo Poesio (in ARRAU 3 each document was looked at by both annotators, and most documents were also independently checked by Massimo Poesio). To prepare the data for the share"
2021.codi-sharedtask.1,W04-0210,1,0.761359,"j didn’t fit [her]i . However, many other types of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associati"
2021.codi-sharedtask.1,D12-1071,1,0.763158,"e-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November"
2021.codi-sharedtask.1,poesio-artstein-2008-anaphoric,1,0.800941,"lopment in the trial phase, whereas all other datasets were used for training.6 A limitation of most resources annotated for anaphora is that they mostly focus on expository text. The one substantial dataset of anaphoric relations in dialogue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the"
2021.codi-sharedtask.1,N19-1176,1,0.849468,"tings, and evaluation metrics in Section 4, and submission details in Section 5. This is followed by details of the baselines in Section 6 and participating systems in Section 7. We present a discussion of the performance of the systems on different tasks and sub-corpora in Section 8, and finally conclude this paper in Section 9. 2 1993), as in (2). These are also cases of plural identity coreference, but to sets composed of two or more entities introduced by separate noun phrases. Such references are annotated in, e.g., ARRAU (Uryupina et al., 2020), GUM (Zeldes, 2017) and Phrase Detectives (Poesio et al., 2019). (2) [John]1 met [Mary]2 . [He]1 greeted [her]2 . [They]1,2 went to the movies. Discourse deixis In ONTONOTES, event anaphora, a subtype of discourse deixis (Webber, 1991; Kolhatkar et al., 2018) is marked, as in (3) (where [that] arguably refers to the event of a white rabbit with pink ears running past Alice) but not the whole range of abstract anaphora, illustrated by, e.g., (4), where again arguably [this] refers to the fact that the Rabbit was able to talk. (Both examples from the Phrase Detectives corpus (Poesio et al., 2019).) (3) So she was considering in her own mind (as well as she"
2021.codi-sharedtask.1,2020.emnlp-main.686,0,0.790191,"f 55 individual participants registered for the CODI - CRAC 2021 shared task on CodaLab.21 Among them, five teams submitted results for Task 1, three submitted results for Task 2, and two submitted results for Task 3. Teams UTD_NLP, KU_NLP, DFKI_TalkingRobots, Emory_NLP, and INRIA submitted system description papers. We summarize their approaches below (and in Table 2): Baselines UTD_NLP participated in all three tasks. For identity anaphora, they deployed a pipeline architecture consisting of a mention detection component and an entity coreference component. The coreference component extends Xu and Choi (2020)’s implementation of Lee et al. (2018) by modifying the objective so that it can output singleton clusters, and enforces dialogue-specific constraints. They setup a similar architecture for discourse deixis. However, they slightly modified the objective function in Xu and Choi (2020) by classifying each span as a candidate anaphor, a candidate antecedent, or a non-mention in the mention detection stage, and resolving only candidate anaphors to candidate antecedents later. The team used a multi-pass sieve approach for bridging resolution to target same-head bridging links, with Yu and Poesio (2"
2021.codi-sharedtask.1,2020.lrec-1.1,1,0.560581,"ities that are often missing in news or Wikipedia articles, which form a large chunk of current datasets for coreference resolution. There has been some research on coreference in dialogue (Byron, 2002; Eckert and Strube, 2001; Müller, 2008), but very limited in scope (primarily related to pronominal interpretation), due to the lack of suitable corpora. The one language for which substantial corpora of coreference in dialogue exist is French: the ANCOR corpus (Muzerelle et al., 2014) has enabled the development of an end-to-end neural model for coreference interpretation in dialogue by Grobol (2020). For English, the one resource we are aware of fully annotated for anaphoric reference is the TRAINS corpora included in the ARRAU corpus (Uryupina et al., 2020). The objective of the CODI - CRAC 2021 Shared In this paper, we provide an overview of the CODI - CRAC 2021 Shared Task. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on"
2021.codi-sharedtask.1,2020.coling-main.538,1,0.858079,"A; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves m"
2021.codi-sharedtask.1,D19-1062,0,0.248851,"duced (antecedent). For discourse-old mentions, an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on"
2021.codi-sharedtask.1,2021.naacl-main.329,1,0.738872,"Missing"
2021.codi-sharedtask.1,P16-1216,0,0.0628664,"Missing"
2021.codi-sharedtask.1,2020.coling-main.315,1,0.910842,"in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Ar"
2021.codi-sharedtask.1,C18-1003,0,0.0784215,"USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Di"
2021.codi-sharedtask.1,P19-1566,0,0.250523,", an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and"
2021.codi-sharedtask.1,2021.acl-short.59,0,0.0207201,"gue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the second release (Uryupina et al., 2020), the guidelines for bridging were extended and genericity was also annotated using the GNOME guidelines, but a complete new manual was not produced. However, a fairly extensive description ca"
2021.codi-sharedtask.1,Q18-1042,0,0.0216415,"t al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics Task in Anaphora Resolution in Dialogue1 was to provide participants with the opportunity to develop automated approaches for corefer"
2021.codi-sharedtask.1,P15-1137,0,0.0532448,"eixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et"
2021.codi-sharedtask.2,D09-1120,0,0.0434047,"e rule in Stanford’s discourse processing sieve posits two mentions as coreferent if they are both pronouns and are produced by the same speaker. Sieves are ordered by their precision, with the most precise sieve appearing first. To resolve a set of mentions in a document, the resolver makes multiple passes over them: in the i-th pass, it attempts to use only the rules in the i-th sieve to find an antecedent for each mention mk . Specifically, when searching for an antecedent for mk , its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). The partial clustering of the mentions created in the i-th pass is then passed to the i + 1-th pass. Hence, later passes can exploit the information computed by previous passes, but a coreference link established earlier cannot be removed later. Our sieve-based approach to bridging resolution differs from the conventional approach described above in two key aspects. First, rather than order the sieves by precision, we order them so that they collectively achieve the best performance on the test sets. Second, later sieves only attempt to resolve mentions that have not been resolved by earlier"
2021.codi-sharedtask.2,P19-1066,0,0.0255531,"Missing"
2021.codi-sharedtask.2,P14-5010,0,0.0153265,"used to train sieve y. ing mentions. We employ a turn window when generating training instances, meaning that training instances will be generated from two mentions only if the distance between them is within a certain number of turns. We treat the turn window as a tunable parameter. Test instances are created in the same manner as the training instances. The four mention-pair models are trained using different features. Table 10 provides a description of these features, the motivation behind their design, as well as the features used to train each mention-pair model. We use Stanford CoreNLP (Manning et al., 2014) and spaCy (Honnibal et al., 2020) to extract the linguistic information needed to compute the features. A few features deserve mention. Feature 4, which is used by the singularsingular sieve, is a set of features encoding the presence of conflicting article pairs, which are manually identified via our inspection of the official development sets. Most conflicting pairs are composed of a definite article and an indefinite article, so the presence of a conflicting article pair makes the corresponding mentions less likely to be coreferent (because they differ in definiteness). At the same time, h"
2021.codi-sharedtask.2,2021.codi-sharedtask.1,1,0.717028,"deixis resolution. 1 • for entity coreference, we employ a pipeline architecture where we perform mention detection prior to coreference resolution. Our mention detection model is adapted from Xu and Choi’s (2020) implementation of Lee et al.’s span-based model. For coreference resolution, we extend Xu and Choi’s coreference model by (1) adding a sentence distance feature; (2) modifying the objective function used by the model so that it can learn to output singleton clusters; and (3) introducing non-coreference constraints for the dialogue domain. Introduction The CODI-CRAC 2021 shared task (Khosla et al., 2021), which focuses on anaphora resolution in dialogue, provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis/abstract anaphora resolution. While the CRAC 2018 shared task (Poesio et al., 2018) provides the same three tracks, the two shared tasks differ by the genre they focus on: CRAC 2018 focuses primarily on text, whereas CODI-CRAC 2021 focuses exclusively on spoken dialogue. Not only has entity coreference resolution been an active area of research in the NLP community in the past few decades, but recent years have seen considerable progress on"
2021.codi-sharedtask.2,P02-1014,1,0.46207,"employ five learning-based sieves, including one neural sieve and four &quot;same head&quot; sieves. 4.1.1.2 Same-Head Sieves 4.1.1.1 Neural Sieve Next, we design four sieves, all of which focus on establishing bridging links between two mentions that have the same head lemma. We therefore refer to them as same-head sieves. We focus on samehead sieves as opposed to different-head sieves because the former are arguably less challenging to design than the latter, especially in the Predicted setting where no gold mentions are given. Each sieve operates by combining a mentionpair model (Soon et al., 2001; Ng and Cardie, 2002), which in our case is a binary classifier that determines whether two mentions having the same head are involved in a bridging relation, with a closest-first single-link clustering algorithm, which selects as the antecedent of an anaphoric candidate the closest preceding mention that is classified as its bridging antecedent. Motivated in part by the same-head bridging resolution rules developed by Rösiger (2018), we divide the same-head bridging links into four groups based on whether the anaphor, mi , is singular or plural and whether the candidate antecedent, mj , is singular or plural, and"
2021.codi-sharedtask.2,2021.naacl-main.131,1,0.718755,"system needs to first identify all of the entity mentions that likely correspond to anaphors and antecedents, then perform bridging resolution on the predicted mentions; and (2) the Gold phase, which is essentially the same as the Predicted phase except that bridging resolution is performed on the given gold mentions. Below we describe our approach and our official test results. 4.1 Approach We employ a multi-pass sieve approach to bridging resolution. Our decision to employ a sieve-based approach is motivated in part by its successful application to bridging resolution in our previous work (Kobayashi and Ng, 2021), where we achieved state-of-the-art results by applying a rule-based 24 perform mention extraction. 4.1.1 ing precision. In contrast, a lower dummy score makes it less likely for the anaphoric candidate to be resolved to the dummy antecedent, thereby potentially reducing precision and improving recall. Sieves We employ five learning-based sieves, including one neural sieve and four &quot;same head&quot; sieves. 4.1.1.2 Same-Head Sieves 4.1.1.1 Neural Sieve Next, we design four sieves, all of which focus on establishing bridging links between two mentions that have the same head lemma. We therefore refe"
2021.codi-sharedtask.2,W11-1902,0,0.0547356,"can be said for &quot;which&quot;. However, the resolution results of &quot;this&quot; and &quot;it&quot; are comparatively less consistent across datasets. For example, the resolution of &quot;it&quot; appears to be much better on LIGHT and Persuasion than on AMI and Switchboard. 4 sieve followed by a learning-based sieve. The multi-pass sieve approach to entity coreference resolution, which was originally proposed by members of the Stanford NLP Group (Raghunathan et al., 2010), received a lot of attention in the coreference research community after their team won the CoNLL 2011 shared task on Unrestricted Coreference Resolution (Lee et al., 2011). Briefly, a sieve is composed of one or more heuristic rules. When applied to entity coreference resolution, each rule extracts a coreference relation between two mentions based on one or more conditions. For example, one rule in Stanford’s discourse processing sieve posits two mentions as coreferent if they are both pronouns and are produced by the same speaker. Sieves are ordered by their precision, with the most precise sieve appearing first. To resolve a set of mentions in a document, the resolver makes multiple passes over them: in the i-th pass, it attempts to use only the rules in the"
2021.codi-sharedtask.2,D10-1048,0,0.0548848,"uent anaphors in all test sets, followed by &quot;this&quot; and &quot;which&quot;. As for resolution of the most frequent anaphors, the results for &quot;that&quot; are consistently among the best, and the same can be said for &quot;which&quot;. However, the resolution results of &quot;this&quot; and &quot;it&quot; are comparatively less consistent across datasets. For example, the resolution of &quot;it&quot; appears to be much better on LIGHT and Persuasion than on AMI and Switchboard. 4 sieve followed by a learning-based sieve. The multi-pass sieve approach to entity coreference resolution, which was originally proposed by members of the Stanford NLP Group (Raghunathan et al., 2010), received a lot of attention in the coreference research community after their team won the CoNLL 2011 shared task on Unrestricted Coreference Resolution (Lee et al., 2011). Briefly, a sieve is composed of one or more heuristic rules. When applied to entity coreference resolution, each rule extracts a coreference relation between two mentions based on one or more conditions. For example, one rule in Stanford’s discourse processing sieve posits two mentions as coreferent if they are both pronouns and are produced by the same speaker. Sieves are ordered by their precision, with the most precise"
2021.codi-sharedtask.2,W18-0703,0,0.0278481,"lenging to design than the latter, especially in the Predicted setting where no gold mentions are given. Each sieve operates by combining a mentionpair model (Soon et al., 2001; Ng and Cardie, 2002), which in our case is a binary classifier that determines whether two mentions having the same head are involved in a bridging relation, with a closest-first single-link clustering algorithm, which selects as the antecedent of an anaphoric candidate the closest preceding mention that is classified as its bridging antecedent. Motivated in part by the same-head bridging resolution rules developed by Rösiger (2018), we divide the same-head bridging links into four groups based on whether the anaphor, mi , is singular or plural and whether the candidate antecedent, mj , is singular or plural, and create one sieve for each group of bridging links. More specifically, the four sieves are: singularsingular (both mentions are singular), singularplural (mi is singular and mj is plural), pluralsingular (mi is plural and mj is singular), and plural-plural (both mentions are plural). To train the mention-pair models, we use the SVM learner implemented in the SVMlight software package (Joachims, 1998). The trainin"
2021.codi-sharedtask.2,D17-1018,0,0.0455729,"namely entity coreference resolution, bridging resolution, and discourse deixis/abstract anaphora resolution. While the CRAC 2018 shared task (Poesio et al., 2018) provides the same three tracks, the two shared tasks differ by the genre they focus on: CRAC 2018 focuses primarily on text, whereas CODI-CRAC 2021 focuses exclusively on spoken dialogue. Not only has entity coreference resolution been an active area of research in the NLP community in the past few decades, but recent years have seen considerable progress on entity coreference because of the development of span-based neural models (Lee et al., 2017, 2018). Compared to entity coreference, bridging resolution and discourse deixis resolution are much less studied, and hence they are arguably the more interesting tracks of this shared task. In particular, a relevant question is: can the successes of span-based models be extended from entity coreference to bridging resolution and discourse deixis resolution? We participated in all three tracks of the shared task. For bridging and discourse deixis resolution, we submitted results based on both predicted mentions and gold mentions. Given the recent successes of span-based neural entity corefer"
2021.codi-sharedtask.2,J01-4004,0,0.618817,"g recall. Sieves We employ five learning-based sieves, including one neural sieve and four &quot;same head&quot; sieves. 4.1.1.2 Same-Head Sieves 4.1.1.1 Neural Sieve Next, we design four sieves, all of which focus on establishing bridging links between two mentions that have the same head lemma. We therefore refer to them as same-head sieves. We focus on samehead sieves as opposed to different-head sieves because the former are arguably less challenging to design than the latter, especially in the Predicted setting where no gold mentions are given. Each sieve operates by combining a mentionpair model (Soon et al., 2001; Ng and Cardie, 2002), which in our case is a binary classifier that determines whether two mentions having the same head are involved in a bridging relation, with a closest-first single-link clustering algorithm, which selects as the antecedent of an anaphoric candidate the closest preceding mention that is classified as its bridging antecedent. Motivated in part by the same-head bridging resolution rules developed by Rösiger (2018), we divide the same-head bridging links into four groups based on whether the anaphor, mi , is singular or plural and whether the candidate antecedent, mj , is s"
2021.codi-sharedtask.2,N18-2108,0,0.105004,"Missing"
2021.codi-sharedtask.2,2020.emnlp-main.686,0,0.79042,"sm (y) + sc (x, y) sm (x) = FFNNm (gx ) sc (x, y) = FFNNc (gx , gy , φ(x, y)) where gx and gy denote the span embeddings of x and y, FFNN(·) denotes a feedforward neural network, and φ(x, y) encodes the speaker information from the metadata as well as the segment distance between the two spans.1 Xu and Choi describe several higher-order inference (HOI) approaches that can be added to the basic end-to-end coreference model. We do not employ any HOI approaches because (1) Xu and Choi found that when using SpanBERT as the encoder, the impact of HOI is negative to marginal; Baseline: An Overview Xu and Choi (2020) reimplement the end-to-end coreference model introduced by Lee et al. (2018). For each mention span x, the model learns a distribution P (y) over possible antecedents y ∈ Y(x): es(x,y) s(x,y 0 ) y 0 ∈Y(x) e P (y) = P where s(x, y) is a pairwise score that incorporates three factors: (1) sm (x), a score that indicates how likely span x is a mention; (2) sm (y), a score that indicates how likely span y is a mention; and (3) 1 Each document is split into independent segments with a maximum size of 512 tokens. 17 (2) in preliminary experiments, we found that better results could be achieved witho"
2021.codi-sharedtask.2,2021.codi-sharedtask.8,1,0.854388,"Missing"
2021.codi-sharedtask.2,2020.coling-main.315,0,0.0849369,"ftware package (Joachims, 1998). The training instances for a sieve include those that (1) correspond to same-head pairs and (2) match the singularity/plurality conditions on the anaphor and the candidate antecedents for the sieve under consideration. For instance, the training instances for the singularsingular sieve include only those training instances for which the mentions have the same head and are both singular. Positive training instances are created by pairing each anaphoric candidate with a gold antecedent or preceding mentions that are coreferent with the gold antecedent, following Yu and Poesio (2020). Negative training instances are created by pairing anaphoric mentions with preceding mentions that are not correct antecedents or by pairing non-anaphoric mentions with precedOur neural sieve uses Yu and Poesio’s (2020) multitask learning (MTL) based neural bridging resolver, which has achieved state-of-the-art results on standard evaluation corpora for bridging resolution.5 Yu and Poesio presented two extensions to Kantor and Globerson’s (2019) span-based neural entity coreference model. First, they provided gold mentions as input to the model, meaning that the model needs to learn the span"
2021.codi-sharedtask.2,2020.emnlp-main.536,1,0.504025,"minary experiments, we found that better results could be achieved without HOI. 2.2 Group 1 2 3 4 5 6 7 8 Approaches Next, we describe two approaches to entity coreference resolution. 2.2.1 End-to-End Approach We extend the aforementioned coref-hoi model with the following modifications. Sentence distance We hypothesize that recency plays a role in resolution, so we add the sentence distance between two spans into φ(x, y) as another feature. Type prediction Since the official scorer penalizes a mention e in the system output if e is not a referring entity mention, we follow our previous work (Lu and Ng, 2020) and extend the model so that it can predict the type of each span, where the type can be N ULL (for non-entity spans), R E FERRING (for referring entity mentions), or N ON REFERRING (for non-referring entity mentions), and subsequently remove from the output any spans that are predicted to be N ULL or N ON - REFERRING. Type prediction proceeds as follows. For each span x, we pass its representation gx to a FFNN, which outputs a vector otx of dimension 3. Each element otx (t) of otx denotes the likelihood that span x belongs to type t. The span type tx is then determined by the type with the h"
2021.codi-sharedtask.8,2021.codi-sharedtask.3,0,0.130531,"r at least two reasons. First, for entity coreference resolution and discourse deixis resolution, the latter of which is treated as a general case of event coreference, 2 Entity Coreference Resolution In this section, we analyze the results of the four teams that participated in the anaphora resolution track and submitted a shared task paper, namely the team from Emory University (Xu and Choi, 2021) (henceforth Emory), the team from the University of Texas at Dallas (Kobayashi et al., 2021) (henceforth UTD), the team from Korea University (Kim et al., 2021) (henceforth KU), and the DFKI team (Anikina et al., 2021) (henceforth DFKI). ∗ *Equal contribution 71 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 71–95 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics LIGHT Emory UTD KU DFKI AMI Persuasion Switchboard P R F P R F P R F P R F 89.2 92.3 85.6 84.8 92.5 91.6 92.8 82.6 90.8 92.0 89.1 83.7 82.2 86.6 79.4 75.4 90.2 78.6 89.3 65.8 86.0 82.4 84.0 70.3 90.6 91.3 83.3 79.8 90.7 89.7 92.5 77.5 90.6 90.5 87.7 78.6 85.3 89.2 78.7 79.3 89.8 86.1 89.8 77.9 87.5 87.6 83.8 78.6 Table 1: Entity coreferenc"
2021.codi-sharedtask.8,N18-2001,0,0.0217803,"Missing"
2021.codi-sharedtask.8,2020.acl-main.132,0,0.0386992,"Missing"
2021.codi-sharedtask.8,2021.codi-sharedtask.1,1,0.710697,"describe our cross-team analysis for the three tracks, namely entity coreference (Section 2), bridging (Section 3), and discourse deixis (Section 4). We present our conclusions and observations in Section 5. The CODI-CRAC 2021 shared task is the first shared task that focuses exclusively on anaphora resolution in dialogue and provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis resolution. We perform a cross-task analysis of the systems that participated in the shared task in each of these tracks. 1 Introduction The CODI-CRAC 2021 shared task (Khosla et al., 2021), which focuses on anaphora resolution in dialogue, provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis/abstract anaphora resolution. Among these three tracks, bridging resolution and discourse deixis resolution are relatively under-studied problems. This is particularly so in the context of dialogue processing. This shared task is therefore of potential interest to researchers in the discourse and dialogue communities, particularly researchers in anaphora resolution who intend to work on problems beyond identity coreference. Our goal in this"
2021.codi-sharedtask.8,2021.codi-sharedtask.4,0,0.0494097,"Missing"
2021.codi-sharedtask.8,2021.codi-sharedtask.2,1,0.854388,"Missing"
2021.codi-sharedtask.8,2020.coling-main.331,1,0.894956,"Missing"
2021.codi-sharedtask.8,W11-1901,0,0.106608,"Missing"
2021.codi-sharedtask.8,D13-1027,0,0.0145781,"arly researchers in anaphora resolution who intend to work on problems beyond identity coreference. Our goal in this paper is to perform a cross-team analysis of the systems participating in the three tracks of the shared task. Our analysis is partly quantitative, where we attempt to draw conclusions based on statistics computed using the outputs of the systems, and partly qualitative, where we discuss the strengths and weaknesses of the systems based on our manual inspection of these outputs. While several attempts have been made to perform an analysis of different coreference systems (e.g., Kummerfeld and Klein (2013), Lu and Ng (2020)), we note that conducting an insightful analysis of these systems is inherently challenging for at least two reasons. First, for entity coreference resolution and discourse deixis resolution, the latter of which is treated as a general case of event coreference, 2 Entity Coreference Resolution In this section, we analyze the results of the four teams that participated in the anaphora resolution track and submitted a shared task paper, namely the team from Emory University (Xu and Choi, 2021) (henceforth Emory), the team from the University of Texas at Dallas (Kobayashi et al"
2021.codi-sharedtask.8,2021.codi-sharedtask.5,0,0.0127684,"two phases: (1) the Predicted phase, where a system needs to first identify all of the entity mentions that likely correspond to anaphors and antecedents, then perform bridging resolution on the predicted mentions; and (2) the Gold phase, which is essentially the same as the Predicted phase except that bridging resolution is performed on the given gold mentions. In this section, we analyze the performance of the teams that participated in the bridging resolution track. The UTD team (Kobayashi et al., 2021) and the KU team (Kim et al., 2021) participated in both phases, whereas the INRIA team (Renner et al., 2021) only participated in the Gold phase. In other words, two teams participated in the Predicted phase, and three teams participated in the Gold phase. We will use their team name to refer to the bridging resolution systems they developed. To make it clear which phase a system was developed for, we will augment the team name with a superscript that encodes the phase. For instance, we will use UTDP and UTDG to refer to the systems the UTD team developed for the Predicted phase and the Gold phase respectively. 81 LIGHT P R AMI F P Persuasion R F Switchboard P R F P R F P Recognition Resolution Reco"
2021.codi-sharedtask.8,D17-1018,0,0.0722775,"Missing"
2021.codi-sharedtask.8,M95-1005,0,0.771441,"Missing"
2021.codi-sharedtask.8,N18-2108,0,0.0685584,"Missing"
2021.codi-sharedtask.8,2020.emnlp-main.686,0,0.578916,"Missing"
2021.codi-sharedtask.8,2020.emnlp-main.536,1,0.50499,"resolution who intend to work on problems beyond identity coreference. Our goal in this paper is to perform a cross-team analysis of the systems participating in the three tracks of the shared task. Our analysis is partly quantitative, where we attempt to draw conclusions based on statistics computed using the outputs of the systems, and partly qualitative, where we discuss the strengths and weaknesses of the systems based on our manual inspection of these outputs. While several attempts have been made to perform an analysis of different coreference systems (e.g., Kummerfeld and Klein (2013), Lu and Ng (2020)), we note that conducting an insightful analysis of these systems is inherently challenging for at least two reasons. First, for entity coreference resolution and discourse deixis resolution, the latter of which is treated as a general case of event coreference, 2 Entity Coreference Resolution In this section, we analyze the results of the four teams that participated in the anaphora resolution track and submitted a shared task paper, namely the team from Emory University (Xu and Choi, 2021) (henceforth Emory), the team from the University of Texas at Dallas (Kobayashi et al., 2021) (hencefor"
2021.codi-sharedtask.8,2021.codi-sharedtask.6,0,0.0206966,"ve been made to perform an analysis of different coreference systems (e.g., Kummerfeld and Klein (2013), Lu and Ng (2020)), we note that conducting an insightful analysis of these systems is inherently challenging for at least two reasons. First, for entity coreference resolution and discourse deixis resolution, the latter of which is treated as a general case of event coreference, 2 Entity Coreference Resolution In this section, we analyze the results of the four teams that participated in the anaphora resolution track and submitted a shared task paper, namely the team from Emory University (Xu and Choi, 2021) (henceforth Emory), the team from the University of Texas at Dallas (Kobayashi et al., 2021) (henceforth UTD), the team from Korea University (Kim et al., 2021) (henceforth KU), and the DFKI team (Anikina et al., 2021) (henceforth DFKI). ∗ *Equal contribution 71 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 71–95 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics LIGHT Emory UTD KU DFKI AMI Persuasion Switchboard P R F P R F P R F P R F 89.2 92.3 85.6 84.8 92.5 91.6 92.8 82.6 90.8 92"
2021.codi-sharedtask.8,H05-1004,0,0.159285,"Missing"
2021.codi-sharedtask.8,P16-1060,0,0.040375,"Missing"
2021.codi-sharedtask.8,2020.crac-1.7,0,0.018526,"Missing"
2021.codi-sharedtask.8,W12-4501,0,0.046776,"ests provided by the organizers on their systems so that additional insights into their behavior can be gained. For instance, since mention extraction performance has a significant correlation with resolution performance, we will not be able to quantify its impact on resolution performance or directly compare different models in terms of their resolution performance that is not affected by their mention detection performance unless we provide a system with gold mentions. Hence, a useful diagnostic test, which was employed in the CoNLL-2012 shared task on Unrestricted Multilingual Coreference (Pradhan et al., 2012), involves running the systems on the test data when gold mentions are given. Another useful diagnostic test, which involves running the bridging resolvers and the discourse deixis resolvers on the test data when gold anaphors are given, would allow us to directly compare the resolvers in terms of their antecedent selection performance. While this shared task has a Gold phase for the bridging track and the discourse deixis track in which gold mentions are given, these gold mentions are somewhat different from what one would expect. Specifically, while the participants expected to be given task"
2021.emnlp-main.103,K16-1024,0,0.0195068,"coreference resolvers were developed in the tract from a document the event triggers, each pre-neural NLP era, focusing primarily on feature of which is a word/phrase that expresses the ocengineering (Ahn, 2006; Chen et al., 2009; Chen and Ji, 2009; McConky et al., 2012; Araki et al., currence of an event, and (2) assign an event subtype to each trigger that is chosen from a 2014; Bejan and Harabagiu, 2010, 2014; Chen and Ng, 2014, 2015, 2016; Cybulska and Vossen, corpus-specific subtype inventory. In our example, ev1, ev2, and ev3 are triggered by &quot;hire&quot;, &quot;start&quot;, 2015a,b; Yang et al., 2015; Krause et al., 2016; Lu and &quot;hired&quot; respectively with subtype P ERSON et al., 2016) and adapting the models originally developed for entity coreference to event corefer- NEL _S TART P OSITION. Two mentions cannot be coreferent unless they have the same subtype. ence (Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016, 2017, 2020). Neural event coreference 1 For an empirical analysis of non-neural event coreference models (Choubey and Huang, 2017, 2018, 2021; models, we refer the reader to Chen and Ng (2013). 1368 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1368"
2021.emnlp-main.103,D17-1018,0,0.0297391,"ussion, we present an empirical analysis of our state-of-the-art spanbased event coreference resolver (Lu and Ng, 2021) with the goal of gaining insights into its behavior. We believe that our analysis will not only provide the general NLP audience with a better understanding of the strengths and weaknesses of span-based event coreference models, but also provide coreference researchers with directions for future work.1 Recent years have seen the successful application of span-based neural models to key entity-based information extraction (IE) tasks such as entity coreference resolution (CR) (Lee et al., 2017, 2018) and relation extraction (Luan et al., 2019). Unlike many non-span-based neural models, which typi- 2 Tasks and Definitions cally learn task-specific contextualized word repreIn this section, we define the six tasks to be learned sentations (Peters et al., 2018), span-based models by our span-based event coreference model. are designed to learn task-specific representations The event coreference task involves identifying of text spans. This potentially allows span-based the event mentions in a document that refer to the models to create better representations of the ensame real-world ev"
2021.emnlp-main.103,N18-2108,0,0.052809,"Missing"
2021.emnlp-main.103,liu-etal-2014-supervised,0,0.180262,"12; Araki et al., currence of an event, and (2) assign an event subtype to each trigger that is chosen from a 2014; Bejan and Harabagiu, 2010, 2014; Chen and Ng, 2014, 2015, 2016; Cybulska and Vossen, corpus-specific subtype inventory. In our example, ev1, ev2, and ev3 are triggered by &quot;hire&quot;, &quot;start&quot;, 2015a,b; Yang et al., 2015; Krause et al., 2016; Lu and &quot;hired&quot; respectively with subtype P ERSON et al., 2016) and adapting the models originally developed for entity coreference to event corefer- NEL _S TART P OSITION. Two mentions cannot be coreferent unless they have the same subtype. ence (Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016, 2017, 2020). Neural event coreference 1 For an empirical analysis of non-neural event coreference models (Choubey and Huang, 2017, 2018, 2021; models, we refer the reader to Chen and Ng (2013). 1368 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1368–1380 c November 7–11, 2021. 2021 Association for Computational Linguistics {Apple}en1 said {Tuesday}en2 that {it}en3 will {hire}ev1 {Angela Ahrendts, the chief executive of Burberry}en4 , as a member of {its}en5 executive team. {She}en6 will {start}ev2 working for"
2021.emnlp-main.103,L16-1631,1,0.831557,"t, and (2) assign an event subtype to each trigger that is chosen from a 2014; Bejan and Harabagiu, 2010, 2014; Chen and Ng, 2014, 2015, 2016; Cybulska and Vossen, corpus-specific subtype inventory. In our example, ev1, ev2, and ev3 are triggered by &quot;hire&quot;, &quot;start&quot;, 2015a,b; Yang et al., 2015; Krause et al., 2016; Lu and &quot;hired&quot; respectively with subtype P ERSON et al., 2016) and adapting the models originally developed for entity coreference to event corefer- NEL _S TART P OSITION. Two mentions cannot be coreferent unless they have the same subtype. ence (Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016, 2017, 2020). Neural event coreference 1 For an empirical analysis of non-neural event coreference models (Choubey and Huang, 2017, 2018, 2021; models, we refer the reader to Chen and Ng (2013). 1368 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1368–1380 c November 7–11, 2021. 2021 Association for Computational Linguistics {Apple}en1 said {Tuesday}en2 that {it}en3 will {hire}ev1 {Angela Ahrendts, the chief executive of Burberry}en4 , as a member of {its}en5 executive team. {She}en6 will {start}ev2 working for {Apple}en7 in the {spring}en8 . In"
2021.emnlp-main.103,P17-1009,1,0.896925,"Missing"
2021.emnlp-main.103,2020.aacl-main.66,1,0.865488,"Missing"
2021.emnlp-main.103,M95-1005,0,0.817859,"the easier it is to determine whether a mention is anaphoric. We compute it using both gold and candidate mentions. 3 The event subtypes, argument roles, and event attributes defined for ACE 2005 and KBP 2017 can be found in LDC (2005) and LDC (2016), respectively. 1369 Figure 1: Model structure. 3.2 Evaluation Metrics Results of event coreference are obtained using version 1.8 of the official scorer provided by the KBP 2017 shared task organizers. This scorer reports results in terms of AVG-F, which is the unweighted average of the F-scores of four coreference evaluation metrics, namely MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011). TD results are expressed in terms of F-score, where a trigger is considered correctly detected if it has an exact match with a gold trigger in terms of boundary and event subtype. 3.3 Model ity Prediction Layer to predict its anaphoricity value (i.e., A NAPHORIC or N ON -A NAPHORIC); (3) the Attribute Prediction Layer, which contains a network for predicting the value of each event attribute; (4) the Coreference Prediction Layer, which uses a ranker to link a span to its highestranked candidate antecedent (o"
2021.emnlp-main.103,D19-1585,0,0.0223438,"2 ACE 2005 defines 33 event 2 We excluded ECB+, an extensively used dataset, because of criticisms for its incomplete within-document event coreference annotation (Liu et al., 2014; Choubey and Huang, 2018). subtypes, 30 argument roles, and four event attributes, whereas KBP 2017 defines 18 subtypes, 20 argument roles, and one attribute. For ACE 2005, while the official training set is available, the official test set is not. As a result, previous work defined different train-test partitions over the official training set when evaluating on ACE 2005. We employ the same train-test partition as Wadden et al. (2019). For the experiments involving KBP, we use five corpora (LDC2015E29, E68, E73, E94, and LDC2016E64) as our training set. Among them, we reserve 82 documents for parameter tuning. For evaluation, we use the official KBP 2017 test set. Statistics on ACE 2005 and KBP 2017 are shown in Table 2.3 In addition to general statistics such as the number of event coreference chains, we compute several statistics that aim to better gauge the difficulty of these datasets. The first one is the average entropy of the subtype distribution of an event mention. This statistic could shed light on the difficulty"
2021.findings-emnlp.129,D15-1075,0,0.0113979,"to this two ads belong to the same product category. Beproblem should involve matching the texts and the low we will discuss related work on text matching, images in the two ads. While algorithms for text image matching, and text-image matching, even matching (Yang et al., 2019; Gong et al., 2018; though none of the existing matching algorithms Wang et al., 2017b), image matching (Schroff et al., are specifically developed for ad matching. 2015; Novotný et al., 2017), and text-image matchMany text matching methods use an encoder ing (Zheng et al., 2020; Wang et al., 2019) exist, such as RNNs (Bowman et al., 2015), CNNs (Tan none of them was developed specifically for ads. et al., 2016), recursive networks (Tai et al., 2015) We therefore propose ION, a bimodal method that and Transformer-based networks (Vaswani et al., determines how likely two ads belong to the same 2017; Devlin et al., 2019) to embed input texts into product category, the key highlights of which invectors, possibly enhanced by attention (Parikh clude the design of (1) a semantics-enhanced image et al., 2016; Chen et al., 2017), and then build a region extraction mechanism for identifying the rebinary classifier to determine whether t"
2021.findings-emnlp.129,P17-1152,0,0.0134407,"Many text matching methods use an encoder ing (Zheng et al., 2020; Wang et al., 2019) exist, such as RNNs (Bowman et al., 2015), CNNs (Tan none of them was developed specifically for ads. et al., 2016), recursive networks (Tai et al., 2015) We therefore propose ION, a bimodal method that and Transformer-based networks (Vaswani et al., determines how likely two ads belong to the same 2017; Devlin et al., 2019) to embed input texts into product category, the key highlights of which invectors, possibly enhanced by attention (Parikh clude the design of (1) a semantics-enhanced image et al., 2016; Chen et al., 2017), and then build a region extraction mechanism for identifying the rebinary classifier to determine whether the inputs gion(s) of the image in an ad that is most relevant are similar. An exception is Yang et al. (2019), to the text, and (2) a dual-path fusion attention whose matching method is based on rich alignment method for fusing the information extracted from features. In general, however, the text in ads are the two modalities. often so ambiguous that it is difficult to determine In sum, our contributions in this paper are three which products are promoted. fold. First, we hypothesize t"
2021.findings-emnlp.129,N19-1423,0,0.0672375,"et al., 2018; though none of the existing matching algorithms Wang et al., 2017b), image matching (Schroff et al., are specifically developed for ad matching. 2015; Novotný et al., 2017), and text-image matchMany text matching methods use an encoder ing (Zheng et al., 2020; Wang et al., 2019) exist, such as RNNs (Bowman et al., 2015), CNNs (Tan none of them was developed specifically for ads. et al., 2016), recursive networks (Tai et al., 2015) We therefore propose ION, a bimodal method that and Transformer-based networks (Vaswani et al., determines how likely two ads belong to the same 2017; Devlin et al., 2019) to embed input texts into product category, the key highlights of which invectors, possibly enhanced by attention (Parikh clude the design of (1) a semantics-enhanced image et al., 2016; Chen et al., 2017), and then build a region extraction mechanism for identifying the rebinary classifier to determine whether the inputs gion(s) of the image in an ad that is most relevant are similar. An exception is Yang et al. (2019), to the text, and (2) a dual-path fusion attention whose matching method is based on rich alignment method for fusing the information extracted from features. In general, howe"
2021.findings-emnlp.129,P19-1500,0,0.0245647,"le that reranks the generated image regions extracted by the YOLOv3 object detection module (Redmon and Farhadi, 2018) (Section 3.3). Finally, the model combines the re-ranked image regions and the distributed text representation through a Dual-path Fusion Attention (DFA) layer to obtain a multimodal representation of the ad (Section 3.4). Below we introduce each of these modules in detail. 3.1 Sentence Representation Learning We encode each word in the text portion of the input ad using Transformer (Vaswani et al., 2017), as it has been shown effective in many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019). Given the text, we encode its word sequence and obtain its representations H = {h1 , h2 , . . . hn }, where n is the number of words and hi ∈ Rdmodel . 3.2 Keywords-Guided Selective Encoding Some words in the text portion of an ad contain information that can help us to determine which products are promoted by the ads, and thus are more useful than those words that do not. As an example, the ad shown in Figure 2 contains the text “Spark Wrist with Brand XXX1 , Treasure Your Love Forever”. Here, the words “Spark” and “Wrist” strongly suggest that it may be an ad of something that is sparkling"
2021.findings-emnlp.129,D16-1244,0,0.0779153,"Missing"
2021.findings-emnlp.129,P15-1150,0,0.0251913,"Missing"
2021.findings-emnlp.129,2020.acl-main.349,0,0.0319556,"antic Clue Attention … YOLOv3 Transformer Encoder Spark Your Wrist with Brand *, Treasure Your Love Forever Image Text Figure 2: The framework of the proposed model. and external knowledge (Shi et al., 2019; Wang et al., 2019). Different from work on cross-modal matching, which measures the similarity between different modalities, our work focuses on fusing features from the texts and the image in an ad to create a multimodal representation. Note that there is also related work that aims to generate multimodal vectors containing both text and image features for pre-training or classification (Xu et al., 2020; Abavisani et al., 2020; Lu et al., 2019), in which vectors from different modalities are concatenated to form the multimodal representation. Rather than performing a simple concatenation, our work proposes an attention mechanism to fuse modalities in order to better identify the correspondence between words and image regions. In addition, while existing methods do not determine which words and image regions in an ad are relevant to the product under consideration and which ones are irrelevant/noisy, our method encodes words and extract image regions selectively so that those that are related"
2021.findings-emnlp.129,P19-1465,0,0.0309516,"h to our knowlto determine how likely two ads are for the same edge is an unexplored area of research. product category? Since ads are displayed in the A crucial aspect of our work concerns the deform of creatives that are typically composed of velopment of a method for determining how likely both texts and images, a reasonable solution to this two ads belong to the same product category. Beproblem should involve matching the texts and the low we will discuss related work on text matching, images in the two ads. While algorithms for text image matching, and text-image matching, even matching (Yang et al., 2019; Gong et al., 2018; though none of the existing matching algorithms Wang et al., 2017b), image matching (Schroff et al., are specifically developed for ad matching. 2015; Novotný et al., 2017), and text-image matchMany text matching methods use an encoder ing (Zheng et al., 2020; Wang et al., 2019) exist, such as RNNs (Bowman et al., 2015), CNNs (Tan none of them was developed specifically for ads. et al., 2016), recursive networks (Tai et al., 2015) We therefore propose ION, a bimodal method that and Transformer-based networks (Vaswani et al., determines how likely two ads belong to the same"
2021.naacl-main.356,araki-etal-2014-detecting,0,0.0272887,"We propose a neural event coreference model error propagation problem, knowledge provided by in which event coreference is jointly trained other information extraction (IE) components (e.g., with five tasks: trigger detection, entity corefentity coreference, event arguments) is typically erence, anaphoricity determination, realis deemployed as features for training event coreference tection, and argument extraction. To guide the models (Chen et al., 2009; McConky et al., 2012; learning of this complex model, we incorporate cross-task consistency constraints into the Cybulska and Vossen, 2013; Araki et al., 2014; Liu learning process as soft constraints via designet al., 2014; Peng et al., 2016; Krause et al., 2016; ing penalty functions. In addition, we propose Choubey and Huang, 2017). Oftentimes, these feathe novel idea of viewing entity coreference tures provide limited improvements to event corefand event coreference as a single coreference erence models as they are too noisy to be useful. task, which we believe is a step towards a uniThough less popular than pipelined approaches, fied model of coreference resolution. The resulting model achieves state-of-the-art results bootstrapping approaches"
2021.naacl-main.356,D15-1247,0,0.0200122,"-coreferent. However, identifying potential arguments (which is performed by an en- process cannot be undone in later iterations. tity extraction system), linking arguments to their Joint learning approaches have recently emerged event mentions (which is also performed by an as promising approaches to event coreference owevent extraction system), and determining whether ing to their ability to address error propagation. two event arguments are coreferent (which is the In these approaches, two or more tasks are jointly job of an entity coreference resolver), are all non- trained. For instance, Araki and Mitamura (2015) trivial tasks. Hence, a key challenge in designing learn a joint model for trigger detection and event an event coreference resolver involves determining coreference using a structured perceptron, and Lu how to integrate these noisy components. and Ng (2017) learn a joint model for trigger deOne of the most common approaches to event tection, event coreference, and anaphoricity detercoreference resolution is pipelined approaches, mination using a structured conditional random where a trigger detection component, which iden- field. The key advantage of these models is that the tifies triggers"
2021.naacl-main.356,2020.tacl-1.5,0,0.0166081,"emaining ones all have subtype C ONFLICT _ATTACK. Among the eight entity mentions (en1, . . . , en8), en3 has FACILITY as its type and the remaining ones are all PERSONs. en1 and en2 are the arguments of ev1 filling the roles of ATTACKER and TARGET respectively, whereas en4 is the argument of ev3 having the role ATTACKER. There are two entity coreference chains (one composed of en1 and en4 and the other en5 and en7) and one event coreference chain (ev1 and ev3). span-based models have been successfully applied to a variety of entity-based IE tasks such as entity coreference (Lee et al., 2017; Joshi et al., 2020) and relation extraction (Luan et al., 2019), they have not been applied to event coreference. More formally, our model takes as input a document D represented as a sequence of word tokens, from which we extract all possible intra-sentence spans of up to length L. It simultaneously learns six tasks, which we define below. The trigger detection task aims to assign each span i a subtype label yi . Each yi takes a value in a subtype inventory or N ONE, which indicates that i is not a trigger. The model predicts i’s subtype to be yi∗ = arg maxyt st (i, yt ), where st is a scoring function suggesti"
2021.naacl-main.356,D19-1588,0,0.0114378,"46.9 55.3 56.1 58.7 Anaphoricity F 56.2 51.1 54.3 62.4 63.8 64.5 P − − − − 43.0 50.4 R − − − − 44.5 45.3 F − − − − 43.8 47.7 Realis P 48.0 − − − 70.0 63.7 R 46.9 − − − 53.1 52.0 Argument F 47.4 − − − 60.3 57.3 P − − − − 36.9 32.4 R − − − − 29.9 24.5 F − − − − 33.0 27.9 Entity Coref. CoNLL − − − − 72.6 68.7 Table 2: Results of different resolvers on event coreference and related tasks. Results in rows 1-3 are copied verbatim from the original papers; − indicates the corresponding result is not available. For training, we use document sized minibatches and apply a dropout rate of 0.3. Following Joshi et al. (2019), we use different learning rates for training the task parameters and the SpanBERT parameters. Specifically, the task learning rate is 1 × 10−5 and is decayed linearly, whereas the learning rate for SpanBERT is 2 × 10−4 and is decayed linearly. The hyperparameters in the loss function, λc , λt , λa , λr , and λo , are 1, 1, 0.05, 0.5, and 0.05. 5.2 Results and Discussion barely changes. Since MUC only rewards successful identification of coreference links, the fact that the MUC score is more or less unchanged implies that the improvement does not arise from link identification; rather, the fa"
2021.naacl-main.356,K16-1024,0,0.0173892,"ent coreference is jointly trained other information extraction (IE) components (e.g., with five tasks: trigger detection, entity corefentity coreference, event arguments) is typically erence, anaphoricity determination, realis deemployed as features for training event coreference tection, and argument extraction. To guide the models (Chen et al., 2009; McConky et al., 2012; learning of this complex model, we incorporate cross-task consistency constraints into the Cybulska and Vossen, 2013; Araki et al., 2014; Liu learning process as soft constraints via designet al., 2014; Peng et al., 2016; Krause et al., 2016; ing penalty functions. In addition, we propose Choubey and Huang, 2017). Oftentimes, these feathe novel idea of viewing entity coreference tures provide limited improvements to event corefand event coreference as a single coreference erence models as they are too noisy to be useful. task, which we believe is a step towards a uniThough less popular than pipelined approaches, fied model of coreference resolution. The resulting model achieves state-of-the-art results bootstrapping approaches have been used for event on the KBP 2017 event coreference dataset. coreference resolution, where an eve"
2021.naacl-main.356,W09-4303,0,0.0488828,"n, TX 75083-0688 {ljwinnie,vince}@hlt.utdallas.edu Abstract from the trigger detection component to the event coreference component. To avoid aggravating this We propose a neural event coreference model error propagation problem, knowledge provided by in which event coreference is jointly trained other information extraction (IE) components (e.g., with five tasks: trigger detection, entity corefentity coreference, event arguments) is typically erence, anaphoricity determination, realis deemployed as features for training event coreference tection, and argument extraction. To guide the models (Chen et al., 2009; McConky et al., 2012; learning of this complex model, we incorporate cross-task consistency constraints into the Cybulska and Vossen, 2013; Araki et al., 2014; Liu learning process as soft constraints via designet al., 2014; Peng et al., 2016; Krause et al., 2016; ing penalty functions. In addition, we propose Choubey and Huang, 2017). Oftentimes, these feathe novel idea of viewing entity coreference tures provide limited improvements to event corefand event coreference as a single coreference erence models as they are too noisy to be useful. task, which we believe is a step towards a uniTho"
2021.naacl-main.356,D12-1045,0,0.0540666,"Missing"
2021.naacl-main.356,D17-1226,0,0.0358794,"Missing"
2021.naacl-main.356,P18-1045,0,0.0256555,"Missing"
2021.naacl-main.356,R13-1021,0,0.0222934,"To avoid aggravating this We propose a neural event coreference model error propagation problem, knowledge provided by in which event coreference is jointly trained other information extraction (IE) components (e.g., with five tasks: trigger detection, entity corefentity coreference, event arguments) is typically erence, anaphoricity determination, realis deemployed as features for training event coreference tection, and argument extraction. To guide the models (Chen et al., 2009; McConky et al., 2012; learning of this complex model, we incorporate cross-task consistency constraints into the Cybulska and Vossen, 2013; Araki et al., 2014; Liu learning process as soft constraints via designet al., 2014; Peng et al., 2016; Krause et al., 2016; ing penalty functions. In addition, we propose Choubey and Huang, 2017). Oftentimes, these feathe novel idea of viewing entity coreference tures provide limited improvements to event corefand event coreference as a single coreference erence models as they are too noisy to be useful. task, which we believe is a step towards a uniThough less popular than pipelined approaches, fied model of coreference resolution. The resulting model achieves state-of-the-art results boot"
2021.naacl-main.356,D13-1203,0,0.0187696,"the coreference loss function previously defined by Wiseman et al. (2015) for entity coreference resolution. Specifically, let GOLDc (i) denote the set of spans preceding span i that are coreferent with i, and ycl be arg maxy∈GOLDc (i) sc (i, y). In other words, ycl is the highest scoring (latent) antecedent of i according to sc among all the antecedents of i. The loss function for coreference is defined as: Lc (Θ) = n X i=1 max (∆c (i, j)(1+sc (i, j)−sc (i, ycl )) j∈Y(i) (15) where ∆c (i, j) is a mistake-specific cost function that returns the cost associated with a particular type of error (Durrett and Klein, 2013).1 Intuitively, the loss function penalizes a span i if the predicted antecedent j has a higher score than the correct latent antecedent ycl . We similarly define the loss for trigger detection: Lt (Θ) = Pn P i=1 ˆ l6=yt max(0, ∆t (i, ˆl)(1 + st (i, ˆl) − st (i, yt ))) (16) ˆ where ∆t (i, l) is a mistake-specific cost function that returns the cost associated with a particular type of error.1 Intuitively, the loss function penalizes each span for which each of the wrong subtypes ˆl has a higher score than the correct subtype yt according to st . The task losses for anaphoricity determination,"
2021.naacl-main.356,N19-1085,1,0.786401,"between a set of mentions without distinguishing between entity and event mentions. 2 Related Work Traditional resolvers. Many existing event coreference resolvers, including those that employ the four approaches described in the introduction, are developed in the pre-neural era. resolvers. For a detailed overview of these non-neural resolvers and the wide variety of hand-engineered features they employ, we refer the reader to Lu and Ng (2018). Neural resolvers. Of particular relevance to our work are neural event coreference models (e.g., Nguyen et al. (2016), Choubey and Huang (2017, 2018), Huang et al. (2019)). Unlike their traditional counterparts, neural coreference models can leverIn light of the above discussion, we seek to ad- age the knowledge learned from large unlabeled vance the state of the art in event coreference reso- corpora through pretrained word embeddings or lution by proposing a model that jointly learns six transfer learning. Existing neural event coreference tasks: trigger detection, event coreference, entity models are pipeline-based and seek to learn word coreference, anaphoricity determination, argument representations so that coreferent event mentions extraction, and reali"
2021.naacl-main.356,D17-1018,0,0.0216247,"its subtype, the remaining ones all have subtype C ONFLICT _ATTACK. Among the eight entity mentions (en1, . . . , en8), en3 has FACILITY as its type and the remaining ones are all PERSONs. en1 and en2 are the arguments of ev1 filling the roles of ATTACKER and TARGET respectively, whereas en4 is the argument of ev3 having the role ATTACKER. There are two entity coreference chains (one composed of en1 and en4 and the other en5 and en7) and one event coreference chain (ev1 and ev3). span-based models have been successfully applied to a variety of entity-based IE tasks such as entity coreference (Lee et al., 2017; Joshi et al., 2020) and relation extraction (Luan et al., 2019), they have not been applied to event coreference. More formally, our model takes as input a document D represented as a sequence of word tokens, from which we extract all possible intra-sentence spans of up to length L. It simultaneously learns six tasks, which we define below. The trigger detection task aims to assign each span i a subtype label yi . Each yi takes a value in a subtype inventory or N ONE, which indicates that i is not a trigger. The model predicts i’s subtype to be yi∗ = arg maxyt st (i, yt ), where st is a scor"
2021.naacl-main.356,N18-2108,0,0.0445132,"Missing"
2021.naacl-main.356,D19-1405,0,0.0281811,"te While multi-task learning in a neural network typically allows the different tasks involved to ben- event trigger and may be composed of more than efit from each other via learning shared representa- one token, so that coreferent event mentions have similar span representations. tions, we hypothesize that the model would benefit additional guidance given that the learning task, Constrained learning in neural models. Anwhich involves six tasks, is so complex. Conse- other line of related work concerns the use of conquently, we propose to guide the learning process straints in neural models (Li et al., 2019; Wang by exploiting cross-task consistency constraints. et al., 2020), where constraints are represented as As mentioned above, such consistency constraints first order logic formulas and compiled into the are typically employed in joint inference and rarely loss functions. These models are typically trained in joint learning. Moreover, unlike in joint infer- to minimize the weighted sum of task losses and ence where such constraints are typically imple- constraint losses. Rather than introduce additional mented as hard constraints, we provide flexibility terms in the loss function, we employ"
2021.naacl-main.356,liu-etal-2014-supervised,0,0.0395442,"Missing"
2021.naacl-main.356,P17-1009,1,0.856117,"Missing"
2021.naacl-main.356,2020.aacl-main.66,1,0.742391,"Missing"
2021.naacl-main.356,P15-1137,0,0.0223048,"e L(Θ), whereas the hyperpaarg maxyt st (j, yt ). Intuitively, c1 provides an es- rameters are tuned using grid search to maximize timate of the least amount of adjustment needed AVG-F (the standard event coreference evaluation to make i’s semantic type the same as j’s or the metric; see the next section) on development data. 4509 Task Losses We employ a max-margin loss for each of the six tasks. Defining the coreference loss is slightly tricky since the coreference annotations for each document are provided in the form of clusters. We adopt the coreference loss function previously defined by Wiseman et al. (2015) for entity coreference resolution. Specifically, let GOLDc (i) denote the set of spans preceding span i that are coreferent with i, and ycl be arg maxy∈GOLDc (i) sc (i, y). In other words, ycl is the highest scoring (latent) antecedent of i according to sc among all the antecedents of i. The loss function for coreference is defined as: Lc (Θ) = n X i=1 max (∆c (i, j)(1+sc (i, j)−sc (i, ycl )) j∈Y(i) (15) where ∆c (i, j) is a mistake-specific cost function that returns the cost associated with a particular type of error (Durrett and Klein, 2013).1 Intuitively, the loss function penalizes a spa"
2021.naacl-main.356,C16-1308,1,0.838736,"d involves multiple tasks, it is typically complex. In therefore not be surprising that errors propagate fact, it is by no means easy to scale such a model 4504 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4504–4514 June 6–11, 2021. ©2021 Association for Computational Linguistics to a large number of tasks because of the high computational complexity involved in learning. Joint inference approaches have also been applied to event coreference resolution. For instance, Chen and Ng (2016) and Lu et al. (2016) first train separate models for entity coreference, trigger detection, argument extraction, and event coreference, then use Integer Linear Programming or Markov Logic Networks to jointly infer the outputs of these tasks subject to (mostly) hard cross-task consistency constraints. For instance, one such hard constraint says that two coreferent event mentions should have the same event subtype. Since the models are trained independently, they cannot benefit from each other and could be noisy. Worse still, performing joint inference using hard constraints over (very) noisy outputs could do more"
2021.naacl-main.356,N19-1308,0,0.0224661,"ACK. Among the eight entity mentions (en1, . . . , en8), en3 has FACILITY as its type and the remaining ones are all PERSONs. en1 and en2 are the arguments of ev1 filling the roles of ATTACKER and TARGET respectively, whereas en4 is the argument of ev3 having the role ATTACKER. There are two entity coreference chains (one composed of en1 and en4 and the other en5 and en7) and one event coreference chain (ev1 and ev3). span-based models have been successfully applied to a variety of entity-based IE tasks such as entity coreference (Lee et al., 2017; Joshi et al., 2020) and relation extraction (Luan et al., 2019), they have not been applied to event coreference. More formally, our model takes as input a document D represented as a sequence of word tokens, from which we extract all possible intra-sentence spans of up to length L. It simultaneously learns six tasks, which we define below. The trigger detection task aims to assign each span i a subtype label yi . Each yi takes a value in a subtype inventory or N ONE, which indicates that i is not a trigger. The model predicts i’s subtype to be yi∗ = arg maxyt st (i, yt ), where st is a scoring function suggesting i’s likelihood of having yi as its subtyp"
2021.naacl-main.356,H05-1004,0,0.207099,"uted over 13146 coreference chains2 . Among these 817 documents, we reserve 82 documents for parameter tuning and use the remaining documents for model training. We report results on the official test set, which consists of 167 documents with 4375 event mentions distributed over 2963 coreference chains. 5.1.2 Results of event coreference, trigger detection and realis detection are obtained using version 1.8 of the official scorer provided by the KBP 2017 organizers. For event coreference, the scorer employs four scoring metrics, MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). Results of trigger detection and realis detection are both expressed in terms of Precision (P), Recall (R) and F-score. The scorer considers (1) a trigger correctly detected if it has an exact match with a gold trigger in terms of boundary and event subtype, and (2) a realis label correctly classified if it has an exact match with a gold trigger in terms of boundary and realis value. Additionally, we express results of both argument extraction and anaphoricity determination in terms of Precision,"
2021.naacl-main.356,D16-1038,0,0.0507112,"Missing"
2021.naacl-main.356,M95-1005,0,0.753901,"etection. 5 5.1 Evaluation with 22894 event mentions distributed over 13146 coreference chains2 . Among these 817 documents, we reserve 82 documents for parameter tuning and use the remaining documents for model training. We report results on the official test set, which consists of 167 documents with 4375 event mentions distributed over 2963 coreference chains. 5.1.2 Results of event coreference, trigger detection and realis detection are obtained using version 1.8 of the official scorer provided by the KBP 2017 organizers. For event coreference, the scorer employs four scoring metrics, MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). Results of trigger detection and realis detection are both expressed in terms of Precision (P), Recall (R) and F-score. The scorer considers (1) a trigger correctly detected if it has an exact match with a gold trigger in terms of boundary and event subtype, and (2) a realis label correctly classified if it has an exact match with a gold trigger in terms of boundary and realis value. Additionally, we express results of both argument extraction and"
2021.naacl-main.356,2020.emnlp-main.51,0,0.0741533,"Missing"
A00-1025,P98-1034,1,0.750103,"Missing"
A00-1025,J93-2004,0,0.0450067,"Missing"
A00-1025,W99-0212,0,0.0196131,"dentifying named entities. Finally, the Cymphony QA system (Srihari and Li, 2000) relies heavily on named entity identification; it also employs standard IR techniques and a shallow parser. In terms of statistical and linguistic knowledge sources employed, the primary difference between these systems and ours is our lack of an adequate named entity tagger. Incorporation of such a tagger will be a focus of future work. In addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (Buckley, 1995) and coreference resolution. Morton (1999), for example, shows that coreference resolution improves passage retrieval for their question-answering system. We also plan to reconsider paragraph-based summaries given their coverage on the test corpus. The most critical area for improvement, however, is the linguistic filters. The semantic type filter will be greatly improved by the addition of a named entity tagger, but we believe that additional gains can be attained by augmenting named entity identification with information from WordNet. Finally, we currently make no attempt to confirm any phrase relations from the query. Without this,"
A00-1025,A00-1023,0,0.0840042,"e AT&T QA system (Singhal et al., 2000), the Qanda system (Breck et al., 2000), and the SyncMatcher system (Oard et al., 2000) all employ vector-space methods from IR, named entity identifiers, and a fairly simple question type determiner. In addition, SyncMatcher uses a broad-coverage dependency parser to enforce phrase relationship constraints. Instead of the vector space model, the LASSO system (Moldovan et al., 2000) uses boolean search operators for paragraph retrieval. Recognition of answer hypotheses in their system relies on identifying named entities. Finally, the Cymphony QA system (Srihari and Li, 2000) relies heavily on named entity identification; it also employs standard IR techniques and a shallow parser. In terms of statistical and linguistic knowledge sources employed, the primary difference between these systems and ours is our lack of an adequate named entity tagger. Incorporation of such a tagger will be a focus of future work. In addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (Buckley, 1995) and coreference resolution. Morton (1999), for example, shows that coreference resolution improves passage re"
A00-1025,E99-1011,0,\N,Missing
A00-1025,C98-1034,1,\N,Missing
A00-1025,X98-1017,1,\N,Missing
C02-1139,M95-1005,0,\N,Missing
C02-1139,N01-1008,0,\N,Missing
C02-1139,C96-1021,0,\N,Missing
C02-1139,J00-4005,0,\N,Missing
C02-1139,P99-1048,0,\N,Missing
C02-1139,P98-1012,0,\N,Missing
C02-1139,C98-1012,0,\N,Missing
C02-1139,J94-4002,0,\N,Missing
C02-1139,P95-1017,0,\N,Missing
C02-1139,P02-1014,1,\N,Missing
C02-1139,J01-4004,0,\N,Missing
C10-1105,P04-1056,0,0.0182248,"put of the baseline model. The idea is to treat the output for each NP, which is a probability distribution over the semantic subtypes, as its prior label/class distribution, and convert it into a posterior label/class distribution by exploiting the available relational information as an additional piece of evidence. For this purpose, we will make use of factor graphs. In this section, we first give a brief overview of factor graphs8 , and show how they can be used to perform joint 7 We use the MaxEnt implementation available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html 8 See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. 934 inference for semantic subtype determination. 4.1 baseline classification model, each variable node is given a factor f (Xi ) = P (Xi = lq ). If no additional factors that model the relation between two nodes/instances are introduced, maximizing the objective function for this graph (by maximizing the product of factors) will find an assignment identical to the one obtained by taking the most probable semantic subtype label assigned to each instance by the baseline classifier. Next, we exploit the relationship between two r"
C10-1105,P05-1045,0,0.104882,"If NPi is governed by a verb, the following three features are derived from the governing verb. First, we employ the string of the governing verb as a feature. Second, we create a feature whose value is the semantic role of the 933 governing verb.3 Finally, to distinguish the different senses of the governing verb, we create a feature whose value is the concatenation of the verb and its WordNet sense number. 3. Semantic (5): We employ five semantic features. First, if NPi is an NE, we create a feature whose value is the NE label of NPi , as determined by the Stanford CRF-based NE recognizer (Finkel et al., 2005). However, if NPi is a nominal, we create a feature that encodes the WordNet semantic class of which it is a hyponym, using the manually determined sense of NPi .4 Moreover, to improve generalization, we employ a feature whose value is the WordNet synset number of the head noun of a nominal. If NPi has a governing verb, we also create a feature whose value is the WordNet synset number of the verb. Finally, if NPi is a nominal, we create a feature based on its WordNet equivalent concept. Specifically, for each entity type defined in ACE 20055 , we create a list containing all the word-sense pai"
C10-1105,N04-1001,0,0.027071,"Missing"
C10-1105,P06-1060,0,0.0557735,"Missing"
C10-1105,P07-1068,1,0.380253,"they are applied in isolation or in combination with the standard approach. 1 Introduction Semantic class determination refers to the task of classifying a noun phrase (NP), be it a name or a nominal, as one of a set of pre-defined semantic classes. A semantic class classifier is a basic text-processing component in many highlevel natural language processing (NLP) applications, including information-extraction (IE) systems and question-answering (QA) systems. In recent years, supervised semantic class determination has been tackled primarily in the context of (1) coreference resolution (e.g., Ng (2007), Huang et al. (2009)), where semantic classes are induced and subsequently used to disallow coreference between semantically incompatible NPs, and (2) the Over the years, NLP researchers have focused on a relatively small number of semantic classes in both NE recognition and mention detection: seven classes in the MUC-6 and MUC-7 NE recognition task, four classes in the CoNLL 2002 and 2003 NE recognition shared task, and seven classes in the ACE 2005 mention detection task. Given that one of the uses of semantic class information is to support NLP applications, it is questionable whether this"
C10-1105,W06-1633,0,0.0105306,"hecks whether NPi is in a particular gazetteer. The eight dictionaries contain pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehi3 We also employ the semantic role that is manually annotated for each NP in the WSJ corpus in OntoNotes. 4 The semantic classes we considered are person, location, organization, date, time, money, percent, and object. 5 The ACE 2005 entity types include person, organization, GPE, facility, location, weapon, and vehicle. 6 Details of how these lists are constructed can be found in Nicolae and Nicolae (2006). cle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). 7. Grammatical (2): We create a feature that encodes the part-of-speech (POS) sequence of NPi obtained via the Stanford POS tagger (Toutanova et al., 2003). In addition, we have a feature that determines whether NPi is a nominal or not. We employ maximum entropy (MaxEnt) modeling7 for training the baseline semantic subtype classifier. MaxEnt is chosen because it provides a probabilistic classification for each instance, which we will need to perform collective c"
C10-1105,N03-1033,0,0.00879306,"the WSJ corpus in OntoNotes. 4 The semantic classes we considered are person, location, organization, date, time, money, percent, and object. 5 The ACE 2005 entity types include person, organization, GPE, facility, location, weapon, and vehicle. 6 Details of how these lists are constructed can be found in Nicolae and Nicolae (2006). cle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). 7. Grammatical (2): We create a feature that encodes the part-of-speech (POS) sequence of NPi obtained via the Stanford POS tagger (Toutanova et al., 2003). In addition, we have a feature that determines whether NPi is a nominal or not. We employ maximum entropy (MaxEnt) modeling7 for training the baseline semantic subtype classifier. MaxEnt is chosen because it provides a probabilistic classification for each instance, which we will need to perform collective classification, as described in the next section. 4 Collective Classification One weakness of the baseline classification model is that it classifies each instance independently. In particular, the model cannot take into account relationships between them that may be helpful for improving"
C10-1105,H92-1045,0,0.0253072,"a probabilistic classification for each instance, which we will need to perform collective classification, as described in the next section. 4 Collective Classification One weakness of the baseline classification model is that it classifies each instance independently. In particular, the model cannot take into account relationships between them that may be helpful for improving classification accuracy. For example, if two NPs are the same string in a given document, then it is more likely than not that they have the same semantic subtype according to the “one sense per discourse” hypothesis (Gale et al., 1992). Incorporating this kind of relational information into the feature set employed by the baseline system is not an easy task, since each feature characterizes only a single NP. To make use of the relational information, one possibility is to design a new learning procedure. Here, we adopt a different approach: we perform collective classification, or joint probabilistic inference, on the output of the baseline model. The idea is to treat the output for each NP, which is a probability distribution over the semantic subtypes, as its prior label/class distribution, and convert it into a posterior"
C10-1105,N06-2015,0,0.0194896,"(0.04). Ordinal (0.6). Cardinal (5.1). War (0.03), Hurricane (0.1), Other (0.24). Plant (0.2). Animal (0.7). Food (1.1), Drug (0.46), Chemical (0.23), Other (0.9). Disease (0.6). Law (0.5). Language (0.2). Address (0.01), Phone (0.04). Game (0.1). Book (0.16), Play (0.04), Song (0.03), Painting (0.01), Other (0.4). Table 1: The 92 semantic subtypes and their corresponding supertypes. a feature whose value is the concatenation of the head of NPi and its WordNet sense number.2 2 We employ the sense number that is manually annotated for each NP in the WSJ corpus as part of the OntoNotes project (Hovy et al., 2006). 2. Verb String (3): If NPi is governed by a verb, the following three features are derived from the governing verb. First, we employ the string of the governing verb as a feature. Second, we create a feature whose value is the semantic role of the 933 governing verb.3 Finally, to distinguish the different senses of the governing verb, we create a feature whose value is the concatenation of the verb and its WordNet sense number. 3. Semantic (5): We employ five semantic features. First, if NPi is an NE, we create a feature whose value is the NE label of NPi , as determined by the Stanford CRF-"
C10-1105,D09-1128,0,0.0107744,"plied in isolation or in combination with the standard approach. 1 Introduction Semantic class determination refers to the task of classifying a noun phrase (NP), be it a name or a nominal, as one of a set of pre-defined semantic classes. A semantic class classifier is a basic text-processing component in many highlevel natural language processing (NLP) applications, including information-extraction (IE) systems and question-answering (QA) systems. In recent years, supervised semantic class determination has been tackled primarily in the context of (1) coreference resolution (e.g., Ng (2007), Huang et al. (2009)), where semantic classes are induced and subsequently used to disallow coreference between semantically incompatible NPs, and (2) the Over the years, NLP researchers have focused on a relatively small number of semantic classes in both NE recognition and mention detection: seven classes in the MUC-6 and MUC-7 NE recognition task, four classes in the CoNLL 2002 and 2003 NE recognition shared task, and seven classes in the ACE 2005 mention detection task. Given that one of the uses of semantic class information is to support NLP applications, it is questionable whether this purpose can be adequ"
C10-2042,W03-1028,0,0.750889,"tent remains untagged. While keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial. Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability"
C10-2042,N09-1070,0,0.304335,"extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. Consequently, we have little understanding of how effective the state-of the-art systems would be on a completely new dataset from a different domain. A few questions arise n"
C10-2042,D09-1027,0,0.390249,"extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. Consequently, we have little understanding of how effective the state-of the-art systems would be on a completely new dataset from a different domain. A few questions arise n"
C10-2042,D09-1137,0,0.0731046,"ntagged. While keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial. Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains"
C10-2042,W03-1805,0,0.159732,"no means trivial. Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. Consequently, we have little understanding of how"
C10-2042,W00-1308,0,0.0302692,"Missing"
C10-2042,P07-1070,0,0.0229445,"dress the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999), Turney (2000; 2003), Hulth (2003), Medelyan et al. (2009)). A disadvantage of supervised approaches The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003)), graph-based ranking (e.g., Zha (2002), Mihalcea and Tarau (2004), Wan et al. (2007), Wan and Xiao (2008), Liu et al. (2009a)), and clustering (e.g., Matsuo and Ishizuka (2004), Liu et al. (2009b)). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. Consequently, we have little understanding of how effective the state-of the-art systems would be on a completely new dataset from a di"
C10-2042,W04-3252,0,\N,Missing
C12-1033,W06-0901,0,0.24358,", namely, (1) trigger identification (e.g., [acquired] should be identified as the trigger of an event); (2) trigger type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of seman"
C12-1033,W09-2209,0,0.136142,"rguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document through the pipeline of components where the output of one component is the input of the subsequent one. Li et al.'s Chinese event extraction system also employs a pipeline architecture,"
C12-1033,N09-2053,0,0.211945,"rguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document through the pipeline of components where the output of one component is the input of the subsequent one. Li et al.'s Chinese event extraction system also employs a pipeline architecture,"
C12-1033,P05-1045,0,0.0173358,"ary subtasks of an ACE event extraction system, namely, (1) trigger identification (e.g., [acquired] should be identified as the trigger of an event); (2) trigger type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic"
C12-1033,P06-1061,0,0.0293466,"extension to the baseline system, respectively. We present evaluation results in Section 6 and conclude in Section 7. 2 Related Work Much attention has been devoted to the event extraction task in the NLP community. In the early years, researchers focused on sentence-level extraction, employing local information from just one sentence (e.g., Grishman et al. (2005), Hardy et al. (2006), Ahn (2006)). However, in many cases local information alone is insufficient to make the right decisions, so later work incorporates more context around a sentence and seeks high level information. For example, Gu and Cercone (2006) and Patwardhan and Riloff (2009) consider broader sentential context. Ji and Grishman (2008) extend the scope to a cluster of topic-related documents and utilize global information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument"
C12-1033,P09-2093,0,0.0969183,"ly years, researchers focused on sentence-level extraction, employing local information from just one sentence (e.g., Grishman et al. (2005), Hardy et al. (2006), Ahn (2006)). However, in many cases local information alone is insufficient to make the right decisions, so later work incorporates more context around a sentence and seeks high level information. For example, Gu and Cercone (2006) and Patwardhan and Riloff (2009) consider broader sentential context. Ji and Grishman (2008) extend the scope to a cluster of topic-related documents and utilize global information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and"
C12-1033,P11-1113,0,0.310152,"[acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document through the pipeline of components where the"
C12-1033,E12-1029,0,0.0240111,"al information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and then include discourse properties to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. For example, Lu and Roth (2012) introduce a novel sequence labeling framework called structured preference modeling, and Huang and Riloff (2012a) propose a bootstrapping solution for argument extraction with little annotated data. 531 As far as work on Chinese event extraction is concerned, Chen and Ji (2009b) point out the Chinesespecific issue of word segmentation errors and create an errata table to allevia"
C12-1033,W09-1704,0,0.182324,"of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document through the pipeline of components where the output of one component is the input of the subsequent one. Li et al.'s Chinese event extraction system also employs a pipeline architecture,"
C12-1033,P08-1030,0,0.0601494,"be identified as the trigger of an event); (2) trigger type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in tes"
C12-1033,D12-1092,0,0.219431,"ent extraction with little annotated data. 531 As far as work on Chinese event extraction is concerned, Chen and Ji (2009b) point out the Chinesespecific issue of word segmentation errors and create an errata table to alleviate this problem, analyzing the impactof different types of features. Chen and Ji (2009a) bootstrap Chinese event extraction with extra information from an English event extraction system using cross-lingual information projection. Ji (2009) extracts cross-lingual predicate clusters and uses a cross-lingual information extraction system to improve Chinese event extraction. Li et al. (2012) explore compositional semantics and discourse consistency to address the unknown trigger problem and word segmentation errors. 3 Baseline System In order to establish a strong baseline Chinese event extraction system adopting the pipeline architecture mentioned in the introduction, we train a classifier for each of the four components by using a feature set that is the union of the features employed by Chen and Ji (2009b) and Li et al. (2012). We augment it with compositional semantics and discourse consistency, the two extensions proposed by Li et al. that aim to improve the trigger identifi"
C12-1033,P10-1081,0,0.425178,"type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document through the pipeline of c"
C12-1033,R11-1002,0,0.349978,", Ahn (2006)). However, in many cases local information alone is insufficient to make the right decisions, so later work incorporates more context around a sentence and seeks high level information. For example, Gu and Cercone (2006) and Patwardhan and Riloff (2009) consider broader sentential context. Ji and Grishman (2008) extend the scope to a cluster of topic-related documents and utilize global information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and then include discourse properties to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. For example, L"
C12-1033,P11-2045,0,0.0604533,", Ahn (2006)). However, in many cases local information alone is insufficient to make the right decisions, so later work incorporates more context around a sentence and seeks high level information. For example, Gu and Cercone (2006) and Patwardhan and Riloff (2009) consider broader sentential context. Ji and Grishman (2008) extend the scope to a cluster of topic-related documents and utilize global information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and then include discourse properties to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. For example, L"
C12-1033,P12-1088,0,0.0357292,"1a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and then include discourse properties to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. For example, Lu and Roth (2012) introduce a novel sequence labeling framework called structured preference modeling, and Huang and Riloff (2012a) propose a bootstrapping solution for argument extraction with little annotated data. 531 As far as work on Chinese event extraction is concerned, Chen and Ji (2009b) point out the Chinesespecific issue of word segmentation errors and create an errata table to alleviate this problem, analyzing the impactof different types of features. Chen and Ji (2009a) bootstrap Chinese event extraction with extra information from an English event extraction system using cross-lingual information"
C12-1033,P07-1075,0,0.0105108,"ion (e.g., [acquired] should be identified as the trigger of an event); (2) trigger type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of the"
C12-1033,P11-1163,0,0.0813176,"level information. For example, Gu and Cercone (2006) and Patwardhan and Riloff (2009) consider broader sentential context. Ji and Grishman (2008) extend the scope to a cluster of topic-related documents and utilize global information from related documents. Gupta and Ji (2009) employ cross-event information to extract implicit time information. Liao and Grishman (2010) leverage document-level cross-event inference; Liao and Grishman (2011a) extract topic features to improve event extraction; and Liao and Grishman (2011b) present a self-training strategy and combine it with global inference. McClosky et al. (2011) use the tree of event-argument relations in a reranking dependency parser to capture global event structure properties. Hong et al. (2011) explore entity type consistency to predict event mentions. Huang and Riloff (2012b) initially identify arguments and then include discourse properties to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. For example, Lu and Roth (2012) introduce a novel sequence labeling framework called structured preference modeling, and Huang and Riloff (2012a) propose a bootstrapping solution for argument"
C12-1033,D09-1016,0,0.398554,"gger of an event); (2) trigger type determination (e.g., [acquired] should be assigned the type Transfer-Money; (3) argument identification (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] are the arguments of this Transfer-Money event); and (4) argument role determination (e.g., Resneft, [Yugansk] and 93.5 [9.35 billion] play the roles of Buyer, Artifact and Price respectively in this event). Compared to the amount of research on English event extraction (e.g., Finkel et al. (2005), Grishman et al. (2005), Ahn (2006), Hardy et al. (2006), Maslennikov and Chua (2007), Ji and Grishman (2008), Patwardhan and Riloff (2009), Liao and Grishman (2010), Hong et al. (2011)), there is considerably less work on Chinese event extraction. Work on end-to-end Chinese event extraction was pioneered by Chen and Ji (2009b), who adopt a pipeline system architecture composed of four components that correspond to the four major subtasks mentioned above. More specifically, in training, they learn a classifier to perform each of the four subtasks independently using primarily lexico-syntactic features but also a couple of semantic features (see Section 3 for an overview of these features); and in testing, they feed a raw document"
C12-1033,W09-1206,0,\N,Missing
C12-2019,W12-4504,1,0.840152,", and entity-based CEAF), whose implementation is provided by the shared task organizers. It is worth mentioning that (1) a resolver is not rewarded for correctly identifying singleton mentions; and (2) a mention is considered correctly extracted if and only if there is an exact phrase match between the gold mention and the extracted mention. In addition, elided pronouns, copulars, and appositive constructions are excluded in this (and the official shared task) evaluation. 3 Our Coreference Resolver In this section, we give an overview of our two-step resolver as used for the shared task (see Chen and Ng (2012) for details). Note that linguistic annotations such as word segmentation and syntactic parses come with the shared task datasets and do not need to be computed separately. Step 1: Mention Detection. To build a mention detector, we employ a two-step approach. First, in the extraction step, we extract mentions from all the NP and QP nodes in syntactic parse trees. Then, in the pruning step, we identify and filter erroneously extracted mentions by employing two types of pruning. In heuristic pruning, we use simple heuristics to prune erroneous mentions. For instance, we prune a candidate mention"
C12-2019,D09-1120,0,0.0193651,"mple, one rule specifies that two mentions are coreferent if the probability that their heads are coreferent according to the training data is greater than t H PU , where t H PU is a threshold determined using the development set. Note that the usual linguistic constraints on non-coreference are applied before a rule in any of the sieves posits two mentions as coreferent. These constraints are implemented as a single noncoreference rule, which specifies that two mentions mi and m j cannot be coreferent if one of the following five conditions holds: (1) they satisfy the i -within-i constraint (Haghighi and Klein, 2009); (2) they refer to different speakers in a dialogue despite being the same string; (3) they are in a copular construction; (4) mi is composed of two NPs connected by an &quot;and&quot; and m j is one of the conjuncts; and (5) the probability that mi and m j are coreferent (as calculated from training data) falls below a certain threshold. Step 3: Postprocessing. We postprocess the coreference partition before sending it to the scoring program. Specifically, we remove from it (1) all coreference links between two mentions in appositive constructions and (2) singleton clusters. 4 Evaluation In this secti"
C12-2019,W11-1902,0,0.0259432,"s, have recently addressed this issue by providing free access to the training and test sets used in the official evaluation (Pradhan et al., 2012). Our goal in this paper is to gain a better understanding of the state of the art in Chinese coreference resolution by providing an extensive empirical analysis of our Chinese resolver, which is ranked first on the Chinese subtask of the CoNLL-2012 shared task. Briefly, our resolver adopts a hybrid rulebased/machine learning approach to coreference resolution, extending the successful rule-based multi-pass sieve approach (Raghunathan et al., 2010; Lee et al., 2011) with lexical features that have proven useful in machine learning approaches (Rahman and Ng, 2011a, 2011b). Our analysis is focused on four issues. 1. Mention detection. Previous work has shown that the quality of the extracted mentions (i.e., the NPs participating in a coreference chain) plays an important role in the performance of a resolver. To what extent is the performance of our resolver limited by the recall and precision of our mention detector? To improve the precision of our mention detector, we need to improve its mention pruning strategy, but to what extent is its precision limit"
C12-2019,W12-4501,0,0.0189919,"etermine the state of the art in Chinese coreference resolution. The reason can be attributed in part to the lack of a standard evaluation dataset: while recently developed Chinese resolvers are typically evaluated on the ACE datasets, different researchers have used different splits of the ACE data for training and testing, making performance comparisons difficult.1 The organizers of the CoNLL-2012 shared task, Modeling Unrestricted Multilingual Coreference in OntoNotes, have recently addressed this issue by providing free access to the training and test sets used in the official evaluation (Pradhan et al., 2012). Our goal in this paper is to gain a better understanding of the state of the art in Chinese coreference resolution by providing an extensive empirical analysis of our Chinese resolver, which is ranked first on the Chinese subtask of the CoNLL-2012 shared task. Briefly, our resolver adopts a hybrid rulebased/machine learning approach to coreference resolution, extending the successful rule-based multi-pass sieve approach (Raghunathan et al., 2010; Lee et al., 2011) with lexical features that have proven useful in machine learning approaches (Rahman and Ng, 2011a, 2011b). Our analysis is focus"
C12-2019,D10-1048,0,0.0146702,"al Coreference in OntoNotes, have recently addressed this issue by providing free access to the training and test sets used in the official evaluation (Pradhan et al., 2012). Our goal in this paper is to gain a better understanding of the state of the art in Chinese coreference resolution by providing an extensive empirical analysis of our Chinese resolver, which is ranked first on the Chinese subtask of the CoNLL-2012 shared task. Briefly, our resolver adopts a hybrid rulebased/machine learning approach to coreference resolution, extending the successful rule-based multi-pass sieve approach (Raghunathan et al., 2010; Lee et al., 2011) with lexical features that have proven useful in machine learning approaches (Rahman and Ng, 2011a, 2011b). Our analysis is focused on four issues. 1. Mention detection. Previous work has shown that the quality of the extracted mentions (i.e., the NPs participating in a coreference chain) plays an important role in the performance of a resolver. To what extent is the performance of our resolver limited by the recall and precision of our mention detector? To improve the precision of our mention detector, we need to improve its mention pruning strategy, but to what extent is"
C12-2019,P11-1082,1,0.838648,"d in the official evaluation (Pradhan et al., 2012). Our goal in this paper is to gain a better understanding of the state of the art in Chinese coreference resolution by providing an extensive empirical analysis of our Chinese resolver, which is ranked first on the Chinese subtask of the CoNLL-2012 shared task. Briefly, our resolver adopts a hybrid rulebased/machine learning approach to coreference resolution, extending the successful rule-based multi-pass sieve approach (Raghunathan et al., 2010; Lee et al., 2011) with lexical features that have proven useful in machine learning approaches (Rahman and Ng, 2011a, 2011b). Our analysis is focused on four issues. 1. Mention detection. Previous work has shown that the quality of the extracted mentions (i.e., the NPs participating in a coreference chain) plays an important role in the performance of a resolver. To what extent is the performance of our resolver limited by the recall and precision of our mention detector? To improve the precision of our mention detector, we need to improve its mention pruning strategy, but to what extent is its precision limited by our current mention pruning strategy? To improve the recall of our mention detector, we need"
C12-2019,J01-4004,0,0.299727,"d by these tools? 3. The coreference algorithm. To better understand our hybrid approach, we focus on three questions. First, do we really need a hybrid approach? In other words, will our approach work equally well without the learning component? Second, how much does each sieve in the multi-pass sieve approach contribute to overall performance? Third, how important is the ordering of the sieves as far as performance is concerned? 4. Comparison with classifier-based approaches. In the shared task, our resolver outperformed those systems that adopted the popularly-used mention-pair (MP) model (Soon et al., 2001), a classifier trained to determine whether two given NPs are coreferent. However, we cannot claim that our coreference algorithm is superior to the MP model because we do not know which component(s) of our resolver (e.g., mention detection, feature computation, resolution) contributed to the superiority. In fact, much of the previous work focuses on comparing systems rather than models/methods. We determine whether our resolution method is better than the MP model if both are given the same set of mentions and features. 1 One may wonder why researchers did not simply follow the same train-tes"
C12-2019,P09-1074,0,0.107247,"postprocess the coreference partition before sending it to the scoring program. Specifically, we remove from it (1) all coreference links between two mentions in appositive constructions and (2) singleton clusters. 4 Evaluation In this section, we conduct extensive experimentation with our resolver in an attempt to shed light on the four issues raised in the introduction. 4.1 Mention Detection and Preprocessing We begin by describing the experimental setup related to the first two issues. The first issue concerns how the performance of our resolver is affected by the quality of the mentions. Stoyanov et al. (2009) show that for English coreference, results obtained using gold mentions (i.e., mentions taken directly from gold-standard coreference annotations) are substantially better than those produced using system mentions (i.e., automatically extracted mentions). While we will explore gold mentions in our Chinese experiments, we seek to gain a better understanding of this issue by considering three types of system mentions. The first type, system mentions from system parses with imperfect pruning, is typically what researchers use to produce end-to-end coreference results. It is composed of mentions"
C12-2045,W11-1701,0,0.478797,"ate which point raised by other people she is responding to. These phrases typically appear in sentences that express concession, as well as in rhetorical questions, where an author questions the validity of other people&apos;s arguments. Hence, for debate stance classification, it is particularly important to interpret a phrase using its context. Unfortunately, existing work on this task has largely failed to take context into account, training a single classifier for stance prediction using shallow features computed primarily from n-grams and dependency parse trees (Somasundaran and Wiebe, 2010; Anand et al., 2011). Motivated by the above discussion, our goal in this paper is to improve the performance of a learning-based debate stance classification system. As we will see below, our approach exploits rich linguistic knowledge that can be divided into two types: (1) knowledge that can be automatically computed and encoded as features for better exploiting contextual information, and (2) knowledge that is acquired from additional manual annotations on the debate posts. Briefly, our approach is composed of three steps: 1. Employing additional linguistic features to train a post-stance classifier. To impro"
C12-2045,J96-2004,0,0.0788806,"rt by manually labeling only the key topics. We define a topic t as a key topic for a post d if (1) t is one of the 10 topics with the highest Tf-Idf value in d and (2) t appears in at least 10 posts. These conditions ensure that t is important for both d and the domain. We then ask two human annotators to annotate each key topic with one of three labels, support, oppose, or neutral, depending on the annotators&apos; perception of the author&apos;s stance on a topic after reading the entire post. The kappa value computed over the two sets of manual annotations is 0.69, indicating substantial agreement (Carletta, 1996). Training and applying a topic-stance classifier. For each key topic with a stance label in a training post, we create one training instance. Each instance is represented by the same set of features that we used to train the post-stance classifier, except that (1) the topic features (Section 4.1.1) and the topic-opinion features (Section 4.1.3) are extracted only for the topic under consideration; and (2) all the features are computed using only the sentences in which the topic appears. After training, we apply the resulting classifier to a test post. Test instances are generated the same way"
C12-2045,W02-1011,0,0.0125476,"অিভমেতর উপর িভি কের িতি ত। িট ব ল আেলািচত িবষেয়র পে -িবপে লখা রচনার উপর চালােনা পরী ার ফলাফল ইি টজার িলিনয়ার া ািমং-এর সােথ যু াব া এই ই ধরেণর উ ত ভাষািবদ ার কাযকািরতা মাণ কের। Keywords: debate stance classification, opinion mining, sentiment analysis. Keywords in Bengali: িবতেকর প িনণয়, ওিপিনয়ন মাইিনং, মতামত িবে ষণ। Proceedings of COLING 2012: Posters, pages 451–460, COLING 2012, Mumbai, December 2012. 451 1 Introduction While much traditional work on opinion mining has involved determining the polarity expressed in a customer review (e.g., whether a review is “thumbs up” or “thumbs down”) (Pang et al., 2002)), researchers have begun exploring new opinion mining tasks in recent years. One such task is debate stance classification: given a post written for a two-sided online debate topic (e.g., “Should abortion be banned?”), determine which of the two sides (i.e., for and against) its author is taking. Debate stance classification is arguably a more challenging task than polarity classification. While in polarity classification sentiment-bearing words and phrases have proven to be useful (e.g., “excellent” correlates strongly with positive polarity), in debate stance classification it is not uncomm"
C12-2045,W04-2401,0,0.0498823,"pproach, where we train a topic-stance classifier to determine an author&apos;s stance on a topic by relying on manual topic-stance annotations. 3. Improving post stance prediction using topic stances. Now that we have topic stances, we want to use them to improve the prediction of post stances. One way to do so is to encode topic stances as additional features for training the post-stance classifier. Another way, which we adopt in this paper, is to perform joint inference over the predictions made by the topicstance classifier and the post-stance classifier using integer linear programming (ILP) (Roth and Yih, 2004). We evaluate our approach on debate posts taken from two domains (Abortion and Gun Rights), and show that both sources of linguistic information we introduce (the additional linguistic features for training the post-stance classifier and the topic stances) significantly improve a baseline classifier trained on Anand et al.&apos;s (2011) features. The rest of the paper is structured as follows. We first discuss related work (Section 2) and our datasets (Section 3). Then we describe our three-step approach to debate stance classification (Section 4). Finally, we evaluate our approach (Section 5). 2"
C12-2045,W10-0214,0,0.5604,"contrast her own view or indicate which point raised by other people she is responding to. These phrases typically appear in sentences that express concession, as well as in rhetorical questions, where an author questions the validity of other people&apos;s arguments. Hence, for debate stance classification, it is particularly important to interpret a phrase using its context. Unfortunately, existing work on this task has largely failed to take context into account, training a single classifier for stance prediction using shallow features computed primarily from n-grams and dependency parse trees (Somasundaran and Wiebe, 2010; Anand et al., 2011). Motivated by the above discussion, our goal in this paper is to improve the performance of a learning-based debate stance classification system. As we will see below, our approach exploits rich linguistic knowledge that can be divided into two types: (1) knowledge that can be automatically computed and encoded as features for better exploiting contextual information, and (2) knowledge that is acquired from additional manual annotations on the debate posts. Briefly, our approach is composed of three steps: 1. Employing additional linguistic features to train a post-stance"
C14-1159,N07-4013,0,0.0204737,", or tests). The ability to classify MRs is indispensable to sound automatic analysis of patient health records. While MR classification is a relatively new task, there has been a lot of work on extracting semantic relations from news articles. Supervised approaches train classifiers on data annotated with the target relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld a"
C14-1159,P07-1073,0,0.0403205,"and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in this paper is to advance the state of the art in MR classification. One of the ma"
C14-1159,J81-4005,0,0.754157,"Missing"
C14-1159,N13-1095,0,0.0136925,"earning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in this paper is to advance the state of the art in MR classification. One of the major challenges in MR classification is the scarcity of labeled data. At first glance, we can mitigate this problem using distant supervision approaches. However, there is difficulty in applying these approaches to MR classification: only one of the relation types defined in the 2010 i2b2 Chal"
C14-1159,P14-5010,0,0.00356621,"d following the second concept in the pair; (4) as a concept type sequence for all the concepts found in the sentence containing the pair; and (5) as a shortest dependency path sequence connecting the two concepts. Table 2 shows an example of these five methods of generating sequences from the T EST concept her exam and the P ROBLEM concept her hyperreflexia in the sentence 2 Other methods for addressing class imbalance, such as over-sampling (Chawla et al., 2002) and cost-sensitive learning (Turney, 1995), can also be employed. 3 To compute the features, we use (1) the Stanford CoreNLP tool (Manning et al., 2014) to obtain POS tags, word lemmas, and dependency structures; (2) GENIA (http://www.nactem.ac.uk/tsujii/GENIA/tagger) to obtain phrase chunks; and (3) SENNA (Collobert et al., 2011) to obtain predicate-argument structures. 1684 Generation Method (1) (2) (3) (4) (5) Sequence RB VB , testc1 RB VBD RB IN problemc2 . ADVP VP ADVP PP postop , testc1 only improve slightly in problemc2 . testc1 problemc2 testc1 –nsubj–> prep &lt;–pobj–problemc2 Table 2: Examples of the five methods of sequence generation. Postop, her exam only improved slightly in her hyperreflexia . Note that for better generalization,"
C14-1159,P09-1113,0,0.0844195,"knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in this paper is to advance the state of the art in MR classification. One of the major challenges in MR classification is the scarcity of labeled data. At first glance, we can mitigate this problem using distant supervision approaches. However, there is difficulty in applying these approaches to MR classifi"
C14-1159,P11-2048,0,0.0158093,"way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in this paper is to advance the state of the art in MR classification. One of the major challenges in MR classification is the scarcity of labeled data. At first glance, we can mitigate this problem using distant supervision approaches. However, there is difficulty in applying these approaches to MR classification: only one of the rela"
C14-1159,P06-1015,0,0.0203806,"supervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in this paper is to advance the state of the art in MR cla"
C14-1159,P02-1006,0,0.0307556,"r-intensive process, researchers have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). However, the resulting patterns often suffer from semantic drift and low precision. Recent years have seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated relation instances by extracting their labels from relation instances in a knowledge base such as Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). Our goal in t"
C14-1159,W04-2401,0,0.0311195,"e systems are not applicable to all of the test instances. If this happens, the corresponding system(s) will return a vector in which all of its elements are set to 0. 6 Enforcing Global Consistency So far we have had an ensemble that, given a test instance, returns the probability that it belongs to each of the 11 classes. Since the test instances are classified independently of each other, there is no guarantee that the resulting classifications are globally consistent. To enforce global consistency, we employ global constraints implemented in the Integer Linear Programming (ILP) framework (Roth and Yih, 2004). Since our constraints are intra-sentential, we formulate one ILP program for each sentence s in each training summary. Each ILP program contains 11×Ns variables, where Ns is the number of test instances formed from the concept pairs in s. In other words, there is one binary indicator variable xi,j,r for each relation class r of each test instance inst formed from concept i and concept j, which will be set to 1 by the ILP solver if and only if it thinks inst should belong to class r. Our objective is to maximize the linear combination of these variables and their corresponding probabilities g"
C14-1159,N06-1039,0,0.0151008,"cepts (problems, treatments, or tests). The ability to classify MRs is indispensable to sound automatic analysis of patient health records. While MR classification is a relatively new task, there has been a lot of work on extracting semantic relations from news articles. Supervised approaches train classifiers on data annotated with the target relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Moon"
C14-1159,P05-1053,0,0.0604309,"tion, an information extraction task in the clinical domain that was recently defined in the 2010 i2b2/VA Challenge (Uzuner et al., 2011), involves determining the relation between a pair of medical concepts (problems, treatments, or tests). The ability to classify MRs is indispensable to sound automatic analysis of patient health records. While MR classification is a relatively new task, there has been a lot of work on extracting semantic relations from news articles. Supervised approaches train classifiers on data annotated with the target relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively lea"
C14-1159,D07-1076,0,0.0990076,"cal domain that was recently defined in the 2010 i2b2/VA Challenge (Uzuner et al., 2011), involves determining the relation between a pair of medical concepts (problems, treatments, or tests). The ability to classify MRs is indispensable to sound automatic analysis of patient health records. While MR classification is a relatively new task, there has been a lot of work on extracting semantic relations from news articles. Supervised approaches train classifiers on data annotated with the target relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While unsupervised approaches can use a large amount of unannotated data and extract a large number of relations, it may not be easy to map the resulting relations to those needed for a given knowledge base. One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances, a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin"
C16-1308,W06-0901,0,0.602113,"ng probability distribution:   X 1 θi Nfi (ω) (1) PM (ω) = exp  Z fi where Nfi (ω) is the number of groundings of fi that evaluate to True given a world ω (an assignment of {0, 1} to all ground atoms). The use of first-order logic enables the user to succinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s syste"
C16-1308,D15-1247,0,0.218432,"uth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-available Chinese event coreference resolver. Not all such corpora were carefully annotated: as Liu et al. (2014) pointed out, OntoNo"
C16-1308,araki-etal-2014-detecting,0,0.139499,"of first-order logic enables the user to succinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoC"
C16-1308,P10-1143,0,0.195016,"ccinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-availa"
C16-1308,J14-2004,0,0.423539,"Missing"
C16-1308,W09-3208,0,0.631541,"1 θi Nfi (ω) (1) PM (ω) = exp  Z fi where Nfi (ω) is the number of groundings of fi that evaluate to True given a world ω (an assignment of {0, 1} to all ground atoms). The use of first-order logic enables the user to succinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s syst"
C16-1308,chen-ng-2014-sinocoreferencer,1,0.866903,"rpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-available Chinese event coreference resolver. Not all such corpora were carefully annotated: as Liu et al. (2014) pointed out, OntoNotes and ECB have only been partially annotated with event coreference links, for instance. 4 Baseline System Our pipeline-based baseline system has five steps: Step 1: Entity extraction. Our entity extraction model jointly identifies the entity mentions and their entity types. We train this model using CRF++2 , treating each sentence as a word sequence. Specifically, we create one instance for each word w and assign it a class label that indicates wh"
C16-1308,I11-1012,0,0.0610881,"number of groundings of fi that evaluate to True given a world ω (an assignment of {0, 1} to all ground atoms). The use of first-order logic enables the user to succinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the"
C16-1308,W97-1311,0,0.600036,"Markov network represents the following probability distribution:   X 1 θi Nfi (ω) (1) PM (ω) = exp  Z fi where Nfi (ω) is the number of groundings of fi that evaluate to True given a world ω (an assignment of {0, 1} to all ground atoms). The use of first-order logic enables the user to succinctly represent prior, relational knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et"
C16-1308,D12-1045,0,0.406047,"al knowledge about the application domain, while the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-available Chinese event corefere"
C16-1308,D12-1092,0,0.0367721,"Missing"
C16-1308,liu-etal-2014-supervised,0,0.406675,"Bank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-available Chinese event coreference resolver. Not all such corpora were carefully annotated: as Liu et al. (2014) pointed out, OntoNotes and ECB have only been partially annotated with event coreference links, for instance. 4 Baseline System Our pipeline-based baseline system has five steps: Step 1: Entity extraction. Our entity extraction model jointly identifies the entity mentions and their entity types. We train this model using CRF++2 , treating each sentence as a word sequence. Specifically, we create one instance for each word w and assign it a class label that indicates whether it begins an entity mention with type tj (B-tj ), is inside an entity mention with type tj (I-tj ), or is outside an ent"
C16-1308,H05-1004,0,0.585689,"l test set.5 For Chinese, since the ACE 2005 test set is not publicly available, we report five-fold cross validation results on the ACE 2005 training corpus. For each fold experiment, we employ three folds for classifier training, one fold for development (parameter tuning), and one fold for testing. To evaluate event coreference performance on KBP, we follow the official KBP evaluation and employ four commonly-used scoring measures as implemented in version 1.7 of the official scorer provided by the KBP 2015 organizers, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores.6 5 Since the KBP 2015 corpus was not annotated with event arguments and entity coreference links, we train our entity mention extractor, our entity coreference resolver, and our event argument identification and role classification model on two LDC corpora provided by the TAC KBP 2015 task organizers (LDC2015E29 and LDC2015E68), as permitted by the rules of the shared task. 6 The official KBP scorer is available at http://cairo.lti.cs.cmu.edu/kbp/2015/event/scoring . 3271 Metric B3 CEAFe MUC BLANC Englis"
C16-1308,P14-5010,0,0.0184773,"e document; whether the sentence containing the the triggers of ev1 and ev2 are identical if this is a discussion forum document. Argument whether ev1 and ev2 have arguments with the same role; whether the arguments have the same head word; features whether they are in the same coreference chains; whether they have the same modifiers; the roles and number of the arguments that only appear in ev1 ; and the roles and number of the arguments that only appear in ev2 . Table 2: Features used in the English baseline system. POS tags, constituent parses and dependency parses are provided by CoreNLP (Manning et al., 2014). For all uses of WordNet (Fellbaum, 1998), only the first synset is used. 5 Joint Model In this section, we describe our MLN-based joint model for event coreference resolution. 5.1 MLN Structure Figure 1 shows our proposed MLN for event coreference resolution. It has five predicates subdivided into three categories: query, hidden and evidence. The query predicate EventCoref(d,t1 ,t2 ) is true when two event mentions t1 and t2 in document d are coreferent. The hidden predicates are those that cannot be directly observed in the data. Our model contains three hidden predicates: (1) Trigger(d,t,p"
C16-1308,P16-1060,0,0.0147226,"identification and subtyping. A closer examination of the outputs reveals that our resolver is comparatively better at extracting two types of coreference links that are traditionally considered difficult to extract. The first type involves triggers that are lexically different. For example, in the text segment “The former mayor of Detroit, 7 The CoNLL scorer is available at https://github.com/conll/reference-coreference-scorers . As is commonly known, CEAFe sometimes produces unintuitive scores. Specifically, the CEAFe F-score may drop as more coreference links are correctly identified. See Moosavi and Strube (2016) for a detailed discussion. 8 3272 Michigan was sentenced to 28 years in prison . . . Prosecutors asked for a minimum of 28 years for Kilpatrick, who resigned from the mayor’s office in 2008 . . .”, the link between event mentions triggered by former and resigned, both of which have type Personnel.End-position, is discovered by our resolver but not the baseline. The second type involves links between event mentions that are far from each other. 6.3 Error Analysis To better understand how to improve our MLN-based resolver and to provide directions for future work, we conduct a qualitative analy"
C16-1308,P14-2006,1,0.837838,"CoNLL = 40.82 Table 4: Results for event coreference resolution on KBP 2015 and ACE 2005. English/KBP 2015 Baseline MLNs Prec Rec F1 Prec Rec 65.05 51.43 57.45 67.97 50.51 F1 57.95 Chinese/ACE 2005 Baseline MLNs Prec Rec F1 Prec Rec 67.08 56.44 61.30 66.39 57.37 F1 61.55 Table 5: Results for event trigger identification and subtyping on KBP 2015 and ACE 2005. To evaluate event coreference performance on ACE, we follow previous work on event coreference (e.g., Yang et al. (2015)) and employ the aforementioned four scoring measures as implemented in the latest version (v8) of the CoNLL scorer (Pradhan et al., 2014), as well as the CoNLL score, which is the unweighted average of the MUC, B 3 , and CEAFe F-scores.7 To our knowledge, there is only one difference between the implementations of the four scoring measures in the two scorers: while the CoNLL scorer considers an event mention correctly detected as long as it has an exact match with a gold event mention in terms of its left and right boundaries, the KBP 2015 scorer is stricter in that it considers an event mention correctly detected by additionally requiring that its event subtype be correctly determined. 6.2 Results and Discussion The left half"
C16-1308,J01-4004,0,0.498316,"Missing"
C16-1308,M95-1005,0,0.287526,"ing 30 training documents, and report results on the official test set.5 For Chinese, since the ACE 2005 test set is not publicly available, we report five-fold cross validation results on the ACE 2005 training corpus. For each fold experiment, we employ three folds for classifier training, one fold for development (parameter tuning), and one fold for testing. To evaluate event coreference performance on KBP, we follow the official KBP evaluation and employ four commonly-used scoring measures as implemented in version 1.7 of the official scorer provided by the KBP 2015 organizers, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores.6 5 Since the KBP 2015 corpus was not annotated with event arguments and entity coreference links, we train our entity mention extractor, our entity coreference resolver, and our event argument identification and role classification model on two LDC corpora provided by the TAC KBP 2015 task organizers (LDC2015E29 and LDC2015E68), as permitted by the rules of the shared task. 6 The official KBP scorer is available at http://cairo.lti.cs.cmu.edu/kbp/2015/eve"
C16-1308,Q15-1037,0,0.671541,"the weights help model uncertainty in the truth of the first-order logic sentences. 3.2 Related Work Existing within-document English event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), McConky et al. (2012), Chen and Ji (2009), S. and Arock (2012)), OntoNotes (e.g., Chen et al. (2011)) the (not publicly-available) Intelligence Community (IC) corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014)); the ECB corpus (e.g., Bejan and Harabagiu (2010; 2014), Lee et al. (2012)) and its extension, ECB+ (e.g.,Yang et al. (2015)); and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpus is the one used in the KBP 2015 Event Nugget Detection and Coreference shared task, in which the best performers are RPI’s system (Hong et al., 2015), LCC’s system (Monahan et al., 2015), and UI-CCG’s system (Sammons et al., 2015). Among these corpora, ACE is the only one that is additionally composed of event coreference-annotated Chinese documents. It has been used to train SinoCoreferencer (Chen and Ng, 2014), a publicly-available Chinese event coreference resolver. Not all such corpora were carefully"
chen-ng-2014-sinocoreferencer,M95-1005,0,\N,Missing
chen-ng-2014-sinocoreferencer,W09-3208,0,\N,Missing
chen-ng-2014-sinocoreferencer,W97-1311,0,\N,Missing
chen-ng-2014-sinocoreferencer,J01-4001,0,\N,Missing
chen-ng-2014-sinocoreferencer,D12-1045,0,\N,Missing
chen-ng-2014-sinocoreferencer,H05-1004,0,\N,Missing
chen-ng-2014-sinocoreferencer,W11-1902,0,\N,Missing
chen-ng-2014-sinocoreferencer,D09-1120,0,\N,Missing
chen-ng-2014-sinocoreferencer,I11-1012,0,\N,Missing
chen-ng-2014-sinocoreferencer,P10-1143,0,\N,Missing
chen-ng-2014-sinocoreferencer,J01-4004,0,\N,Missing
chen-ng-2014-sinocoreferencer,D10-1085,0,\N,Missing
chen-ng-2014-sinocoreferencer,W09-4303,0,\N,Missing
chen-ng-2014-sinocoreferencer,C12-1033,1,\N,Missing
chen-ng-2014-sinocoreferencer,D10-1048,0,\N,Missing
D07-1023,P06-3002,0,0.375467,"on algorithm. 1 See http://www.utdallas.edu/~sajib/posDatasets.html. Tag JJ JJR JJS NN NNS RB VB VBD VBG VBZ Description Adjective Adjective, comparative Adjective, superlative Singular noun Plural noun Adverb Verb, non-3rd ps. sing. present Verb, past tense or past participle Verb, gerund/present participle Verb, 3rd ps. sing. present Treebank tags JJ JJR JJS NN, NNP NNS, NNPS RB VB, VBP VBD, VBN VBG VBZ Table 1: The English tagset Using morphological information. Perhaps due to the overly simplistic methods employed to compute morphological information, morphology has only been used as what Biemann (2006) called add-on’s in existing POS induction algorithms, which remain primarily distributional in nature. In contrast, our approach more tightly integrates morphology into the distributional framework. As we will see, we train SVM classifiers using both morphological and distributional features to select seed words for our bootstrapping algorithm, effectively letting SVM combine these two sources of information and perform automatic feature weighting. Another appealing feature of our approach is that when labeling each unlabeled word with its POS tag, an SVM classifier also returns a numeric val"
D07-1023,W00-0717,0,0.48988,"be tagged with its POS) as a context vector that encodes its left and right context, (2) clustering distributionally similar words, and (3) manually labeling each cluster with a POS tag by inspecting the members of the cluster. This distributional approach works under the assumption that the context vector of each word encodes sufficient information for enabling accurate word clustering. However, many words are distributionally unreliable: due to data sparseness, they occur infrequently and hence their context vectors do not capture reliable statistical information. To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster. The goal of this paper is to propose a new bootstrapping approach to unsupervised POS induction that can operate in a resource-scarce setting. Most notably, our approach aims to improve the quality of the seed clusters by employing seed words that are both distributionally and morphologically reliable. In particular, we present a novel method for combining morphologic"
D07-1023,E03-1009,0,0.117078,"uction algorithms have also attempted to incorporate morphological information into the distributional framework, but our work differs from these in two respects. Computing morphological information. Previous POS induction algorithms have attempted to derive morphological information from dictionaries (Haji , 2000) and knowledge-based morphological analyzers (Duh and Kirchhoff, 2006). However, these resources are generally not available for resource-scarce languages. Consequently, researchers have attempted to derive morphological information heuristically (e.g., Cucerzan and Yarowsky (2000), Clark (2003), Freitag (2004)). For instance, Cucerzan and Yarowsky (2000) posit a character sequence x as a suffix if there exists a sufficient number of distinct words w in the vocabulary such that the concatentations wx are also in the vocabulary. It is conceivable that such heuristically computed morphological information can be inaccurate, thus rendering the usefulness of a more accurate morphological analyzer. To address this problem, we exploit morphological information provided by an unsupervised word segmentation algorithm. 1 See http://www.utdallas.edu/~sajib/posDatasets.html. Tag JJ JJR JJS NN N"
D07-1023,P00-1035,0,0.0207714,"k Several unsupervised POS induction algorithms have also attempted to incorporate morphological information into the distributional framework, but our work differs from these in two respects. Computing morphological information. Previous POS induction algorithms have attempted to derive morphological information from dictionaries (Haji , 2000) and knowledge-based morphological analyzers (Duh and Kirchhoff, 2006). However, these resources are generally not available for resource-scarce languages. Consequently, researchers have attempted to derive morphological information heuristically (e.g., Cucerzan and Yarowsky (2000), Clark (2003), Freitag (2004)). For instance, Cucerzan and Yarowsky (2000) posit a character sequence x as a suffix if there exists a sufficient number of distinct words w in the vocabulary such that the concatentations wx are also in the vocabulary. It is conceivable that such heuristically computed morphological information can be inaccurate, thus rendering the usefulness of a more accurate morphological analyzer. To address this problem, we exploit morphological information provided by an unsupervised word segmentation algorithm. 1 See http://www.utdallas.edu/~sajib/posDatasets.html. Tag J"
D07-1023,N07-1020,1,0.81859,"asks is simple for a truly resource-scarce language for which we do not have a dictionary or a knowledgebased morphological analyzer. As mentioned in the introduction, our proposed solution to both tasks is to use an unsupervised morphological analyzer that can be built just from an unannotated corpus. In particular, we have implemented an unsupervised morphological analyzer that outperforms Goldsmith’s (2001) Linguistica and Creutz and Lagus’s (2005) Morfessor for our English and Bengali datasets and compares favorably to the bestperforming morphological parsers in MorphoChallenge 20053 (see Dasgupta and Ng (2007)). Given the segmentation of each word and the most frequent 30 suffixes 4 provided by our morphological analyzer, our clustering algorithm operates by (1) clustering the similar suffixes and then (2) assigning words to each cluster based on the suffixes a word combines with. To cluster similar suffixes, we need to define the similarity between two suffixes. Informally, we say that two suffixes x and y are similar if a word that combines with x also combines with y and vice versa. In practice, we will rarely posit two suffixes as similar under this definition unless we assume access to a compl"
D07-1023,W06-1647,0,0.0288415,"d words from each cluster (Section 5), and bootstrap each cluster using a weakly supervised learner (Section 6). Finally, we present evaluation results in Section 7 and conclusions in Section 8. 2 Related Work Several unsupervised POS induction algorithms have also attempted to incorporate morphological information into the distributional framework, but our work differs from these in two respects. Computing morphological information. Previous POS induction algorithms have attempted to derive morphological information from dictionaries (Haji , 2000) and knowledge-based morphological analyzers (Duh and Kirchhoff, 2006). However, these resources are generally not available for resource-scarce languages. Consequently, researchers have attempted to derive morphological information heuristically (e.g., Cucerzan and Yarowsky (2000), Clark (2003), Freitag (2004)). For instance, Cucerzan and Yarowsky (2000) posit a character sequence x as a suffix if there exists a sufficient number of distinct words w in the vocabulary such that the concatentations wx are also in the vocabulary. It is conceivable that such heuristically computed morphological information can be inaccurate, thus rendering the usefulness of a more"
D07-1023,C04-1052,0,0.704203,"hms have also attempted to incorporate morphological information into the distributional framework, but our work differs from these in two respects. Computing morphological information. Previous POS induction algorithms have attempted to derive morphological information from dictionaries (Haji , 2000) and knowledge-based morphological analyzers (Duh and Kirchhoff, 2006). However, these resources are generally not available for resource-scarce languages. Consequently, researchers have attempted to derive morphological information heuristically (e.g., Cucerzan and Yarowsky (2000), Clark (2003), Freitag (2004)). For instance, Cucerzan and Yarowsky (2000) posit a character sequence x as a suffix if there exists a sufficient number of distinct words w in the vocabulary such that the concatentations wx are also in the vocabulary. It is conceivable that such heuristically computed morphological information can be inaccurate, thus rendering the usefulness of a more accurate morphological analyzer. To address this problem, we exploit morphological information provided by an unsupervised word segmentation algorithm. 1 See http://www.utdallas.edu/~sajib/posDatasets.html. Tag JJ JJR JJS NN NNS RB VB VBD VBG"
D07-1023,J01-2001,0,0.431737,"Missing"
D07-1023,N06-1041,0,0.409278,"Missing"
D07-1023,A00-2013,0,0.0804628,"Missing"
D07-1023,J93-2004,0,0.02855,"Missing"
D07-1023,J97-3003,0,0.107403,"Missing"
D07-1023,E95-1020,0,0.857409,"al Language Processing and Computational c Natural Language Learning, pp. 218–227, Prague, June 2007. 2007 Association for Computational Linguistics emphasis on resource-scarce languages, our approach does not rely on any language resources. In particular, the morphological information that it exploits is provided by an unsupervised morphological analyzer. It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schütze (1995) and Clark (2000)). To understand the role and significance of morphology, it is important to first understand why the distributional approach works well for English. Recall from the above that the distributional approach assumes that the information encoded in the context vector of each word, which typically consists of the 250 most frequent words of a given language, is sufficient for accurately clustering the words. This approach works well for English because the most frequent English words are composed primarily of closed-class words such as “to” and “is”, which provide strong clues to th"
D07-1023,P05-1044,0,0.120698,"confident the word is labeled. This opens up the possibility of having a human improve our automatically constructed lexicon by manually checking those entries that are tagged with low confidence by an SVM classifier. Recently, there have been attempts to perform (mostly) unsupervised POS tagging without relying on a POS lexicon. Haghighi and Klein’s (2006) prototype-driven approach requires just a few prototype examples for each POS tag, exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative log-linear model for POS tagging. Smith and Eisner (2005) train a log-linear model for POS tagging in an unsupervised manner using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples created by perturbing e). 3 The English and Bengali Tagsets Given our focus on automatically labeling open class words, our English and Bengali tagsets are designed to essentially cover all of the open-class 220 Tag JJ NN NN2 NN6 NN7 NNP NNS NNSH VB VBN Description Adjective Singular noun 2nd order inflectional noun 6th order inflectional noun 7th order inflectional noun Proper noun Plural noun"
D08-1067,H05-1013,0,0.590441,"Missing"
D08-1067,N07-1030,0,0.210791,"uations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised and weakly supervised approaches for the automatic processing of resource-scarce languages has become more important than ever. In fact, several popular weakly supervised learning algorithms s"
D08-1067,P07-1107,0,0.407239,", pages 640–649, c Honolulu, October 2008. 2008 Association for Computational Linguistics inducing coreference partitions on unlabeled documents, rather than classifying mention pairs, via EM clustering (Section 2). In fact, our model combines the best of two worlds: it operates at the document level, while exploiting essential linguistic constraints on coreferent mentions (e.g., gender and number agreement) provided by traditional pairwise classification models. For comparison purposes, we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by Haghighi and Klein (2007), discuss its potential weaknesses and consequently propose three modifications to their model (Section 3). Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model (Section 4). 2 Coreference as EM Clustering In this section, we will explain how we recast unsupervised coreference resolution as EM clustering. We begin by introducing some of the definitions and notations that we will use in this paper. 2.2 The Model As mentioned previously, our generative model operates at the document level, ind"
D08-1067,H05-1003,0,0.039716,"hich mentions (i.e., noun phrases) refer to which real-world entities. The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised a"
D08-1067,N04-4009,0,0.0611463,"able to languages for which little or no annotated data exists. Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised and weakly supervised approaches for the automatic processing of resource-scarce languages has become more important than ever. In fact, several popular weakly supervised learning algorithms such as self-training, co-training (Blum and Mitchell, 1998), and EM (Dempster et al., 1977) have been applied to coreference resolution (Ng and Cardie, 2003) and the related task of pronoun resolution (M¨uller et al., 2002; Kehler et al., 2004; Cherry and Bergsma, 2005). Given a small number of coreference-annotated documents and a large number of unlabeled documents, these weakly supervised learners aim to incrementally augment the labeled data by iteratively training a classifier1 on the labeled data and using it to label mention pairs randomly drawn from the unlabeled documents as COREFERENT or NOT COREFERENT. However, classifying mention pairs using such iterative approaches is undesirable for coreference resolution: since the non-coreferent mention pairs significantly outnumber their coreferent counterparts, the resulting clas"
D08-1067,P04-1018,0,0.736405,"alization. We use a small amount of labeled data for parameter initialization for the two models. Specifically, for evaluations on the BNEWS test data, we use as labeled data one randomly-chosen document from the BNEWS training set, which has 58 true mentions and 102 system mentions. Similarly for NWIRE, where the chosen document has 42 true mentions and 72 system mentions. For our model, we use the labeled document to initialize the parameters. Also, we set N (the number of most probable partitions) to 50 and δ (the start penalty used in the Bell tree) to 0.8, the latter being recommended by Luo et al. (2004). For H&K’s model, we use the labeled data to tune the concentration parameter α. While H&K set α to 0.4 without much explanation, a moment’s thought reveals that the choice of α should reflect the fraction of mentions that appear in a singleton cluster. We therefore estimate this value from the labeled document, yielding 0.4 for true mentions (which is consistent with H&K’s choice) and 0.7 for system mentions. The remaining parameters, the λ’s, are all set to e−4 , following H&K. In addition, as is commonly done in Bayesian approaches, we do not sample entities directly from the conditional d"
D08-1067,H05-1004,0,0.790677,"sults only for the test sets of BNEWS and NWIRE, but verified that the same performance trends can be observed on NPAPER as well. Unlike H&K, who report results using only true mentions (extracted from the answer keys), we show results for true mentions as well as system mentions that were extracted by an in-house noun phrase chunker. The relevant statistics of the BNEWS and NWIRE test sets are shown in Table 3. Scoring programs. To score the output of the coreference models, we employ the commonly-used MUC scoring program (Vilain et al., 1995) and the recently-developed CEAF scoring program (Luo, 2005). In the MUC scorer, recall is computed as 646 Number of documents Number of true mentions Number of system mentions BNEWS 51 2608 5424 NWIRE 29 2630 5197 Table 3: Statistics of the BNEWS and NWIRE test sets the percentage of coreference links in the reference partition that appear in the system partition; precision is computed in the same fashion as recall, except that the roles of the reference partition and the system partition are reversed. As a link-based scoring program, the MUC scorer (1) does not reward successful identification of singleton entities and (2) tends to under-penalize par"
D08-1067,P02-1045,0,0.0833472,"Missing"
D08-1067,P02-1014,1,0.879154,"bly to the modified model. 1 Introduction Coreference resolution is the problem of identifying which mentions (i.e., noun phrases) refer to which real-world entities. The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance"
D08-1067,N03-1023,1,0.534346,"s, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised and weakly supervised approaches for the automatic processing of resource-scarce languages has become more important than ever. In fact, several popular weakly supervised learning algorithms such as self-training, co-training (Blum and Mitchell, 1998), and EM (Dempster et al., 1977) have been applied to coreference resolution (Ng and Cardie, 2003) and the related task of pronoun resolution (M¨uller et al., 2002; Kehler et al., 2004; Cherry and Bergsma, 2005). Given a small number of coreference-annotated documents and a large number of unlabeled documents, these weakly supervised learners aim to incrementally augment the labeled data by iteratively training a classifier1 on the labeled data and using it to label mention pairs randomly drawn from the unlabeled documents as COREFERENT or NOT COREFERENT. However, classifying mention pairs using such iterative approaches is undesirable for coreference resolution: since the non-coreferent m"
D08-1067,N06-1025,0,0.294389,"., noun phrases) refer to which real-world entities. The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised and weakly supervised approac"
D08-1067,J01-4004,0,0.971579,"and compares favorably to the modified model. 1 Introduction Coreference resolution is the problem of identifying which mentions (i.e., noun phrases) refer to which real-world entities. The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given t"
D08-1067,W02-1040,0,0.00926133,"Missing"
D08-1067,J00-4003,0,0.0798338,"for a given mention with a non-zero probability if and only if the corresponding entity and the head of the mention agree in gender and number. We hypothesize that this modification would improve precision. Pronoun-only salience. In Section 3.2.3, we motivate the need for salience using pronouns only, since proper names can to a large extent be resolved using string-matching facilities and are not particularly sensitive to salience. Nominals (especially definite descriptions), though more sensitive to salience than names, can also be resolved by simple stringmatching heuristics in many cases (Vieira and Poesio, 2000; Strube et al., 2002). Hence, we hypothesize that the use of salience for names and nominals would adversely affect their resolution performance, as incorporating salience could diminish the role of string match in the resolution process, according to the sampling equations. Consequently, we modify H&K’s model by limiting the application of salience to the resolution of pronouns only. We hypothesize that this change would improve precision. 4 Evaluation 4.1 Experimental Setup To evaluate our EM-based model and H&K’s model, we use the ACE 2003 coreference corpus, which is composed of three sec"
D08-1067,M95-1005,0,0.954412,"nd a test set. Due to space limitations, we will present evaluation results only for the test sets of BNEWS and NWIRE, but verified that the same performance trends can be observed on NPAPER as well. Unlike H&K, who report results using only true mentions (extracted from the answer keys), we show results for true mentions as well as system mentions that were extracted by an in-house noun phrase chunker. The relevant statistics of the BNEWS and NWIRE test sets are shown in Table 3. Scoring programs. To score the output of the coreference models, we employ the commonly-used MUC scoring program (Vilain et al., 1995) and the recently-developed CEAF scoring program (Luo, 2005). In the MUC scorer, recall is computed as 646 Number of documents Number of true mentions Number of system mentions BNEWS 51 2608 5424 NWIRE 29 2630 5197 Table 3: Statistics of the BNEWS and NWIRE test sets the percentage of coreference links in the reference partition that appear in the system partition; precision is computed in the same fashion as recall, except that the roles of the reference partition and the system partition are reversed. As a link-based scoring program, the MUC scorer (1) does not reward successful identificati"
D08-1067,P03-1023,0,0.0809443,"del. 1 Introduction Coreference resolution is the problem of identifying which mentions (i.e., noun phrases) refer to which real-world entities. The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years. The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)) to the development of rich linguistic features (e.g., Ji et al. (2005), Ponzetto and Strube (2006)) and the exploitation of advanced techniques that involve joint learning (e.g., Daum´e III and Marcu (2005)) and joint inference (e.g., Denis and Baldridge (2007)) for coreference resolution and a related extraction task. The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists. Given the growing importance of multi-lingual pro"
D08-1067,N04-1001,0,\N,Missing
D08-1067,N07-1011,0,\N,Missing
D08-1067,E06-1007,0,\N,Missing
D08-1067,C96-1021,0,\N,Missing
D08-1067,N03-1033,0,\N,Missing
D08-1067,C08-1121,0,\N,Missing
D08-1067,D08-1031,0,\N,Missing
D08-1067,D08-1069,0,\N,Missing
D08-1067,C04-1033,0,\N,Missing
D08-1067,C02-1139,1,\N,Missing
D08-1067,P99-1048,0,\N,Missing
D08-1067,P98-1012,0,\N,Missing
D08-1067,C98-1012,0,\N,Missing
D08-1067,P08-1096,0,\N,Missing
D08-1067,J95-2003,0,\N,Missing
D08-1067,W05-0612,0,\N,Missing
D08-1067,J94-4002,0,\N,Missing
D08-1067,P09-1074,0,\N,Missing
D08-1067,P03-2012,0,\N,Missing
D08-1067,P08-1002,0,\N,Missing
D08-1067,J86-3001,0,\N,Missing
D08-1067,P98-2143,0,\N,Missing
D08-1067,C98-2138,0,\N,Missing
D08-1067,N04-1037,0,\N,Missing
D08-1067,P05-1045,0,\N,Missing
D08-1067,W04-0707,0,\N,Missing
D09-1061,W02-1011,0,0.0218193,"e, Sentiment-wise, or Otherwise? Identifying the Hidden Dimension for Unsupervised Text Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract where the goal is to cluster (or classify) a set of documents (e.g., reviews) according to the polarity (e.g., “thumbs up” or “thumbs down”) expressed by the author in an unsupervised manner. Despite the large amount of recent work on sentiment analysis and opinion mining, much of it has focused on supervised methods (e.g., Pang et al. (2002), Kim and Hovy (2004), Mullen and Collier (2004)). One weakness of these existing supervised polarity classification systems is that they are typically domain- and language-specific. Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a paralle"
D09-1061,D08-1014,0,0.0065067,"y are typically domain- and language-specific. Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is “similar” enough to the target domain (Blitzer et al., 2007). When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives. Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community. Turney’s (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is use"
D09-1061,W09-1110,0,0.0239747,"along the sentiment dimension, possibly in an interactive manner. One way to do this would be to ask the user to annotate a small number of reviews with polarity information, possibly through an active learning procedure to minimize human intervention (Dredze and Crammer, 2008). Another way would be to have the user explicitly identify the relevant features (in our case, the sentiment-bearing words) at the beginning of the clustering process (Liu et al., 2004), or incrementally construct the set of relevant features in an interactive fashion (Bekkerman et al., 2007; Raghavan and Allan, 2007; Roth and Small, 2009). In addition, the user may supply constraints on which pairs of documents must or must not appear in the same cluster (Wagstaff et al., 2001), or simply tell the algorithm whether two clusters should be merged or split during the clustering process (Balcan and Blum, 2008). It is worth noting that many of these feedback mechanisms were developed by machine learning researchers for general clustering tasks and not for sentiment-based clustering. 2 Spectral Clustering When given a clustering task, an important question to ask is: which clustering algorithm should we use? A popular choice is k-me"
D09-1061,P07-1056,0,0.372226,"one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is “similar” enough to the target domain (Blitzer et al., 2007). When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives. Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community. Turney’s (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically. While traditional work on text clustering"
D09-1061,P02-1053,0,0.00524102,"Missing"
D09-1061,D08-1058,0,0.023955,"in- and language-specific. Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is “similar” enough to the target domain (Blitzer et al., 2007). When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives. Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community. Turney’s (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict"
D09-1061,P08-2059,0,0.0108274,"s the sentiment dimension hidden (i.e., less prominent) as far as clustering is concerned. Therefore, there is no guarantee that the clustering algorithm will automatically produce a sentiment-based clustering of the reviews. Hence, it is important for a user to provide feedback on the clustering process to ensure that the reviews are clustered along the sentiment dimension, possibly in an interactive manner. One way to do this would be to ask the user to annotate a small number of reviews with polarity information, possibly through an active learning procedure to minimize human intervention (Dredze and Crammer, 2008). Another way would be to have the user explicitly identify the relevant features (in our case, the sentiment-bearing words) at the beginning of the clustering process (Liu et al., 2004), or incrementally construct the set of relevant features in an interactive fashion (Bekkerman et al., 2007; Raghavan and Allan, 2007; Roth and Small, 2009). In addition, the user may supply constraints on which pairs of documents must or must not appear in the same cluster (Wagstaff et al., 2001), or simply tell the algorithm whether two clusters should be merged or split during the clustering process (Balcan"
D09-1061,C04-1200,0,0.107883,"r Otherwise? Identifying the Hidden Dimension for Unsupervised Text Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract where the goal is to cluster (or classify) a set of documents (e.g., reviews) according to the polarity (e.g., “thumbs up” or “thumbs down”) expressed by the author in an unsupervised manner. Despite the large amount of recent work on sentiment analysis and opinion mining, much of it has focused on supervised methods (e.g., Pang et al. (2002), Kim and Hovy (2004), Mullen and Collier (2004)). One weakness of these existing supervised polarity classification systems is that they are typically domain- and language-specific. Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a parallel corpus/machine tran"
D09-1061,W04-3253,0,\N,Missing
D09-1101,P98-1012,0,0.0740885,", person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5 , a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3 -CEAF (Luo, 2005). Evaluation 5.1 Experimental Setup Corpus. We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Webblog (wb), Usenet (un), and conversational telephone speech (cts). The number of documents belonging to each source is shown in Table 2. For evaluation, we partition the 599 documents into a training set and a test set f"
D09-1101,P99-1048,0,0.0296764,"een the cluster and the mention) but also lexical and grammatical compatibility, for instance. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., Mu¨ ller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and c"
D09-1101,D08-1031,0,0.53105,"e when Xb is true between mk and at least half (but not all) of the mentions in cj ; and (4) ALL - Xb is true when Xb is true between mk and each mention in cj . Hence, for each Xb , exactly one of these four cluster-level features evaluates to true. [Barack Obama]11 nominated [Hillary Rodham Clinton]22 as [[his]13 secretary of state]34 on [Monday]45 . [He]16 ... Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1. These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically. As can be seen, the features are divided into four blocks. The first two blocks consist of features that describe the properties of mj and mk , respectively, and the last two blocks of features describe the relationship between mj and mk . The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent. If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a skewed class distribution that will typically"
D09-1101,P08-1002,0,0.0228197,"cular, we attempt to determine the compatibility between a cluster and an active mention, using factors that determine not only salience (e.g., the distance between the cluster and the mention) but also lexical and grammatical compatibility, for instance. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., Mu¨ ller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new class"
D09-1101,N07-1011,0,0.419025,"before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj . Each instance i(mj , mk ) represents mj and 970 an active mention mk is coreferent with a partial cluster cj that precedes mk . Each training instance, i(cj , mk ), represents cj and mk . The features for an instance can be divided into two types: (1) features that describe mk (i.e, those shown in the second block of Table 1), and (2) cluster-level features, which describe the relationship between cj and mk . Motivated by previous work (Luo et al., 2004; Culotta et al., 2007; Yang et al., 2008), we create cluster-level features from mention-pair features using four predicates: NONE, MOST- FALSE, MOST- TRUE, and ALL . Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued. Then, for each resulting binaryvalued feature Xb , we create four binary-valued cluster-level features: (1) NONE - Xb is true when X b is false between mk and each mention in cj ; (2) MOST- FALSE - X b is true when X b is true between mk and less than half (but at least one) of the mentio"
D09-1101,N07-1030,0,0.233055,"s of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly and not independently. Entity-mention coreference models. Luo et al. (2004) represent one of the earliest attempts to investigate learning-based entity-mention models. They use the ANY predicate to generate clusterlevel features as follows: given a binary-valued feature X defined over a pair of mentions, they introduce an ANY- X cluster-level feature, which has the value TRUE if X is true between the active mention and any mention in the preceding cluster under consideration. Contr"
D09-1101,D08-1069,0,0.743479,"to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in their expressiveness: the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., Clinton). To address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is most probable given an active mention (e.g., Denis and Baldridge (2008)). Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear whic"
D09-1101,P05-1045,0,0.0126875,"of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): Tokens in a window of 7: {wi−3 , . . . , wi+3 }. Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al. (1999)). Morphological (8): wi ’s prefixes and suffixes of length one, two, three, and four. Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5 , a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scorin"
D09-1101,N04-1001,0,0.0217908,"ted to the ranker. If the additional test instance is assigned the highest rank value by the ranker, then mk is classified as discourse-new and will not be resolved. Otherwise, mk is linked to the cluster that has the highest rank. As before, all partial clusters preceding mk are formed incrementally based on the predictions of the ranker for the first k − 1 mentions. 5 bn 60 bc 226 nw 106 wl 119 un 49 cts 39 Table 2: Statistics for the ACE 2005 corpus tomatically identified mentions). To extract system mentions from a test text, we trained a mention extractor on the training texts. Following Florian et al. (2004), we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention. Hence, to learn the extractor, we create one training instance for each token in a training text and derive its class value (one of b, i, and o) from the annotated data. Each instance represents wi , the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7):"
D09-1101,J95-2003,0,0.252295,"ention model, and the mention-ranking model. For illustrative purposes, we will use the text segment shown in Figure 1. Each mention m in the segment is annotated as [m]cid mid , where mid is the mention id and cid is the id of the cluster to which m belongs. As we can see, the mentions are partitioned into four sets, with Barack Obama, his, and he in one cluster, and each of the remaining mentions in its own cluster. Mention ranking. The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al. (1995), Walker et al. (1998), and Mitkov (2002)). However, mention ranking has been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and Baldridge (2008) train a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, 3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj . Each instance i(mj , mk ) represents mj and 970 an active mention mk is coreferent with a partial cluster c"
D09-1101,J86-3001,0,0.370759,"gned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters. By contrast, our resolution strategy is learned without applying hand-coded conBridging the gap between machine-learning approaches and linguistically-motivated approaches to coreference resolution. While machine learning approaches to coreference resolution have received a lot of attention since the mid90s, popular learning-based coreference frameworks such as the mention-pair model are arguably rather unsatisfactory from a linguistic point of view. In particular, they have not leveraged 969 which ranks only two candidate antecedents at a time. Unlike ours,"
D09-1101,N04-1037,0,0.0218631,"We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP advan"
D09-1101,C96-1021,0,0.077408,"like ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. straints in a separate filtering step. In particular, we attempt to determine the compatibility between a cluster and an active mention, using factors that determine not only salience (e.g., the distance between the cluster and the mention) but also lexical and grammatical compatibility, for instance. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., Mu¨ ller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There"
D09-1101,J94-4002,0,0.0395001,"s — by a large margin. Also, our joint-learning approach to discourse-new detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture. Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see McCallum and Wellner (2004)). 2 Related Work Heuristic-based cluster ranking. As mentioned previously, the work most related to ours is Lappin and Leass (1994), whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster. Nevertheless, Lappin and Leass’s work differs from ours in several respects. First, they only tackle pronoun resolution rather than the full coreference task. Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolver"
D09-1101,P04-1018,0,0.317785,"s to be considered simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best). To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al. (2004), Yang et al. (2004)). Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster. Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models. Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking modTraditional learning-based"
D09-1101,H05-1004,0,0.468291,"d honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5 , a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3 -CEAF (Luo, 2005). Evaluation 5.1 Experimental Setup Corpus. We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Webblog (wb), Usenet (un), and conversational telephone speech (cts). The number of documents belonging to each source is shown in Table 2. For evaluation, we partition the 599 documents into a training set and a test set following a 80/20 ratio, en"
D09-1101,P98-2143,0,0.14337,"oal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster. Nevertheless, Lappin and Leass’s work differs from ours in several respects. First, they only tackle pronoun resolution rather than the full coreference task. Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters. By contrast, our resolution strategy is learned without applying hand-coded conBridging the gap between machine-learning approaches and linguistically-motivated approaches to coreference resolution. While machi"
D09-1101,E06-1007,0,0.0440923,"Missing"
D09-1101,C02-1139,1,0.920453,"entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, c Singapore, 6-7 August 2009."
D09-1101,P02-1014,1,0.913116,"entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, c Singapore, 6-7 August 2009."
D09-1101,W04-0707,0,0.060726,"e clusters is the best to link to an active mention using a learned cluster ranker. In addition, we show how discourse-new detection (i.e., the task of determining whether a mention introduces a new entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework. It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see Poesio et al. (2004) for an overview). As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)). Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem. In sum, we believe our work makes three main contributions to coreference resolution: Revealing the importance of adopting the right model. While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et a"
D09-1101,N06-1025,0,0.171672,"ow our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP advances in discourse-based anaph"
D09-1101,J01-4004,0,0.972642,"mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In 968 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, c Singapo"
D09-1101,P09-1074,0,0.441124,"p.sourceforge.net grams. As we can see, this baseline achieves Fscores of 54.3–70.0 and 53.4–62.5 for true mentions and system mentions, respectively. There is a complication, however. When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key. If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). However, this is not the case when system mentions are used. The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. We propose a simple solution to this problem: we remove all and only those twinless system mentions that are singletons before applying B3 and CEAF. The reason is simple: since the coreference resolver has successfully identified these mentions as singletons, it should not be penalized, and remov"
D09-1101,N03-1033,0,0.0233229,"he annotated data. Each instance represents wi , the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): Tokens in a window of 7: {wi−3 , . . . , wi+3 }. Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al. (1999)). Morphological (8): wi ’s prefixes and suffixes of length one, two, three, and four. Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5 , a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 prec"
D09-1101,P03-2012,0,0.0321075,"nedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., Mu¨ ller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly and not independently. Entity-mention corefe"
D09-1101,M95-1005,0,0.261281,"hat are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5 , a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3 -CEAF (Luo, 2005). Evaluation 5.1 Experimental Setup Corpus. We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Webblog (wb), Usenet (un), and conversational telephone speech (cts). The number of documents belonging to each source is shown in Table 2. For evaluation, we partition the 599 documents into a"
D09-1101,P03-1023,0,0.169781,"Missing"
D09-1101,C04-1033,0,0.111254,"simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best). To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al. (2004), Yang et al. (2004)). Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster. Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models. Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking modTraditional learning-based coreference resolve"
D09-1101,P08-1096,0,0.52918,"erview). As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)). Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem. In sum, we believe our work makes three main contributions to coreference resolution: Revealing the importance of adopting the right model. While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et al., 2004; Yang et al., 2008), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches. This suggests that the use of an appropriate learning framework can bring us a long way towards highperformance coreference resolution. The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking. We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6. Proposing a simple, yet effective"
D10-1023,P05-1018,0,0.0469773,"-of-the-art essay scoring software such as e-rater V.2 (Attali and Burstein, 2006) employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension. Our goal in this paper is to develop a computational model for the organization of student es229 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics says. While many models of text coherence have been developed in recent years (e.g., Barzilay and Lee (2004), Barzilay and Lapata (2005), Soricut and Marcu (2006), Elsner et al. (2007)), the same is not true for text organization. One reason is the availability of training and test data for coherence modeling. Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text. On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models. We believe that student essays are an ideal source of well- and poorly-organized texts. We evaluate our organization"
D10-1023,W02-1022,0,0.0387589,"assign the paragraph the label having the most support.1 1 Space limitations preclude a complete listing of these para5 Heuristic-Based Organization Scoring Having applied labels to each paragraph in an essay, how can we use these labels to predict the essay’s score? Recall that the importance of each paragraph label stems not from the label itself, but from the sequence of labels it appears in. Motivated by this observation, we exploit a technique that is commonly used in bioinformatics — sequence alignment. While sequence alignment has also been used in text and paraphrase generation (e.g., Barzilay and Lee (2002; 2003)), it has not been extensively applied to other areas of language processing, including essay scoring. In this section, we will present two heuristic approaches to organization scoring, one based on aligning paragraph sequences and the other on aligning sentence sequences. 5.1 Aligning Paragraph Sequences As mentioned above, our first approach to heuristic organization scoring involves aligning paragraph sequences. Specifically, this approach operates in two steps. Given an essay e in the test set, we (1) find the k essays in the training set that are most similar to e via paragraph seq"
D10-1023,N03-1003,0,0.0296352,"Missing"
D10-1023,N04-1015,0,0.0264369,"n of essay quality, state-of-the-art essay scoring software such as e-rater V.2 (Attali and Burstein, 2006) employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension. Our goal in this paper is to develop a computational model for the organization of student es229 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics says. While many models of text coherence have been developed in recent years (e.g., Barzilay and Lee (2004), Barzilay and Lapata (2005), Soricut and Marcu (2006), Elsner et al. (2007)), the same is not true for text organization. One reason is the availability of training and test data for coherence modeling. Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text. On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models. We believe that student essays are an ideal source of well- and poorly-organized texts."
D10-1023,H05-1091,0,0.0252836,"design heuristics to extract the desired parse-based features from parse trees. For certain tasks, designing a good set of heuristics can be time-consuming and sometimes difficult. On the other hand, SVMs enable a parse tree to be employed directly as a structured feature, obviating the need to design heuristics to extract information from potentially complex structures. However, structured features have only been applied to a handful of NLP tasks such as semantic role labeling (Moschitti, 2004), syntactic parsing and named entity identification (Collins and Duffy, 2002), relation extraction (Bunescu and Mooney, 2005), and coreference resolution (Versley et al., 2008). Our goal here is to explore this rarely-exploited capability of SVMs for the task of essay scoring. While the vast majority of previous NLP work on using structured features have involved tree kernels, we employ a kernel that is rarely investigated in NLP: string kernels (Lodhi et al., 2002). Informally, a string kernel aims to efficiently compute the similarity between two strings (or sequences) of symbols based on the similarity of their subsequences. We apply string kernels to essay scoring as follows: we represent each essay using its pa"
D10-1023,P02-1034,0,0.0463653,"from a parse tree as features, we will need to design heuristics to extract the desired parse-based features from parse trees. For certain tasks, designing a good set of heuristics can be time-consuming and sometimes difficult. On the other hand, SVMs enable a parse tree to be employed directly as a structured feature, obviating the need to design heuristics to extract information from potentially complex structures. However, structured features have only been applied to a handful of NLP tasks such as semantic role labeling (Moschitti, 2004), syntactic parsing and named entity identification (Collins and Duffy, 2002), relation extraction (Bunescu and Mooney, 2005), and coreference resolution (Versley et al., 2008). Our goal here is to explore this rarely-exploited capability of SVMs for the task of essay scoring. While the vast majority of previous NLP work on using structured features have involved tree kernels, we employ a kernel that is rarely investigated in NLP: string kernels (Lodhi et al., 2002). Informally, a string kernel aims to efficiently compute the similarity between two strings (or sequences) of symbols based on the similarity of their subsequences. We apply string kernels to essay scoring"
D10-1023,N07-1055,0,0.0128095,"(Attali and Burstein, 2006) employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension. Our goal in this paper is to develop a computational model for the organization of student es229 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics says. While many models of text coherence have been developed in recent years (e.g., Barzilay and Lee (2004), Barzilay and Lapata (2005), Soricut and Marcu (2006), Elsner et al. (2007)), the same is not true for text organization. One reason is the availability of training and test data for coherence modeling. Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text. On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models. We believe that student essays are an ideal source of well- and poorly-organized texts. We evaluate our organization model on a data set of 1003 essays annotated wi"
D10-1023,N04-1024,0,0.621233,"eakness of many existing essay scoring engines such as IntelliMetric (Elliot, 2001) and Intelligent Essay Assessor (Landauer et al., 2003) is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, and relevance to prompt (Higgins et al., 2004). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Nevertheless, there is an essay scoring dimension for which few computational models have been developed — organization. Organization refers to the structure of an essay. A high score on organization means that writers introduce a topic, state their position on that topic, support their position, and conclude, often by restating their position (Silva, 1993). A well-organized essay is structured in a way that logically develops a"
D10-1023,P04-1043,0,0.0183944,"we did with the linear kernel. Hence, if we want to use information from a parse tree as features, we will need to design heuristics to extract the desired parse-based features from parse trees. For certain tasks, designing a good set of heuristics can be time-consuming and sometimes difficult. On the other hand, SVMs enable a parse tree to be employed directly as a structured feature, obviating the need to design heuristics to extract information from potentially complex structures. However, structured features have only been applied to a handful of NLP tasks such as semantic role labeling (Moschitti, 2004), syntactic parsing and named entity identification (Collins and Duffy, 2002), relation extraction (Bunescu and Mooney, 2005), and coreference resolution (Versley et al., 2008). Our goal here is to explore this rarely-exploited capability of SVMs for the task of essay scoring. While the vast majority of previous NLP work on using structured features have involved tree kernels, we employ a kernel that is rarely investigated in NLP: string kernels (Lodhi et al., 2002). Informally, a string kernel aims to efficiently compute the similarity between two strings (or sequences) of symbols based on th"
D10-1023,P06-2103,0,0.0309292,"ftware such as e-rater V.2 (Attali and Burstein, 2006) employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension. Our goal in this paper is to develop a computational model for the organization of student es229 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics says. While many models of text coherence have been developed in recent years (e.g., Barzilay and Lee (2004), Barzilay and Lapata (2005), Soricut and Marcu (2006), Elsner et al. (2007)), the same is not true for text organization. One reason is the availability of training and test data for coherence modeling. Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text. On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models. We believe that student essays are an ideal source of well- and poorly-organized texts. We evaluate our organization model on a data set of 10"
D10-1023,C08-1121,0,0.0167495,"atures from parse trees. For certain tasks, designing a good set of heuristics can be time-consuming and sometimes difficult. On the other hand, SVMs enable a parse tree to be employed directly as a structured feature, obviating the need to design heuristics to extract information from potentially complex structures. However, structured features have only been applied to a handful of NLP tasks such as semantic role labeling (Moschitti, 2004), syntactic parsing and named entity identification (Collins and Duffy, 2002), relation extraction (Bunescu and Mooney, 2005), and coreference resolution (Versley et al., 2008). Our goal here is to explore this rarely-exploited capability of SVMs for the task of essay scoring. While the vast majority of previous NLP work on using structured features have involved tree kernels, we employ a kernel that is rarely investigated in NLP: string kernels (Lodhi et al., 2002). Informally, a string kernel aims to efficiently compute the similarity between two strings (or sequences) of symbols based on the similarity of their subsequences. We apply string kernels to essay scoring as follows: we represent each essay using its paragraph function label sequence, and employ a strin"
D11-1099,P95-1017,0,0.131464,"reference research for more than a decade since its appearance in the mid-1990s, a number of new coreference models have been proposed in recent years. To investigate whether these newer, presumably more sophisticated, coreference models can better exploit the automatically acquired information-status information, we will evaluate the usefulness of information-status information when used in combination with two different coreference models, the aforementioned mention-pair model and the recently-developed cluster-ranking model. 7.1.1 Mention-Pair Model The mention-pair (MP) model, proposed by Aone and Bennett (1995) and McCarthy and Lehnert (1995), is a classifier that determines whether two NPs are co-referring or not. Each instance i(NPj , NPk ) corresponds to two NPs, NPj and NPk , and is represented by 39 features. Table 1 of Rahman and Ng (2009) contains a detailed description of these features. Linguistically, they can be divided into four categories: string-matching, grammatical, semantic, and positional. They can also be categorized based on whether they are relational or not. Specifically, relational features capture the relationship between NPj and NPk , whereas non-relational features capture"
D11-1099,P99-1048,0,0.0324443,"pact of knowledge of anaphoricity on the information-status classifiers. NP is anaphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experime"
D11-1099,N07-1011,0,0.0450378,"ker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding cluster) can be employed. Since the CR model ranks preceding clusters, a training instance i(cj , NPk ) represents a preceding cluster cj and an anaphoric NP NPk . Each instance consists of features that are computed based solely on NPk as well as cluster-level features, which describe the relationship between cj and NPk . Motivated in part by Culotta et al. (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOSTFALSE, MOST- TRUE, and ALL. Specifically, for each relational feature X, we first convert X into an equivalent set of binary-valued features if it is multivalued. Then, for each resulting binary-valued feature Xb , we create four binary-valued cluster-level features: (1) NONE - Xb is true when Xb is false between NPk and each NP in cj ; (2) MOST- FALSE - Xb is true when Xb is true between NPk and less than half (but at least one) of the NPs in cj ; (3) MOST- TRUE X b is true when"
D11-1099,N07-1030,0,0.0732018,"-status classifiers. NP is anaphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity features we assembled in step"
D11-1099,D08-1069,0,0.0258917,"but not how good a candidate antecedent is relative to other candidates. So, it fails to answer the critical question of which candidate antecedent is most probable. Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision. 7.1.2 Cluster-Ranking Model The cluster-ranking (CR) model, proposed by Rahman and Ng (2009), addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) and the mention-ranking model (e.g., Denis and Baldridge (2008)). Specifically, the CR model ranks the preceding clusters for an 4 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. 1076 active NP so that the highest-ranked cluster is the one to which the active NP should be linked. Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding clus"
D11-1099,P04-1018,0,0.0557686,"nly determines how good a candidate antecedent is relative to the active NP, but not how good a candidate antecedent is relative to other candidates. So, it fails to answer the critical question of which candidate antecedent is most probable. Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision. 7.1.2 Cluster-Ranking Model The cluster-ranking (CR) model, proposed by Rahman and Ng (2009), addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) and the mention-ranking model (e.g., Denis and Baldridge (2008)). Specifically, the CR model ranks the preceding clusters for an 4 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. 1076 active NP so that the highest-ranked cluster is the one to which the active NP should be linked. Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level"
D11-1099,H05-1004,0,0.0231599,"mationstatus classifier to each discourse entity in the test set, and have the coreference models resolve all and only those NPs that are labeled as old by the information-status classifier. Our decision to allow the coreference models to resolve only the old entities is motivated by the fact that med and new entities have not been previously introduced in the conversation and therefore do not have antecedents. The NPs used by the coreference models are the same as those accessible to the information-status classifier. We employ two scoring programs, B3 (Bagga and Baldwin, 1998) and φ3 -CEAF (Luo, 2005), to score the output of a coreference model. Given a goldstandard (i.e., key) partition, KP , and a systemgenerated (i.e., response) partition, RP , B3 computes the recall and precision of each NP and averages these values at the end. Specifically, for each NP, NPj , B3 first computes the number of NPs that appear in both KPj and RPj , the clusters containing NPj in KP and RP , respectively, and then divides this number by |KPj |and |RPj |to obtain the recall and precision of NPj , respectively. On the other hand, CEAF finds the best one-to-one alignment between the key clusters and the respo"
D11-1099,P04-1043,0,0.0472066,"d its context. Note that using two labels, X and Y , enables the kernel to distinguish the discourse entity under consideration from its context within this substructure. In addition, we simply use a single node (Y) to represent the discourse entity, since any NP-internal information has presumably been captured by the flat features. We compute these structured features using hand-annotated parse trees. While structured features have been employed for a multitude of tasks in syntax, semantics, and information extraction such as syntactic parsing (e.g., Collins (2002)), semantic parsing (e.g., Moschitti (2004)), named entity recognition (e.g., Cumby and Roth (2003), and relation extraction (e.g., Zelenko et al. (2003)), the same is not true for discourse processing tasks. We hope that our use of structured features for information-status classification can promote their use in discourse processing. 5.3 Combining Kernels Recall that the flat features are computed using a linear kernel, while the structured features are computed using a tree kernel. If we want our learner to make use of more than one of these types of features, 1073 we need to employ a composite kernel to combine them. Specifically,"
D11-1099,C02-1139,1,0.881477,"n dependent on whether the entity was previously mentioned in the dialogue. On the other hand, med and new are similar in that it may sometimes be difficult even for a human to determine whether certain entities should be labeled as med or new, since the decision depends on whether she believes these entities are generally known or not. 6.2 Relation to Anaphoricity Determination Anaphoricity determination refers to the task of determining whether an NP is anaphoric or not, where an NP is considered anaphoric if it is part of a (nonsingleton) coreference chain but is not the head of the chain (Ng and Cardie, 2002). In other words, an old med new Accuracy Anaphoricity R P F 91.4 86.6 88.9 84.3 63.1 72.2 30.8 66.4 42.1 74.7 Baseline+Ana R P F 91.3 87.3 89.3 84.9 64.1 73.1 31.1 66.9 42.5 75.1 Baseline+Lexical+Ana R P F 90.8 91.7 91.3 92.3 64.7 76.1 32.9 68.7 44.5 77.6 Baseline+Both+Ana R P F 92.8 94.9 93.9 88.7 71.1 78.9 34.1 71.7 46.2 82.0 Table 5: Impact of knowledge of anaphoricity on the information-status classifiers. NP is anaphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entit"
D11-1099,P04-1020,1,0.825374,"information-status classifiers. NP is anaphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity fe"
D11-1099,N09-1065,1,0.666339,"it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity features we assembled in step (1). Results with the anaphoricity"
D11-1099,nissim-etal-2004-annotation,0,0.167201,"Missing"
D11-1099,W06-1612,0,0.238685,"describe our baseline system, which adopts a machine learning approach to determining the information status of a discourse entity. Building SVM classifiers for information-status determination. We employ the support vector machine (SVM) learner as implemented in the SVMlight package (Joachims, 1999) to train three binary classifiers, one for predicting each of the three possible classes (i.e., new, old, and med), using a linear kernel in combination with the oneversus-all training scheme.2 Each training instance represents a single NP and consists of the seven morpho-syntactic features that Nissim (2006) used in her learning-based approach (see Table 2 for an overview). Following Nissim, we extract the NPs directly from the gold-standard annotations, but the features are computed entirely automatically. 2 SVM was chosen because it provides the option to employ kernels. The reason why we train three binary classifiers rather than just one multi-class classifier (using SVMmulticlass ) is that SVMmulticlass does not permit the use of a non-linear kernel, which we will need to incorporate structured features later on. 1071 Feature full prev mention mention time partial prev mention determiner NP"
D11-1099,D09-1101,1,0.782894,"ion and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity features we assembled in step (1). Results with the anaphoricity features are shown in Table 5. Under Anaphoricity, we have the results obtained using only the 29 anaphoricity features. As we can see, these results are comparable to those obtained using the Baseline features. Comparing each of Baseline+Ana and Baseline+Lexical+Ana with the corresponding experiments in Table 3, we see that the addition of anaphoricity features yields a mild performanc"
D11-1099,J01-4004,0,0.523452,"Missing"
D11-1099,P98-2204,0,0.0505298,"re not widely known to researchers working outside the area of discourse processing, in this section we will explain them in more detail. The terms old and new information have meant a variety of things over the years (Allerton, 1978; Prince, 1981; Horn, 1986). Since we use Nissim et al.’s (2004) corpus for training and evaluation, the definitions of these concepts we present here are those that Nissim et al. used to annotate their corpus. According to Nissim et al., their definitions are built upon Prince’s (1981), and the categorization into old, new, and mediated entities resemble those of Strube (1998) and Eckert and Strube (2001). Old. As mentioned before, an entity is old if it is both known to the hearer and has been mentioned in the conversation. More precisely, an entity is old if (1) it is coreferential with an entity introduced earlier, (2) it is a generic pronoun, or (3) it is a personal pronoun referring to the dialogue participants. To exemplify, consider the following sentences. (1) (2) I was angry that he destroyed my tent. You cannot leave until the test is over. In Example 1, my is an old entity because it is coreferent with I. In Example 2, You is an old entity because it is"
D11-1099,P03-2012,0,0.0926198,"phoricity on the information-status classifiers. NP is anaphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anap"
D11-1099,C08-1121,0,0.0362941,"naphoric if and only if it has an antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity features we assembled in step (1). Results with the a"
D11-1099,J00-4003,0,0.369419,"Missing"
D11-1099,P08-1096,0,0.0372033,"good a candidate antecedent is relative to the active NP, but not how good a candidate antecedent is relative to other candidates. So, it fails to answer the critical question of which candidate antecedent is most probable. Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision. 7.1.2 Cluster-Ranking Model The cluster-ranking (CR) model, proposed by Rahman and Ng (2009), addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) and the mention-ranking model (e.g., Denis and Baldridge (2008)). Specifically, the CR model ranks the preceding clusters for an 4 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. 1076 active NP so that the highest-ranked cluster is the one to which the active NP should be linked. Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., fea"
D11-1099,D09-1102,0,0.110216,"antecedent. Given this definition, anaphoricity determination bears resemblance to information-status classification. For instance, an old entity is anaphoric, since it has been introduced earlier in the conversation and therefore have an antecedent. Similarly, a new or med entity is non-anaphoric, since the entity has not been previously introduced in the conversation and therefore cannot have an antecedent. There has been a lot of recent work on anaphoricity determination (e.g., Bean and Riloff (1999), Uryupina (2003), Ng (2004), Denis and Baldridge (2007), Versley et al. (2008), Ng (2009), Zhou and Kong (2009)). Given the similarity between this task and information-status classification, a natural question is: will the anaphoricity features previously developed by coreference researchers be helpful for information-status classification? To answer this question, we (1) assemble a feature set composed of the 26 anaphoricity features previously used by Rahman and Ng (2009),3 and then (2) repeat the experiments in Table 3, except that we augment the feature set used in each of these experiments with the anaphoricity features we assembled in step (1). Results with the anaphoricity features are shown in"
D11-1099,D08-1067,1,\N,Missing
D11-1099,C98-2199,0,\N,Missing
D11-1099,P02-1062,0,\N,Missing
D12-1071,P98-1013,0,0.0303568,", it is unlikely that Google will return meaningful counts to us. The reason is that both candidate antecedents in the sentence are proper names belonging to the same type (which in this case is P ERSON). However, in some cases, we may be able to generate more meaningful queries from such kind of sentences. Consider the following sentence: (4) John killed Jim, so he was arrested. To generate meaningful queries, we make one observation: John and Jim played different roles in a kill event. Hence, we can replace these proper names with their roles. We propose to obtain these roles from FrameNet (Baker et al., 1998). More generally, for each proper name e in a given sentence, we (1) determine the event in which e is involved (using the Stanford dependency parser); (2) search for the FrameNet frame corresponding to the event as well as e’s role in the event; and (3) replace the name with its FrameNet role. In our example, since both names are involved in the kill event, we retrieve the FrameNet frame for kill. Given that John and Jim are the subject and object of kill, we can extract their semantic roles directly from the frame, which are killer and victim, respectively.6 Consequently, we replace the two"
D12-1071,P06-1005,0,0.023603,"exical Features We exploit the coreference-annotated training documents by creating lexical features from them. These lexical features can be divided into two categories, depending on whether they are computed based on the candidate antecedents. Let us begin with the antecedent-independent features. Assuming that W is an arbitrary word in a sentence S that is not part of a candidate antecedent and Conn is the connective in S, we create three types of binary-valued antecedent-independent features, namely (1) unigrams, where we create one 9 We use the same formula as described in Section 4.2 of Bergsma and Lin (2006) to compute MI values. feature for each W ; (2) word pairs, where we create features by pairing each W appearing before Conn with each W appearing after Conn, excluding adjective-noun and noun-adjective pairs10 ; and (3) word triples, where we augment each word pair in (2) with Conn. The value of each feature f indicates the presence or absence of f in S. Next, we compute the antecedent-dependent features. Let (1) HC1 and HC2 be the head words of candidate antecedents C1 and C2 , respectively; (2) VC1 , VC2 , and VA be the verbs governing C1 , C2 , and the target pronoun A, respectively; and ("
D12-1071,P02-1011,0,0.030198,"nt machine that can truly understand natural language. To sum up, an important contribution of our work is that it opens up a new line of research involving a problem whose solution requires a deeper understanding of a text. With recent advances in knowledge extraction from text, we believe that time is ripe to tackle this problem. It is worth noting that some researchers have focused on other kinds of anaphors that are hard to resolve, including bridging anaphors (e.g., Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), an"
D12-1071,P08-1090,0,0.015773,"able 2. 4.1 Narrative Chains Consider the following sentence: (2) Ed punished Tim because he tried to escape. Humans resolve he to Tim by exploiting the world knowledge that someone who tried to escape is bad and therefore should be punished. Such kind of knowledge can be extracted from narrative chains. Narrative chains are partially ordered sets of events centered around a common protagonist, aiming to encode the kind of knowledge provided by scripts (Schank and Abelson, 1977). While scripts are hand-written, narrative chains can be learned from unannotated text. Below is a chain learned by Chambers and Jurafsky (2008): borrow-s invest-s spend-s pay-s raise-s lend-s As we can see, a narrative chain is composed of a sequence of events (verbs) together with the roles of the protagonist. Here, “s” denotes the subject role, even though a chain can contain a mix of “s” and “o” (the object role). From this chain, we know that the person who borrows something (probably money) may invest, spend, pay, or lend it. We employ narrative chains to heuristically predict the antecedent for the target pronoun, and encode the prediction as a feature. The heuristic decision procedure operates as follows. Given a sentence, we"
D12-1071,E09-1018,0,0.0306846,"mplies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference 778 researcher, but it is indeed the state of"
D12-1071,de-marneffe-etal-2006-generating,0,0.0356684,"Missing"
D12-1071,D08-1069,0,0.0428046,"Missing"
D12-1071,D09-1120,0,0.0532557,"in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference 778 researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora such as MUC, ACE, and OntoNotes (Bagga, 1998; Haghighi and Klein, 2009). In fact, the Stanford coreference resolver (Lee et al., 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. In fact, being able to automatically resolve difficult pronouns has broader implications in artifici"
D12-1071,W03-2604,0,0.0366131,"ume below that there are two candidate antecedents per sentence. 3 Machine Learning Framework Since our goal is to determine which of the two candidate antecedents is the correct antecedent for the target pronoun in each sentence, our system assumes as input the sentence, the target pronoun, and the two candidate antecedents. We employ machine learning to combine the features derived from different knowledge sources. Specifically, we employ a ranking-based approach. Ranking-based approaches have been shown to outperform their classification-based counterparts (Denis and Baldridge, 2007, 2008; Iida et al., 2003; Yang et al., 2003). Given a pronoun and two candidate antecedents, we aim to train a ranking model that ranks the two candidates such that the correct antecedent is assigned a higher rank. More formally, given training sentence Sk containing target pronoun Ak , correct antecedent Ck and incorrect antecedent Ik , we create two feature vectors, xCAk and xIAk , where xCAk is generated from Ak and Ck , and xIAk is generated from Ak and Ik . The training set consists of ordered pairs of feature vectors (xCAk , xIAk ), and the goal of the training procedure is to acquire a ranker that minimizes th"
D12-1071,W11-1913,0,0.0227229,"me in the rest of the paper that i1 and i2 are the feature vectors corresponding to the first candidate antecedent and the second candidate an2 Throughout the paper, the subject/object of an event refers to its deep rather than surface subject/object. We determine the grammatical role of an NP using the Stanford dependency parser (de Marneffe et al., 2006) and a set of simple heuristics. 3 We employ narrative chains of length 12, which are available from http://cs.stanford.edu/people/ nc/schemas/schemas-size12. 4 For an alternative way of using narrative chains for coreference resolution, see Irwin et al. (2011). tecedent, respectively.5 For our running example, since Tim is predicted to be the antecedent of he, the value of NC in i2 is 1, and its value in i1 is 0. For notational convenience, we write NC(i1 )=0 and NC(i2 )=1, and will follow this convention when describing the features in the rest of the paper. Finally, we note that NC(i1 ) and NC(i2 ) will both be set to zero if (1) the pronoun and the antecedents do not participate in events, or (2) no narrative chains can be extracted in step 4 above, or (3) step 4 enables us to extract more than one chain and these chains indicate that the candid"
D12-1071,W11-1902,0,0.0554922,"Missing"
D12-1071,W03-1023,0,0.0371952,"of G3(i1 ) and G3(i2 ) is 1, then G4(i1 )=G3(i1 ) and G4(i2 )=G3(i2 ); else G4(i1 )=G4(i2 )=0. The role of the threshold x should be obvious: it ensures that a heuristic decision is made only if the difference between the counts for the two queries are sufficiently large, because otherwise there is no reason for us to prefer one candidate antecedent to the other. In all of our experiments, we set x to 20. Note that other researchers have also used lexicosyntactic patterns to generate search queries for bridging anaphora resolution (e.g., Poesio et al. (2004)), other-anaphora resolution (e.g., Modjeska et al. (2003)), and learning selectional preferences for pronoun resolution (e.g., Yang et al. (2005)). However, in each of these three cases, the target relations (e.g., the part-whole relation in the case of bridging anaphora resolution, and the subject-verb and verb-object relations in the case of selectional preferences) are specific enough that they can be effectively captured by specific patterns. For example, to determine whether the wheel is part of the car in bridging anaphora resolution, Poesio et al. employ queries of the form “X of Y”, where X and Y would be replaced with the wheel and the car,"
D12-1071,P07-1103,0,0.0448265,"Missing"
D12-1071,poesio-kabadjov-2004-general,0,0.0608356,"lexically identical but have different antecedents. The presence of twins implies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be sur"
D12-1071,P04-1019,0,0.0511279,"rve for people”. Hence, being able to make progress on this task enables us to move one step closer to building an intelligent machine that can truly understand natural language. To sum up, an important contribution of our work is that it opens up a new line of research involving a problem whose solution requires a deeper understanding of a text. With recent advances in knowledge extraction from text, we believe that time is ripe to tackle this problem. It is worth noting that some researchers have focused on other kinds of anaphors that are hard to resolve, including bridging anaphors (e.g., Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2"
D12-1071,N06-1025,0,0.0614328,", Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the targ"
D12-1071,W11-1901,0,0.0833099,"Missing"
D12-1071,qiu-etal-2004-public,0,0.0402678,"icult pronouns in them are lexically identical but have different antecedents. The presence of twins implies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pro"
D12-1071,D10-1048,0,0.0285005,"e the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference 778 researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in stan"
D12-1071,D09-1101,1,0.72334,"on of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference 778 researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns"
D12-1071,P11-1082,1,0.365828,"ed by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct antecedent for each sentence she composed. Note that a s"
D12-1071,P03-1022,0,0.102774,"Missing"
D12-1071,H05-2018,0,0.0142667,"L2(i2 )=positive-negative. To compute HPOL3 for a given instance, we simply take its HPOL2 value and append the connective to it. Using (5a) as an example, HPOL3(i1 )=positive-positive-even-though and HPOL3(i1 )=positive-negative-even-though. 4.5 Machine-Learned Polarity In the previous subsection, we compute the polarity of a word by updating its prior polarity heuristically with contextual information. We hypothesized that polarity could be computed more accurately by employing a sentiment analyzer that can capture richer contextual information. For this reason, we employ 783 OpinionFinder (Wilson et al., 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. Given a sentence and the polarity values of the phrases annotated by OpinionFinder, we determine the rank values of the pronoun and the two candidate antecedents by mapping them to the polarized phrases using the dependency relations provided by the Stanford dependency parser. We create three binary features, LPOL1, LPOL2, and LPOL3, whose values are computed in the same way as HPOL1, HPOL2, and HPOL3, respectively, except that the computation here is based on the machine-lear"
D12-1071,H05-1044,0,0.0207053,"L2(i2 )=positive-negative. To compute HPOL3 for a given instance, we simply take its HPOL2 value and append the connective to it. Using (5a) as an example, HPOL3(i1 )=positive-positive-even-though and HPOL3(i1 )=positive-negative-even-though. 4.5 Machine-Learned Polarity In the previous subsection, we compute the polarity of a word by updating its prior polarity heuristically with contextual information. We hypothesized that polarity could be computed more accurately by employing a sentiment analyzer that can capture richer contextual information. For this reason, we employ 783 OpinionFinder (Wilson et al., 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. Given a sentence and the polarity values of the phrases annotated by OpinionFinder, we determine the rank values of the pronoun and the two candidate antecedents by mapping them to the polarized phrases using the dependency relations provided by the Stanford dependency parser. We create three binary features, LPOL1, LPOL2, and LPOL3, whose values are computed in the same way as HPOL1, HPOL2, and HPOL3, respectively, except that the computation here is based on the machine-lear"
D12-1071,H89-1033,0,0.627418,"efers to as the Winograd Schema Challenge1 , could serve as a conceptually and practically appealing alternative to the well-known Turing Test (Turing, 1 Levesque (2011) defines a Winograd Schema as a small reading comprehension test involving the question of which of the two candidate antecedents for the definite pronoun in a given sentence is its correct antecedent. Levesque names this challenge after Winograd because of his pioneering attempt to use a well-known pair of twin sentences — specifically the first pair in Table 1 — to illustrate the difficulty of natural language understanding (Winograd, 1972). Strictly speaking, we are addressing a relaxed version of the Challenge: while Levesque focuses solely on definite pronouns whose resolution requires background knowledge not expressed in the words of a sentence, we do not impose such a condition on a sentence. 1950). The reason should perhaps be clear given the above discussion: this is an easy task for a subject who can “understand” natural language but a challenging task for one who can only make intelligent guesses. Levesque believes that “with a very high probability”, anything that can resolve correctly a series of difficult pronouns “"
D12-1071,P03-1023,0,0.0143907,"e are two candidate antecedents per sentence. 3 Machine Learning Framework Since our goal is to determine which of the two candidate antecedents is the correct antecedent for the target pronoun in each sentence, our system assumes as input the sentence, the target pronoun, and the two candidate antecedents. We employ machine learning to combine the features derived from different knowledge sources. Specifically, we employ a ranking-based approach. Ranking-based approaches have been shown to outperform their classification-based counterparts (Denis and Baldridge, 2007, 2008; Iida et al., 2003; Yang et al., 2003). Given a pronoun and two candidate antecedents, we aim to train a ranking model that ranks the two candidates such that the correct antecedent is assigned a higher rank. More formally, given training sentence Sk containing target pronoun Ak , correct antecedent Ck and incorrect antecedent Ik , we create two feature vectors, xCAk and xIAk , where xCAk is generated from Ak and Ck , and xIAk is generated from Ak and Ik . The training set consists of ordered pairs of feature vectors (xCAk , xIAk ), and the goal of the training procedure is to acquire a ranker that minimizes the number of violatio"
D12-1071,P05-1021,0,0.0234192,"0. The role of the threshold x should be obvious: it ensures that a heuristic decision is made only if the difference between the counts for the two queries are sufficiently large, because otherwise there is no reason for us to prefer one candidate antecedent to the other. In all of our experiments, we set x to 20. Note that other researchers have also used lexicosyntactic patterns to generate search queries for bridging anaphora resolution (e.g., Poesio et al. (2004)), other-anaphora resolution (e.g., Modjeska et al. (2003)), and learning selectional preferences for pronoun resolution (e.g., Yang et al. (2005)). However, in each of these three cases, the target relations (e.g., the part-whole relation in the case of bridging anaphora resolution, and the subject-verb and verb-object relations in the case of selectional preferences) are specific enough that they can be effectively captured by specific patterns. For example, to determine whether the wheel is part of the car in bridging anaphora resolution, Poesio et al. employ queries of the form “X of Y”, where X and Y would be replaced with the wheel and the car, respectively. On the other hand, we are not targeting a particular type of relation. Ra"
D12-1071,C98-1013,0,\N,Missing
D12-1071,P10-2029,0,\N,Missing
D12-1071,D08-1067,1,\N,Missing
D13-1135,P08-1090,0,0.0311994,"lver with the following information to help it make these decisions. First, we find the head word of each candidate antecedent, Headc . Then we form two strings, Headc + P redz and Headc + P redz + Obj (if the object of the VP is present). Finally, we employ them as binary lexical features, setting their feature values to 1 if and only if they can be extracted from the instance under consideration. The training data can be used to determine which of these features are useful.2 Narrative event chains. A narrative event chain is a partially ordered set of events related by a common protagonist (Chambers and Jurafsky, 2008). For example, we can infer from the chain ""borrow-s invests spend-s lend-s"" that a person who borrows (pre2 We tried to apply Kehler et al.'s (2004) and Yang et al.'s (2005) methods to learn Chinese selectional preferences from unlabeled data, but without success. 1362 sumably money) can invest it, spend it, or lend it to other people.3 Consider the following example: E2: 国家 ∗pro∗ (The country gives our department money, but all ∗pro∗ provides is exactly what we worked for.) In E2, ∗pro∗ is coreferent with 国家 (The country), and the presence of the narrative event chain − (gives−provides) sugg"
D13-1135,N04-4009,0,0.0848679,"Missing"
D13-1135,D10-1086,0,0.698453,"to refer to a real-world entity. An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding noun phrases (NPs) in the associated text. Unlike overt pronouns, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. We aim to improve the state of the art in Chinese AZP resolution by proposing two extensions. First, while previous approaches to this task have primarily focused on employing positional and syntactic features (e.g., Zhao and Ng (2007) [Z&N], Kong and Zhou (2010) [K&Z]), we exploit a richer set of features for capturing the context of an AZP and its candidate antecedents. Second, to alleviate the difficulty of resolving an AZP to an antecedent far away from it, we break down the process into smaller, intermediate steps, where we allow coreference links between AZPs to be established. We apply our two extensions to a state-of-the-art Chinese AZP resolver proposed by Z&N and evaluate the resulting resolver on the OntoNotes corpus. Experimental results show that this resolver significantly outperforms both Z&N's resolver and another state-of-the-art reso"
D13-1135,E06-1015,0,0.0144467,"Missing"
D13-1135,J01-4004,0,0.30187,"Missing"
D13-1135,C10-2158,0,0.0132679,"tor IP; whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears; whether z is the first or last ZP of the sentence; whether z is in the headline of the text. Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate antecedent of z. V is the VP node following z in the parse tree. To implement the classification step, we train a classifier using SVMlight (Joachims, 1999) to distinguish AZPs from non-AZPs. We employ 32 features, 13 of which were proposed by Z&N and 19 of which were proposed by Yang and Xue (2010). A brief description of these features can be found in Table 1. 2.2 Two Baseline AZP Resolvers The Zhao and Ng (2007) [Z&N] baseline. In our implementation of the Z&N baseline, we use SVMlight to train a mention-pair model for determining whether an AZP z and a candidate antecedent of z are coreferent. We consider all NPs preceding z that do not have the same head as its parent NP in the parse tree to be z's candidate antecedents. We use Soon et al.'s (2001) method to create training instances: we create a positive instance between an AZP, z, and its closest overt antecedent, and we create a"
D13-1135,P05-1021,0,0.362088,"Missing"
D13-1135,D07-1057,0,0.720189,"tically null form is used to refer to a real-world entity. An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding noun phrases (NPs) in the associated text. Unlike overt pronouns, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. We aim to improve the state of the art in Chinese AZP resolution by proposing two extensions. First, while previous approaches to this task have primarily focused on employing positional and syntactic features (e.g., Zhao and Ng (2007) [Z&N], Kong and Zhou (2010) [K&Z]), we exploit a richer set of features for capturing the context of an AZP and its candidate antecedents. Second, to alleviate the difficulty of resolving an AZP to an antecedent far away from it, we break down the process into smaller, intermediate steps, where we allow coreference links between AZPs to be established. We apply our two extensions to a state-of-the-art Chinese AZP resolver proposed by Z&N and evaluate the resulting resolver on the OntoNotes corpus. Experimental results show that this resolver significantly outperforms both Z&N's resolver and a"
D13-1135,I08-1004,0,0.238312,"rent, (2) use Soon et al.'s method to create training instances, and (3) resolve an AZP to its closest coreferent candidate antecedent. Unlike Z&N, however, K&Z use the SVMlight−T K learning algorithm (Moschitti, 1 When resolving a gold AZP z, if none of the preceding candidate antecedents is classified as coreferent with it, we resolve it to the candidate that has the highest coreference likelihood with it. Here, we employ the signed distance from the SVM hyperplane to measure the coreference likelihood. 2006) to train their model, employing a parse subtree known as a dynamic expansion tree (Zhou et al., 2008) as a structured feature to represent an instance. 3 Extension 1: Novel Features We propose three kinds of features to better capture the context of an AZP, as described below. Antecedent compatibility. AZPs are omitted subjects that precede VP nodes in a sentence's parse tree. From the VP node, we can extract its head verb (P redz ) and the head of its object NP (Obj), if any. Note that P redz and Obj contain important contextual information for an AZP. Next, observe that if a NP is coreferent with an AZP, it should be able to fill the AZP's gap and be compatible with the gap's context. Consi"
D14-1083,P11-1099,0,0.0785152,"dict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moe"
D14-1083,W11-1701,0,0.364387,"ow. N-gram features. We encode each unigram and bigram collected from the training sentences as a 753 binary feature indicating the n-gram’s presence or absence in a given sentence. stating her counter-argument. FollowsQuote is a binary feature that indicates whether a sentence follows a sentence for which the IsQuote feature value is true. Intuitively, a sentence following a quote is likely to present a counter-argument. Dependency-based features. To capture the inter-word relationships that n-grams may not, we employ the dependency-based features previously used for stance classification in Anand et al. (2011). These features have three variants. In the first variant, the pair of arguments involved in each dependency relation extracted by a dependency parser is used as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Positional feature. We split each post into four parts (such that each part"
D14-1083,P13-2142,1,0.918277,"s examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (1) determining the reason(s) associat"
D14-1083,C08-2004,0,0.0152449,"Missing"
D14-1083,W14-2107,0,0.267946,"Missing"
D14-1083,P11-1151,0,0.0208009,"et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (1) determining th"
D14-1083,W13-3514,1,0.951462,"s examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (1) determining the reason(s) associat"
D14-1083,I13-1191,1,0.808636,"s examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (1) determining the reason(s) associat"
D14-1083,P06-2063,0,0.00846205,"s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000). The argument types for legal documents refer to legal factors which are either pro-plaintiff or pro-defendant (Br¨uninghaus and Ashley, 2005). For instance, for trade secret law cases, factors such as Waiver-of-Confidentiality and Disclosure-in-Public-Forum refer to certain facts strengthening the claim of one of the sides participating in a case. Sentiment analysis. RC resembles certain tasks in sentiment analysis. One such task is pro and con reason classification in reviews (Kim and Hovy, 2006), where sentences containing opinions as well as reasons justifying the opinions are to be extracted and classified as PRO, CON, or NONE. Hence, this task focuses on categorizing sentences into coarse-grained, high-level groups (e.g., PRO vs. CON, POSITIVE vs. NEGATIVE), but does not attempt to subcategorize the PRO and CON classes into fine-grained reason classes, unlike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to a"
D14-1083,P07-1055,0,0.0112624,"al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reas"
D14-1083,P04-1035,0,0.00322791,"review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stance"
D14-1083,P09-1095,1,0.788985,"rom (1) the lack of access to background knowledge, (2) the failure to process complex discourse structures, and (3) the failure to process sarcastic statements and rhetorical questions. We present two examples for each of these three major sources of error from the ABO and OBA domains in Table 5. In each example, we show their predicted (P) and gold (G) labels. 6 Related Work In this section, we discuss related work in the areas of document-level RC, argument recognition, textual entailment in online debates, argumentation mining, and sentiment analysis. Document-level reason classification. Persing and Ng (2009) apply a multi-label text classification approach to document-level RC of aviation safety incident reports. Given a set of pre-defined reasons, their RC system seeks to identify the reasons that can explain why the incident described in a given report occurred. Their work is different from ours in at least two respects. First, while our posts occur in post sequences (which can be profitably exploited in RC, for example, as in J3), their incident reports were written independently of each other. Second, they do not perform sentence-level RC, as the lack of sentence-level reason annotations in t"
D14-1083,W04-2401,0,0.0270482,"ince this is the only RC system that is not stance-specific. For the SC system, we employ P2. Since the SC system and the RC system are trained independently of each other, their outputs may not be consistent. For instance, an inconsistency arises if a post is labeled as for but one or more of its reasons are associated with the opposing stance. In fact, an inconsistency can arise in the output of the RC system alone: reasons associated with both stances may be assigned by the RC systems to different sentences of a given post. To enforce consistency, we apply integer linear programming (ILP) (Roth and Yih, 2004). We formulate one ILP program for each debate post. Each ILP program contains two post-stance variables (xf or and xagainst ) and |T |∗ |LR |reason variables (i.e., one indicator variable zt,r for each reason class r and each sentence t), where |T |is the number of sentences in the post and |LR |is the number of reason labels. Our objective is to maximize the linear combination of these variables and their corresponding probabilities assigned by their respective classifiers (see (2) below) subject to two types of constraints, the integrity constraints and the post-reason constraints. The inte"
D14-1083,P09-1026,0,0.375291,"y-internal discussions (Walker et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguist"
D14-1083,W00-1302,0,0.0709773,"Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, de"
D14-1083,W06-1639,0,0.291016,"s such as congressional debates and company-internal discussions (Walker et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, c October 25-29, 2014, Doha,"
D14-1083,N13-1100,0,0.0140214,"the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpu"
D14-1083,N12-1072,0,0.442588,"why she supports abortion, namely it’s a woman’s right to abort and unwanted babies are threat to their parents’ future, which are mentioned in the first and third sentences in the post respectively. Our goal in this paper is to examine post- and sentence-level reason classification (RC) in ideological debates. Many online debaters use emotional languages, which may involve sarcasm and insults, to express their points, thereby making RC and SC in ideological debates potentially more challenging than that in other debate settings such as congressional debates and company-internal discussions (Walker et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al."
D14-1083,P10-2062,0,0.0153608,"ike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiment"
D14-1083,N07-1033,0,0.0286155,"nd con reason classification in reviews (Kim and Hovy, 2006), where sentences containing opinions as well as reasons justifying the opinions are to be extracted and classified as PRO, CON, or NONE. Hence, this task focuses on categorizing sentences into coarse-grained, high-level groups (e.g., PRO vs. CON, POSITIVE vs. NEGATIVE), but does not attempt to subcategorize the PRO and CON classes into fine-grained reason classes, unlike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et"
D14-1083,D08-1013,0,0.00903122,"erm rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classi"
D14-1083,J96-2004,0,\N,Missing
D14-1084,P06-1005,0,0.0516325,"Missing"
D14-1084,E09-1018,0,0.375313,"p://code.google.com/p/ctbparser/ 767 ``` ``` Pronoun neuter Antecedent ```` neuter feminine masculine 0.864 0.065 0.130 feminine masculine 0.018 0.930 0.041 0.117 0.005 0.828 first second third singular plural 0.861 0.26 ``` ``` Pronoun first Antecedent ```` plural 0.139 0.74 first second third Table 4: Learned values of P (pN um |cN um , l=1). 5.3 third 0.119 0.766 0.077 0.025 0.016 0.634 0.417 0.75 0.437 second third 0.525 0.23 0.229 0.057 0.02 0.334 Table 6: Learned values of P (pP er |cP er , l=1) (different speakers). NUMBER When computing the Number of a candidate antecedent in English, Charniak and Elsner (2009) rely on part-of-speech information. For example, NN and NNP denote singular nouns, whereas NNS and NNPS denote plural nouns. However, Chinese part-of-speech tags do not provide such information. Hence, we need a different method for finding the Number of a candidate antecedent c in Chinese. If c is a pronoun, we look up its Number in Table 1. If c is a named entity, its Number is singular. If c is a common noun, we infer its Number from its string: if the string ends with 们 or is modified by a quantity word (e.g., 一些, 许多), c is plural; otherwise, c is singular. Table 4 shows the learned value"
D14-1084,D13-1135,1,0.71136,"ibute values p′a and c′a , we compute P (p, k, c, l=1) P (l=1|p, k, c) = ∑ ′ c′ ∈C P (p, k, c , l=1) ∏ P (pa |ca , l=1) ∗ P (l=1|kc ) ∏ ≈ ∑ a∈A ′ c′ ∈C a∈A P (pa |ca , l=1) ∗ P (l=1|kc′ ) (6) 2 This assumption is reasonable because it is fairly easy to determine which pronoun can be used to refer to a given NP. 765 Count(p′a , c′a , l=1) as follows: ∑ Count(p′a , c′a , l=1) = Although we can now apply our generative model to resolve AZPs, the resolution procedure P (l=1|p, k, c) can be improved further. The improvement is p,c:pa =p′a ,ca =c′a motivated by a problem we observed previously (8) (Chen and Ng, 2013): an AZP and its closest anSimilarly, P (l=1|kc ) is estimated as follows: tecedent can sometimes be far away from each other, thus making it difficult to correctly resolve Count(kc , l=1) + θ P (l=1|kc ) = (9) the AZP. To address this problem, we employ the Count(kc ) + θ ∗ 2 following resolution procedure in our experiments. where Count(kc ) is the number of times kc apGiven a test document, we process its AZPs in a pears in the training data, and Count(kc , l=1) is left-to-right manner. As soon as we resolve an the expected number of times kc is the context surAZP to a preceding NP c, we fi"
D14-1084,W05-0612,0,0.0863146,"Missing"
D14-1084,J95-2003,0,0.814284,"Missing"
D14-1084,P00-1022,0,0.725569,"dges to only those maximal or modifier NPs that precede find textually distant antecedents for ZPs (Chen p in the associated text and are at most two senand Ng, 2013). tences away from it.1 k is the context surroundZP resolution for other languages. There have ing p as well as every candidate antecedent c in been rule-based and supervised machine learning C; kc is the context surrounding p and candidate approaches for resolving ZPs in other languages. antecedent c; and l is a binary variable indicatFor example, to resolve ZPs in Spanish texts, ing whether c is the correct antecedent of p. The Ferrández and Peral (2000) proposed a set of handset A = {N um, Gen, P er, Ani} has four elecrafted rules that encode preferences for candidate ments, which correspond to Number, Gender, antecedents. In addition, supervised approaches Person and Animacy respectively. a is an athave been extensively employed to resolve ZPs tribute in A. Finally, pa and ca are the attribute in Korean (e.g., Han (2006)), Japanese (e.g., Seki values of p and c with respect to a respectively. et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and 4.2 Training Poesio (2011), Sasano and Kurohashi (2"
D14-1084,P11-1081,0,0.308201,"s that encode preferences for candidate ments, which correspond to Number, Gender, antecedents. In addition, supervised approaches Person and Animacy respectively. a is an athave been extensively employed to resolve ZPs tribute in A. Finally, pa and ca are the attribute in Korean (e.g., Han (2006)), Japanese (e.g., Seki values of p and c with respect to a respectively. et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and 4.2 Training Poesio (2011), Sasano and Kurohashi (2011)), and Our model estimates P (p, k, c, l), the probability Italian (e.g., Iida and Poesio (2011)). of seeing (1) the overt pronoun p; (2) the context k surrounding p and its candidate antecedents; (3) 3 Chinese Overt Pronouns a candidate antecedent c of p; and (4) whether c is Since our approach relies heavily on Chinese the correct antecedent of p. Since we estimate this overt pronouns, in this section we introduce them probability from a raw, unannotated corpus, we are by describing their four grammatical attributes, effectively treating p, k, and c as observed data and namely Number, Gender, Person and Anil as hidden data. macy. Number has two values, singular and Owing to the presenc"
D14-1084,C02-1078,0,0.79076,"Missing"
D14-1084,P06-1079,0,0.679876,"to resolve ZPs in Spanish texts, ing whether c is the correct antecedent of p. The Ferrández and Peral (2000) proposed a set of handset A = {N um, Gen, P er, Ani} has four elecrafted rules that encode preferences for candidate ments, which correspond to Number, Gender, antecedents. In addition, supervised approaches Person and Animacy respectively. a is an athave been extensively employed to resolve ZPs tribute in A. Finally, pa and ca are the attribute in Korean (e.g., Han (2006)), Japanese (e.g., Seki values of p and c with respect to a respectively. et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and 4.2 Training Poesio (2011), Sasano and Kurohashi (2011)), and Our model estimates P (p, k, c, l), the probability Italian (e.g., Iida and Poesio (2011)). of seeing (1) the overt pronoun p; (2) the context k surrounding p and its candidate antecedents; (3) 3 Chinese Overt Pronouns a candidate antecedent c of p; and (4) whether c is Since our approach relies heavily on Chinese the correct antecedent of p. Since we estimate this overt pronouns, in this section we introduce them probability from a raw, unannotated corpus, we are by describing their four gra"
D14-1084,P05-1021,0,0.0378285,"Missing"
D14-1084,P09-2022,0,0.352632,"Missing"
D14-1084,D07-1057,0,0.906698,"likely to be generated from an antecedent with the same speaker than one with a different speaker. In the different-speaker case, a first (second) person pronoun is most likely to be generated from a second (first) person pronoun. 6 Context Features To fully specify our model, we need to describe how to represent kc , which is needed to compute P (l=1|kc ). Recall that kc encodes the context surrounding candidate antecedent c and the associated pronoun p. As described below, we represent kc using eight features, some of which are motivated by previous work on supervised AZP resolution (e.g., Zhao and Ng (2007), Chen and Ng (2013)). Note that (1) all but feature 1 are computed based on syntactic parse trees, and (2) features 2, 3, 6, and 8 are ternary-valued features. PERSON Finally, we compute the Person of a candidate antecedent c. Similar to Charniak and Elsner (2009), we set 我 (I) and 我们 (we) to first person, 你 (singular you) and 你们 (plural you) to second person, and everything else to third person. We estimate two sets of probabilities P (pP er |cP er , l=1), one where p and c are from the same speaker, and the other where they are from different speakers.7 This is based on our observation that"
D14-1084,W03-1024,0,0.768709,"iable indicatFor example, to resolve ZPs in Spanish texts, ing whether c is the correct antecedent of p. The Ferrández and Peral (2000) proposed a set of handset A = {N um, Gen, P er, Ani} has four elecrafted rules that encode preferences for candidate ments, which correspond to Number, Gender, antecedents. In addition, supervised approaches Person and Animacy respectively. a is an athave been extensively employed to resolve ZPs tribute in A. Finally, pa and ca are the attribute in Korean (e.g., Han (2006)), Japanese (e.g., Seki values of p and c with respect to a respectively. et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and 4.2 Training Poesio (2011), Sasano and Kurohashi (2011)), and Our model estimates P (p, k, c, l), the probability Italian (e.g., Iida and Poesio (2011)). of seeing (1) the overt pronoun p; (2) the context k surrounding p and its candidate antecedents; (3) 3 Chinese Overt Pronouns a candidate antecedent c of p; and (4) whether c is Since our approach relies heavily on Chinese the correct antecedent of p. Since we estimate this overt pronouns, in this section we introduce them probability from a raw, unannotated corpus, we are by descri"
D14-1084,I08-1004,0,0.242292,"imate this task are based on supervised learning. Zhao 我们 (we) plural neuter first animate masculine third animate and Ng (2007) are the first to employ a supervised 他们 (they) plural 她们 (they) plural feminine third animate learning approach to Chinese ZP resolution. They 它们 (they) plural neuter third inanimate trained an AZP resolver by employing syntactic and positional features in combination with a de- Table 1: Attribute values of Chinese overt pronouns. cision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive con4 The Generative Model volution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More re4.1 Notation cently, we extended Zhao and Ng&apos;s feature set with Let p be an overt pronoun in P R, the set of the novel features that encode the context surrounding 10 overt pronouns described in Section 3. C, the a ZP and its candidate antecedents, and exploited set of candidate antecedents of p, contains all and the coreference links between ZPs as bridges to only those maximal or modifier NPs that precede find textually distant antecedents for ZPs (Chen p in the associated text and are at most two senand Ng, 2013). tences away from it"
D14-1084,N04-1037,0,0.061155,"Missing"
D14-1084,W12-4501,0,0.0741731,"ee baselines are state-of-the-art supervised AZP resolvers, which include our own resolver (Chen and Ng, 2013) as well as our re-implementations of Zhao and Ng&apos;s (2007) resolver and Kong and Zhou&apos;s (2010) resolver. The test set results of these seven baseline resolvers when evaluated under the three aforementioned evaluation settings are shown in Table 8. The system AZPs employed by the rulebased resolvers are obtained using our rule-based Evaluation 7.1 Experimental Setup Datasets. We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task (Pradhan et al., 2012). In the CoNLL-2012 data, the training set and development set contain ZP coreference annotations, but the test set does not. Therefore, we train our models on the training set and perform evaluation on the development set. Statistics on the datasets are shown in Table 7. The documents in these datasets come from six sources, namely Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). 8 We sort the candidate antecedents of p as follows. We first consider the subject candidate antecedents in the same sentence as p from ri"
D14-1084,C10-1102,0,0.0606335,"Missing"
D14-1084,I11-1085,0,0.485631,"rández and Peral (2000) proposed a set of handset A = {N um, Gen, P er, Ani} has four elecrafted rules that encode preferences for candidate ments, which correspond to Number, Gender, antecedents. In addition, supervised approaches Person and Animacy respectively. a is an athave been extensively employed to resolve ZPs tribute in A. Finally, pa and ca are the attribute in Korean (e.g., Han (2006)), Japanese (e.g., Seki values of p and c with respect to a respectively. et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and 4.2 Training Poesio (2011), Sasano and Kurohashi (2011)), and Our model estimates P (p, k, c, l), the probability Italian (e.g., Iida and Poesio (2011)). of seeing (1) the overt pronoun p; (2) the context k surrounding p and its candidate antecedents; (3) 3 Chinese Overt Pronouns a candidate antecedent c of p; and (4) whether c is Since our approach relies heavily on Chinese the correct antecedent of p. Since we estimate this overt pronouns, in this section we introduce them probability from a raw, unannotated corpus, we are by describing their four grammatical attributes, effectively treating p, k, and c as observed data and namely Number, Gender"
D14-1084,D10-1086,0,\N,Missing
D14-1084,chen-ng-2014-sinocoreferencer,1,\N,Missing
D14-1090,W06-0901,0,0.12723,"part of the Message Understanding Conferences (MUCs), the Automatic Content Extraction (ACE) evaluations, and the BioNLP shared 832 tasks on event extraction. Previous work on event extraction can be broadly divided into two categories, one focusing on the development of features (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. Th"
D14-1090,W11-1828,0,0.133584,"Missing"
D14-1090,W13-2003,0,0.0634834,"Missing"
D14-1090,W09-1402,0,0.137989,"Missing"
D14-1090,W13-2014,0,0.0529568,"Missing"
D14-1090,C12-1033,1,0.273044,"2-dependent manner following engagement of CD40 . . . (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme={HOIL-1L interacting protein,CD40} E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1 , p−1) ∧ Word(w2 , p) ∧ Word(w3 , p + 1) ⇒ Type(p, T ). For any given position (p), this formula has W 3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, curre"
D14-1090,W02-1001,0,0.060848,"the number of training examples of some types (e.g., N one) far outnumber other types. To rectify this ill-conditioning problem (Singla and Domingos, 2005; Lowd and Domingos, 2007), we divide the gradient with the number of true groundings in the data, namely, we compute the gradient using (Ew (nj )−nj ) . nj Another key issue with using Equation (3) is that computing Ew (nj ) requires performing inference over the MLN. This step is intractable, #P-complete in the worst case. To circumvent this problem and for fast, scalable training, we instead propose to use the voted perceptron algorithm (Collins, 2002; Singla and Domingos, 2005). This algorithm approximates Ew (nj ) by counting the number of satisfied groundings of each formula in the MAP assignment. Computing the MAP assignment is much easier (although still NP-hard in the worst case) than computing Ew (nj ), and as a result the Testing In the testing phase, we combine BioMLN with the output of the pipeline model (see Section 3) to obtain a new MLN, which we refer to as BioMLN+ . For every candidate trigger, the SVM trigger classifier outputs a vector of signed confidence values (which is proportional to the distance from the separating h"
D14-1090,P09-2093,0,0.17167,"re-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Li"
D14-1090,W13-2004,0,0.0564016,"Missing"
D14-1090,E12-1029,0,0.07166,"ollowing engagement of CD40 . . . (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme={HOIL-1L interacting protein,CD40} E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1 , p−1) ∧ Word(w2 , p) ∧ Word(w3 , p + 1) ⇒ Type(p, T ). For any given position (p), this formula has W 3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems ten"
D14-1090,P08-1030,0,0.0403167,"tegories, one focusing on the development of features (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model."
D14-1090,W09-1401,0,0.0413667,"extract events from Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high-dimensional features suffer from error propagation as the model is generally not rich enough for joint inference. 2.2 The Genia Event Extraction Task The BioNLP Shared Task (BioNLP-ST) series (Kim et al. (2009), Kim et al. (2011a) and N´edellec et al. (2013)) is designed to tackle the problem of extracting structured information from the biomedical literature. The Genia Event Extraction task is arguably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series. The 2009 edition of the Genia task (Kim et al., 2009) was conducted on the Genia event corpus (Kim et al., 2008), which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edition (Kim et al., 2011b) augmented the dataset t"
D14-1090,W11-1801,0,0.0974135,"Missing"
D14-1090,W11-1802,0,0.0622153,"Missing"
D14-1090,D12-1092,0,0.00894177,"0 . . . (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme={HOIL-1L interacting protein,CD40} E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1 , p−1) ∧ Word(w2 , p) ∧ Word(w3 , p + 1) ⇒ Type(p, T ). For any given position (p), this formula has W 3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highl"
D14-1090,W13-2015,0,0.0296119,"Missing"
D14-1090,P13-1145,0,0.0805588,"Missing"
D14-1090,P13-1008,0,0.564993,"ce fragment ID Event Type Trigger Arguments E11 Binding recruited Theme={HOIL-1L interacting protein,CD40} E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1 , p−1) ∧ Word(w2 , p) ∧ Word(w3 , p + 1) ⇒ Type(p, T ). For any given position (p), this formula has W 3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified mode"
D14-1090,P10-1081,0,0.160108,"ased approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem a"
D14-1090,R11-1002,0,0.404425,"Missing"
D14-1090,W11-1806,0,0.039453,"ing high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of th"
D14-1090,N10-1004,0,0.0198588,"ion shared task. Note that this task is the most important one for Genia and therefore has the most active participation. Statistics on the datasets are shown in Table 2. All our evaluations use the online tool provided by the shared task organizers. We report scores obtained using the approximate span, recursive evaluation. To generate features, we employ the supporting resources provided by the organizers. Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model (McClosky, 2010). Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser (Miyao and Tsujii, 2008) and the aforementioned BLLIP parser that uses the selftrained biomedical model, which results in two sets of dependency features. For MAP inference, we use Gurobi, a parallelized ILP solver. After inference, a postprocessing step is required to generate biomedical events from the extracted triggers and arguments. Specifically, for binding events, we employ a learning-based method similar to Bj¨orne and Salakoski (2011), while for the other events, we employ a"
D14-1090,J08-1002,0,0.021145,"stics on the datasets are shown in Table 2. All our evaluations use the online tool provided by the shared task organizers. We report scores obtained using the approximate span, recursive evaluation. To generate features, we employ the supporting resources provided by the organizers. Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model (McClosky, 2010). Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser (Miyao and Tsujii, 2008) and the aforementioned BLLIP parser that uses the selftrained biomedical model, which results in two sets of dependency features. For MAP inference, we use Gurobi, a parallelized ILP solver. After inference, a postprocessing step is required to generate biomedical events from the extracted triggers and arguments. Specifically, for binding events, we employ a learning-based method similar to Bj¨orne and Salakoski (2011), while for the other events, we employ a rule-based approach similar to Bj¨orne et al. (2009). Both the SVM baseline system and the combined MLN+SVM system employ the same post"
D14-1090,W13-2001,0,0.0496349,"Missing"
D14-1090,W13-2010,0,0.0487006,"Missing"
D14-1090,D09-1016,0,0.186551,"pment of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between ev"
D14-1090,P12-1088,0,0.0123944,"d, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approach to extract biomedical events; and Ritter et al. (2012) used unsupervised learning to extract events from Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high"
D14-1090,N10-1123,0,0.659047,"m on the BioNLP’13 Genia shared task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics . . . demonstrated th"
D14-1090,P11-1163,0,0.661114,"ing high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of th"
D14-1090,W11-1825,0,0.0375955,"Missing"
D14-1090,D11-1001,0,0.801579,"ed task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics . . . demonstrated that HOIL-1L interacting prot"
D14-1090,W11-1807,0,0.493561,"ed task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics . . . demonstrated that HOIL-1L interacting prot"
D14-1090,W09-1406,0,0.353628,"approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) im"
D14-1090,W11-1808,0,0.042895,"Missing"
D14-1090,J03-4003,0,\N,Missing
D14-1090,C10-1088,0,\N,Missing
D14-1090,W13-2002,0,\N,Missing
D14-1119,I13-1191,1,0.779557,"with similar demographic information. Previous work has attempted to predict such latent features (e.g., Rao and Yarowsky (2010), Burger et al. (2011)) rather than employing them for opinion mining tasks. Second, we exploit inter-comment constraints to help us perform joint inference over votes on different questions. Note that previous work on debate stance recognition has also employed constraints to improve the inference process. Specifically, in stance prediction, it is typical to employ so-called author constraints (e.g., Thomas et al. (2006), Bansal et al. (2008), Walker et al. (2012a), Hasan and Ng (2013)), which specify that two documents written by the same author for the same topic should have the same stance. However, in vote prediction, author constraints are not useful because a user is not permitted to cast more than one vote per question, unlike in stance prediction, where users may engage in a debate and therefore post more than once per debate topic. Consequently, we propose two new types of constraints for exploiting inter topic user voting patterns. One constraint involves pairs of authors and the other involves pairs of questions. These constraints are also potentially useful for"
D14-1119,C08-2004,0,0.0398163,"Missing"
D14-1119,W11-1709,0,0.0197425,"0; Walker et al., 2012b), we also present our classifiers with features representing the first lemmatized unigram, bigram, and trigram appearing in each comment. These may be useful in our task when, for example, a user’s comment begins with or entirely consists of a restatement of the answer she chose. So if the possible answers for a given question are “Yes” and “No”, a user might write in her comment “Yes. Because ...”, and this would make the “CueWord:Yes” feature useful for classifying this comment. Emotion Frequency. For each word in a comment, we used the NRC Emotion Word Lexicon 1129 (Mohammad and Yang, 2011) to discover if the word conveys any emotion. Then, for each emotion or sentiment covered by the lexicon (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, positive, or negative) ei , we construct a feai) ture ei : C(e total describing how much of the comment consists of words conveying emotion ei , where C(ei ) is the count of words in the comment bearing emotion ei and total is the number of words in the comment. To understand why this feature may be useful, consider the question “Does Sarah Palin deserve VP?” We suspect that users who post comments laden with words associat"
D14-1119,P11-1151,0,0.0129471,"social media sites typically allow users to comment on multiple topics. Note that enforcing constraints involving two questions is by no means trivial, as the possible class values associated with the two comments may not necessarily be the same. Another contribution of our work lies in our adaptation of the label propagation algorithm (Zhu and Ghahramani, 2002) to enforce constraints for vote prediction. Recall that existing stance classification approaches enforce constraints using minimum cut (Thomas et al., 2006), integer linear programming (Lu et al., 2012), and loopy belief propagation (Burfoot et al., 2011). Our decision to employ label propagation stems in part from the inability of loopy belief propagation and integer linear programming to efficiently process the nearly one million comments we have, and in part from the inability of the traditional two-way minimum cut algorithm to handle multiclass classification. It is worth noting, however, that other variations of the label propagation algorithm have been proposed for unrelated NLP tasks such as automatically harvesting temporal facts from the web (e.g., Wang et al. (2011) and Wang et al. (2012)). While we are the first to address the vote"
D14-1119,de-marneffe-etal-2006-generating,0,0.0151829,"Missing"
D14-1119,W10-0214,0,0.0670441,"s about what kind of content/ads related to the question the user would like to see. Unfortunately, a major difficulty of vote prediction arises from the casual nature of discussion in social media. A comment often contains insufficient information for inferring the user’s vote, or in some cases may even be entirely absent. In light of this difficulty, we exploit two additional types of information in the prediction process. First, we employ demographic features derived from user profiles. Demographic features may be broadly useful for other opinion mining tasks such as stance classification (Somasundaran and Wiebe, 2010), as many social media websites like CreateDebate1 allow users to create profiles with similar demographic information. Previous work has attempted to predict such latent features (e.g., Rao and Yarowsky (2010), Burger et al. (2011)) rather than employing them for opinion mining tasks. Second, we exploit inter-comment constraints to help us perform joint inference over votes on different questions. Note that previous work on debate stance recognition has also employed constraints to improve the inference process. Specifically, in stance prediction, it is typical to employ so-called author cons"
D14-1119,W06-1639,0,0.619886,"l media websites like CreateDebate1 allow users to create profiles with similar demographic information. Previous work has attempted to predict such latent features (e.g., Rao and Yarowsky (2010), Burger et al. (2011)) rather than employing them for opinion mining tasks. Second, we exploit inter-comment constraints to help us perform joint inference over votes on different questions. Note that previous work on debate stance recognition has also employed constraints to improve the inference process. Specifically, in stance prediction, it is typical to employ so-called author constraints (e.g., Thomas et al. (2006), Bansal et al. (2008), Walker et al. (2012a), Hasan and Ng (2013)), which specify that two documents written by the same author for the same topic should have the same stance. However, in vote prediction, author constraints are not useful because a user is not permitted to cast more than one vote per question, unlike in stance prediction, where users may engage in a debate and therefore post more than once per debate topic. Consequently, we propose two new types of constraints for exploiting inter topic user voting patterns. One constraint involves pairs of authors and the other involves pair"
D14-1119,N12-1072,0,0.0714255,"ers to create profiles with similar demographic information. Previous work has attempted to predict such latent features (e.g., Rao and Yarowsky (2010), Burger et al. (2011)) rather than employing them for opinion mining tasks. Second, we exploit inter-comment constraints to help us perform joint inference over votes on different questions. Note that previous work on debate stance recognition has also employed constraints to improve the inference process. Specifically, in stance prediction, it is typical to employ so-called author constraints (e.g., Thomas et al. (2006), Bansal et al. (2008), Walker et al. (2012a), Hasan and Ng (2013)), which specify that two documents written by the same author for the same topic should have the same stance. However, in vote prediction, author constraints are not useful because a user is not permitted to cast more than one vote per question, unlike in stance prediction, where users may engage in a debate and therefore post more than once per debate topic. Consequently, we propose two new types of constraints for exploiting inter topic user voting patterns. One constraint involves pairs of authors and the other involves pairs of questions. These constraints are also"
D14-1119,P12-2046,0,0.0212919,"al., 2012), and loopy belief propagation (Burfoot et al., 2011). Our decision to employ label propagation stems in part from the inability of loopy belief propagation and integer linear programming to efficiently process the nearly one million comments we have, and in part from the inability of the traditional two-way minimum cut algorithm to handle multiclass classification. It is worth noting, however, that other variations of the label propagation algorithm have been proposed for unrelated NLP tasks such as automatically harvesting temporal facts from the web (e.g., Wang et al. (2011) and Wang et al. (2012)). While we are the first to address the vote prediction task, other researchers have previously used social media to predict the outcomes of various events, primarily by analyzing Twitter data. For example, Tumasjan et al. (2010) and Gayo-Avello et al. (2011) performed the related task of predicting the outcomes of elections. Rather than predicting election outcomes, O’Connor et al. (2010) focused on finding correlations between measures derived from tweets and the outcomes of political events like elections and polls. Finally, Asur and Huberman (2010) predicted movies’ box office success. Th"
D14-1119,D11-1120,0,\N,Missing
D15-1087,P04-1043,0,0.334438,"nes for the first time spatial relations on objects in motion. Extracting spatial relations on objects in motion, or MOVELINKs, is very challenging. The challenge stems in part from the fact that a MOVELINK involves two mandatory participants (with roles mover and trigger) and up to six optional participants (with other semantic roles). As 758 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 758–768, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2.1 nathan et al., 2010; Lee et al., 2013) and tree kernels (Moschitti, 2004; Moschitti, 2006). Recall that a sieve-based approach is composed of a pipeline of sieves ordered by precision, where the decisions made by earlier sieves can be exploited by later sieves in order to incrementally construct a complex structure. When applied to MOVELINK identification, we can create a sieve for identifying each role, so that (1) spatial elements corresponding to different roles are incrementally added to a MOVELINK , and (2) earlier attachment decisions can be exploited as additional contextual information by later sieves. Hence, compared to a joint approach, a sieve-based app"
D15-1087,S13-2096,0,0.0170351,"OVELINKs (because its optional participants are extracted independently of each other by these classifiers). Broadly speaking, existing spatial relation extraction systems have adopted either a pipeline approach or a joint approach to these subtasks. Given a set of spatial elements, a pipeline spatial relation extraction system (1) extracts the triggers, (2) determines whether a spatial relation exists between each extracted trigger and each of the remaining spatial elements, and (3) classifies the role of each non-trigger in each pair of spatially-related elements (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2015). The major weakness of pipeline approaches is that errors in trigger identification can propagate to the relation classification component, whose errors can in turn propagate to the role labeling component. To address this weakness, Roberts et al. (2012; 2013) investigated joint approaches. Given a set of spatial elements with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In oth"
D15-1087,E06-1015,0,0.259396,"time spatial relations on objects in motion. Extracting spatial relations on objects in motion, or MOVELINKs, is very challenging. The challenge stems in part from the fact that a MOVELINK involves two mandatory participants (with roles mover and trigger) and up to six optional participants (with other semantic roles). As 758 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 758–768, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2.1 nathan et al., 2010; Lee et al., 2013) and tree kernels (Moschitti, 2004; Moschitti, 2006). Recall that a sieve-based approach is composed of a pipeline of sieves ordered by precision, where the decisions made by earlier sieves can be exploited by later sieves in order to incrementally construct a complex structure. When applied to MOVELINK identification, we can create a sieve for identifying each role, so that (1) spatial elements corresponding to different roles are incrementally added to a MOVELINK , and (2) earlier attachment decisions can be exploited as additional contextual information by later sieves. Hence, compared to a joint approach, a sieve-based approach achieves com"
D15-1087,H05-1091,0,0.0804681,"nd (2) earlier attachment decisions can be exploited as additional contextual information by later sieves. Hence, compared to a joint approach, a sieve-based approach achieves computational tractability by modeling partial, rather than full dependencies among the spatial elements. While a sieve-based approach allows us to exploit additional contextual information provided by earlier sieves, we still have to specify how we encode such contextual information. Motivated by the successful application of tree kernels to relation extraction (e.g., Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005), Zhou et al. (2007)), we propose to (1) encode the syntactic context in which the spatial elements extracted by the sieves appear using a syntactic parse tree, and then (2) employ the tree as an (additional) structured feature for training the classifier associated with each sieve. This novel combination of sieves and tree-based structured features results in what we call an expanding parse tree. Specifically, as a spatial element for a MOVELINK is extracted by a (role-specific) sieve, it will be added to the structured feature for the classifier associated with the following sieve. In other"
D15-1087,S15-2150,0,0.0118963,"weakness, Roberts et al. (2012; 2013) investigated joint approaches. Given a set of spatial elements with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will output a 1 if and only if (1) the elements in the set form a relation and (2) their roles in the relation are correct. The systems participating in SpaceEval all seem to be in favor of joint approaches (D’Souza and Ng, 2015; Nichols and Botros, 2015; Salaberri et al., 2015). 4 4.1 Training the Baseline Classifiers In this subsection, we describe how we train the Baseline classifiers, which include one classifier for identifying QSLINKs and OLINKs (Section 4.1.1) and seven classifiers for identifying MOVELINK s (Section 4.1.2). 4.1.1 The LINK Classifier We collapse QSLINKs and OLINKs to a single relation type, LINK, identifying these two types of links using the LINK classifier. To understand why we can do this, recall from Section 2.1 that in QS LINK s and OLINK s, the trigger has to be a spatial signal element having a semantic type att"
D15-1087,Q14-1022,0,0.0789052,"hen we will choose the role assigned by the classifier associated with the earlier sieve. Given the above discussion, it should be clear that the ordering of the sieves is important. Typically, sieves are ordered by precision, with the hope of reducing the number of erroneous deci4 See Footnote 3 for the set of values of C and J used for parameter tuning. 5 We use the distance from the hyperplane as a measure of an SVM classifier’s confidence. 4.2 Applying the Baseline Classifiers 762 sions passed from the earlier sieves to the later sieves (e.g., Raghunathan et al. (2010), Lee et al. (2013), Chambers et al. (2014)). Motivated by this observation, we order the sieves as follows. We set sieve 0 to be the LINK classifier and sieve 1 to be the (trigger, mover) classifier, and then order the remaining sieves by precision. Specifically, we compute the precision of each sieve on the development data, then add sieves into the pipeline in decreasing order of precision. 5.2 (a) (b) Figure 2: Syntactic parse trees for two example sentences containing MOVELINKs. each spatial element, effectively attaching it to its grandparent. Finally, for better generalization, we replace each spatial element with its role. Sinc"
D15-1087,W13-0503,0,0.0267062,"ve. In other words, the parse tree corresponding to the structured feature will keep expanding as we move along the sieves in the pipeline. This contrasts with previous applications of tree kernels, where a structured feature is created from a static parse subtree for extracting exactly two arguments involved in a relation. To our knowledge, this is the first attempt to combine sieves and parse trees to create expanding trees to extract complex relations involving multiple arguments. 2 The SpaceEval Corpus We use as our corpus the SpaceEval training corpus, which is a subset of ISO-SpaceBank (Pustejovsky and Yocum, 2013). The corpus consists of 59 travel narratives annotated with seven types of spatial elements (Table 1) and three types of spatial relations (Table 2), following the ISOSpace (2012) annotation specifications. Different types of spatial elements have different attributes. The only attribute that is relevant to our work is semantic type, which is one of the attributes of a spatial entity. Semantic type expresses the type of the relation it triggers and can take one of three values: topological, directional, or both. What is missing in Table 2 about spatial relations is that each element participa"
D15-1087,P04-1054,0,0.0958564,"tally added to a MOVELINK , and (2) earlier attachment decisions can be exploited as additional contextual information by later sieves. Hence, compared to a joint approach, a sieve-based approach achieves computational tractability by modeling partial, rather than full dependencies among the spatial elements. While a sieve-based approach allows us to exploit additional contextual information provided by earlier sieves, we still have to specify how we encode such contextual information. Motivated by the successful application of tree kernels to relation extraction (e.g., Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005), Zhou et al. (2007)), we propose to (1) encode the syntactic context in which the spatial elements extracted by the sieves appear using a syntactic parse tree, and then (2) employ the tree as an (additional) structured feature for training the classifier associated with each sieve. This novel combination of sieves and tree-based structured features results in what we call an expanding parse tree. Specifically, as a spatial element for a MOVELINK is extracted by a (role-specific) sieve, it will be added to the structured feature for the classifier associated with the"
D15-1087,S15-2146,1,0.874807,"Missing"
D15-1087,D10-1048,0,0.0564072,"different roles to the same spatial element, then we will choose the role assigned by the classifier associated with the earlier sieve. Given the above discussion, it should be clear that the ordering of the sieves is important. Typically, sieves are ordered by precision, with the hope of reducing the number of erroneous deci4 See Footnote 3 for the set of values of C and J used for parameter tuning. 5 We use the distance from the hyperplane as a measure of an SVM classifier’s confidence. 4.2 Applying the Baseline Classifiers 762 sions passed from the earlier sieves to the later sieves (e.g., Raghunathan et al. (2010), Lee et al. (2013), Chambers et al. (2014)). Motivated by this observation, we order the sieves as follows. We set sieve 0 to be the LINK classifier and sieve 1 to be the (trigger, mover) classifier, and then order the remaining sieves by precision. Specifically, we compute the precision of each sieve on the development data, then add sieves into the pipeline in decreasing order of precision. 5.2 (a) (b) Figure 2: Syntactic parse trees for two example sentences containing MOVELINKs. each spatial element, effectively attaching it to its grandparent. Finally, for better generalization, we repla"
D15-1087,S13-2044,0,0.0257547,"Missing"
D15-1087,S12-1056,0,0.258418,"Missing"
D15-1087,W13-0119,0,0.0417617,"Missing"
D15-1087,S15-2145,0,0.0136229,"2012; 2013) investigated joint approaches. Given a set of spatial elements with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will output a 1 if and only if (1) the elements in the set form a relation and (2) their roles in the relation are correct. The systems participating in SpaceEval all seem to be in favor of joint approaches (D’Souza and Ng, 2015; Nichols and Botros, 2015; Salaberri et al., 2015). 4 4.1 Training the Baseline Classifiers In this subsection, we describe how we train the Baseline classifiers, which include one classifier for identifying QSLINKs and OLINKs (Section 4.1.1) and seven classifiers for identifying MOVELINK s (Section 4.1.2). 4.1.1 The LINK Classifier We collapse QSLINKs and OLINKs to a single relation type, LINK, identifying these two types of links using the LINK classifier. To understand why we can do this, recall from Section 2.1 that in QS LINK s and OLINK s, the trigger has to be a spatial signal element having a semantic type attribute. If its semantic t"
D15-1087,S12-1048,0,0.0241249,"sacrificing computational tractability? To address this question, we combine two ideas that have been successfully applied to a variety of information extraction tasks, namely multi-pass sieves (RaghuIntroduction Spatial relation extraction is the task of determining the relation among a set of spatial elements. Although it has thus far received much less attention than temporal relation extraction, there has been a surge of interest in it in recent years, as evidenced by the organization of the three shared tasks on spatial relation extraction, namely the spatial role labeling tasks in 2012 (Kordjamshidi et al., 2012) and 2013 (Kolomiyets et al., 2013), as well as this year’s SpaceEval task (Pustejovsky et al., 2015). The task has also evolved over the years, with new types of spatial elements and/or spatial relations being defined in each shared task. For instance, while the first two shared tasks have focused on extracting spatial relations between stationary objects, SpaceEval examines for the first time spatial relations on objects in motion. Extracting spatial relations on objects in motion, or MOVELINKs, is very challenging. The challenge stems in part from the fact that a MOVELINK involves two manda"
D15-1087,J13-4004,0,0.161936,"stationary objects, SpaceEval examines for the first time spatial relations on objects in motion. Extracting spatial relations on objects in motion, or MOVELINKs, is very challenging. The challenge stems in part from the fact that a MOVELINK involves two mandatory participants (with roles mover and trigger) and up to six optional participants (with other semantic roles). As 758 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 758–768, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2.1 nathan et al., 2010; Lee et al., 2013) and tree kernels (Moschitti, 2004; Moschitti, 2006). Recall that a sieve-based approach is composed of a pipeline of sieves ordered by precision, where the decisions made by earlier sieves can be exploited by later sieves in order to incrementally construct a complex structure. When applied to MOVELINK identification, we can create a sieve for identifying each role, so that (1) spatial elements corresponding to different roles are incrementally added to a MOVELINK , and (2) earlier attachment decisions can be exploited as additional contextual information by later sieves. Hence, compared to a"
D15-1087,D07-1076,0,0.0260472,"ecisions can be exploited as additional contextual information by later sieves. Hence, compared to a joint approach, a sieve-based approach achieves computational tractability by modeling partial, rather than full dependencies among the spatial elements. While a sieve-based approach allows us to exploit additional contextual information provided by earlier sieves, we still have to specify how we encode such contextual information. Motivated by the successful application of tree kernels to relation extraction (e.g., Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005), Zhou et al. (2007)), we propose to (1) encode the syntactic context in which the spatial elements extracted by the sieves appear using a syntactic parse tree, and then (2) employ the tree as an (additional) structured feature for training the classifier associated with each sieve. This novel combination of sieves and tree-based structured features results in what we call an expanding parse tree. Specifically, as a spatial element for a MOVELINK is extracted by a (role-specific) sieve, it will be added to the structured feature for the classifier associated with the following sieve. In other words, the parse tre"
D15-1087,S15-2149,0,\N,Missing
dsouza-ng-2014-annotating,S07-1014,0,\N,Missing
dsouza-ng-2014-annotating,S10-1010,0,\N,Missing
dsouza-ng-2014-annotating,J96-2004,0,\N,Missing
dsouza-ng-2014-annotating,verhagen-etal-2006-annotation,0,\N,Missing
E09-1041,E06-3004,0,0.0614568,"Missing"
E09-1041,E06-1002,0,0.109077,"Missing"
E09-1041,E03-1009,0,0.0363744,"art-of-speech tagging and named entity recognition. 1 Introduction While research in natural language processing has gained a lot of momentum in the past several decades, much of this research effort has been focusing on only a handful of politically-important 1 See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions a"
E09-1041,E99-1001,0,0.0908457,"Missing"
E09-1041,W99-0613,0,0.137266,"Missing"
E09-1041,N01-1006,0,0.102576,"Missing"
E09-1041,W99-0612,0,0.291961,"Missing"
E09-1041,D07-1074,0,0.0205465,"ength2 , and (2) select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTH ERS (i.e., anything other than PER , ORG and LOC ). 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes. Hence, these affixe"
E09-1041,P07-2056,0,0.0171826,"aking it difficult to reproduce the results of these NE recognizers. Second, it is not clear how comprehensive these lists are. Relying on comprehensive lists that comprise a large portion of the names in the test set essentially reduces the NER problem to a dictionary-lookup problem, which is arguably not very interesting from a research perspective. In addition, many existing learning-based Bengali NE recognizers have several common weaknesses. First, they use as features pseudo-affixes, which are created by extracting the first n and the last n characters of a word (where 1 ≤ n ≤ 4) (e.g., Dandapat et al. (2007)). While affixes encode essential grammatical information in Bengali due to its morphological richness, this extraction method is arguably too ad-hoc and does not cover many useful affixes. Second, they typically adopt a pipelined NER architecture, performing POS tagging prior to NER and encoding the resulting not-so-accurate POS information as a feature. In other words, errors in POS tagging are propagated to the NE recognizer via the POS feature, thus limiting its performance. 2 Related Work Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-inde"
E09-1041,P05-2004,0,0.0606172,"Missing"
E09-1041,I08-2077,0,0.0614337,"Missing"
E09-1041,J01-2001,0,0.00896943,"st. Generating and annotating the titles Recall that each Wikipedia article has been optionally assigned to one or more categories by its creator and/or editors. We use these categories to help annotate the title of an article. Specifically, if an article has a category whose name starts with “Born on” or “Death on,” we label the corresponding title with PER. Similarly, if it has a category whose name starts with “Cities of” or “Countries of,” we 2 The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). 3 See http://bn.wikipedia.org. In our experiments, we used the Bengali Wikipedia dump obtained on October 22, 2007. 356 NE Class PER LOC ORG Keywords “born,” “died,” “one,” “famous” “city,” “area,” “population,” “located,” “part of” “establish,” “situate,” “publish” “Anna” is labeled with each NE class, and set its final label to be the most frequent NE class. We put all these annotated tokens into a token list. If the title list and the token list have an element in common, we remove the element from the token list, since we have a higher confidence in the labels of the titles. Merging the"
E09-1041,P07-1094,0,0.0247939,"d a lot of momentum in the past several decades, much of this research effort has been focusing on only a handful of politically-important 1 See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions and particles such as “in” and “to”) are realized as inflections in Bengali, and the absence of these informative words implies that the cont"
E09-1041,N06-1041,0,0.0215711,"research in natural language processing has gained a lot of momentum in the past several decades, much of this research effort has been focusing on only a handful of politically-important 1 See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions and particles such as “in” and “to”) are realized as inflections in Bengali, and the ab"
E09-1041,D07-1073,0,0.0236296,"select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTH ERS (i.e., anything other than PER , ORG and LOC ). 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes. Hence, these affixes could serve as useful feat"
E09-1041,W96-0213,0,0.40885,"Missing"
E09-1041,E95-1020,0,0.187458,"Missing"
E09-1041,P06-2100,0,0.0667767,"Missing"
E09-1041,D07-1068,0,0.0129795,"e is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTH ERS (i.e., anything other than PER , ORG and LOC ). 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes. Hence, these affixes could serve as useful features for training POS an"
E09-1042,E95-1020,0,0.288851,"Missing"
E09-1042,P05-1044,0,0.0512425,"ervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible paramet"
E09-1042,W00-0717,0,0.107741,"ection 3 gives an introduction to G&G’s fully-Bayesian approach to unsupervised POS tagging. In Section 4, we describe our two extensions to G&G’s approach. Section 5 presents experimental results on Bengali POS tagging, focusing on evaluating the effective2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an u"
E09-1042,D08-1109,0,0.0221655,"Missing"
E09-1042,E03-1009,0,0.0569714,"be emitted from the state corresponding to ti . (1) The question, then, is: which priors on θ would allow the acquisition of skewed distributions? To 2 When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger. See Goldberg et al. (2008) for details. 365 From the closed form in Equation 3, given previous outcomes, we can compute the tag transition and output probabilities of the model as follows: P (ti |t−i , α) = n(ti−2 ,ti−1 ,ti ) + α n(ti−2 ,ti−1 ) + T α P (wi |ti , t−i , w−i , β) = n(ti ,wi ) + β nti + Wti β number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)). To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffixbased POS lexicon. Such a suffix-based HMM, however, may suffer from over-generalization. To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&G’s framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the wor"
E09-1042,D07-1023,1,0.816883,"m the state corresponding to ti . (1) The question, then, is: which priors on θ would allow the acquisition of skewed distributions? To 2 When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger. See Goldberg et al. (2008) for details. 365 From the closed form in Equation 3, given previous outcomes, we can compute the tag transition and output probabilities of the model as follows: P (ti |t−i , α) = n(ti−2 ,ti−1 ,ti ) + α n(ti−2 ,ti−1 ) + T α P (wi |ti , t−i , w−i , β) = n(ti ,wi ) + β nti + Wti β number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)). To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffixbased POS lexicon. Such a suffix-based HMM, however, may suffer from over-generalization. To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&G’s framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the word-based POS lexicon; oth"
E09-1042,P08-1085,0,0.233942,"∼ Dirichlet(α) ∼ Dirichlet(β) where wi and ti denote the i-th word and tag. With a tagset of size T (including a special tag used as sentence delimiter), each of the tag transition distributions has T components. For the output symbols, each of the ω (ti ) has Wti components, where Wti denotes the number of word types that can be emitted from the state corresponding to ti . (1) The question, then, is: which priors on θ would allow the acquisition of skewed distributions? To 2 When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger. See Goldberg et al. (2008) for details. 365 From the closed form in Equation 3, given previous outcomes, we can compute the tag transition and output probabilities of the model as follows: P (ti |t−i , α) = n(ti−2 ,ti−1 ,ti ) + α n(ti−2 ,ti−1 ) + T α P (wi |ti , t−i , w−i , β) = n(ti ,wi ) + β nti + Wti β number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)). To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constra"
E09-1042,J01-2001,0,0.032722,"discriminative prediction. 4.1 Induced Suffix Emission For morphologically-rich languages like Bengali, a lot of grammatical information (e.g., POS) is expressed via suffixes. In fact, several approaches to unsupervised POS induction for morphologicallyrich languages have exploited the observation that some suffixes can only be associated with a small Incorporating suffix-based output distributions Finally, we extend our trigram model by introduc3 The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). 366 P (ti |t−i , w, α, β) ∝ n(ti ,wi ) + β n(ti−2 ,ti−1 ,ti ) + α n(ti−1 ,ti ,ti+1 ) + I(ti−2 = ti−1 = ti = ti+1 ) + α . . nti + Wti β n(ti−2 ,ti−1 ) + T α n(ti−1 ,ti ) + I(ti−2 = ti−1 = ti ) + T α . n(ti ,ti+1 ,ti+2 ) + I(ti−2 = ti = ti+2 , ti−1 = ti+1 ) + I(ti−1 = ti = ti+1 = ti+2 ) + α n(ti ,ti+1 ) + I(ti−2 = ti , ti−1 = ti+1 ) + I(ti−1 = ti = ti+1 ) + T α Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx values are computed from the current values of all tags except for ti . Here, I(arg) is a function that returns 1 if arg is true an"
E09-1042,P07-1094,0,0.481795,"lly allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data. The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectationmaximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972). More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&G] as a viable alternative to the traditional maximumlikelihood-based HMM approach. While unsupervised POS taggers adopting both approaches have 1 Introduction Unsupervised POS tagging requires neither manual encoding of tagging heuristics nor the availability of data labeled with POS information. Rather, an unsupervised POS tagger operates by only assuming as input a POS lexicon, which consists of a list of possible POS tags for each word. As we can see from the partial POS lexicon for English in Figure 1, “the” is unambiguous with respect to POS tagging, since it can only be a"
E09-1042,N06-1041,0,0.0627012,"xtensions to G&G’s approach. Section 5 presents experimental results on Bengali POS tagging, focusing on evaluating the effective2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similari"
E09-1042,D07-1031,0,0.35293,"MM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning. Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007). Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging. However, their work departs from existing Bayesian approaches to POS tagging in that they (1) introduce a new sparse prior on the distribution over tags for each word, (2) extend the Latent Dirichlet Allocation model, and (3) explicitly model ambiguity class. While their tagging model, like Goldwater and Griffiths’s, assumes as input an incomplete POS lexicon and a large unlabeled corpus, they consider their approach “semisupervised” simply because of the hu"
E09-1042,J94-2001,0,0.815028,"anized as follows. Section 2 presents related work on unsupervised approaches to POS tagging. Section 3 gives an introduction to G&G’s fully-Bayesian approach to unsupervised POS tagging. In Section 4, we describe our two extensions to G&G’s approach. Section 5 presents experimental results on Bengali POS tagging, focusing on evaluating the effective2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i"
E09-1042,N09-1010,0,\N,Missing
E12-1081,P98-1013,0,0.0616767,"rst identify all the triples that are instances of the part-whole relation using regular expressions. Next, we create clusters of relation arguments, such that each pair of arguments in a cluster has a part-whole relation. This is easy: since part-whole is a transitive relation (i.e., &lt;A,part,B&gt; and &lt;B,part,C&gt; implies &lt;A,part,C&gt;), we cluster the arguments by taking the transitive closure of these relation instances. Then, given an NP NPi in the test set, we assign med/part to it if there is a preceding NP NPj such that the two NPs are in the same argument cluster. In Rule 14, we use FrameNet (Baker et al., 1998) to determine whether med/situation should be assigned to an NP, NPi . Specifically, we check whether it fills an argument of a frame set up by a preceding NP, NPj , or verb. To exemplify, let us assume that NPj is “capital punishment”. We search for “punishment” in FrameNet to access the appropriate frame, which in this case is “rewards and punishments”. This frame contains a list of arguments together with examples. If NPi is one of these arguments, we assign med/situation to NPi , since it is involved in a situation (described by a frame) that is set up by a preceding NP/verb. In Rule 15, w"
E12-1081,D11-1142,0,0.0154083,"bset or a specific portion of an entity or concept mentioned earlier in the dialogue. Examples include “another bedroom”, “different color”, “somebody else”, “any place”, “one of them”, and “most other cities”. Condition 3 of the rule, which checks whether the head noun of the NP has been mentioned previously, is a good test for identity coreference, but since all the old entities have suppos802 edly been identified by the preceding rules, it becomes a reasonable test for set-subset relations. For convenience, we identify part-whole relations in Rule 12 based on the output produced by ReVerb (Fader et al., 2011), an open information extraction system.6 The output contains, among other things, relation instances, each of which is represented as a triple, &lt;A,rel,B&gt;, where rel is a relation, and A and B are its arguments. To preprocess the output, we first identify all the triples that are instances of the part-whole relation using regular expressions. Next, we create clusters of relation arguments, such that each pair of arguments in a cluster has a part-whole relation. This is easy: since part-whole is a transitive relation (i.e., &lt;A,part,B&gt; and &lt;B,part,C&gt; implies &lt;A,part,C&gt;), we cluster the arguments"
E12-1081,C08-1033,0,0.0268764,"teedman (2000)), and in part in the benefits it can potentially bring to NLP applications. One task that could benefit from knowledge of IS is identity coreference: since new entities by definition have not been previously referred to, an NP marked as new does not need to be resolved, thereby improving the precision of a coreference resolver. Knowledge of fine-grained or subcategorized IS is valuable for other NLP tasks. For instance, an NP marked as set signifies that it is in a set-subset relation with its antecedent, thereby providing important clues for bridging anaphora resolution (e.g., Gasperin and Briscoe (2008)). Despite the potential usefulness of IS in NLP tasks, there has been little work on learning the IS of discourse entities. To investigate the plausibility of learning IS, Nissim et al. (2004) annotate a set of Switchboard dialogues with such information2 , and subsequently present a 2 These and other linguistic annotations on the Switchboard dialogues were later released by the LDC as part of the NXT corpus, which is described in Calhoun et al. (2010). 798 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 798–807, c Avignon, Fr"
E12-1081,W11-1902,0,0.0644107,"ent of old subtypes to NPs. For instance, Rule 1 identifies instances of old/general, which comprises the personal pronouns referring 4 Not all NPs have an IS type/subtype. For instance, a pleonastic “it” does not refer to any real-world entity and therefore does not have any IS, and so are nouns such as “course” in “of course”, “accident” in “by accident”, etc. to the dialogue participants. Note that this and several other rules rely on coreference information, which we obtain from two sources: (1) chains generated automatically using the Stanford Deterministic Coreference Resolution System (Lee et al., 2011)5 , and (2) manually identified coreference chains taken directly from the annotated Switchboard dialogues. Reporting results using these two ways of obtaining chains facilitates the comparison of the IS determination results that we can realistically obtain using existing coreference technologies against those that we could obtain if we further improved existing coreference resolvers. Note that both sources provide identity coreference chains. Specifically, the gold chains were annotated for NPs belonging to old/identity and old/ident generic. Hence, these chains can be used to distinguish be"
E12-1081,nissim-etal-2004-annotation,0,0.892859,"roduction A linguistic notion central to discourse processing is information status (IS). It describes the extent to which a discourse entity, which is typically referred to by noun phrases (NPs) in a dialogue, is available to the hearer. Different definitions of IS have been proposed over the years. In this paper, we adopt Nissim et al.’s (2004) proposal, since it is primarily built upon Prince’s (1992) and Eckert and Strube’s (2001) well-known definitions, and is empirically shown by Nissim et al. to yield an annotation scheme for IS in dialogue that has good reproducibility.1 Specifically, Nissim et al. (2004) adopt a threeway classification scheme for IS, defining a discourse entity as (1) old to the hearer if it is known to the hearer and has previously been referred to in the dialogue; (2) new if it is unknown to her and 1 It is worth noting that several IS annotation schemes have been proposed more recently. See G¨otze et al. (2007) and Riester et al. (2010) for details. has not been previously referred to; and (3) mediated (henceforth med) if it is newly mentioned in the dialogue but she can infer its identity from a previously-mentioned entity. To capture finergrained distinctions for IS, Nis"
E12-1081,W06-1612,0,0.18757,"Missing"
E12-1081,D11-1099,1,0.589356,"Missing"
E12-1081,riester-etal-2010-recursive,0,0.2674,"since it is primarily built upon Prince’s (1992) and Eckert and Strube’s (2001) well-known definitions, and is empirically shown by Nissim et al. to yield an annotation scheme for IS in dialogue that has good reproducibility.1 Specifically, Nissim et al. (2004) adopt a threeway classification scheme for IS, defining a discourse entity as (1) old to the hearer if it is known to the hearer and has previously been referred to in the dialogue; (2) new if it is unknown to her and 1 It is worth noting that several IS annotation schemes have been proposed more recently. See G¨otze et al. (2007) and Riester et al. (2010) for details. has not been previously referred to; and (3) mediated (henceforth med) if it is newly mentioned in the dialogue but she can infer its identity from a previously-mentioned entity. To capture finergrained distinctions for IS, Nissim et al. allow an old or med entity to have a subtype, which subcategorizes an old or med entity. For instance, a med entity has the subtype set if the NP that refers to it is in a set-subset relation with its antecedent. IS plays a crucial role in discourse processing: it provides an indication of how a discourse model should be updated as a dialogue is"
E12-1081,C98-1013,0,\N,Missing
H01-1054,H01-1065,0,0.0170363,"ocuments into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also noted this problem, we have focused our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary. In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collecte"
H01-1054,W00-0405,0,0.0152076,"ortant difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 4000 vs. 5000), whereas we have implemented rules for doing so. Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also n"
H01-1054,J93-2004,0,0.0355149,"o template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89). The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text. The RIPTIDES system for the most part employs a traditional IE architecture [4]. In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations. A preprocessor first finds sentences and tokens. For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems. Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document. The extraction phase of the system identifies domain-specific relations among relevant entities in the text. It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns. Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training — one set comprises texts relevant to the domain of"
H01-1054,J98-3005,0,0.690231,"earch efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques. This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization. (RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization.) Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques. In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system. We conclude with a brief discussion of related and ongoing work. 2. SYSTEM DESIGN Figure 1 depicts the IE-supported summarization system. The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario te"
H01-1054,W98-1428,1,0.601029,"as however, nevertheless, etc. We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid. Figure 4. 200 word summary of actual IE output, with emphasis on Red Cross cases. We then perform a randomized local search for a good set of sentences according to these scoring criteria. 2.2.4 Implementation The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex’s Exemplars Framework [13]. The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations. In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a “rough draft” of the summary text. In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text. In the second generation step, the Exemplars text polishing component makes use of this markup to trigger s"
H01-1054,A00-2018,0,\N,Missing
I11-1052,H05-1004,0,0.0216077,"ve error is reduced by 5.1 and 6.7 according to B3 and CEAF, respectively. Experimental Setup Corpus. We employ in our evaluation a dataset comprising 147 coreference-annotated Switchboard dialogues, which contain a total of 68,992 NPs.4 We partition the dialogues into a training set (117 dialogues) and a test set (30 dialogues). We extract the NPs and the parse trees directly from the gold-standard annotations, but the coreference features are computed entirely automatically. Scoring programs. We employ two commonlyused coreference scoring programs, B3 (Bagga and Baldwin, 1998) and φ3 -CEAF (Luo, 2005), both of which report results in terms of recall (R), precision (P), and F-measure (F). 4.2 Results and Discussion The baseline mention-pair model. We employ as our first baseline the MP model, which is trained using the procedure described in Section 2.1. Given that our goal is to examine the effectiveness of the tree-based and path-based features for the joint CR model, one may wonder why the results of the MP model are relevant to our investigation. Recall from the introduction that we chose to improve the joint CR model with the two types of features derived from syntactic parses because"
I11-1052,D08-1031,0,0.0329744,"Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {altaf,vince}@hlt.utdallas.edu Abstract second component, the resolution system, which identifies an antecedent for the NP. This resolver is typically implemented by training a mention-pair (MP) model, which is a binary classifier that determines whether a pair of NPs are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b)). While this architecture is popularly adopted by coreference researchers and was implemented even within recently developed coreference resolvers (e.g., Bengtson and Roth (2008), Stoyanov et al. (2009)), neither the architecture itself nor its aforementioned implementation is satisfactory for at least two reasons. First, in this pipeline architecture, anaphoricity determination is performed prior to coreference resolution, so errors in anaphoricity determination can propagate to the downstream coreference component and adversely affect its performance (Ng and Cardie, 2002a). Second, the MP coreference model is fundamentally weak in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model"
I11-1052,P04-1043,0,0.0385926,"NP-internal information (e.g., gender) has been captured by the flat features. While this parse substructure ignores the words in NPk , these unigrams could be useful for determining its anaphoricity, as a learner may learn from coreference-annotated data that “it” only has a moderate probability of being anaphoric, and that “the contrary” from the phrase “on the contrary” is never anaphoric. As a result, we augment the set of flat features in i(NULL, NPk ) with the unigrams extracted from NPk . Step 2: Recasting Ranking as Classification Existing implementations of SVMs, such as SVMlight-TK (Moschitti, 2004), allow us to combine flat and (structured) tree-based features to train a classifier by designing appropriate kernels. Hence, if we were to train an SVM classifier, all we need to do is to design a kernel. However, we are given a ranking problem, and it is not immediately clear how an SVM can learn a ranking model in the presence of tree-based features. Our approach to this problem is to reduce the given ranking problem to an equivalent classification problem. Once we have a classification problem, all we need to do is to design a kernel for training a classifier, as mentioned above. To reduc"
I11-1052,C02-1139,1,0.703124,"on et al. (2001), Ng and Cardie (2002b)). While this architecture is popularly adopted by coreference researchers and was implemented even within recently developed coreference resolvers (e.g., Bengtson and Roth (2008), Stoyanov et al. (2009)), neither the architecture itself nor its aforementioned implementation is satisfactory for at least two reasons. First, in this pipeline architecture, anaphoricity determination is performed prior to coreference resolution, so errors in anaphoricity determination can propagate to the downstream coreference component and adversely affect its performance (Ng and Cardie, 2002a). Second, the MP coreference model is fundamentally weak in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model is trained to compare the NP to be resolved (henceforth the active NP) against a candidate antecedent, it only determines how good the candidate is relative to the active NP, not how good the candidate is relative to other candidates. In light of the aforementioned problems, researchers have proposed a number of solutions: • To address the error propagation problem, researchers have proposed joint"
I11-1052,P02-1014,1,0.80374,"on et al. (2001), Ng and Cardie (2002b)). While this architecture is popularly adopted by coreference researchers and was implemented even within recently developed coreference resolvers (e.g., Bengtson and Roth (2008), Stoyanov et al. (2009)), neither the architecture itself nor its aforementioned implementation is satisfactory for at least two reasons. First, in this pipeline architecture, anaphoricity determination is performed prior to coreference resolution, so errors in anaphoricity determination can propagate to the downstream coreference component and adversely affect its performance (Ng and Cardie, 2002a). Second, the MP coreference model is fundamentally weak in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model is trained to compare the NP to be resolved (henceforth the active NP) against a candidate antecedent, it only determines how good the candidate is relative to the active NP, not how good the candidate is relative to other candidates. In light of the aforementioned problems, researchers have proposed a number of solutions: • To address the error propagation problem, researchers have proposed joint"
I11-1052,P02-1034,0,0.167649,"Missing"
I11-1052,W04-0707,0,0.0157382,"r (1979), coreference resolution can be decomposed into two complementary subtasks: “(1) identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice”. These two subtasks are commonly known as anaphoricity determination and anaphora resolution, both of which have recently been tackled using machine learning techniques. More specifically, anaphoricity determination is typically tackled by training an anaphoricity classifier, which determines whether an NP is anaphoric or not (e.g., Poesio et al. (2004), Zhou and Kong (2009)). If so, the NP is passed to the 465 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 465–473, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP et al. (2008)). tures in a ranking model that employs joint learning. With the increasingly important role structured features and ranking models play in natural language learning, we believe that a method for combining flat and structured features for training a ranker would be of particular interest to natural language processing (NLP) researchers.1 Second, motivated in part b"
I11-1052,D09-1101,1,0.848026,"in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model is trained to compare the NP to be resolved (henceforth the active NP) against a candidate antecedent, it only determines how good the candidate is relative to the active NP, not how good the candidate is relative to other candidates. In light of the aforementioned problems, researchers have proposed a number of solutions: • To address the error propagation problem, researchers have proposed joint inference (Denis and Baldridge, 2007) and joint learning (Rahman and Ng, 2009) for anaphoricity determination and coreference resolution. • To address the expressiveness problem resulting from making a coreference decision based on only two NPs, researchers have proposed the entity-mention model, where coreference decisions are made by determining whether an NP belongs to a preceding coreference cluster (e.g., Luo et al. (2004), Yang Recent research efforts have led to the development of a state-of-the-art supervised coreference model, the cluster-ranking model. However, it is not clear whether the features that have been shown to be useful when employed in traditional"
I11-1052,N07-1011,0,0.0221708,", is compared in turn to each preceding NP, NPj , from right to left, and NPj is selected as its antecedent if the pair is classified as coreferent. The process ends as soon as an antecedent is found for NPk or the beginning of the text is reached. 2.2 Since the CR model ranks preceding clusters, a training instance i(cj , NPk ) represents a preceding cluster cj and an anaphoric NP NPk . Each instance consists of two types of features: (1) features that are computed based solely on NPk , and (2) cluster-level features, which describe the relationship between cj and NPk . Motivated in part by Culotta et al. (2007), we create cluster-level features from the relational features in our 39-feature set using four logical predicates: NONE, MOSTFALSE, MOST- TRUE, and ALL. Specifically, for each relational feature X, we first convert X into an equivalent set of binary-valued features if it is multi-valued. Then, for each resulting binaryvalued feature Xb , we create four binary-valued cluster-level features: (1) NONE - Xb is true when X b is false between NPk and each NP in cj ; (2) MOST- FALSE - X b is true when X b is true between NPk and less than half (but at least one) of the NPs in cj ; (3) MOST- TRUE -"
I11-1052,D11-1099,1,0.712122,"eature. The reason is that a complex tree may make it difficult for the SVM learner to make generalizations: the more complex the tree is, the less likely it is to find similar trees in other instances. To strike a better balance between having a rich representation of context and improving the learner’s ability to generalize, we extract a substructure from a parse tree and use it as the value of the structured feature of an instance. This substructure was previously shown to be useful when used as a structured feature for training a classifier for determining the information status of an NP (Rahman and Ng, 2011). Given an instance i(NULL, NPk ), we extract the substructure from the parse tree containing NPk as follows. Let n(NPk ) be the root of the subtree that spans all and only the words in NPk , and let P arent(n(NPk )) be its immediate parent node. We (1) take the subtree rooted at P arent(n(NPk )), (2) replace each leaf node in this subtree with a node labeled X, (3) replace the child nodes of n(NPk ) with a leaf node labeled Y, and (4) use the subtree rooted at P arent(n(NPk )) as the structured feature for i(NULL, NPk ). Figure 1 illustrates this substructure extraction procedure via an examp"
I11-1052,N07-1030,0,0.0210859,"the MP coreference model is fundamentally weak in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model is trained to compare the NP to be resolved (henceforth the active NP) against a candidate antecedent, it only determines how good the candidate is relative to the active NP, not how good the candidate is relative to other candidates. In light of the aforementioned problems, researchers have proposed a number of solutions: • To address the error propagation problem, researchers have proposed joint inference (Denis and Baldridge, 2007) and joint learning (Rahman and Ng, 2009) for anaphoricity determination and coreference resolution. • To address the expressiveness problem resulting from making a coreference decision based on only two NPs, researchers have proposed the entity-mention model, where coreference decisions are made by determining whether an NP belongs to a preceding coreference cluster (e.g., Luo et al. (2004), Yang Recent research efforts have led to the development of a state-of-the-art supervised coreference model, the cluster-ranking model. However, it is not clear whether the features that have been shown t"
I11-1052,J01-4004,0,0.49498,"Missing"
I11-1052,D08-1069,0,0.0407209,"Missing"
I11-1052,P09-1074,0,0.0134206,"gy Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {altaf,vince}@hlt.utdallas.edu Abstract second component, the resolution system, which identifies an antecedent for the NP. This resolver is typically implemented by training a mention-pair (MP) model, which is a binary classifier that determines whether a pair of NPs are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b)). While this architecture is popularly adopted by coreference researchers and was implemented even within recently developed coreference resolvers (e.g., Bengtson and Roth (2008), Stoyanov et al. (2009)), neither the architecture itself nor its aforementioned implementation is satisfactory for at least two reasons. First, in this pipeline architecture, anaphoricity determination is performed prior to coreference resolution, so errors in anaphoricity determination can propagate to the downstream coreference component and adversely affect its performance (Ng and Cardie, 2002a). Second, the MP coreference model is fundamentally weak in that (1) the information extracted from two NPs may not be sufficient for making an informed coreference decision and (2) since the model is trained to compare t"
I11-1052,D09-1120,0,0.0141672,"ost of the linguistic features for coreference resolution were developed and evaluated in the context of the MP model, and thus it is not clear whether these features would fare similarly when used in combination with the joint CR model. Motivated by this observation, our goal in this paper is to examine the value of features derived from syntactic parses for the joint CR model. Note that parse-based features have been investigated extensively for the MP model. For example, they have been used to implement Binding Constraints (e.g., Luo and Zitouni (2005)) and encode syntactic salience (e.g., Haghighi and Klein (2009)). Rather than re-evaluate them for the CR model, we investigate two types of parse-based features that we believe are particularly interesting. First, we employ parse trees directly as structured features for the joint CR model. The main advantage of employing tree-based structured features is simplicity: we no longer need to design heuristics to extract the desired features (e.g., salience, Binding Constraints) from the parse trees, as designing heuristics can be time-consuming and sometimes difficult for certain tasks. Note, however, that previous attempts have employed structured features"
I11-1052,P09-1073,0,0.0390906,"Missing"
I11-1052,C08-1121,0,0.0144545,"two types of parse-based features that we believe are particularly interesting. First, we employ parse trees directly as structured features for the joint CR model. The main advantage of employing tree-based structured features is simplicity: we no longer need to design heuristics to extract the desired features (e.g., salience, Binding Constraints) from the parse trees, as designing heuristics can be time-consuming and sometimes difficult for certain tasks. Note, however, that previous attempts have employed structured features to train an MP model for anaphora resolution (Yang et al., 2006; Versley et al., 2008) and an anaphoricity classifier in the aforementioned pipeline architecture (Zhou and Kong, 2009). In both cases, the structured features are combined with their non-structured (i.e., flat) counterparts via a composite kernel and used to train a classification model. What is interesting for us to investigate in this paper, however, is the question of how to combine flat and structured fea2 The Baseline Coreference Model This section describes the Baseline CR model. Since the CR model is a natural extension of the 1 The dual form of Collins and Duffy’s (2002) ranking algorithm can also combine"
I11-1052,P06-1006,0,0.0426772,"el, we investigate two types of parse-based features that we believe are particularly interesting. First, we employ parse trees directly as structured features for the joint CR model. The main advantage of employing tree-based structured features is simplicity: we no longer need to design heuristics to extract the desired features (e.g., salience, Binding Constraints) from the parse trees, as designing heuristics can be time-consuming and sometimes difficult for certain tasks. Note, however, that previous attempts have employed structured features to train an MP model for anaphora resolution (Yang et al., 2006; Versley et al., 2008) and an anaphoricity classifier in the aforementioned pipeline architecture (Zhou and Kong, 2009). In both cases, the structured features are combined with their non-structured (i.e., flat) counterparts via a composite kernel and used to train a classification model. What is interesting for us to investigate in this paper, however, is the question of how to combine flat and structured fea2 The Baseline Coreference Model This section describes the Baseline CR model. Since the CR model is a natural extension of the 1 The dual form of Collins and Duffy’s (2002) ranking algo"
I11-1052,H05-1083,0,0.0197806,"), little is known about the joint CR model. In particular, most of the linguistic features for coreference resolution were developed and evaluated in the context of the MP model, and thus it is not clear whether these features would fare similarly when used in combination with the joint CR model. Motivated by this observation, our goal in this paper is to examine the value of features derived from syntactic parses for the joint CR model. Note that parse-based features have been investigated extensively for the MP model. For example, they have been used to implement Binding Constraints (e.g., Luo and Zitouni (2005)) and encode syntactic salience (e.g., Haghighi and Klein (2009)). Rather than re-evaluate them for the CR model, we investigate two types of parse-based features that we believe are particularly interesting. First, we employ parse trees directly as structured features for the joint CR model. The main advantage of employing tree-based structured features is simplicity: we no longer need to design heuristics to extract the desired features (e.g., salience, Binding Constraints) from the parse trees, as designing heuristics can be time-consuming and sometimes difficult for certain tasks. Note, ho"
I11-1052,P04-1018,0,0.0521707,"Missing"
I11-1052,P08-1096,0,0.0365492,"Missing"
I11-1052,D09-1102,0,0.196432,"resolution can be decomposed into two complementary subtasks: “(1) identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice”. These two subtasks are commonly known as anaphoricity determination and anaphora resolution, both of which have recently been tackled using machine learning techniques. More specifically, anaphoricity determination is typically tackled by training an anaphoricity classifier, which determines whether an NP is anaphoric or not (e.g., Poesio et al. (2004), Zhou and Kong (2009)). If so, the NP is passed to the 465 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 465–473, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP et al. (2008)). tures in a ranking model that employs joint learning. With the increasingly important role structured features and ranking models play in natural language learning, we believe that a method for combining flat and structured features for training a ranker would be of particular interest to natural language processing (NLP) researchers.1 Second, motivated in part by lexical semantics re"
I11-1052,D08-1067,1,\N,Missing
I13-1100,C12-2019,1,0.842602,"ejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been made to address these questions in the context of event coreference. Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 1 Introduction Event coreference resolution is the task of determining which even"
I13-1100,W12-4504,1,0.835276,"ejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been made to address these questions in the context of event coreference. Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 1 Introduction Event coreference resolution is the task of determining which even"
I13-1100,C12-1033,1,0.812462,"ejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been made to address these questions in the context of event coreference. Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 1 Introduction Event coreference resolution is the task of determining which even"
I13-1100,W09-4303,0,0.0606825,"h of its upstream components limiting its performance? Second, what are the major types of errors that are attributable to (and therefore can be fixed by improving) the resolution algorithm? We address these two questions by presenting a systematic analysis of a state-of-the-art Chinese event coreference resolver. Our decision to focus on Chinese can be attributed in part to the lack of publicly available results on Chinese event coreference resolution. In particular, to our knowledge, almost all recent work on event coreference has reported results for English (e.g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010"
I13-1100,I11-1012,0,0.16071,"mance? Second, what are the major types of errors that are attributable to (and therefore can be fixed by improving) the resolution algorithm? We address these two questions by presenting a systematic analysis of a state-of-the-art Chinese event coreference resolver. Our decision to focus on Chinese can be attributed in part to the lack of publicly available results on Chinese event coreference resolution. In particular, to our knowledge, almost all recent work on event coreference has reported results for English (e.g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no a"
I13-1100,W97-1311,0,0.791386,"rent in the output of each of its upstream components limiting its performance? Second, what are the major types of errors that are attributable to (and therefore can be fixed by improving) the resolution algorithm? We address these two questions by presenting a systematic analysis of a state-of-the-art Chinese event coreference resolver. Our decision to focus on Chinese can be attributed in part to the lack of publicly available results on Chinese event coreference resolution. In particular, to our knowledge, almost all recent work on event coreference has reported results for English (e.g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Rec"
I13-1100,D12-1045,0,0.330478,"are the major types of errors that are attributable to (and therefore can be fixed by improving) the resolution algorithm? We address these two questions by presenting a systematic analysis of a state-of-the-art Chinese event coreference resolver. Our decision to focus on Chinese can be attributed in part to the lack of publicly available results on Chinese event coreference resolution. In particular, to our knowledge, almost all recent work on event coreference has reported results for English (e.g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been m"
I13-1100,H05-1004,0,0.109371,"Missing"
I13-1100,P10-1144,0,0.0145043,"97), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been made to address these questions in the context of event coreference. Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 1 Introduction Event coreference resolution is the task of det"
I13-1100,J01-4004,0,0.260289,"Missing"
I13-1100,P09-1074,0,0.0191202,"g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On the other hand, no attempts have been made to address these questions in the context of event coreference. Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 1 Introduction Event coreference res"
I13-1100,M95-1005,0,0.127129,"Missing"
I13-1100,P10-1143,0,0.037852,"mponents limiting its performance? Second, what are the major types of errors that are attributable to (and therefore can be fixed by improving) the resolution algorithm? We address these two questions by presenting a systematic analysis of a state-of-the-art Chinese event coreference resolver. Our decision to focus on Chinese can be attributed in part to the lack of publicly available results on Chinese event coreference resolution. In particular, to our knowledge, almost all recent work on event coreference has reported results for English (e.g., Humphreys et al. (1997), Chen et al. (2009), Bejan and Harabagiu (2010), Chen et al. (2011), Lee et al. (2012)).1 Hence, the results in the paper can serve as a baseline against which future work on Chinese event coreference can be compared. It is worth mentioning that similar questions were faced by entity coreference researchers, who have reached a point where it is crucial to answer these questions in order to make further progress. For that reason, a number of recent research papers have focused on these questions via analyzing the inner workings of entity coreference resolvers (e.g., Stoyanov et al. (2009), Recasens and Hovy (2010), Chen and Ng (2012a)). On"
I13-1100,W09-3208,0,0.546661,"Missing"
I13-1191,W11-1701,0,0.567496,"Missing"
I13-1191,P98-1013,0,0.0579078,"Missing"
I13-1191,C08-2004,0,0.0731403,"Missing"
I13-1191,P11-1151,0,0.129147,"Missing"
I13-1191,C12-2045,1,0.885242,"Missing"
I13-1191,C10-2100,0,0.251035,"Missing"
I13-1191,I11-1082,0,0.0271384,"Missing"
I13-1191,W10-0214,0,0.600306,"Missing"
I13-1191,W06-1639,0,0.202583,"Missing"
I13-1191,N12-1072,0,0.445371,"Missing"
I13-1191,N10-1097,0,0.0326498,"Missing"
I13-1191,D10-1102,0,0.143618,"Missing"
I13-1191,C98-1013,0,\N,Missing
I13-1193,W10-4305,0,0.209093,"Missing"
I13-1193,H05-1004,0,0.683141,"on Coreference resolution is the task of determining which mentions in a text or dialogue refer to the same real-world entity. Designing appropriate evaluation metrics for coreference resolution is an important and challenging task. Since there is no consensus on which existing coreference evaluation metric is the best, the organizers of the CoNLL-2011 and CoNLL-2012 shared tasks on unrestricted coreference (Pradhan et al., 2011, 2012) decided to take the average of the scores computed by three coreference evaluation metrics, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as the official score of a participating coreference resolver. One weakness shared by virtually all existing coreference evaluation metrics is that they are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. In other words, while MUC, B3 , and CEAF were designed for evaluating coreference resolvers, their linguistic agnosticity implies that they can be used to evaluate any clustering task, including those that are not linguistic in nature.1 1 This statement is also true for BLANC (Recasens and Hovy, 2011), a Rand Index-based coreference"
I13-1193,W11-1901,0,0.0496999,"evaluated using linguistically agnostic metrics. Consequently, we propose a framework for incorporating linguistic awareness into commonly-used coreference evaluation metrics. 1 Introduction Coreference resolution is the task of determining which mentions in a text or dialogue refer to the same real-world entity. Designing appropriate evaluation metrics for coreference resolution is an important and challenging task. Since there is no consensus on which existing coreference evaluation metric is the best, the organizers of the CoNLL-2011 and CoNLL-2012 shared tasks on unrestricted coreference (Pradhan et al., 2011, 2012) decided to take the average of the scores computed by three coreference evaluation metrics, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as the official score of a participating coreference resolver. One weakness shared by virtually all existing coreference evaluation metrics is that they are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. In other words, while MUC, B3 , and CEAF were designed for evaluating coreference resolvers, their linguistic agnosticity implies that they can be used to ev"
I13-1193,W12-4501,0,0.123424,"Missing"
I13-1193,M95-1005,0,0.917597,"into commonly-used coreference evaluation metrics. 1 Introduction Coreference resolution is the task of determining which mentions in a text or dialogue refer to the same real-world entity. Designing appropriate evaluation metrics for coreference resolution is an important and challenging task. Since there is no consensus on which existing coreference evaluation metric is the best, the organizers of the CoNLL-2011 and CoNLL-2012 shared tasks on unrestricted coreference (Pradhan et al., 2011, 2012) decided to take the average of the scores computed by three coreference evaluation metrics, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as the official score of a participating coreference resolver. One weakness shared by virtually all existing coreference evaluation metrics is that they are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. In other words, while MUC, B3 , and CEAF were designed for evaluating coreference resolvers, their linguistic agnosticity implies that they can be used to evaluate any clustering task, including those that are not linguistic in nature.1 1 This statement is also true for BLANC (Reca"
I17-1060,P14-5010,0,0.00424888,"tive model. The error type for which each feature is originally developed is shown in parentheses. with an UA severity of 0. Unclear Justification (UJ) As with UAs, UJs are often very short. For this reason, we k-means cluster the sentence counts in our training set justifications, and label arguments whose justifications fall into the lowest cluster with an UJ severity of 2. As in the previous error, this rule takes precedence over other rules. To identify arguments with UJ severities of 1 or 0, we first dependency and discourse parse our assertions and justifications using Stanford CoreNLP (Manning et al., 2014) and Lin et al.’s (2014) PDTB-style discourse parser, respectively. Using the dependency parse, we identify the assertion’s main subject, which we assume is the first word that is a child in an nsubj or nsubjpass relationship. Next, we count the number of times words that lemmatically match the subject appear in the first argument of a contingencycause discourse relation in the justification. Finally, we k-means cluster these counts, assigning arguments with justifications in the highest cluster an UJ severity of 0, and those in the lowest cluster a severity of 1. These rules make sense becaus"
I17-1060,W11-1709,0,0.011885,"detailed, we encode as a feature the length in tokens and sentences of an argument’s assertion and justification. Word category-based features: For each of the following categories of words/tokens, we employ as features the absolute count and frequency per token in an argument’s justification: (1) definite and indefinite articles and first and second person pronouns, both of which we learned in Section 5.1 can be useful for detecting lack of objectivity; (2) question marks and quotations, which indicate how an argument is structured; (3) positive and negative sentiment words as determined by Mohammad and Yang (2011) since excessive emotion can also signal a lack of objectivity; (4) URLs, since these may be another way of citing evidence; (5) hedge words15 , which can be used to express argument uncertainty; and (6) phrases that indicate the author is giving an example (“e.g.”, “for instance”, “for example”). Evaluation In this section, we evaluate our approach to persuasiveness prediction. Since there is an element of randomness in our algorithm and the baselines (in which arguments get labeled), we report results using 5 repetitions of 5-fold cross validation. 6.1 Scoring Metrics We employ four evaluati"
I17-1060,W15-0515,0,0.0222185,"e previous one, when applying UA heuristics, rules with greater severity take precedence. Finally, we k-means cluster the counts of assertion content lemmas appearing in the justification and assertion lengths.10 If an argument is not in the lowest cluster in either of these, it gets labeled 7 We note that these lexical features are potentially specific to this particular domain. There have been a number of works examining objectivity and subjectivity that go beyond lexical features and use syntactic structures (Riloff and Wiebe, 2003; Wilson et al., 2005) and emotional and factual arguments (Oraby et al., 2015). 8 Since more than one heuristic might apply to a given argument, we leave an argument unlabeled if the heuristics tell us to apply inconsistent labels to it. This is also how we handle contradictory heuristics for the remaining errors. 9 We develop heuristics for extracting references from the justification. See the Appendix for these heuristics. 10 We use k = 6 for assertion length clustering because assertions vary greatly in length. https://languagetool.org/ Unless otherwise noted, k = 4 in k-means clustering. 597 1 2 3 4 5 6 7 8 9 10 # of grammar errors per sentence in justification (GE)"
I17-1060,D16-1129,0,0.0173992,"ion, which explains in an average of 6.9 sentences why the author believes her assertion. We ask two native speakers of English to annotate each of the 1,208 arguments with a persuasiveness score after familiarizing them with the (topicand domain-independent) scoring rubric (see Table 2). Specifically, we ask our annotators to score each argument’s persuasiveness on a scale of 1−6. The example argument in Table 1 gets a persuasiveness score of 2 because it could be expressed more clearly. Related Work There have been several recent attempts to address tasks related to argument persuasiveness. Habernal and Gurevych (2016a,b) rank a pair of arguments w.r.t. persuasiveness, but ranking alone cannot tell us how persuasive an argument is. Persing and Ng (2015) score a student essay based on whether it makes a (un)convincing argument for its thesis. Using the conversations in the ChangeMyView subreddit, Tan et al. (2016) study factors affecting whether a challenger can successfully persuade a commenter to change the view she expressed in her original post. While Wei et al. (2016) also predict the persuasiveness of debate posts, their work differs from ours in several aspects. First, many of their de1 See http://ww"
I17-1060,P15-1053,1,0.817463,"of the 1,208 arguments with a persuasiveness score after familiarizing them with the (topicand domain-independent) scoring rubric (see Table 2). Specifically, we ask our annotators to score each argument’s persuasiveness on a scale of 1−6. The example argument in Table 1 gets a persuasiveness score of 2 because it could be expressed more clearly. Related Work There have been several recent attempts to address tasks related to argument persuasiveness. Habernal and Gurevych (2016a,b) rank a pair of arguments w.r.t. persuasiveness, but ranking alone cannot tell us how persuasive an argument is. Persing and Ng (2015) score a student essay based on whether it makes a (un)convincing argument for its thesis. Using the conversations in the ChangeMyView subreddit, Tan et al. (2016) study factors affecting whether a challenger can successfully persuade a commenter to change the view she expressed in her original post. While Wei et al. (2016) also predict the persuasiveness of debate posts, their work differs from ours in several aspects. First, many of their de1 See http://www.hlt.utdallas.edu/ ˜persingq/Debate/ for a complete list of our annotations. 2 595 http://idebate.org/ Score 6 5 4 3 2 1 Description of A"
I17-1060,P16-1150,0,0.0657096,"ion, which explains in an average of 6.9 sentences why the author believes her assertion. We ask two native speakers of English to annotate each of the 1,208 arguments with a persuasiveness score after familiarizing them with the (topicand domain-independent) scoring rubric (see Table 2). Specifically, we ask our annotators to score each argument’s persuasiveness on a scale of 1−6. The example argument in Table 1 gets a persuasiveness score of 2 because it could be expressed more clearly. Related Work There have been several recent attempts to address tasks related to argument persuasiveness. Habernal and Gurevych (2016a,b) rank a pair of arguments w.r.t. persuasiveness, but ranking alone cannot tell us how persuasive an argument is. Persing and Ng (2015) score a student essay based on whether it makes a (un)convincing argument for its thesis. Using the conversations in the ChangeMyView subreddit, Tan et al. (2016) study factors affecting whether a challenger can successfully persuade a commenter to change the view she expressed in her original post. While Wei et al. (2016) also predict the persuasiveness of debate posts, their work differs from ours in several aspects. First, many of their de1 See http://ww"
I17-1060,P16-2032,0,0.0227934,"ressed more clearly. Related Work There have been several recent attempts to address tasks related to argument persuasiveness. Habernal and Gurevych (2016a,b) rank a pair of arguments w.r.t. persuasiveness, but ranking alone cannot tell us how persuasive an argument is. Persing and Ng (2015) score a student essay based on whether it makes a (un)convincing argument for its thesis. Using the conversations in the ChangeMyView subreddit, Tan et al. (2016) study factors affecting whether a challenger can successfully persuade a commenter to change the view she expressed in her original post. While Wei et al. (2016) also predict the persuasiveness of debate posts, their work differs from ours in several aspects. First, many of their de1 See http://www.hlt.utdallas.edu/ ˜persingq/Debate/ for a complete list of our annotations. 2 595 http://idebate.org/ Score 6 5 4 3 2 1 Description of Argument Persuasiveness A very persuasive, clear argument. It would persuade most previously uncommitted readers and is devoid of problems that might detract from its persuasiveness or make it difficult to understand. A persuasive, or only pretty clear argument. It would persuade most previously uncommitted readers, but may"
I17-1060,W03-1014,0,\N,Missing
I17-1060,E17-1070,0,\N,Missing
J08-3006,W99-0613,0,0.212973,"Missing"
J08-3006,P92-1017,0,0.0651989,"ng for Computational Linguistics Steven Abney (University of Michigan) Boca Raton, FL: Chapman & Hall / CRC (Computer science and data analysis series, edited by David Madigan et al.), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7, $79.95, £44.99 Reviewed by Vincent Ng University of Texas at Dallas Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classiﬁcation (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin ¨ 2004) or NLP texts (e.g., Manning and Schutze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one ha"
J08-3006,P95-1026,0,0.182989,"88-559-7, $79.95, £44.99 Reviewed by Vincent Ng University of Texas at Dallas Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classiﬁcation (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin ¨ 2004) or NLP texts (e.g., Manning and Schutze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney’s book Semisupervised Learning for Computational Linguistics"
K15-1024,N07-1033,0,0.162173,"e extend a supervised baseline employing only word pairs and LDA-induced topics as features (see Section 4) with two types of humansupplied knowledge. First, we employ annotator rationales. In the context of requirements traceability, rationales are human-annotated words or phrases in a pair of high- and low-level requirements that motivated a human annotator to establish a link between the two. In other words, rationales contain the information relevant to the establishment of a link. Therefore, using them could allow a learner to focus on the relevant portions of a requirement. Motivated by Zaidan et al. (2007), we employ rationales to create additional training instances for the learner. Second, we employ an ontology hand-built by a domain expert. A sample ontology built for the Pine domain is shown in Table 1. As we can see, the ontology contains a verb clustering and a noun clustering: the verbs are clustered by the function they perform, whereas a noun cluster corresponds to a (domain-specific) semantic type. We employ 1 Note that both the rationales and the words/phrases in the ontology could help the learner by allowing it to focus on relevant materials in a given pair of requirements. Neverth"
K15-1024,de-marneffe-etal-2006-generating,0,0.0666787,"Missing"
L16-1631,W06-0901,0,0.0901384,"and conclusions in Section 7. 2. Related Work Early work on event coreference resolution was primarily evaluated on the MUC and ACE corpora, both of which contained within-document event coreference links. While event coreference research in MUC was limited to several scenarios such as terrorist attacks, management succession and resignation (e.g., Humphreys et al. (1997)), the ACE program takes a further step towards processing more finegrained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006), Chen and Ng (2013; 2014)). Improvements to this standard approach include the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) proposed an unsupervised probabilistic model for event coreference resolution that rivaled its supervised counterparts when evaluated on the ACE corpus. There have also been attempts to evaluate within-document event coreference resolvers o"
L16-1631,D15-1247,0,0.04462,"on. While calling their approach a joint approach, they employ neither joint learning nor joint inference. A closer look at the ECB corpus reveals that within-document coreference links are only partially annotated (Liu et al., 2014): in almost all documents only the first few sentences are annotated with entity and event coreference links. In response to the missing links problem, the ECB+ corpus (Cybulska and Vossen, 2014b), an extension to ECB, was created. ECB+ was used by Yang et al. (2015) to evaluate their hierarchical distance-dependent Bayesian event coreference model. More recently, Araki and Mitamura (2015) have evaluated their event coreference system on the ProcessBank corpus (Berant et al., 2014), a corpus of 200 paragraphs taken from a biology textbook. Specifically, they performed event trigger identification and event coreference resolution simultaneously using a structured perceptron. The newest event coreference corpus is perhaps the one used in the KBP 2015 Event Nugget Detection and Coreference shared task. The teams that achieved the highest scores have adopted different strategies for this task. RPI’s system viewed the event nugget coreference space as an undirected weighted graph in"
L16-1631,araki-etal-2014-detecting,0,0.0722073,"nymy relations extracted from WordNet). The IC corpus, which at the time of writing is not yet publicly available, is different from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach was recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Bejan and Harabagiu (2010; 2014) evaluated their unsupervised nonparametric models on the EventCorefBank (ECB) corpus, which is composed of documents annotated with both within-document and cross-document event coreference links. Lee et al. (2012) extended the ECB corpus by annotating it with entity coreference links, which allow them to propose a “joint” method that iteratively performs entity coreference and event coreference by allowing one model to make use of the partial results produced so far for the other model in each iteration. Whil"
L16-1631,P10-1143,0,0.0771076,"e of writing is not yet publicly available, is different from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach was recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Bejan and Harabagiu (2010; 2014) evaluated their unsupervised nonparametric models on the EventCorefBank (ECB) corpus, which is composed of documents annotated with both within-document and cross-document event coreference links. Lee et al. (2012) extended the ECB corpus by annotating it with entity coreference links, which allow them to propose a “joint” method that iteratively performs entity coreference and event coreference by allowing one model to make use of the partial results produced so far for the other model in each iteration. While calling their approach a joint approach, they employ neither joint learning"
L16-1631,J14-2004,0,0.228113,"Missing"
L16-1631,D14-1159,0,0.0129997,"erence. A closer look at the ECB corpus reveals that within-document coreference links are only partially annotated (Liu et al., 2014): in almost all documents only the first few sentences are annotated with entity and event coreference links. In response to the missing links problem, the ECB+ corpus (Cybulska and Vossen, 2014b), an extension to ECB, was created. ECB+ was used by Yang et al. (2015) to evaluate their hierarchical distance-dependent Bayesian event coreference model. More recently, Araki and Mitamura (2015) have evaluated their event coreference system on the ProcessBank corpus (Berant et al., 2014), a corpus of 200 paragraphs taken from a biology textbook. Specifically, they performed event trigger identification and event coreference resolution simultaneously using a structured perceptron. The newest event coreference corpus is perhaps the one used in the KBP 2015 Event Nugget Detection and Coreference shared task. The teams that achieved the highest scores have adopted different strategies for this task. RPI’s system viewed the event nugget coreference space as an undirected weighted graph in which the nodes represent all the event nuggets and the edge weight indicates coreference con"
L16-1631,Q14-1022,0,0.0635132,"Missing"
L16-1631,W09-3208,0,0.493652,"ited to several scenarios such as terrorist attacks, management succession and resignation (e.g., Humphreys et al. (1997)), the ACE program takes a further step towards processing more finegrained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006), Chen and Ng (2013; 2014)). Improvements to this standard approach include the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) proposed an unsupervised probabilistic model for event coreference resolution that rivaled its supervised counterparts when evaluated on the ACE corpus. There have also been attempts to evaluate within-document event coreference resolvers on other corpora, such as OntoNotes (Pradhan et al., 2007). For instance, Chen et al. (2011) trained multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun-noun coreference) on the OntoNot"
L16-1631,I13-1100,1,0.887192,"ions in Section 7. 2. Related Work Early work on event coreference resolution was primarily evaluated on the MUC and ACE corpora, both of which contained within-document event coreference links. While event coreference research in MUC was limited to several scenarios such as terrorist attacks, management succession and resignation (e.g., Humphreys et al. (1997)), the ACE program takes a further step towards processing more finegrained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006), Chen and Ng (2013; 2014)). Improvements to this standard approach include the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) proposed an unsupervised probabilistic model for event coreference resolution that rivaled its supervised counterparts when evaluated on the ACE corpus. There have also been attempts to evaluate within-document event coreference resolvers on other corpora, su"
L16-1631,chen-ng-2014-sinocoreferencer,1,0.90683,"Missing"
L16-1631,N15-1116,1,0.822583,"Humphreys et al. (1997)), the ACE program takes a further step towards processing more finegrained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006), Chen and Ng (2013; 2014)). Improvements to this standard approach include the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) proposed an unsupervised probabilistic model for event coreference resolution that rivaled its supervised counterparts when evaluated on the ACE corpus. There have also been attempts to evaluate within-document event coreference resolvers on other corpora, such as OntoNotes (Pradhan et al., 2007). For instance, Chen et al. (2011) trained multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun-noun coreference) on the OntoNotes corpus. However, since event coreference links and entity coreference links are not distinguis"
L16-1631,I11-1012,0,0.0228278,"de the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) proposed an unsupervised probabilistic model for event coreference resolution that rivaled its supervised counterparts when evaluated on the ACE corpus. There have also been attempts to evaluate within-document event coreference resolvers on other corpora, such as OntoNotes (Pradhan et al., 2007). For instance, Chen et al. (2011) trained multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun-noun coreference) on the OntoNotes corpus. However, since event coreference links and entity coreference links are not distinguished in OntoNotes, Chen et al. made the simplifying assumption that event coreference chains are all and only those coreference chains that involve at least one verb when performing event coreference on OntoNotes. Researchers have also employed other corpora when evaluating their event coreference resolvers. For instance, Cybulska"
L16-1631,P15-2049,1,0.8912,"Missing"
L16-1631,D15-1087,1,0.887494,"Missing"
L16-1631,W97-1311,0,0.258534,"he corpus used in the official KBP 2015 Event Nugget Detection and Coreference task. Sections 4 and 5 discuss our baseline system and our multi-pass sieve approach to event coreference resolution. Finally, we present evaluation results in Section 6 and conclusions in Section 7. 2. Related Work Early work on event coreference resolution was primarily evaluated on the MUC and ACE corpora, both of which contained within-document event coreference links. While event coreference research in MUC was limited to several scenarios such as terrorist attacks, management succession and resignation (e.g., Humphreys et al. (1997)), the ACE program takes a further step towards processing more finegrained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006), Chen and Ng (2013; 2014)). Improvements to this standard approach include the use of (1) feature weighting to train a better model (McConky et al., 2012), and (2) graph-based clustering algorithms to produce event coreference clusters (e.g., Chen and Ji (2009), Sangeetha and Arock (2012)). Despite the successes of supervised approaches, Chen and Ng (2015) propo"
L16-1631,D12-1045,0,0.135639,"rm coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach was recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Bejan and Harabagiu (2010; 2014) evaluated their unsupervised nonparametric models on the EventCorefBank (ECB) corpus, which is composed of documents annotated with both within-document and cross-document event coreference links. Lee et al. (2012) extended the ECB corpus by annotating it with entity coreference links, which allow them to propose a “joint” method that iteratively performs entity coreference and event coreference by allowing one model to make use of the partial results produced so far for the other model in each iteration. While calling their approach a joint approach, they employ neither joint learning nor joint inference. A closer look at the ECB corpus reveals that within-document coreference links are only partially annotated (Liu et al., 2014): in almost all documents only the first few sentences are annotated with"
L16-1631,J13-4004,0,0.0728609,"Missing"
L16-1631,liu-etal-2014-supervised,0,0.662265,"resolution, event coreference resolution is not only much less studied, but it is arguably more challenging. The challenge stems in part from the fact that an event coreference resolver typically lies towards the end of the standard information extraction pipeline, assuming as input the noisy outputs of its upstream components. More specifically, an event coreference resolver assumes as input not only the event triggers, their types/subtypes, and their arguments, but also entity coreference information. Different corpora have been used to train and evaluate event coreference resolvers, but as Liu et al. (2014) pointed out, not all of them were carefully annotated. As will be discussed in more detail in Section 2, OntoNotes and ECB have only be partially annotated with event coreference links. Among the publicly-available corpus, the ACE 2005 corpus is arguably the one that is most complete with respect to the annotation of event coreference links. In fact, the majority of recent work on event coreference was evaluated on the ACE 2005 corpus. As an event coreference corpus, ACE 2005 has a major weakness: it adopts a strict notion of event identity. Specifically, two event mentions were annotated as"
L16-1631,H05-1004,0,0.214343,"Missing"
L16-1631,P14-5010,0,0.00470216,"es: lexical, syntactic and semantic. Lexical: word unigrams (wi−2 ,wi−1 ,wi ,wi+1 ,wi+2 ); word bigrams (wi−1 wi , wi wi+1 ); word trigrams (wi−2 wi−1 wi , wi−1 wi wi+1 , wi wi+1 wi+2 ); the part-of-speech tag of wi ; lemmatized word unigrams, bigrams and trigrams. Syntactic: depth of wi ’s node in its syntactic parse tree; the path from the leaf node of wi to the root in its syntactic parse tree; the phrase structure expanded by the parent of wi ’s node; the phrase type of wi ’s node. We compute the syntactic features based on the syntactic parse trees returned by Stanford’s CoreNLP package (Manning et al., 2014). Semantic: the WordNet synset id of wi ; the WordNet synset ids of wi ’s hypernym, its parent, and its grandparent. When computing these semantic features, we only use the synset corresponding to wi ’s first sense. We improve the recall of event trigger detection in a postprocessing process as follows. First, we construct a wordlist containing triggers that appear infrequently (less than 10 times) in the training data and do not belong more than one subtype according to the training data. For example, the word “hijack” appears only a few times in the training data but is always labeled as “Co"
L16-1631,D10-1048,0,0.0873328,"Missing"
L16-1631,W15-0812,0,0.050838,"ail in Section 2, OntoNotes and ECB have only be partially annotated with event coreference links. Among the publicly-available corpus, the ACE 2005 corpus is arguably the one that is most complete with respect to the annotation of event coreference links. In fact, the majority of recent work on event coreference was evaluated on the ACE 2005 corpus. As an event coreference corpus, ACE 2005 has a major weakness: it adopts a strict notion of event identity. Specifically, two event mentions were annotated as coreferent if and only if “they had the same agent(s), patient(s), time, and location” (Song et al., 2015), and their event attributes (polarity, modality, genericity, and tense) are not incompatible. This is arguably an overly strict definition of event coreference, as some event mentions are intuitively coreferent even if their time and/or location arguments are not identical. The new KBP 2015 event coreference corpus was created in response to the aforementioned weakness of the ACE 2005 corpus (Song et al., 2015). It was annotated using the Rich ERE guidelines, which are arguably more realistic in the sense that they mimic more closely a human’s judgment of whether two event mentions are corefe"
L16-1631,J01-4004,0,0.340477,"r in the constituent parse tree; the phrase type of the trigger; the path from the candidate argument to the trigger in the constituent parse tree; the dependency path from the candidate argument to the trigger. To create test instances, we pair each candidate event mention (i.e., an event mention whose trigger was identified in Section 4.1) with each of its candidate event arguments. The test instances are represented using the same set of features as the training instances. 4.3. Event Coreference Resolution This component identifies event coreference links by combining a mention-pair model (Soon et al., 2001), which is a binary classifier that determines whether two event mentions are co-referring or not, with a closest-first single-link clustering algorithm, which selects as the antecedent of an event mention e the closest preceding event mention that is classified as coreferent with e. We train the mentionpair model using the libSVM software package (Chang and Lin, 2001) as follows. We first divide the training documents of the KBP 2015 coreference corpus into two sets: a 128-document training set for model training, and a 30document development set for jointly tuning the regularization paramete"
L16-1631,M95-1005,0,0.771386,"Missing"
L16-1631,Q15-1037,0,0.152389,"reference by allowing one model to make use of the partial results produced so far for the other model in each iteration. While calling their approach a joint approach, they employ neither joint learning nor joint inference. A closer look at the ECB corpus reveals that within-document coreference links are only partially annotated (Liu et al., 2014): in almost all documents only the first few sentences are annotated with entity and event coreference links. In response to the missing links problem, the ECB+ corpus (Cybulska and Vossen, 2014b), an extension to ECB, was created. ECB+ was used by Yang et al. (2015) to evaluate their hierarchical distance-dependent Bayesian event coreference model. More recently, Araki and Mitamura (2015) have evaluated their event coreference system on the ProcessBank corpus (Berant et al., 2014), a corpus of 200 paragraphs taken from a biology textbook. Specifically, they performed event trigger identification and event coreference resolution simultaneously using a structured perceptron. The newest event coreference corpus is perhaps the one used in the KBP 2015 Event Nugget Detection and Coreference shared task. The teams that achieved the highest scores have adopted"
L16-1695,W09-1206,0,0.0172306,"Missing"
L16-1695,W04-2401,0,0.025808,"were tackled using the so-called pipeline architecture: the given task is broken into a series of sub-tasks such that the output of one sub-task is an input to the next sub-task in the sequence. This pipeline architecture is appealing for various reasons, including modularity, modeling convenience, and manageable computational complexity. However, it suffers from the error propagation problem: errors made in one sub-task are propagated to the next sub-task in the sequence. Realizing this weakness, researchers have turned to joint inference approaches such as Integer Linear Programming (ILP) (Roth and Yih, 2004) and Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which often outperform their pipeline counterparts. These joint inference approaches enable manual specification of constraints. These constraints effectively allow incorporation of background knowledge into NLP systems, addressing the aforementioned error propagation problem by allowing the downstream components to influence the upstream components in a pipelined system architecture. To date, MLNs have been underused in NLP applications, and are arguably much less popular among NLP researchers than ILP. While NLP researchers who des"
L16-1695,D14-1090,1,0.889554,"Missing"
L16-1695,P13-1161,0,0.142285,"se (how easy is it to encode constraints?), scalability (how efficient is the inference procedure, especially when applied to large problems?), and performance (how well do these inference algorithms perform?). To facilitate the comparison, we will discuss how ILP and MLNs can be applied to the task of fine-grained opinion extraction. While many tasks could have been chosen, we chose this task not only for its importance but also for the fact that its sophistication allows us to demonstrate the differences between the two inference frameworks. In fact, while ILP has been applied to this task (Yang and Cardie, 2013) (henceforth Y&C), MLNs have not, so it would be informative for us to consider how MLNs can be applied to it. To our knowledge, this is the first systematic comparison of ILP and MLNs on an NLP task. It is worth mentioning that while our discussion is centered on this task, many of our conclusions and recommendations are generally applicable to other NLP tasks. The rest of this paper is organized as follows. Section 2 provides some background information, including the task of fine-grained opinion extraction, the corpus, and the two inference algorithms. Section 3 describes the constraints th"
L18-1585,P13-1025,0,0.0445802,"Missing"
L18-1585,P14-5010,0,0.00259087,"y tasks, glv performs better on the most frequent class than ngr but worse on the second most frequent class. 5.1. Feature Sets 6. Error Analysis In our preliminary experiments, we experiment with two feature sets. We plan to conduct experiments with additional features in the final version of the paper. N-gram features. We encode each lemmatized and unlemmatized unigram and bigram collected from the training comments as a binary feature. In a similar manner, we include the unigram and bigram along with their POS tag as in Xu et al. (2012a). To extract these features we used Stanford CoreNLP (Manning et al., 2014). GloVe Embeddings (glv). Word embeddings were created to overcome certain problems with the bag of words (BOW) representation, like sparsity, and weight in correlations of semantically similar words. For this reason, and following Nobota et al. (Nobata et al., 2016), we create a distributed representation of the comments by averaging the word vector of each lowercase token in the comment found in the Twitter corpus pre-trained GloVe vectors (Pennington et al., 2014). The resulting comment vector representation is a 200 dimensional array that is concatenated with the existing BOW. In order to"
L18-1585,P16-2065,0,0.49636,"Missing"
L18-1585,K15-1032,0,0.0948021,"l modeling of trolling. 2. Related Work In this section, we discuss related work in the areas of trolling, bullying, abusive language detection and polite3701 ness, as they partially address the problem presented in this work. In the realm of psychology, Bishop (2013; 2014) elaborate a deep description of a troll’s personality, motivations, effects on the community that trolls interfere in and the criminal and psychological aspects of trolls. Their main focus are flaming (trolls), and hostile and aggressive interactions between users (O’sullivan and Flanagin, 2003). On the computational side, Mihaylov et al. (2015b; 2015a; 2016) address the problem of identifying opinion manipulation trolls, including paid trolls in news community forums. Not only do they focus solely on troll identification, but the major difference with this work is that all their predictions are based on non-linguistic information such as number of votes, dates, number of comments and so on. In a networks related framework, Kumar et al. (2014) and Guha et al. (2004) present a methodology to identify malicious individuals in a network based solely on the network’s properties rather than on the textual content of comments. Cambria et"
L18-1585,R15-1058,0,0.0740782,"l modeling of trolling. 2. Related Work In this section, we discuss related work in the areas of trolling, bullying, abusive language detection and polite3701 ness, as they partially address the problem presented in this work. In the realm of psychology, Bishop (2013; 2014) elaborate a deep description of a troll’s personality, motivations, effects on the community that trolls interfere in and the criminal and psychological aspects of trolls. Their main focus are flaming (trolls), and hostile and aggressive interactions between users (O’sullivan and Flanagin, 2003). On the computational side, Mihaylov et al. (2015b; 2015a; 2016) address the problem of identifying opinion manipulation trolls, including paid trolls in news community forums. Not only do they focus solely on troll identification, but the major difference with this work is that all their predictions are based on non-linguistic information such as number of votes, dates, number of comments and so on. In a networks related framework, Kumar et al. (2014) and Guha et al. (2004) present a methodology to identify malicious individuals in a network based solely on the network’s properties rather than on the textual content of comments. Cambria et"
L18-1585,I13-1066,0,0.197345,"Missing"
L18-1585,D14-1162,0,0.0913813,"Missing"
L18-1585,R15-1086,0,0.130898,"Missing"
L18-1585,H05-1044,0,0.159156,"Missing"
L18-1585,N12-1084,0,0.299782,"Missing"
L18-1585,N13-1082,0,0.0488016,"Missing"
L18-1588,P14-1119,1,0.828763,"or Illustration with Example Figure 1 shows part of an example document1 . In this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present keyphrase extraction systems using this example. Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that frequently appears in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word(Hasan and Ng, 2014). It is not easy to reject a non-keyphrase containing a word with a high term frequency: many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. To be more concrete, consider the news article in Figure 1. The word Cattle has a significant presence in the document. Consequently, the system not only correctly predict British cattle as a keyphrase, but also erroneously predict cattle industry, cattle feed, and cattle brain as keyphrases, yielding overgeneration errors. 1 Documen"
L18-1588,W03-1028,0,0.216536,"GME (Ferragina and Scaiella, 2010) to obtain the underlying concepts in documents. TAGME is a powerful topic annotator. It identifies meaningful sequences of words in a short text and link them to Underlined: Keyphrase annotated to Wikipedia Entity by TagMe a pertinent Wikipedia page, as shown in Figure 1. These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts. 3.1.2. Lexical Unit Selection This step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun groups. We follow (Wan and Xiao, 2008a) and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova et al., 2003), and then extract the noun groups whose pattern is zero or more adjectives followed by one or more nouns. The pattern can be represented using regular expressions as follows (JJ) ∗ (N N |N N S|N N P |N N P S)+ where JJ indicates adjectives and various forms of nouns are represented using NN, NNS and NNP . 3.1.3. Graph building"
L18-1588,D09-1027,0,0.0331304,"se two logic, a candidate satisfying the constrains of Step 3 is not likely to be picked in the best keyphrase set Ω, so we can prune it before the optimalization process. 4. 4.1. Experiments and Results Corpora The DUC-2001 dataset (Over, 2001), which is a collection of 308 news articles, is annotated by (Wan and Xiao, 2008b). The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by (Hulth, 2003) and later by Mihalcea and (Mihalcea and Tarau, 2004) and (Liu et al., 2009). The NUS Keyphrase Corpus (Nguyen and Kan, 2007) includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams. 1: while |Ω |&lt; k"
L18-1588,D10-1036,0,0.137779,"ning data to get better generalization performance, more and more researchers focus on unsupervised methods. Traditional methods of unsupervised keyphrase extraction mostly focus on getting information of document from word frequency and document structure(Hasan and Ng, 2014), however, after years of attempting, the performance seems very hard to be improved any more. Based on this observation, it is reasonable to suspect that the document itself possibly cannot provide enough information for keyphrase extraction task. To get good coverage of the main topics of the document, Topical PageRank (Liu et al., 2010) started to adopt topical information in automatic keyphrase extraction. The main idea of Topical PageRank is to extract the top topics of the document using LDA, then sum over the scores of a candidate phrase under each topic to be the final score. The main problems with Topical PageRank are: First, The topics are too general. Second, since they are using LDA, they only classify the words to several topics, but don’t know what the topics exactly are. However, the topical information we need for keyphrase extraction should be precise. As shown in Figure 1, the difference between a correct keyp"
L18-1588,N03-1033,0,0.178262,"pertinent Wikipedia page, as shown in Figure 1. These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts. 3.1.2. Lexical Unit Selection This step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun groups. We follow (Wan and Xiao, 2008a) and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova et al., 2003), and then extract the noun groups whose pattern is zero or more adjectives followed by one or more nouns. The pattern can be represented using regular expressions as follows (JJ) ∗ (N N |N N S|N N P |N N P S)+ where JJ indicates adjectives and various forms of nouns are represented using NN, NNS and NNP . 3.1.3. Graph building We build a semantic graph G = [V ; E] in which the set of vertices V is the union of the concept set C and the candidate keyphrase set P —i.e., V = P ∪ C. In the graph, each unique concept c ∈ C or candidate keyphrase p ∈ P for document d corresponds to a node. The node"
L18-1588,C08-1122,0,0.065812,"E is a powerful topic annotator. It identifies meaningful sequences of words in a short text and link them to Underlined: Keyphrase annotated to Wikipedia Entity by TagMe a pertinent Wikipedia page, as shown in Figure 1. These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts. 3.1.2. Lexical Unit Selection This step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun groups. We follow (Wan and Xiao, 2008a) and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova et al., 2003), and then extract the noun groups whose pattern is zero or more adjectives followed by one or more nouns. The pattern can be represented using regular expressions as follows (JJ) ∗ (N N |N N S|N N P |N N P S)+ where JJ indicates adjectives and various forms of nouns are represented using NN, NNS and NNP . 3.1.3. Graph building We build a semantic graph G = [V ; E] in which the set of vertices V is the union of the con"
N03-1023,W01-0501,1,\N,Missing
N03-1023,M95-1005,0,\N,Missing
N03-1023,E03-1008,0,\N,Missing
N03-1023,W02-1008,1,\N,Missing
N03-1023,N01-1023,0,\N,Missing
N03-1023,P01-1005,0,\N,Missing
N03-1023,J01-4004,0,\N,Missing
N03-1023,W99-0613,0,\N,Missing
N03-1023,P02-1046,0,\N,Missing
N03-1023,P02-1045,0,\N,Missing
N07-1020,P03-1036,0,0.182348,"is another class of unsupervised morphological learning algorithms whose design is driven by the Minimum Description Length (MDL) principle. Specifically, EM is used to iteratively segment a list of words using some predefined heuristics until the length of the morphological grammar converges to a minimum. Brent et al. (1995) are the first to introduce an informationtheoretic notion of compression to represent the MDL framework. Goldsmith (2001) also adopts the MDL approach, providing a new compression system that incorporates signatures when measuring the length of the morphological grammar. Creutz (2003) proposes a probabilistic maximum a posteriori formulation that uses prior distributions of morpheme length and frequency to measure the goodness of an induced morpheme, achieving better results for Finnish but worse results for English in comparison to Goldsmith’s Linguistica. 3 The Basic Morpheme Induction Algorithm Our unsupervised segmentation algorithm is composed of two steps: (1) inducing prefixes, suffixes and roots from a vocabulary that consists of words taken from a large corpus, and (2) segmenting a word using these induced morphemes. This section describes our basic morpheme induc"
N07-1020,W98-1239,0,0.173434,"lem, we automatically acquire allomorphs and orthographic change rules from an unannotated corpus. These rules also allow us to output the actual segmentation of the words that exhibit spelling changes during morpheme attachment, thus avoiding the segmentation of “denial” as “deni”+”al”, as is typically done in existing morphological parsers. In addition to addressing the aforementioned problems, our segmentation algorithm has two appealing features. First, it can segment words with any number of morphemes, whereas many analyzers can only be applied to words with one root and one suffix (e.g. DéJean (1998), Snover and Brent (2001)). Second, it exhibits robust performance even when inducing morphemes from a very large vocabulary, whereas Goldsmith’s (2001) and Freitag’s (2005) morphological analyzers perform well only when a small vocabulary is employed, showing deteriorating performance as the vocabulary size increases. The rest of this paper is organized as follows. Section 2 presents related work on unsupervised morphological analysis. In Section 3, we describe our basic morpheme induction algorithm. We then show how to exploit the induced morphemes to (1) detect incorrect attachments by usin"
N07-1020,W05-0617,0,0.0368265,"ough very successful, knowledge-based morphological analyzers operate by relying on manually designed segmentation heuristics (e.g. Koskenniemi (1983)), which require a lot of linguistic expertise and are time-consuming to construct. As a result, research in morphological analysis has exhibited a shift to unsupervised approaches, in which a word is typically segmented based on morphemes that are automatically induced from an unannotated corpus. Unsupervised approaches have achieved considerable success for English and many European languages (e.g. Goldsmith (2001), Schone and Jurafsky (2001), Freitag (2005)). The recent PASCAL Challenge on Unsupervised Segmentation of Words into Morphemes1 has further intensified interest in this problem, selecting as target languages English as well as two highly agglutinative languages, Turkish and Finnish. However, the evaluation of the Challenge reveals that (1) the success of existing unsupervised morphological parsers does not carry over to the two agglutinative languages, and (2) no segmentation algorithm achieves good performance for all three languages. Motivated by these state-of-the-art results, our goal in this paper is to develop an unsupervised mor"
N07-1020,J01-2001,0,0.843455,"aningbearing elements of natural languages. Though very successful, knowledge-based morphological analyzers operate by relying on manually designed segmentation heuristics (e.g. Koskenniemi (1983)), which require a lot of linguistic expertise and are time-consuming to construct. As a result, research in morphological analysis has exhibited a shift to unsupervised approaches, in which a word is typically segmented based on morphemes that are automatically induced from an unannotated corpus. Unsupervised approaches have achieved considerable success for English and many European languages (e.g. Goldsmith (2001), Schone and Jurafsky (2001), Freitag (2005)). The recent PASCAL Challenge on Unsupervised Segmentation of Words into Morphemes1 has further intensified interest in this problem, selecting as target languages English as well as two highly agglutinative languages, Turkish and Finnish. However, the evaluation of the Challenge reveals that (1) the success of existing unsupervised morphological parsers does not carry over to the two agglutinative languages, and (2) no segmentation algorithm achieves good performance for all three languages. Motivated by these state-of-the-art results, our goal in"
N07-1020,N01-1024,0,0.599019,"nts of natural languages. Though very successful, knowledge-based morphological analyzers operate by relying on manually designed segmentation heuristics (e.g. Koskenniemi (1983)), which require a lot of linguistic expertise and are time-consuming to construct. As a result, research in morphological analysis has exhibited a shift to unsupervised approaches, in which a word is typically segmented based on morphemes that are automatically induced from an unannotated corpus. Unsupervised approaches have achieved considerable success for English and many European languages (e.g. Goldsmith (2001), Schone and Jurafsky (2001), Freitag (2005)). The recent PASCAL Challenge on Unsupervised Segmentation of Words into Morphemes1 has further intensified interest in this problem, selecting as target languages English as well as two highly agglutinative languages, Turkish and Finnish. However, the evaluation of the Challenge reveals that (1) the success of existing unsupervised morphological parsers does not carry over to the two agglutinative languages, and (2) no segmentation algorithm achieves good performance for all three languages. Motivated by these state-of-the-art results, our goal in this paper is to develop an"
N07-1020,P01-1063,0,0.237752,"ically acquire allomorphs and orthographic change rules from an unannotated corpus. These rules also allow us to output the actual segmentation of the words that exhibit spelling changes during morpheme attachment, thus avoiding the segmentation of “denial” as “deni”+”al”, as is typically done in existing morphological parsers. In addition to addressing the aforementioned problems, our segmentation algorithm has two appealing features. First, it can segment words with any number of morphemes, whereas many analyzers can only be applied to words with one root and one suffix (e.g. DéJean (1998), Snover and Brent (2001)). Second, it exhibits robust performance even when inducing morphemes from a very large vocabulary, whereas Goldsmith’s (2001) and Freitag’s (2005) morphological analyzers perform well only when a small vocabulary is employed, showing deteriorating performance as the vocabulary size increases. The rest of this paper is organized as follows. Section 2 presents related work on unsupervised morphological analysis. In Section 3, we describe our basic morpheme induction algorithm. We then show how to exploit the induced morphemes to (1) detect incorrect attachments by using relative corpus frequen"
N07-1020,P00-1027,0,0.709803,"on method, we need to determine that the attachment of the morpheme “ate” to the root word “candid” is incorrect. In this section, we propose a simple yet novel idea of using relative corpus frequency to determine whether the attachment of a morpheme to a root word is plausible or not. Consider again the two words “candidate” and “candid”. While “candidate” occurs 6380 times in our corpus, “candid” occurs only 119 times. This frequency disparity can be an important clue to determining that there is no morphological relation between “candidate” and “candid”. Similar observation is also made by Yarowsky and Wicentowski (2000), who successfully employ relative frequency similarity or disparity to rank candidate VBD/VB pairs (e.g. “sang”/“sing”) that are irregular in nature. Unlike Yarowsky and Wicentowski, however, our goal is to detect incorrect affix attachments and improve morphological analysis. Our incorrect attachment detection algorithm, which exploits frequency disparity, is based on the following hypothesis: if a word w is formed by attaching an affix m to a root word r, then the corpus frequency of w is likely to be less than that of r (i.e. the frequency ratio of w to r is less than one). In other words,"
N09-1065,P99-1048,0,0.612026,"Missing"
N09-1065,J96-1002,0,0.0149132,"approaches to anaphoricity determination. Section 4 provides the details of our graph-cut-based approach. Finally, we present evaluation results in Section 5 and conclude in Section 6. 2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. 2.1 The Standard Machine Learning Approach We use maximum entropy (MaxEnt) classification (Berger et al., 1996) in conjunction with the 33 features described in Ng (2007) to acquire a model, PC , for determining the probability that two mentions, mi and mj , are coreferent. Hence, PC (mi , mj ) = P (COREFERENT |mi , mj ). In the rest of the paper, we will refer to PC (mi , mj ) as the pairwise coreference probability between mi and mj . To generate training instances, we employ Soon et al.’s (2001) procedure, relying on the training texts to create (1) a positive instance for each anaphoric mention, mj , and its closest antecedent, mi ; and (2) a negative instance for mj paired with each of the interve"
N09-1065,P08-1002,0,0.480059,"Missing"
N09-1065,N07-1030,0,0.746915,"ricity classifier by exploiting the dependency between anaphoricity determination and coreference resolu575 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575–583, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tion. For instance, noting that Ng and Cardie’s anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized. As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier. While this ILP approach and Ng’s (2004) approach to improving the output of an anaphoricity classifier both result in increased coreference performance, they have complementary strengths and weaknesses. Specifically, Ng’s approach can directly optimize the desired coreference evaluation metric, but by treating the coreference"
N09-1065,P08-2012,0,0.736504,"the dependency between anaphoricity determination and coreference resolu575 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575–583, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tion. For instance, noting that Ng and Cardie’s anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized. As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier. While this ILP approach and Ng’s (2004) approach to improving the output of an anaphoricity classifier both result in increased coreference performance, they have complementary strengths and weaknesses. Specifically, Ng’s approach can directly optimize the desired coreference evaluation metric, but by treating the coreference system as a black box during"
N09-1065,C96-1021,0,0.0713444,"Missing"
N09-1065,J94-4002,0,0.711253,"Missing"
N09-1065,P04-1018,0,0.30911,"(m1 , m4 ), PC (m3 , m4 )), according to Luo’s algorithm. The score of this partition is the product of these four probabilities, two provided by PA and two by PC . As can be seen, a partition is penalized whenever a mention that is unlikely to be anaphoric (according to PA ) is being resolved to some antecedent according to the partition. Nevertheless, it is computationally infeasible to score all possible partitions given a set of mentions, as the number of partitions is exponential in the number of mentions. To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al. (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. In essence, only the most promising nodes in the tree are expanded at each step of the search process, where the “promise” of a node is defined in terms of the probabilities provided by PA and PC , as described above. Details of this process can be found in Luo et al. (2004). 578 Denis and Baldridge (2007) (mi ,mj )∈P + X mj ∈M cA ¯A j · yj + c j · (1 − yj ) subject to a set of manually-specified linear constraints. D&B specify four types of constraints: (1) each indicator variable can tak"
N09-1065,H05-1004,0,0.1711,"(NPAPER), and Newswire (NWIRE). Each section is in turn composed of a training set and a test set. For each section, we train an anaphoricity model, PA , and a coreference model, PC , on the training set, and evaluate PC (when used in combination with different approaches to anaphoricity determination) on the test set. As noted before, the mentions used are extracted automatically using an in-house NP chunker. Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005). 5.2 Results and Discussions “No Anaphoricity” baseline. Our first baseline is the learning-based coreference system described in Section 2, which does not employ any anaphoricity determination algorithm. Results using the MUC scorer and the CEAF scorer are shown in row 1 of Tables 1 and 2, respectively. As we can see, MUC F-score ranges from 55.0 to 61.7 and CEAF F-score ranges from 55.3 to 61.2. Duplicated Ng and Cardie (2002a) baseline. Next, we evaluate our second baseline, which is N&C’s coreference system. As seen from row 2 of Tables 1 and 2, MUC F-score ranges from 50.5 to 60.0 and CE"
N09-1065,N07-1010,0,0.804205,"anaphoric, and let an independentlytrained coreference system resolve only those mentions that are classified as anaphoric. Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric. One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution. For instance, when searching for the best partition of a set of mentions, Luo (2007) combines the probabilities returned by an anaphoricity model and a coreference model to score a coreference partition, such that a partition is penalized whenever an anaphoric mention is resolved. Another, arguably more popular, solution is to “improve” the output of the anaphoricity classifier by exploiting the dependency between anaphoricity determination and coreference resolu575 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575–583, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tion. For instance, n"
N09-1065,E06-1007,0,0.166767,"Missing"
N09-1065,C02-1139,1,0.631404,"tions do not have an antecedent and therefore do not need to be resolved. Previous work on anaphoricity determination can be broadly divided into two categories (see Poesio et al. (2004) for an overview). Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it (using heuristics [e.g., Paice and Husk (1987), On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution. For instance, Ng and Cardie (2002a) train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independentlytrained coreference system resolve only those mentions that are classified as anaphoric. Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric. One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution. For i"
N09-1065,P02-1014,1,0.687533,"tions do not have an antecedent and therefore do not need to be resolved. Previous work on anaphoricity determination can be broadly divided into two categories (see Poesio et al. (2004) for an overview). Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it (using heuristics [e.g., Paice and Husk (1987), On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution. For instance, Ng and Cardie (2002a) train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independentlytrained coreference system resolve only those mentions that are classified as anaphoric. Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric. One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution. For i"
N09-1065,P04-1020,1,0.249597,"ference model to score a coreference partition, such that a partition is penalized whenever an anaphoric mention is resolved. Another, arguably more popular, solution is to “improve” the output of the anaphoricity classifier by exploiting the dependency between anaphoricity determination and coreference resolu575 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575–583, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tion. For instance, noting that Ng and Cardie’s anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized. As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier. While this ILP approach and Ng’s (2004) approach to improving the output of an anaphoricity classifier"
N09-1065,P04-1035,0,0.0308217,"Missing"
N09-1065,W04-0707,0,0.184072,"identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice.” The first task is nowadays typically formulated as an anaphoricity determination task, which aims to classify whether a given mention is anaphoric or not. Knowledge of anaphoricity could improve the precision of a coreference system, since non-anaphoric mentions do not have an antecedent and therefore do not need to be resolved. Previous work on anaphoricity determination can be broadly divided into two categories (see Poesio et al. (2004) for an overview). Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it (using heuristics [e.g., Paice and Husk (1987), On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution. For instance, Ng and Cardie (2002a) train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independentlytrained coreference system resolve only those mentions that ar"
N09-1065,N06-1025,0,0.0392444,"five existing approaches, none performs consistently better than the others. The rest of the paper is organized as follows. Section 2 describes our learning-based coreference system. In Section 3, we give an overview of the five baseline approaches to anaphoricity determination. Section 4 provides the details of our graph-cut-based approach. Finally, we present evaluation results in Section 5 and conclude in Section 6. 2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. 2.1 The Standard Machine Learning Approach We use maximum entropy (MaxEnt) classification (Berger et al., 1996) in conjunction with the 33 features described in Ng (2007) to acquire a model, PC , for determining the probability that two mentions, mi and mj , are coreferent. Hence, PC (mi , mj ) = P (COREFERENT |mi , mj ). In the rest of the paper, we will refer to PC (mi , mj ) as the pairwise coreference probability between mi and mj . To generate training instances, we empl"
N09-1065,J01-4004,0,0.446291,"n et al.’s (2001) procedure, relying on the training texts to create (1) a positive instance for each anaphoric mention, mj , and its closest antecedent, mi ; and (2) a negative instance for mj paired with each of the intervening mentions, mi+1 , mi+2 ,. . ., mj−1 . When training the feature-weight parameters of the MaxEnt model, we use 100 iterations of the improved iterative scaling (IIS) algorithm (Della Pietra et al., 1997) together with a Gaussian prior to prevent overfitting. After training, the coreference model is used to select an antecedent for each mention in a test text. Following Soon et al. (2001), we select as the antecedent of each mention, mj , the closest preceding mention that is classified as coreferent with mj , where mention pairs with pairwise probabilities of at least 0.5 are considered coreferent. If no such mention exists, no antecedent will be selected for mj . In essence, we use a closest-first clustering algorithm to impose a partitioning on the mentions. probability that a mention is anaphoric. In the rest of this section, we provide an overview of the five baseline approaches to anaphoricity determination. We will characterize each approach along two dimensions: (1) wh"
N09-1065,P03-2012,0,0.117554,"of these features). 1 The classification of a training instance — one of ANAPHORIC or NOT ANAPHORIC — is derived directly from the coreference chains in the associated training text. Like the coreference model, the anaphoricity model is trained by running 100 iterations of IIS with a Guassian prior. The resulting model is then applied to a test text to determine the 1 While we train the anaphoricity model using the Ng and Cardie (2002a) feature set, it should be clear that any features that are useful for distinguishing anaphoric and non-anaphoric mentions can be used (e.g., those proposed by Uryupina (2003) and Elsner and Charniak (2007)). 577 3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA , simply using the anaphoricity information it provides as hard constraints for coreference resolution. Specifically, the coreference system resolves only those mentions that are determined as anaphoric by PA , where a mention is classified as anaphoric if the classification threshold is at least 0.5. 3.2 Ng (2004) evaluation metric. 3.4 3.3 As mentioned before, Denis and Baldridge (D&B) aim to improve the outputs of PA and PC by employing Integer Linear Programming (ILP) to perform"
N09-1065,C08-1121,0,0.0351848,"Missing"
N09-1065,M95-1005,0,0.521855,"ctions: Broadcast News (BNEWS), Newspaper (NPAPER), and Newswire (NWIRE). Each section is in turn composed of a training set and a test set. For each section, we train an anaphoricity model, PA , and a coreference model, PC , on the training set, and evaluate PC (when used in combination with different approaches to anaphoricity determination) on the test set. As noted before, the mentions used are extracted automatically using an in-house NP chunker. Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005). 5.2 Results and Discussions “No Anaphoricity” baseline. Our first baseline is the learning-based coreference system described in Section 2, which does not employ any anaphoricity determination algorithm. Results using the MUC scorer and the CEAF scorer are shown in row 1 of Tables 1 and 2, respectively. As we can see, MUC F-score ranges from 55.0 to 61.7 and CEAF F-score ranges from 55.3 to 61.2. Duplicated Ng and Cardie (2002a) baseline. Next, we evaluate our second baseline, which is N&C’s coreference system. As seen from row 2 of Tables 1 and 2, MUC F-score"
N09-1065,P07-1067,0,0.04689,"ne performs consistently better than the others. The rest of the paper is organized as follows. Section 2 describes our learning-based coreference system. In Section 3, we give an overview of the five baseline approaches to anaphoricity determination. Section 4 provides the details of our graph-cut-based approach. Finally, we present evaluation results in Section 5 and conclude in Section 6. 2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. 2.1 The Standard Machine Learning Approach We use maximum entropy (MaxEnt) classification (Berger et al., 1996) in conjunction with the 33 features described in Ng (2007) to acquire a model, PC , for determining the probability that two mentions, mi and mj , are coreferent. Hence, PC (mi , mj ) = P (COREFERENT |mi , mj ). In the rest of the paper, we will refer to PC (mi , mj ) as the pairwise coreference probability between mi and mj . To generate training instances, we employ Soon et al.’s (20"
N12-1090,S10-1022,0,0.0997798,"ore drops statistically significantly13 to 54.6 (Spanish) and 63.4 (Italian).14 Best systems in the shared task. To determine whether the upper bounds established by our supervised systems are reasonable, we show the results of the best-performing resolvers participating in the shared task for both languages under the goldstandard and regular settings in rows 3 and 4 of Tables 3 and 4. Since none of the participating systems achieved the best score over all four scorers, we report the performance of the system that has the highest average F-score. According to the shared task website, TANL-1 (Attardi et al., 2010) achieved the best average F-score in the regular setting for Spanish, whereas SUCRE (Kobdani and Sch¨utze, 2010) outperformed others in the remaining settings. Comparing these best shared task results with our supervised results in rows 1 and 2, we see that our average F-score for Spanish/Gold is worse than its shared task counterpart by 0.7 points, but otherwise our system outperforms in other settings w.r.t. average F-score, specifically by 5.0 points for Spanish/Regular (due to a better MUC F-score), by 3.4– 4.7 points for Italian (due to better CEAF, B3 , and BLANC scores). Overall, these"
N12-1090,P98-1012,0,0.330496,"he mentions have the same grammatical role 19 NE MATCH determines whether both are NEs and have the same NE type 20 SR MATCH determines whether the mentions have the same semantic role 21 ALIAS determines whether one mention is an abbreviation or an acronym of the other determines whether both mentions are pronominal and have the same person 22 PERSON MATCH 23 LEXICAL the concatenation of the heads of the two mentions Table 1: Feature set for coreference resolution. Scoring programs. To score the output of a coreference resolver, we employ four scoring programs, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), φ3 -CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011), which were downloaded from the shared task website (see Footnote 10). Gold-standard versus regular settings. The format of each data set follows that of a typical CoNLL shared task data set. In other words, each row corresponds to a word in a document; moreover, all but the last column contain the linguistic features computed for the words, and the last column stores the coreference information. Some of the features were computed via automatic means, but some were extracted from human annotations. Given this distinction, the shared t"
N12-1090,P11-1061,0,0.0340467,"nnotations from a resourcerich language to a resource-scarce language was originally proposed by Yarowsky and Ngai (2001) and subsequently developed by others (e.g., Resnik (2004), Hwa et al. (2005)). These projection algorithms assume as input a parallel corpus for the source language and the target language. Given the recent availability of machine translation (MT) services on the Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves. MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian. On the other hand, Postolache et al. (2006) apply a word align"
N12-1090,D08-1069,0,0.0549973,"instance i(mj , mk ) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) or a ranking model (e.g., Denis and Baldridge (2008), Rahman and Ng (2009)). 724 5 Evaluation We evaluate our MT-based projection approach for each of the three settings described in Section 3. 5.1 Experimental Setup Data sets. We use the Spanish and Italian data sets from the SemEval-2010 shared task on Coreference Resolution in Multiple Languages.11 Each data set is composed of a training set and a test set. Statistics of these data sets are shown in Table 2. Spanish Training Set Statistics number of mentions 78779 number of non-singleton clusters 48681 number of singleton clusters 37336 Test Set Statistics number of mentions 14133 number of"
N12-1090,P07-1107,0,0.0360609,"ck is to employ an unsupervised or heuristic approach to coreference resolution, especially in light of the fact that they have recently started to rival their supervised counterparts. However, by adopting these approaches, we are simply replacing the corpus annotation bottleneck by another, possibly equally serious, bottleneck, the knowledge acquisition bottleneck. Specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules (e.g., Mitkov (1999), Poon and Domingos (2008), Raghunathan et al. (2010)) or sophisticated generative models (e.g., Haghighi and Klein (2007,2010), Ng (2008)) to combine the available knowledge sources. One could argue that designing coreference rules and generative models may not be as timeconsuming as annotating a large coreference corpus. This may be true for a well-studied language like English, where we can easily compose a rule that disallows coreference between two mentions if they disagree in number and gender, for instance. However, computing these features may not be as simple as we hope for a language like Chinese: the lack of morphology complicates the determination of number information, and the fact that most Chinese"
N12-1090,N10-1061,0,0.0444222,"Missing"
N12-1090,A00-1020,0,0.701911,"e Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves. MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian. On the other hand, Postolache et al. (2006) apply a word alignment algorithm to project the hand-annotated English coreference chains and then manually fix the projection errors on the Romanian side. Hence, their goal is different from ours in at least two respects. First, while they employ significant knowledge of the target language to create a clean coreference corpus, we examine the quality of coreferenceannotated data created via an entirely aut"
N12-1090,S10-1018,0,0.0716669,"Missing"
N12-1090,P11-1079,0,0.0413129,"Missing"
N12-1090,2005.mtsummit-papers.11,0,0.0549611,"g the mentions sequentially enables us to ensure that an English mention is not mapped to a 1 See http://translate.google.com. See http://www.cs.utah.edu/nlp/reconcile. We use the resolver pre-trained on the Wolverhampton corpus. 3 Other methods for projecting mentions can be found in Postolache et al. (2006), for example. 2 722 French text span that has already been mapped to by a previously-processed English mention.4 To align English and French words, we trained a word alignment model using GIZA++5 (Och and Ney, 2000) on a parallel corpus comprising the English-French section of Europarl6 (Koehn, 2005) as well as all the French texts (and their translated English counterparts) for which we want to automatically create coreference chains. Following common practice, we stemmed the parallel corpus using the Porter stemmer (Porter, 1980) in order to reduce data sparseness. However, even with stemming, we found that many English words were not aligned to any French words by the resulting alignment model. This would prevent many English mentions from being projected to the French side, potentially harming the recall of the French coreference annotations. To improve alignment coverage, we retraine"
N12-1090,P04-1018,0,0.0331405,"g or not (e.g., Soon et al. (2001), Ng and Cardie (2002)).9 Each instance i(mj , mk ) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) or a ranking model (e.g., Denis and Baldridge (2008), Rahman and Ng (2009)). 724 5 Evaluation We evaluate our MT-based projection approach for each of the three settings described in Section 3. 5.1 Experimental Setup Data sets. We use the Spanish and Italian data sets from the SemEval-2010 shared task on Coreference Resolution in Multiple Languages.11 Each data set is composed of a training set and a test set. Statistics of these data sets are shown in Table 2. Spanish Training Set Statistics number of mentions 78779 number of non-singleton clusters 48681 number of single"
N12-1090,H05-1004,0,0.114733,"l role 19 NE MATCH determines whether both are NEs and have the same NE type 20 SR MATCH determines whether the mentions have the same semantic role 21 ALIAS determines whether one mention is an abbreviation or an acronym of the other determines whether both mentions are pronominal and have the same person 22 PERSON MATCH 23 LEXICAL the concatenation of the heads of the two mentions Table 1: Feature set for coreference resolution. Scoring programs. To score the output of a coreference resolver, we employ four scoring programs, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), φ3 -CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011), which were downloaded from the shared task website (see Footnote 10). Gold-standard versus regular settings. The format of each data set follows that of a typical CoNLL shared task data set. In other words, each row corresponds to a word in a document; moreover, all but the last column contain the linguistic features computed for the words, and the last column stores the coreference information. Some of the features were computed via automatic means, but some were extracted from human annotations. Given this distinction, the shared task organizers defined"
N12-1090,P07-1123,0,0.0635903,"d Ngai (2001) and subsequently developed by others (e.g., Resnik (2004), Hwa et al. (2005)). These projection algorithms assume as input a parallel corpus for the source language and the target language. Given the recent availability of machine translation (MT) services on the Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves. MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian. On the other hand, Postolache et al. (2006) apply a word alignment algorithm to project the hand-annotated English coreference chains and then manually fix the projection"
N12-1090,P02-1014,1,0.652236,"correspond to six European languages. Each data set comprises not only training and test documents that are coreference-annotated, but also a number of word-based linguistic features from which we derive mention-based linguistic features for training a resolver. In this section, we will describe how this resolver is trained and then applied to generate coreference chains for unseen documents. Training the coreference classifier. As our coreference model, we train a mention-pair model, which is a classifier that determines whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002)).9 Each instance i(mj , mk ) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) or a ranking mode"
N12-1090,D08-1067,1,0.87459,"or heuristic approach to coreference resolution, especially in light of the fact that they have recently started to rival their supervised counterparts. However, by adopting these approaches, we are simply replacing the corpus annotation bottleneck by another, possibly equally serious, bottleneck, the knowledge acquisition bottleneck. Specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules (e.g., Mitkov (1999), Poon and Domingos (2008), Raghunathan et al. (2010)) or sophisticated generative models (e.g., Haghighi and Klein (2007,2010), Ng (2008)) to combine the available knowledge sources. One could argue that designing coreference rules and generative models may not be as timeconsuming as annotating a large coreference corpus. This may be true for a well-studied language like English, where we can easily compose a rule that disallows coreference between two mentions if they disagree in number and gender, for instance. However, computing these features may not be as simple as we hope for a language like Chinese: the lack of morphology complicates the determination of number information, and the fact that most Chinese first names are"
N12-1090,P00-1056,0,0.242762,"ention.3 We process the English mentions in the text in a left-to-right manner, as processing the mentions sequentially enables us to ensure that an English mention is not mapped to a 1 See http://translate.google.com. See http://www.cs.utah.edu/nlp/reconcile. We use the resolver pre-trained on the Wolverhampton corpus. 3 Other methods for projecting mentions can be found in Postolache et al. (2006), for example. 2 722 French text span that has already been mapped to by a previously-processed English mention.4 To align English and French words, we trained a word alignment model using GIZA++5 (Och and Ney, 2000) on a parallel corpus comprising the English-French section of Europarl6 (Koehn, 2005) as well as all the French texts (and their translated English counterparts) for which we want to automatically create coreference chains. Following common practice, we stemmed the parallel corpus using the Porter stemmer (Porter, 1980) in order to reduce data sparseness. However, even with stemming, we found that many English words were not aligned to any French words by the resulting alignment model. This would prevent many English mentions from being projected to the French side, potentially harming the re"
N12-1090,D08-1068,0,0.0361507,"lver can be trained. One may argue that a potential solution to this corpus annotation bottleneck is to employ an unsupervised or heuristic approach to coreference resolution, especially in light of the fact that they have recently started to rival their supervised counterparts. However, by adopting these approaches, we are simply replacing the corpus annotation bottleneck by another, possibly equally serious, bottleneck, the knowledge acquisition bottleneck. Specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules (e.g., Mitkov (1999), Poon and Domingos (2008), Raghunathan et al. (2010)) or sophisticated generative models (e.g., Haghighi and Klein (2007,2010), Ng (2008)) to combine the available knowledge sources. One could argue that designing coreference rules and generative models may not be as timeconsuming as annotating a large coreference corpus. This may be true for a well-studied language like English, where we can easily compose a rule that disallows coreference between two mentions if they disagree in number and gender, for instance. However, computing these features may not be as simple as we hope for a language like Chinese: the lack of"
N12-1090,postolache-etal-2006-transferring,0,0.361715,"speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian. On the other hand, Postolache et al. (2006) apply a word alignment algorithm to project the hand-annotated English coreference chains and then manually fix the projection errors on the Romanian side. Hence, their goal is different from ours in at least two respects. First, while they employ significant knowledge of the target language to create a clean coreference corpus, we examine the quality of coreferenceannotated data created via an entirely automatic process, determining quality by the performance of the resolver trained on the data. Second, unlike ours, neither of these attempts is at the level of defining a technology for proje"
N12-1090,D10-1048,0,0.0280548,"ay argue that a potential solution to this corpus annotation bottleneck is to employ an unsupervised or heuristic approach to coreference resolution, especially in light of the fact that they have recently started to rival their supervised counterparts. However, by adopting these approaches, we are simply replacing the corpus annotation bottleneck by another, possibly equally serious, bottleneck, the knowledge acquisition bottleneck. Specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules (e.g., Mitkov (1999), Poon and Domingos (2008), Raghunathan et al. (2010)) or sophisticated generative models (e.g., Haghighi and Klein (2007,2010), Ng (2008)) to combine the available knowledge sources. One could argue that designing coreference rules and generative models may not be as timeconsuming as annotating a large coreference corpus. This may be true for a well-studied language like English, where we can easily compose a rule that disallows coreference between two mentions if they disagree in number and gender, for instance. However, computing these features may not be as simple as we hope for a language like Chinese: the lack of morphology complicates the"
N12-1090,D09-1101,1,0.833571,"onds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) or a ranking model (e.g., Denis and Baldridge (2008), Rahman and Ng (2009)). 724 5 Evaluation We evaluate our MT-based projection approach for each of the three settings described in Section 3. 5.1 Experimental Setup Data sets. We use the Spanish and Italian data sets from the SemEval-2010 shared task on Coreference Resolution in Multiple Languages.11 Each data set is composed of a training set and a test set. Statistics of these data sets are shown in Table 2. Spanish Training Set Statistics number of mentions 78779 number of non-singleton clusters 48681 number of singleton clusters 37336 Test Set Statistics number of mentions 14133 number of non-singleton clusters"
N12-1090,W09-2411,0,0.0582703,"Missing"
N12-1090,J01-4004,0,0.296705,"six data sets that correspond to six European languages. Each data set comprises not only training and test documents that are coreference-annotated, but also a number of word-based linguistic features from which we derive mention-based linguistic features for training a resolver. In this section, we will describe how this resolver is trained and then applied to generate coreference chains for unseen documents. Training the coreference classifier. As our coreference model, we train a mention-pair model, which is a classifier that determines whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002)).9 Each instance i(mj , mk ) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (20"
N12-1090,P09-1074,0,0.076483,"may not be immediately clear why the exploitation of the mention extractor in this setting may yield better coreference annotations than those produced in Setting 1. To see the reason, recall that one source of errors inherent in a projection approach is word alignment errors. In Setting 1, when we tried to project English mentions to the French text, word 723 alignment errors would adversely affect the ability of the NP projection algorithm to correctly define the boundaries of the French mentions. Since coreference performance depends crucially on the ability to correctly identify mentions (Stoyanov et al., 2009), the presence of word alignment errors implies that the resulting French coreference annotations could score poorly even if the English coreference annotations produced by Reconcile were of high quality. In the current setting, on the other hand, we reduce the sensitivity of coreference performance to word alignment errors via the use of the French mention extractor to produce more accurate French mention boundaries. 3.3 Setting 3: Additional Taggers Available Finally, we consider a setting that is the least resource-scarce of the three. We assume that in addition to a French mention extracto"
N12-1090,P10-2029,0,0.0240022,". Below we assume that English and French are our source and target languages, respectively. 3.1 Setting 1: No French Taggers Available In this setting, we assume that we do not have access to any French tagger that we can exploit to improve projection. Hence, all we can do is to employ the three steps involved in the projection approach as described at the beginning of this section to create coreference-annotated data for French. Specifically, we translate a French text to an English text using GoogleTranslate1 , and create coreference chains for the translated English text using Reconcile2 (Stoyanov et al., 2010). To project mentions from English to French, we first align the English and French words in each pair of parallel sentences, and then project the English mentions onto the French text using the alignment. However, since the alignment is noisy, the French words to which the words in the English mention are aligned may not form a contiguous text span. To fix this problem, we follow Yarowsky and Ngai (2001) and use the smallest text span that covers all the aligned French words to create the French mention.3 We process the English mentions in the text in a left-to-right manner, as processing the"
N12-1090,M95-1005,0,0.723983,"MATCH determines whether the mentions have the same grammatical role 19 NE MATCH determines whether both are NEs and have the same NE type 20 SR MATCH determines whether the mentions have the same semantic role 21 ALIAS determines whether one mention is an abbreviation or an acronym of the other determines whether both mentions are pronominal and have the same person 22 PERSON MATCH 23 LEXICAL the concatenation of the heads of the two mentions Table 1: Feature set for coreference resolution. Scoring programs. To score the output of a coreference resolver, we employ four scoring programs, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), φ3 -CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011), which were downloaded from the shared task website (see Footnote 10). Gold-standard versus regular settings. The format of each data set follows that of a typical CoNLL shared task data set. In other words, each row corresponds to a word in a document; moreover, all but the last column contain the linguistic features computed for the words, and the last column stores the coreference information. Some of the features were computed via automatic means, but some were extracted from human annotations. Given"
N12-1090,P08-1096,0,0.018073,"n et al. (2001), Ng and Cardie (2002)).9 Each instance i(mj , mk ) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1. As we can see, each feature is either relational, capturing the relation between mj and mk , or non-relational, capturing the linguistic property of mk . The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9 Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al. (2008)) or a ranking model (e.g., Denis and Baldridge (2008), Rahman and Ng (2009)). 724 5 Evaluation We evaluate our MT-based projection approach for each of the three settings described in Section 3. 5.1 Experimental Setup Data sets. We use the Spanish and Italian data sets from the SemEval-2010 shared task on Coreference Resolution in Multiple Languages.11 Each data set is composed of a training set and a test set. Statistics of these data sets are shown in Table 2. Spanish Training Set Statistics number of mentions 78779 number of non-singleton clusters 48681 number of singleton clusters 37336 T"
N12-1090,N01-1026,0,0.317699,"e the extent to which projection, which does not require that we have any knowledge of the target language, can push the limits of multilingual coreference res721 olution. If our results indicate that projection is a promising approach, then the automatic coreference annotations it produces can be used to augment the manual annotations that capture the properties specific to the target language, thus alleviating the corpus annotation bottleneck. 2 Related Work on Projection The idea of projecting annotations from a resourcerich language to a resource-scarce language was originally proposed by Yarowsky and Ngai (2001) and subsequently developed by others (e.g., Resnik (2004), Hwa et al. (2005)). These projection algorithms assume as input a parallel corpus for the source language and the target language. Given the recent availability of machine translation (MT) services on the Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves. MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et a"
N12-1090,D08-1063,0,0.0709401,"urce-scarce language was originally proposed by Yarowsky and Ngai (2001) and subsequently developed by others (e.g., Resnik (2004), Hwa et al. (2005)). These projection algorithms assume as input a parallel corpus for the source language and the target language. Given the recent availability of machine translation (MT) services on the Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves. MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian. On the other hand, Postolache et al. (2006) apply a word alignment algorithm to project the hand-annotated English c"
N12-1090,C98-1012,0,\N,Missing
N12-1090,S10-1001,0,\N,Missing
N13-1112,P07-2044,0,0.887939,"eas the pair (invasion, rise) has type Before). 14 relation types are defined and used to annotate the temporal relations in the TimeBank corpus. Table 1 provides a brief description of these relation types and the relevant statistics. In our experiments, we assume that our temporal relation classification system is given an eventevent or event-time pair that is known to belong to one of the 14 relation types defined in TimeBank and aims to determine its relation type. Following previous evaluations of the temporal relation classification task on the TimeBank corpus (e.g., Mani et al. (2006), Chambers et al. (2007)) and in TempEval1/2, we assume as input gold events and time expressions. Unlike Mani et al. (2006) and Chambers et al. (2007), who focus on six relation types (Simultaneous, Before, IBefore, Begins, Ends, and Includes), we report results on 14 relation types. Note that the aforementioned six relation types are chosen by (1) discarding During, During Inv, and Identity, and (2) combining the two relation types in each of the five pairs, namely (Before, After), (IBefore, IAfter), (Includes, Is Included), (Begins, Begun By), and (Ends, Ended By), into a single type because they are inverses of e"
N13-1112,W04-3205,0,0.0614925,"uation of temporal relation classification systems. Our goal in this paper is to advance the state of the art in temporal relation classification. Our work differs from existing work with respect to both the Our approach to temporal relation classification can be distinguished from existing approaches in two respects. The first involves a large-scale expansion of the linguistic features made available to the classification system. Recall that existing approaches have relied primarily on morphosyntactic features as well as a few semantic features extracted from WordNet synsets and VerbOcean’s (Chklovski and Pantel, 2004) semantic relations. On the other hand, we propose not only novel lexical and grammatical features, but also sophisticated features involving semantics and discourse. Most notably, we propose (1) semantic features encoding a variety of semantic relations, including PropBank-style predicate-argument relations as well as those extracted from the Merriam-Webster dictionary, and (2) discourse features encoding automatically computed Penn Discourse TreeBank (PDTB) style (Prasad et al., 2008) discourse relations. Second, while the vast majority of existing approaches to temporal relation classificat"
N13-1112,S10-1076,0,0.0947369,"2 in the associated text and (2) (e1 ,e2 ) belongs to one of the 14 TimeBank temporal relation types. We create one training instance for each event-event/event-time pair in a training document that satisfies the two conditions above, labeling it with the relation type that exists between e1 and e2 . To build a strong baseline, we represent each instance using 68 linguistic features modeled after the top-performing temporal relation classification systems on TimeBank (e.g., Mani et al. (2006), Chambers et al. (2007)) and in the TempEval shared tasks (e.g., Min et al. (2007), Pus¸cas¸u (2007), Ha et al. (2010), Llorens et al. (2010), Mirroshandel and 920 Ghassem-Sani (2011)).1 These features can be divided into six categories, as described below. Lexical (5). The strings of e1 and e2 , the head words of e1 and e2 , and a binary feature indicating whether e1 and e2 have the same string. Grammatical (33). The POS tags of the head words of e1 and e2 , the POS tags of the five tokens preceding and following e1 and e2 , the POS bigram formed from the head word of e1 and its preceding token, the POS bigram formed from the head word of e2 and its preceding token, the POS tag pair formed from the head word"
N13-1112,E95-1035,0,0.166537,"Missing"
N13-1112,W94-0100,0,0.629594,"Missing"
N13-1112,S10-1063,0,0.246775,"d text and (2) (e1 ,e2 ) belongs to one of the 14 TimeBank temporal relation types. We create one training instance for each event-event/event-time pair in a training document that satisfies the two conditions above, labeling it with the relation type that exists between e1 and e2 . To build a strong baseline, we represent each instance using 68 linguistic features modeled after the top-performing temporal relation classification systems on TimeBank (e.g., Mani et al. (2006), Chambers et al. (2007)) and in the TempEval shared tasks (e.g., Min et al. (2007), Pus¸cas¸u (2007), Ha et al. (2010), Llorens et al. (2010), Mirroshandel and 920 Ghassem-Sani (2011)).1 These features can be divided into six categories, as described below. Lexical (5). The strings of e1 and e2 , the head words of e1 and e2 , and a binary feature indicating whether e1 and e2 have the same string. Grammatical (33). The POS tags of the head words of e1 and e2 , the POS tags of the five tokens preceding and following e1 and e2 , the POS bigram formed from the head word of e1 and its preceding token, the POS bigram formed from the head word of e2 and its preceding token, the POS tag pair formed from the head words of e1 and e2 , the pr"
N13-1112,P06-1095,0,0.89235,"has type After, whereas the pair (invasion, rise) has type Before). 14 relation types are defined and used to annotate the temporal relations in the TimeBank corpus. Table 1 provides a brief description of these relation types and the relevant statistics. In our experiments, we assume that our temporal relation classification system is given an eventevent or event-time pair that is known to belong to one of the 14 relation types defined in TimeBank and aims to determine its relation type. Following previous evaluations of the temporal relation classification task on the TimeBank corpus (e.g., Mani et al. (2006), Chambers et al. (2007)) and in TempEval1/2, we assume as input gold events and time expressions. Unlike Mani et al. (2006) and Chambers et al. (2007), who focus on six relation types (Simultaneous, Before, IBefore, Begins, Ends, and Includes), we report results on 14 relation types. Note that the aforementioned six relation types are chosen by (1) discarding During, During Inv, and Identity, and (2) combining the two relation types in each of the five pairs, namely (Before, After), (IBefore, IAfter), (Includes, Is Included), (Begins, Begun By), and (Ends, Ended By), into a single type becaus"
N13-1112,S07-1046,0,0.0308966,"time pair such that (1) e1 precedes e2 in the associated text and (2) (e1 ,e2 ) belongs to one of the 14 TimeBank temporal relation types. We create one training instance for each event-event/event-time pair in a training document that satisfies the two conditions above, labeling it with the relation type that exists between e1 and e2 . To build a strong baseline, we represent each instance using 68 linguistic features modeled after the top-performing temporal relation classification systems on TimeBank (e.g., Mani et al. (2006), Chambers et al. (2007)) and in the TempEval shared tasks (e.g., Min et al. (2007), Pus¸cas¸u (2007), Ha et al. (2010), Llorens et al. (2010), Mirroshandel and 920 Ghassem-Sani (2011)).1 These features can be divided into six categories, as described below. Lexical (5). The strings of e1 and e2 , the head words of e1 and e2 , and a binary feature indicating whether e1 and e2 have the same string. Grammatical (33). The POS tags of the head words of e1 and e2 , the POS tags of the five tokens preceding and following e1 and e2 , the POS bigram formed from the head word of e1 and its preceding token, the POS bigram formed from the head word of e2 and its preceding token, the PO"
N13-1112,R11-1030,0,0.012619,"her e1 and e2 have the same string. Grammatical (33). The POS tags of the head words of e1 and e2 , the POS tags of the five tokens preceding and following e1 and e2 , the POS bigram formed from the head word of e1 and its preceding token, the POS bigram formed from the head word of e2 and its preceding token, the POS tag pair formed from the head words of e1 and e2 , the prepositional lexeme of the prepositional phrase (PP) if e1 is headed by a PP (Chambers et al., 2007), the prepositional lexeme of the PP if e2 is headed by a PP, the prepositional lexeme of the PP if e1 is governed by a PP (Mirroshandel and Ghassem-Sani, 2011), the prepositional lexeme of the PP if e2 is governed by a PP, the POS of the head of the verb phrase (VP) if e1 is governed by a VP, the POS of the head of the VP if e2 is governed by a VP, whether e1 syntactically dominates e2 (Chambers et al., 2007), and the shortest path from e1 to e2 in the associated syntactic parse tree. We obtain parse trees and POS tags using the Stanford CoreNLP tool.2 1 Note, however, that these features were designed for the arguably simpler 6-class temporal relation classification tasks. 2 http://nlp.stanford.edu/software/ corenlp.shtml Entity attributes (13). Th"
N13-1112,prasad-etal-2008-penn,0,0.00938219,"syntactic features as well as a few semantic features extracted from WordNet synsets and VerbOcean’s (Chklovski and Pantel, 2004) semantic relations. On the other hand, we propose not only novel lexical and grammatical features, but also sophisticated features involving semantics and discourse. Most notably, we propose (1) semantic features encoding a variety of semantic relations, including PropBank-style predicate-argument relations as well as those extracted from the Merriam-Webster dictionary, and (2) discourse features encoding automatically computed Penn Discourse TreeBank (PDTB) style (Prasad et al., 2008) discourse relations. Second, while the vast majority of existing approaches to temporal relation classification are learning-based, we propose a system architecture in which we combine a learning-based approach and a rule-based approach. Our motivation behind adopting a hybrid approach stems from two hypotheses. First, a rule-based method could better handle the skewed class distribution underlying the dataset for 918 Proceedings of NAACL-HLT 2013, pages 918–927, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics our 14-class classification problem. Second, bet"
N13-1112,S07-1108,0,0.0630311,"Missing"
N13-1112,S07-1014,0,0.51329,"Missing"
N13-1112,miltsakaki-etal-2004-penn,0,\N,Missing
N13-1112,S10-1010,0,\N,Missing
N15-1116,W06-0901,0,0.290802,"city of event mentions. 2 Related Work Almost all existing approaches to event coreference are developed for English. These approaches can broadly be divided into three categories. Within-document coreference is the most popularly investigated and arguably the most important event coreference task. While early work in MUC (e.g., Humphreys et al. (1997)) is limited to several scenarios, ACE takes a further step towards processing more fine-grained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006)). Improvements to this standard approach include the use of feature weighting to train a better model (McConky et al., 2012), and graph-based clustering algorithms to produce event coreference clusters (Chen and Ji, 2009; Sangeetha and Arock, 2012). Chen et al. (2011) train multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun1098 noun coreference) on the OntoNotes corpus (Pradhan et al., 2007). However, OntoNotes is only partially annotated with event coreference links, and Chen et al. further make the simplifying as"
N15-1116,araki-etal-2014-detecting,0,0.184745,"mmunity (IC) corpus (Hovy et al., 2013). The IC corpus, which at the time of writing is not yet publicly available, is different from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach is recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Cross-document coreference is first investigated by Bagga and Baldwin (1999), who represent an event mention as a vector of its context words and determine whether two event mentions are coreferent based on the cosine similarity of their vectors. Bejan and Harabagiu (2010; 2014) and Lee et al. (2012) propose nonparametric models and a joint entity and event coreference model respectively for within- and cross-document event coreference, evaluating their models on the ECB corpus. However, ECB ""is annotated mainly for cross-document coreference"
N15-1116,W99-0201,0,0.172284,"erent from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach is recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Cross-document coreference is first investigated by Bagga and Baldwin (1999), who represent an event mention as a vector of its context words and determine whether two event mentions are coreferent based on the cosine similarity of their vectors. Bejan and Harabagiu (2010; 2014) and Lee et al. (2012) propose nonparametric models and a joint entity and event coreference model respectively for within- and cross-document event coreference, evaluating their models on the ECB corpus. However, ECB ""is annotated mainly for cross-document coreference"" and many difficult cases of within-document coreference are not annotated (Liu et al., 2014). Naughton (2009) and Elkhlifi and"
N15-1116,P98-1013,0,0.159774,"reference In this paper, we examine Chinese event coreference resolution. While English event coreference is under-investigated, Chinese event coreference is much less studied than English event coreference. In terms of task definition, there is no difference between English and Chinese event coreference. However, the design of high-performance Chinese event coreference resolvers is complicated in part by the lack of large-scale lexical knowledge bases. Recent work by Bejan and Harabagiu (2010; 2014) has shown that the semantic information extracted from WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998) significantly contributed to the performance of their English event coreference resolver. While the lack of comparable lexical knowledge bases in Chinese can be mitigated in part by the use of event coreference annotated data, we focus on a challenging version of the task --- unsupervised Chinese event coreference resolution. Specifically, our goal is to learn an event coreference model without using data annotated with event coreference links. When evaluated on the Chinese portion of the ACE 2005 corpus, our unsupervised probabilistic model for event coreference resolution rivals its state-o"
N15-1116,P10-1143,0,0.192303,"arguments (which is the job of an entity coreference resolver), are all non-trivial tasks. In other words, end-to-end event coreference In this paper, we examine Chinese event coreference resolution. While English event coreference is under-investigated, Chinese event coreference is much less studied than English event coreference. In terms of task definition, there is no difference between English and Chinese event coreference. However, the design of high-performance Chinese event coreference resolvers is complicated in part by the lack of large-scale lexical knowledge bases. Recent work by Bejan and Harabagiu (2010; 2014) has shown that the semantic information extracted from WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998) significantly contributed to the performance of their English event coreference resolver. While the lack of comparable lexical knowledge bases in Chinese can be mitigated in part by the use of event coreference annotated data, we focus on a challenging version of the task --- unsupervised Chinese event coreference resolution. Specifically, our goal is to learn an event coreference model without using data annotated with event coreference links. When evaluated on the Chinese"
N15-1116,J14-2004,0,0.478872,"Missing"
N15-1116,W09-3208,0,0.785521,"e most popularly investigated and arguably the most important event coreference task. While early work in MUC (e.g., Humphreys et al. (1997)) is limited to several scenarios, ACE takes a further step towards processing more fine-grained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006)). Improvements to this standard approach include the use of feature weighting to train a better model (McConky et al., 2012), and graph-based clustering algorithms to produce event coreference clusters (Chen and Ji, 2009; Sangeetha and Arock, 2012). Chen et al. (2011) train multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun1098 noun coreference) on the OntoNotes corpus (Pradhan et al., 2007). However, OntoNotes is only partially annotated with event coreference links, and Chen et al. further make the simplifying assumption that event coreference chains are all and only those coreference chains that involve at least one verb. More recently, Cybulska and Vossen (2012) and Goyal et al. (2013) have performed event coreference using sem"
N15-1116,chen-ng-2014-sinocoreferencer,1,0.782011,"B ""is annotated mainly for cross-document coreference"" and many difficult cases of within-document coreference are not annotated (Liu et al., 2014). Naughton (2009) and Elkhlifi and Faiz (2009) have worked on sentence-level event coreference, where the goal is to determine whether two sentences containing event mentions are coreferent. Somewhat unfortunately, simplifying assumptions have to be made when a sentence containing multiple noncoreferent event mentions is encountered. Compared to English event coreference, there has been much less work on Chinese event coreference. SinoCoreferencer (Chen and Ng, 2014), a publiclyavailable ACE-style within-document event coreference resolver for Chinese that achieves state-of-theart results, employs a supervised approach where a classifier is trained to determine whether two event mentions are coreferent. We will compare our unsupervised model against this supervised resolver. 3 沙米里与其子在上午交通尖峰时间 [离] 家时，遭到 [暗杀]。这次 [攻击] 再次显示叛乱分子能力。 Shameri and his son were [assassinated] during morning rush hour when [leaving] home. This [attack] once again demonstrated the insurgents&apos; ability. ACE Event Coreference In this section, we overview the ACE 2005 event coreference t"
N15-1116,I11-1012,0,0.195802,"most important event coreference task. While early work in MUC (e.g., Humphreys et al. (1997)) is limited to several scenarios, ACE takes a further step towards processing more fine-grained events. Most ACE event coreference resolvers are supervised, training a pairwise model to determine whether two event mentions are coreferent (e.g., Ahn (2006)). Improvements to this standard approach include the use of feature weighting to train a better model (McConky et al., 2012), and graph-based clustering algorithms to produce event coreference clusters (Chen and Ji, 2009; Sangeetha and Arock, 2012). Chen et al. (2011) train multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun1098 noun coreference) on the OntoNotes corpus (Pradhan et al., 2007). However, OntoNotes is only partially annotated with event coreference links, and Chen et al. further make the simplifying assumption that event coreference chains are all and only those coreference chains that involve at least one verb. More recently, Cybulska and Vossen (2012) and Goyal et al. (2013) have performed event coreference using semantic relations (e.g., hyponymy relations extrac"
N15-1116,P13-2083,0,0.0179016,"ithms to produce event coreference clusters (Chen and Ji, 2009; Sangeetha and Arock, 2012). Chen et al. (2011) train multiple classifiers to handle coreference between event mentions of different syntactic types (e.g., verb-noun coreference, noun1098 noun coreference) on the OntoNotes corpus (Pradhan et al., 2007). However, OntoNotes is only partially annotated with event coreference links, and Chen et al. further make the simplifying assumption that event coreference chains are all and only those coreference chains that involve at least one verb. More recently, Cybulska and Vossen (2012) and Goyal et al. (2013) have performed event coreference using semantic relations (e.g., hyponymy relations extracted from WordNet) and distributional semantic information, respectively, on the Intelligence Community (IC) corpus (Hovy et al., 2013). The IC corpus, which at the time of writing is not yet publicly available, is different from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of"
N15-1116,W13-1203,0,0.0447744,"un coreference, noun1098 noun coreference) on the OntoNotes corpus (Pradhan et al., 2007). However, OntoNotes is only partially annotated with event coreference links, and Chen et al. further make the simplifying assumption that event coreference chains are all and only those coreference chains that involve at least one verb. More recently, Cybulska and Vossen (2012) and Goyal et al. (2013) have performed event coreference using semantic relations (e.g., hyponymy relations extracted from WordNet) and distributional semantic information, respectively, on the Intelligence Community (IC) corpus (Hovy et al., 2013). The IC corpus, which at the time of writing is not yet publicly available, is different from the MUC and ACE corpora in that it is annotated with not only full event coreference relations but also partial event coreference relations. Partial coreference is a term coined by Hovy et al. to refer to event relations that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach is recently proposed by Araki et al. (2014) to identify subeven"
N15-1116,W97-1311,0,0.908809,"Missing"
N15-1116,D12-1045,0,0.233794,"that exhibit subtle deviation from the perfect identity of events (e.g., the subset relation, the membership relation). While all of the aforementioned work addresses the full event coreference task, a two-stage approach is recently proposed by Araki et al. (2014) to identify subevent relations from the IC corpus. Cross-document coreference is first investigated by Bagga and Baldwin (1999), who represent an event mention as a vector of its context words and determine whether two event mentions are coreferent based on the cosine similarity of their vectors. Bejan and Harabagiu (2010; 2014) and Lee et al. (2012) propose nonparametric models and a joint entity and event coreference model respectively for within- and cross-document event coreference, evaluating their models on the ECB corpus. However, ECB ""is annotated mainly for cross-document coreference"" and many difficult cases of within-document coreference are not annotated (Liu et al., 2014). Naughton (2009) and Elkhlifi and Faiz (2009) have worked on sentence-level event coreference, where the goal is to determine whether two sentences containing event mentions are coreferent. Somewhat unfortunately, simplifying assumptions have to be made when"
N15-1116,D12-1092,0,0.0177919,"s not designed to be precision-oriented, as it is computed based solely on the triggers and not their surrounding contexts. Below we explain conditions 2 and 3 in more detail. Recall that condition 2 encodes our observation that an event coreference relation may exist between two non-identical trigger words having the same BV if their verb structures are compatible. To understand this condition, let us explain the notion of BVs and how we determine the compatibility of two verb structures. A BV is a single-character Chinese verb, which is the building block of all Chinese verbs. Specifically, Li et al. (2012) observe that, with a few exceptions, a Chinese verb constructed out of a basic verb bv possesses one of six main verb structures: (1) bv (e.g., 逮 (arrest)); (2) bv + verb (e.g., 送到 (deliver), where bv is 送); (3) verb + bv (e.g., 离开 (leave), where bv is 开); (4) bv + complementation (e.g., 进了 (enter), where bv is 进); (5) bv + noun/adjective (e.g., 开枪 (shoot), where bv is 开); (6) noun/adjective + bv (e.g., 轻伤 (slight wound), where bv is 伤). Now, assuming that t1 and t2 are two lexically different trigger words containing the same BV (bv), we say that their verb structures (denoted as vs1 and vs2"
N15-1116,liu-etal-2014-supervised,0,0.389589,"e is first investigated by Bagga and Baldwin (1999), who represent an event mention as a vector of its context words and determine whether two event mentions are coreferent based on the cosine similarity of their vectors. Bejan and Harabagiu (2010; 2014) and Lee et al. (2012) propose nonparametric models and a joint entity and event coreference model respectively for within- and cross-document event coreference, evaluating their models on the ECB corpus. However, ECB ""is annotated mainly for cross-document coreference"" and many difficult cases of within-document coreference are not annotated (Liu et al., 2014). Naughton (2009) and Elkhlifi and Faiz (2009) have worked on sentence-level event coreference, where the goal is to determine whether two sentences containing event mentions are coreferent. Somewhat unfortunately, simplifying assumptions have to be made when a sentence containing multiple noncoreferent event mentions is encountered. Compared to English event coreference, there has been much less work on Chinese event coreference. SinoCoreferencer (Chen and Ng, 2014), a publiclyavailable ACE-style within-document event coreference resolver for Chinese that achieves state-of-theart results, emp"
N15-1116,H05-1004,0,0.575216,"1 Table 1: Statistics on the ACE 2005 Chinese corpus. Evaluation Experimental Setup Dataset. For evaluation, we conduct five-fold cross-validation experiments on the 633 Chinese documents of the ACE 2005 training corpus. Statistics on the corpus are shown in Table 1. Evaluation measures. We report results in terms of recall (R), precision (P), and F-score (F) using the commonly-used coreference evaluation measures given by the CoNLL scorer, namely the link-based MUC scorer (Vilain et al., 1995), the mention-based B3 scorer (Bagga and Baldwin, 1998), the entitybased version of the CEAF scorer (Luo, 2005), and the Rand index-based BLANC scorer (Recasens and Hovy, 2011), after singleton event mentions are removed from the coreference partitions produced by our resolver. We use the latest version (version 8) of the CoNLL scorer2 , which fixes a bug in previous versions (Pradhan et al., 2014). In addition, we report the CoNLL score (Pradhan et al., 2011), which is the unweighted average of the MUC, B3 , and CEAF F-scores. Evaluation setting. We perform an end-to-end evaluation, as it can more accurately reflect the performance of an event coreference resolver when it is used in practice. More spe"
N15-1116,W11-1901,0,0.0247693,"P), and F-score (F) using the commonly-used coreference evaluation measures given by the CoNLL scorer, namely the link-based MUC scorer (Vilain et al., 1995), the mention-based B3 scorer (Bagga and Baldwin, 1998), the entitybased version of the CEAF scorer (Luo, 2005), and the Rand index-based BLANC scorer (Recasens and Hovy, 2011), after singleton event mentions are removed from the coreference partitions produced by our resolver. We use the latest version (version 8) of the CoNLL scorer2 , which fixes a bug in previous versions (Pradhan et al., 2014). In addition, we report the CoNLL score (Pradhan et al., 2011), which is the unweighted average of the MUC, B3 , and CEAF F-scores. Evaluation setting. We perform an end-to-end evaluation, as it can more accurately reflect the performance of an event coreference resolver when it is used in practice. More specifically, to extract the event mentions used in our evaluation, we employ SinoCoreferencer3 , which, as mentioned before, is an end-to-end ACE-style Chinese IE system that achieves state-ofthe-art event coreference results. Specifically, the event triggers needed to compute the trigger-based context features are extracted using SinoCoreferencer&apos;s eve"
N15-1116,P14-2006,1,0.822776,"measures. We report results in terms of recall (R), precision (P), and F-score (F) using the commonly-used coreference evaluation measures given by the CoNLL scorer, namely the link-based MUC scorer (Vilain et al., 1995), the mention-based B3 scorer (Bagga and Baldwin, 1998), the entitybased version of the CEAF scorer (Luo, 2005), and the Rand index-based BLANC scorer (Recasens and Hovy, 2011), after singleton event mentions are removed from the coreference partitions produced by our resolver. We use the latest version (version 8) of the CoNLL scorer2 , which fixes a bug in previous versions (Pradhan et al., 2014). In addition, we report the CoNLL score (Pradhan et al., 2011), which is the unweighted average of the MUC, B3 , and CEAF F-scores. Evaluation setting. We perform an end-to-end evaluation, as it can more accurately reflect the performance of an event coreference resolver when it is used in practice. More specifically, to extract the event mentions used in our evaluation, we employ SinoCoreferencer3 , which, as mentioned before, is an end-to-end ACE-style Chinese IE system that achieves state-ofthe-art event coreference results. Specifically, the event triggers needed to compute the trigger-ba"
N15-1116,D09-1101,1,0.740291,"stic is not applicable to dummy candidates, we assume for simplicity that they are all plausible. Second, by including d as a dummy candidate antecedent for each e, we model anaphoricity determination and event coreference in a joint fashion. If the model resolves e to d, it means that the model posits e as non-anaphoric; on the other hand, if the model resolves e to a non-dummy candidate antecedent c, it means that the model posits e as anaphoric and c as e&apos;s correct antecedent. This joint modeling method has proven effective in earlier work on supervised entity coreference resolution (e.g., Rahman and Ng (2009; 2011)). 4.2.2 M-Step Given P (l=1|e, k, c), the goal of the M-step is to (re)estimate two of the four groups of parameters mentioned above, namely P (et |ct , l=1) and P (fci |l), using maximum likelihood estimation. Specifically, P (et |ct , l=1) is estimated as follows: P (et |ct , l=1) = Count(et , ct , l=1) + θ Count(ct , l=1) + θ ∗ |t| (9) candidate antecedents, and select the one that yields the largest probability. If c is a non-dummy candidate antecedent, we posit c as the antecedent of e; otherwise, we posit e as non-anaphoric. where Count(ct , l=1) is the expected number of times c"
N15-1116,M95-1005,0,0.165162,"s earlier (later) in the document. 6 6.1 Documents Sentences Event mentions Event coreference chains 633 9,967 3,333 2,521 Table 1: Statistics on the ACE 2005 Chinese corpus. Evaluation Experimental Setup Dataset. For evaluation, we conduct five-fold cross-validation experiments on the 633 Chinese documents of the ACE 2005 training corpus. Statistics on the corpus are shown in Table 1. Evaluation measures. We report results in terms of recall (R), precision (P), and F-score (F) using the commonly-used coreference evaluation measures given by the CoNLL scorer, namely the link-based MUC scorer (Vilain et al., 1995), the mention-based B3 scorer (Bagga and Baldwin, 1998), the entitybased version of the CEAF scorer (Luo, 2005), and the Rand index-based BLANC scorer (Recasens and Hovy, 2011), after singleton event mentions are removed from the coreference partitions produced by our resolver. We use the latest version (version 8) of the CoNLL scorer2 , which fixes a bug in previous versions (Pradhan et al., 2014). In addition, we report the CoNLL score (Pradhan et al., 2011), which is the unweighted average of the MUC, B3 , and CEAF F-scores. Evaluation setting. We perform an end-to-end evaluation, as it can"
N15-1116,C98-1013,0,\N,Missing
N15-1116,D08-1067,1,\N,Missing
N16-1164,W06-1651,0,0.0331455,"Missing"
N16-1164,W13-2707,0,0.191828,"ct of our work goes beyond argument mining, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks. 2 Related Work Recall that identifying argumentative discourse structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Below we divide related works into five broad categories based on which of these subtasks they addressed. Argument location identification. Works in this category aimed to classify whether a sentence contains an argument or not (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). The usefulness of existing works is somewhat limited by the task’s coarseness: it won’t tell us which portion of a potentially long sentence contains the argument, for instance, but it can serve as a potentially useful first step in argument mining. Argument component typing. Works in this category focused on determining the type of an argument. The vast majority of previous works perform argument component typing at the sentence level. For instance, Rooney et al. (2012) classified sentences into premises, conclusions, premiseconc"
N16-1164,C14-1141,0,0.109527,"Missing"
N16-1164,P14-5010,0,0.00266927,"loy a two-step approach to the ACI task, where we first heuristically extract argument component candidates (ACCs) from an essay, and then classify each ACC as either a premise, claim, major claim, or non-argumentative, as described below. 4.1.1 Extracting ACCs We extract ACCs by constructing a set of low precision, high recall heuristics for identifying the locations in each sentence where an argument component’s boundaries might occur. The majority of these rules depend primarily on a syntactic parse tree we automatically generated for all sentences in the corpus using the Stanford CoreNLP (Manning et al., 2014) system. Since argument components are a clause-level annotation and therefore a large majority of annotated argument components are substrings 1386 (a) Potential left boundary locations # 1 2 Rule Exactly where the S node begins. After an initial explicit connective, or if the connective is immediately followed by a comma, after the comma. After nth comma that is an immediate child of the S node. After nth comma. 3 4 (b) Potential right boundary locations # 5 Rule Exactly where the S node ends, or if S ends in a punctuation, immediately before the punctuation. If the S node ends in a (possibl"
N16-1164,W14-2104,0,0.0309853,"f a potentially long sentence contains the argument, for instance, but it can serve as a potentially useful first step in argument mining. Argument component typing. Works in this category focused on determining the type of an argument. The vast majority of previous works perform argument component typing at the sentence level. For instance, Rooney et al. (2012) classified sentences into premises, conclusions, premiseconclusions, and non-argumentative components; Teufel (1999) classified each sentence into one of seven rhetorical classes (e.g., claim, result, purpose); Burstein et al. (2003), Ong et al. (2014), and 1385 Falakmasir et al. (2014) assigned argumentative labels (e.g., claim, thesis, conclusion) to an essay’s sentences; Levy et al. (2014) detected sentences that support or attack an article’s topic; Lippi and Torroni (2015; 2016) detected sentences containing claims; and Rinott et al. (2015) detected sentences containing evidence for a given claim. Sentencelevel argument component typing has limitations, however. For example, it can identify sentences containing claims, but it cannot tell how many claims a sentence has or where in the sentence they are. Argument location identification"
N16-1164,W14-2105,0,0.0544247,"Missing"
N16-1164,D15-1110,0,0.0621429,"Missing"
N16-1164,D15-1050,0,0.0490949,"Missing"
N16-1164,W04-2401,0,0.0606263,"84 Proceedings of NAACL-HLT 2016, pages 1384–1394, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics perform argument mining on raw, unannotated essays. Our work makes three contributions. First, we present the first results on end-to-end argument mining in student essays using a pipeline approach, where the ACI task is performed prior to the RI task. Second, to avoid the error propagation problem inherent in the pipeline approach, we perform joint inference over the outputs of the ACI and RI classifiers in an Integer Linear Programming (ILP) framework (Roth and Yih, 2004), where we design constraints to enforce global consistency. Finally, we argue that the typical objective function used extensively in ILP programs for NLP tasks is not ideal for tasks whose primary evaluation metric is F-score, and subsequently propose a novel objective function that enables F-score to be maximized directly in an ILP framework. We believe that the impact of our work goes beyond argument mining, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks. 2 Related Work Recall that identifying argumentative discourse stru"
N16-1164,W10-0214,0,0.0204298,"Missing"
N16-1164,W14-2110,0,0.0605707,"g, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks. 2 Related Work Recall that identifying argumentative discourse structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Below we divide related works into five broad categories based on which of these subtasks they addressed. Argument location identification. Works in this category aimed to classify whether a sentence contains an argument or not (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). The usefulness of existing works is somewhat limited by the task’s coarseness: it won’t tell us which portion of a potentially long sentence contains the argument, for instance, but it can serve as a potentially useful first step in argument mining. Argument component typing. Works in this category focused on determining the type of an argument. The vast majority of previous works perform argument component typing at the sentence level. For instance, Rooney et al. (2012) classified sentences into premises, conclusions, premiseconclusions, and non-argumentative componen"
N16-1164,C14-1142,0,0.353051,"ment component identification (ACI), which consists of identifying the locations and types of the components that make up the arguments (i.e., Major Claims, Claims, and Premises), and (2) relation identification (RI), which involves identifying the type of relation that holds between two argument components (i.e., Support, Attack, None). As a first step towards mining arguments in persuasive essays, In this example, premise (3) supports claim (2), which in turn supports major claim (1). Using their annotated corpus, S&G presented initial results on simplified versions of the ACI and RI tasks (Stab and Gurevych, 2014b). Specifically, they applied their learned ACI classifier to classify only gold argument components (i.e., text spans corresponding to a Major Claim, Claim, or Premise in the gold standard) or sentences that contain no gold argument components (as non-argumentative). Similarly, they applied their learned RI classifier to classify only the relation between two gold argument components. In other words, they simplified both tasks by avoiding the challenging task of identifying the locations of argument components. Consequently, their approach cannot be applied in a realistic setting where the i"
N16-1164,D14-1006,0,0.155828,"ment component identification (ACI), which consists of identifying the locations and types of the components that make up the arguments (i.e., Major Claims, Claims, and Premises), and (2) relation identification (RI), which involves identifying the type of relation that holds between two argument components (i.e., Support, Attack, None). As a first step towards mining arguments in persuasive essays, In this example, premise (3) supports claim (2), which in turn supports major claim (1). Using their annotated corpus, S&G presented initial results on simplified versions of the ACI and RI tasks (Stab and Gurevych, 2014b). Specifically, they applied their learned ACI classifier to classify only gold argument components (i.e., text spans corresponding to a Major Claim, Claim, or Premise in the gold standard) or sentences that contain no gold argument components (as non-argumentative). Similarly, they applied their learned RI classifier to classify only the relation between two gold argument components. In other words, they simplified both tasks by avoiding the challenging task of identifying the locations of argument components. Consequently, their approach cannot be applied in a realistic setting where the i"
N16-1164,W15-4631,0,0.0399368,"ptimizing objective function is general enough to be applied to any ILP-based joint inference tasks. 2 Related Work Recall that identifying argumentative discourse structures consists of (1) identifying the locations and types of the argument components, and (2) identifying how they are related to each other. Below we divide related works into five broad categories based on which of these subtasks they addressed. Argument location identification. Works in this category aimed to classify whether a sentence contains an argument or not (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). The usefulness of existing works is somewhat limited by the task’s coarseness: it won’t tell us which portion of a potentially long sentence contains the argument, for instance, but it can serve as a potentially useful first step in argument mining. Argument component typing. Works in this category focused on determining the type of an argument. The vast majority of previous works perform argument component typing at the sentence level. For instance, Rooney et al. (2012) classified sentences into premises, conclusions, premiseconclusions, and non-argumentative components; Teufel (1999) class"
N16-1164,P13-1161,0,0.0228217,"Missing"
N19-1085,W06-0901,0,0.0221327,"ts, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility L"
N19-1085,araki-etal-2014-detecting,0,0.0839054,"n event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and"
N19-1085,D17-1226,0,0.583099,"Missing"
N19-1085,D15-1247,0,0.312944,"(2013) provide empirical support for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity b"
N19-1085,R13-1021,0,0.547603,"een extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al."
N19-1085,J14-2004,0,0.266736,"een proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifi"
N19-1085,I13-1100,1,0.839226,"e aforementioned challenge, we propose a framework for transferring argument (in)compatibility knowledge to the event coreference resolution system, specifically by adopting the interactive inference network (Gong et al., 2018) as our model structure. The idea is as follows. First, we train a network to determine whether the corresponding arguments of an event mention pair are compatible on automatically labeled training instances collected from a large unlabeled news corpus. Second, to transfer the knowledge of argument (in)compatibility to an 2 Related Work Ablation experiments conducted by Chen and Ng (2013) provide empirical support for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki a"
N19-1085,chen-ng-2014-sinocoreferencer,1,0.911459,"inary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pret"
N19-1085,N15-1116,1,0.936491,"Missing"
N19-1085,P82-1020,0,0.819375,"Missing"
N19-1085,K16-1024,0,0.269017,"the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this"
N19-1085,P17-1171,0,0.0133487,"ihood above threshold θM are added to the new compatible set. On the other hand, mention pairs with a coreference likelihood below θm are added to the initial incompatible set to form the new incompatible set. With the new compatible and incompatible sets, we can start another iteration of transfer learning to train a coreference resolver with improved quality. In this work, we set θM to 0.8 and θm to 0.2. Exact match A binary feature indicating whether a given token appears in the context of both event mentions. This feature is proved useful for several NLP tasks operating on pairs of texts (Chen et al., 2017; Gong et al., 2018; Pan et al., 2018). Trigger position We encode the position of the trigger word by adding a binary feature to indicate whether a given token is a trigger word. 789 Figure 3: Model structure. 4 Encoding layer We pass the sequence of embedding vectors into a biLSTM layer (Hochreiter and Schmidhuber, 1997), resulting in a sequence of hidden vectors of size |h|: hia = biLST M (emb(wai ), hi−1 a ) hib = biLST M (emb(wbi ), hi−1 b ) 4.1 4.1.1 Experimental Setup Corpora We use English Gigaword (Parker et al., 2009) as the unlabeled corpus for argument compatibility learning. This"
N19-1085,D12-1045,0,0.211618,"e coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event mentions extracted from a large unlabeled news corpus. Task definition Given a pair of event mentions (ma , mb ) with related triggers, predict whether their arguments are compatible or not. Here"
N19-1085,W09-3208,0,0.410956,"n party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng"
N19-1085,D16-1038,0,0.544427,"Missing"
N19-1085,liu-etal-2014-supervised,0,0.642134,"systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in or"
N19-1085,L16-1631,1,0.818701,"ether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train"
N19-1085,D14-1162,0,0.079855,"m preprocessing on the input data. Inference layer In the pretraining stage, we feed fev to a fully-connected inference layer to make a binary prediction of argument compatibility. As for the fine-tuning stage, we concatenate an auxiliary feature vector faux to fev before feeding it into the inference layer. faux consists of two features, a one-hot vector that encodes the sentence distance between the two event mentions and the difference of the word embedding vectors of the two triggers. Network structure Each word embedding is initialized with the 300-dimensional pretrained GloVe embedding (Pennington et al., 2014). The character embedding layer is a combination of an 8-dimensional embedding layer and three 1D convolution layers with a kernel size of 5 with 100 filters. The size of the biLSTM layer is 200. The maximum length of a word is 16 characters; shorter words are padded with zero and longer 790 biLSTM (standard) biLSTM (transfer) Interact (standard) Interact (transfer) Interact (transfer, 2nd iter) Interact (transfer, 3rd iter) Jiang et al. (2017) MUC 29.49 33.84 31.12 34.28 35.66 36.05 30.63 B3 43.15 42.91 42.84 42.93 43.20 43.07 43.84 CEAFe 39.91 38.39 39.01 39.95 40.02 39.69 39.86 BLANC 24.15"
N19-1085,P17-1009,1,0.738198,"upport for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument he"
N19-1085,C16-1308,1,0.831745,"the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event mentions extracted from a large unlabeled news corpus. Task definition Given a pair of event mentions (ma , mb ) with related triggers, predict whether their arguments are compatible or not. Here, an event mention"
N19-1085,J01-4004,0,0.456765,"sentence pairs (Gong et al., 2018). Thus, we believe this network is suitable for capturing the argument compatibility between two event mentions. The model consists of the following components: Model inputs The input to the model is a pair of event mentions (ma , mb ), with ma being the antecedent mention of mb : Mention-Pair Event Coreference Model With the argument compatibility classifier trained in the previous stage, we use the labeled event coreference corpus to fine-tune the model into an event coreference resolver. We design the event coreference resolver to be a mention-pair model (Soon et al., 2001), which takes a pair of event mentions as the input and outputs the likelihood of them being coreferent. ma = {wa1 , wa2 , ..., waN } mb = {wb1 , wb2 , ..., wbN } (1) Each event mention is represented by a sequence of N tokens consisting of one trigger word and its context. Here, we take the context to be the words within an n-word window around the trigger. In this work, n is set to 10. With the pairwise event coreference predictions, we further conduct best-first clustering (Ng and Cardie, 2002) on the pairwise results to build the event coreference clusters of each document. Best-first clus"
N19-1085,H05-1004,0,0.604554,"d 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in rows 3 and 4. These results provide suggestive evidence that our proposed transfer learning framework, which utilizes a large unlabeled corpus to perform argument compatibility learning, is effective. Evaluation Metrics We follow the standard evaluation setup adopted in the official evaluation of the KBP event nugget detection and coreference task. This evaluation setup is based on four distinct scoring measures — MUC (Vilain et al., 1995) , B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011) — and the unweighted average of their F-scores (AVG-F). We use AVG-F as the main evaluation measure when comparing system performances. 4.2 Effect of iterative relabeling We achieve another boost in performance by using the trained event coreference resolver to relabel the training samples for argument compatibility learning. The best result is achieved after two iterations (row 5) with an improvement of 2.26 points in AVG-F compared to the standard interactive inference network (row 3). However, we are not able to obtain further gains with more iterations"
N19-1085,P10-1040,0,0.00563563,"teractive inference network can better capture the complex interactions between two event mentions, accounting for the difference in performance. words are cropped. For the interaction layer, we use convolution layers with a kernel size of 3 in combination with max-pooling layers. The size of the inference layer is 128. Sigmoid activation is used for the inference layer, and all other layers use ReLU as the activation function. Event mention detection model For word embeddings, we use the concatenation of a 300dimensional pretrained GloVe embedding and the 50-dimensional embedding proposed by Turian et al. (2010). The character embedding layer is a combination of an 8-dimensional embedding layer and three 1D convolution layers with kernel sizes of 3, 4, 5 with 50 filters. 4.1.3 Effect of transfer learning Regardless of the network structure, we observe a considerable improvement in performance by pretraining the model as an argument compatibility classifier. The biLSTM baseline model achieves an improvement of 1.25 points in AVG-F by doing transfer learning, as can be seen in rows 1 and 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in r"
N19-1085,P14-5010,0,0.00326788,"ence clusters of these event mentions. (2) where emb(w) is the embedding vector of token w. Interaction layer The interaction layer captures the relations between two event mentions based on the hidden vectors ha and hb . The interaction tensor I, a 3-D tensor of shape (N , N , |h|), is calculated by taking the pairwise multiplication of the corresponding hidden vectors: Iij = hia ◦ hjb Evaluation (3) Finally, we apply a multi-layer convolutional neural network to extract the event pair representation vector fev . 4.1.2 Implementation Details Preprocessing We use the Stanford CoreNLP toolkit (Manning et al., 2014) to perform preprocessing on the input data. Inference layer In the pretraining stage, we feed fev to a fully-connected inference layer to make a binary prediction of argument compatibility. As for the fine-tuning stage, we concatenate an auxiliary feature vector faux to fev before feeding it into the inference layer. faux consists of two features, a one-hot vector that encodes the sentence distance between the two event mentions and the difference of the word embedding vectors of the two triggers. Network structure Each word embedding is initialized with the 300-dimensional pretrained GloVe e"
N19-1085,M95-1005,0,0.878772,"VG-F by doing transfer learning, as can be seen in rows 1 and 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in rows 3 and 4. These results provide suggestive evidence that our proposed transfer learning framework, which utilizes a large unlabeled corpus to perform argument compatibility learning, is effective. Evaluation Metrics We follow the standard evaluation setup adopted in the official evaluation of the KBP event nugget detection and coreference task. This evaluation setup is based on four distinct scoring measures — MUC (Vilain et al., 1995) , B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011) — and the unweighted average of their F-scores (AVG-F). We use AVG-F as the main evaluation measure when comparing system performances. 4.2 Effect of iterative relabeling We achieve another boost in performance by using the trained event coreference resolver to relabel the training samples for argument compatibility learning. The best result is achieved after two iterations (row 5) with an improvement of 2.26 points in AVG-F compared to the standard interactive inference network (row 3). However, we are not"
N19-1085,Q15-1037,0,0.304132,"2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event menti"
N19-1085,P02-1014,1,0.591394,"an event coreference resolver. We design the event coreference resolver to be a mention-pair model (Soon et al., 2001), which takes a pair of event mentions as the input and outputs the likelihood of them being coreferent. ma = {wa1 , wa2 , ..., waN } mb = {wb1 , wb2 , ..., wbN } (1) Each event mention is represented by a sequence of N tokens consisting of one trigger word and its context. Here, we take the context to be the words within an n-word window around the trigger. In this work, n is set to 10. With the pairwise event coreference predictions, we further conduct best-first clustering (Ng and Cardie, 2002) on the pairwise results to build the event coreference clusters of each document. Best-first clustering is an agglomerative clustering algorithm that links each event mention to the antecedent event mention with the highest coreference likelihood given the likelihood is above an empirically determined threshold. 3.3 Model Structure Embedding layer We represent each input token by the concatenation of the following components: Word embedding The word representation of the given token. We use pretrained word vectors to initialize the word embedding layer. Character embedding To identify (in)com"
N19-1085,P18-1091,0,0.0199144,"the new compatible set. On the other hand, mention pairs with a coreference likelihood below θm are added to the initial incompatible set to form the new incompatible set. With the new compatible and incompatible sets, we can start another iteration of transfer learning to train a coreference resolver with improved quality. In this work, we set θM to 0.8 and θm to 0.2. Exact match A binary feature indicating whether a given token appears in the context of both event mentions. This feature is proved useful for several NLP tasks operating on pairs of texts (Chen et al., 2017; Gong et al., 2018; Pan et al., 2018). Trigger position We encode the position of the trigger word by adding a binary feature to indicate whether a given token is a trigger word. 789 Figure 3: Model structure. 4 Encoding layer We pass the sequence of embedding vectors into a biLSTM layer (Hochreiter and Schmidhuber, 1997), resulting in a sequence of hidden vectors of size |h|: hia = biLST M (emb(wai ), hi−1 a ) hib = biLST M (emb(wbi ), hi−1 b ) 4.1 4.1.1 Experimental Setup Corpora We use English Gigaword (Parker et al., 2009) as the unlabeled corpus for argument compatibility learning. This corpus consists of the news articles f"
P02-1014,M95-1005,0,\N,Missing
P02-1014,N01-1008,0,\N,Missing
P02-1014,M95-1014,0,\N,Missing
P02-1014,M95-1010,0,\N,Missing
P02-1014,J94-4002,0,\N,Missing
P02-1014,P95-1017,0,\N,Missing
P02-1014,J01-4004,0,\N,Missing
P04-1020,P99-1048,0,0.630449,"oric NP cannot be coreferent with any of its preceding NPs in a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information (Ng and Ca"
P04-1020,J96-1002,0,0.0117368,"Missing"
P04-1020,C96-1021,0,0.277908,"wo NPs can co-refer remains an active area of research in the community. One significant constraint on coreference, the non-anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity inform"
P04-1020,J94-4002,0,0.339639,"tic constraints on when two NPs can co-refer remains an active area of research in the community. One significant constraint on coreference, the non-anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be impr"
P04-1020,C02-1139,1,0.85385,"n a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information (Ng and Cardie, 2002a). Our goal in this paper is to improve learni"
P04-1020,P02-1014,1,0.848237,"n a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information (Ng and Cardie, 2002a). Our goal in this paper is to improve learni"
P04-1020,J01-4004,0,0.853831,"pproach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information (Ng and Cardie, 2002a). Our goal in this paper is to improve learningbased coreference systems using automatically computed anaphoricity information. In particular, we examine two important, yet largely unexplored, issues in anaphoricity determination"
P04-1020,P03-1022,0,0.0133057,"Missing"
P04-1020,J00-4003,0,0.0472089,"significant constraint on coreference, the non-anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively. One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antec"
P04-1020,M95-1005,0,0.41212,"ystem on the test texts. 5 Evaluation In this section, we will compare the effectiveness of four approaches to anaphoricity determination (see the introduction) in improving our baseline coreference system. 5.1 Coreference Without Anaphoricity As mentioned above, we use our coreference system as the baseline system where no explicit anaphoricity determination system is employed. Results using RIPPER and MaxEnt as the underlying learners are shown in rows 1 and 2 of Table 2 where performance is reported in terms of recall, precision, and F-measure using the model-theoretic MUC scoring program (Vilain et al., 1995). With RIPPER, the system achieves an F-measure of 56.3 for BNEWS, 61.8 for NPAPER, and 51.7 for NWIRE. The performance of MaxEnt is comparable to that of RIPPER for the BNEWS and NPAPER data sets but slightly worse for the NWIRE data set. 5.2 Coreference With Anaphoricity The Constraint-Based, Locally-Optimized (CBLO) Approach. As mentioned before, in constraint-based approaches, the automatically computed non-anaphoricity information is used as 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 System Variation Experiments L No RIP Anaphoricity ME ConstraintRIP Based, RIP LocallyME Optimized ME Fe"
P04-1020,P03-1023,0,0.216891,"pes of (non-)anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube and M¨uller (2003), Yang et al. (2003)). Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information. For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information (Ng and Cardie, 2002a). Our goal in this paper is to improve learningbased coreference systems using automatically computed anaphoricity information. In particular, we examine two important, yet largely unexplored, issues in anaphoricity determination for coreference resolution: representation and optimization. Constrain"
P05-1020,P95-1017,0,0.169243,"mmarizes the previous work on coreference resolution that employs the learning algorithms, clustering algorithms, feature sets, and instance creation methods discussed above. With three learners, three training instance creation methods, two feature sets, and three clustering algorithms, we can produce 54 coreference systems in total. Learning algorithm Instance creation method Feature set Clustering algorithm Decision tree learners (C4.5/C5/CART) RIPPER Maximum entropy McCarthy and Lehnert’s Soon et al.’s Ng and Cardie’s Soon et al.’s Ng and Cardie’s Closest-first Best-first Aggressive-merge Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001), Strube et al. (2002), Strube and M¨uller (2003), Yang et al. (2003) Ng and Cardie (2002b) Kehler (1997), Morton (2000), Luo et al. (2004) McCarthy and Lehnert (1995), Aone and Bennett (1995) Soon et al. (2001), Strube et al. (2002), Iida et al. (2003) Ng and Cardie (2002b) Soon et al. (2001) Ng and Cardie (2002b) Soon et al. (2001), Strube et al. (2002) Aone and Bennett (1995), Ng and Cardie (2002b), Iida et al. (2003) McCarthy and Lehnert (1995) Table 1: Summary of the previous work on coreference resolution that employs the learning algorith"
P05-1020,P98-1012,0,0.0360513,"orpus. 4 Evaluation 4.1 Experimental Setup For evaluation purposes, we use the ACE (Automatic Content Extraction) coreference corpus, which is composed of three data sets created from three different news sources, namely, broadcast news (BNEWS), newspaper (NPAPER), and newswire (NWIRE).5 Statistics of these data sets are shown in Table 2. In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the B-CUBED scorer (Bagga and Baldwin, 1998). 4.2 Results Using the MUC Scorer Baseline systems. We employ as our baseline systems two existing coreference resolvers: our duplication of the Soon et al. (2001) system and the Ng and Cardie (2002b) system. Both resolvers adopt the standard machine learning approach and therefore can be characterized using the four elements discussed in Section 3.1. Specifically, Soon et al.’s system employs a decision tree learner to train a coreference classifier on instances created by Soon’s method and represented by Soon’s feature set, coordinating the classification decisions via closest-first cluster"
P05-1020,N04-1038,0,0.155185,"create candidate partitions as in training, and select the highest-ranked partition according to the ranking model to be the final partition.3 The rest of this section describes how we select these learning-based coreference systems and acquire the ranking model. 3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see Kehler (1997) and Bean and Riloff (2004)) and its variants (see Harabagiu et al. (2001) and Luo et al. (2004)). 3 The ranking model breaks ties randomly. used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions. Selecting a coreference system, then, is a matter of instantiating these elements with specific values. Now we need to define the set of allowable values for each of these elements. In particular, we want to define them in such a way that the resulting coreference systems can potentially generate good candidate partitions. Given that machine learn"
P05-1020,J96-1002,0,0.00243833,"grammatical, semantic, and positional. Space limitations preclude a description of these features. Details can be found in Soon et al. (2001). Ng and Cardie expand Soon et al.’s feature set from 12 features to a deeper set of 53 to allow more complex NP string matching operations as well as finer-grained syntactic and semantic compatibility tests. See Ng and Cardie (2002b) for details. Learning algorithms. We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al., 1996). The classification model induced by each of these learners returns a number between 0 and 1 that indicates the likelihood that the two NPs under consideration are coreferent. In this work, NP pairs with class values above 0.5 are considered COREFERENT; otherwise the pair is considered NOT COREFERENT. Clustering algorithms. We employ three clustering algorithms, as described below. The closest-first clustering algorithm selects as the antecedent of NP its closest preceding coreferent NP. If no such NP exists, then NP is assumed to be non-anaphoric (i.e., no antecedent is selected). On the o"
P05-1020,W99-0611,0,0.0589878,"ss data for training coreference classifiers, but at the same time we can employ weakly supervised techniques to bootstrap the classifiers. Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., M¨uller et al. (2002)), and this is also an area that deserves further research. To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al. (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)). Of course, we can also expand our pre-selected set of coreference systems via incorporating additional learning algorithms, clustering algorithms, and feature sets. Once again, we may use previous work to guide our choices. For instance, Iida et al. (2003) and Zelenko et al. (2004) have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers. McCallum and Wellner (2003) and Zelenko et al. (2004) have employed graph-based partitioning algorithms such as correlation clustering (Bansal et al., 2002). Finally, Strube et al."
P05-1020,W02-1001,0,0.0227268,"idate  partitions by means of Joachims’ (2002) SVM package, with all the parameters set to their default values. To create training data, we first generate 54 candidate partitions for each text in the held-out subset as described above and then convert each partition into a training instance consisting of a set of partition-based features and method-based features. Partition-based features are used to characterize a candidate partition and can be derived directly from the partition itself. Following previous work on using global features of candidate structures to learn a ranking model (Collins, 2002), the global (i.e., partition-based) features we consider here are simple functions of the local features that capture the relationship between NP pairs. Specifically, we define our partition-based features in terms of the features in the Ng and Cardie (N&C) feature set (see Section 3.1) as follows. First,  is the  -th nominal feature in let us assume that N&C’s feature set and  is the -th possible value  of . Next, for each  and , we create two partitionbased features,  and    .  is computed over the set of coreferent NP pairs (with respect to the candidate partition), denoting the"
P05-1020,N01-1008,0,0.465185,"our approach differs from the standard approach primarily by (1) explicitly learning a ranker and (2) optimizing for clustering-level accuracy. In this section we will focus on discussing related work along these two dimensions. Ranking candidate partitions. Although we are not aware of any previous attempt on training a available computing resources. 158 ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, Harabagiu et al. (2001) introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions. At each step of this search process, candidate partitions are ranked based on their heuristically computed scores. Optimizing for clustering-level accuracy. Ng and Cardie (2002a) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program. Strube and M¨uller (2003) propose a similar id"
P05-1020,W03-2604,0,0.153307,"coreference systems in total. Learning algorithm Instance creation method Feature set Clustering algorithm Decision tree learners (C4.5/C5/CART) RIPPER Maximum entropy McCarthy and Lehnert’s Soon et al.’s Ng and Cardie’s Soon et al.’s Ng and Cardie’s Closest-first Best-first Aggressive-merge Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001), Strube et al. (2002), Strube and M¨uller (2003), Yang et al. (2003) Ng and Cardie (2002b) Kehler (1997), Morton (2000), Luo et al. (2004) McCarthy and Lehnert (1995), Aone and Bennett (1995) Soon et al. (2001), Strube et al. (2002), Iida et al. (2003) Ng and Cardie (2002b) Soon et al. (2001) Ng and Cardie (2002b) Soon et al. (2001), Strube et al. (2002) Aone and Bennett (1995), Ng and Cardie (2002b), Iida et al. (2003) McCarthy and Lehnert (1995) Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1. 3.2 Learning to Rank Candidate Partitions We train an SVM-based ranker for ranking candidate  partitions by means of Joachims’ (2002) SVM package, with all the parameters set to"
P05-1020,W97-0319,0,0.424275,"ference systems to create candidate partitions as in training, and select the highest-ranked partition according to the ranking model to be the final partition.3 The rest of this section describes how we select these learning-based coreference systems and acquire the ranking model. 3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see Kehler (1997) and Bean and Riloff (2004)) and its variants (see Harabagiu et al. (2001) and Luo et al. (2004)). 3 The ranking model breaks ties randomly. used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions. Selecting a coreference system, then, is a matter of instantiating these elements with specific values. Now we need to define the set of allowable values for each of these elements. In particular, we want to define them in such a way that the resulting coreference systems can potentially generate good candidate partition"
P05-1020,P04-1018,0,0.840082,"good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 1 Introduction Recent research in coreference resolution — the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity — has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube et al. (2002), Yang et al. (2003), Luo et al. (2004)). The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task. Specifically, a classifier is first trained to determine whether two NPs in a document are co-referring or not. A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs, with one cluster for each set of coreferent NPs. Though reasonably successful, this “standard” approach is not as robust as one may think. First, design decisions such as"
P05-1020,P00-1023,0,0.06144,"three learners, three training instance creation methods, two feature sets, and three clustering algorithms, we can produce 54 coreference systems in total. Learning algorithm Instance creation method Feature set Clustering algorithm Decision tree learners (C4.5/C5/CART) RIPPER Maximum entropy McCarthy and Lehnert’s Soon et al.’s Ng and Cardie’s Soon et al.’s Ng and Cardie’s Closest-first Best-first Aggressive-merge Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001), Strube et al. (2002), Strube and M¨uller (2003), Yang et al. (2003) Ng and Cardie (2002b) Kehler (1997), Morton (2000), Luo et al. (2004) McCarthy and Lehnert (1995), Aone and Bennett (1995) Soon et al. (2001), Strube et al. (2002), Iida et al. (2003) Ng and Cardie (2002b) Soon et al. (2001) Ng and Cardie (2002b) Soon et al. (2001), Strube et al. (2002) Aone and Bennett (1995), Ng and Cardie (2002b), Iida et al. (2003) McCarthy and Lehnert (1995) Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1. 3.2 Learning to Rank Candidate Partitions We train"
P05-1020,P02-1045,0,0.0353995,"Missing"
P05-1020,W02-1008,1,0.783209,"tition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 1 Introduction Recent research in coreference resolution — the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity — has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube et al. (2002), Yang et al. (2003), Luo et al. (2004)). The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task. Specifically, a classifier is first trained to determine whether two NPs in a document are co-referring or not. A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs, with one cluster for each set of coreferent NPs. Though reasonably successful, this “standard” approach is n"
P05-1020,P02-1014,1,0.817519,"tition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 1 Introduction Recent research in coreference resolution — the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity — has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube et al. (2002), Yang et al. (2003), Luo et al. (2004)). The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task. Specifically, a classifier is first trained to determine whether two NPs in a document are co-referring or not. A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs, with one cluster for each set of coreferent NPs. Though reasonably successful, this “standard” approach is n"
P05-1020,J01-4004,0,0.947667,"propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 1 Introduction Recent research in coreference resolution — the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity — has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube et al. (2002), Yang et al. (2003), Luo et al. (2004)). The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task. Specifically, a classifier is first trained to determine whether two NPs in a document are co-referring or not. A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs, with one cluster for each set of coreferent NPs. Though reasonably successful, this “st"
P05-1020,P03-1022,0,0.017134,"Missing"
P05-1020,W02-1040,0,0.0633687,"Missing"
P05-1020,M95-1005,0,0.744627,"74 29 20528 Table 2: Statistics for the ACE corpus. 4 Evaluation 4.1 Experimental Setup For evaluation purposes, we use the ACE (Automatic Content Extraction) coreference corpus, which is composed of three data sets created from three different news sources, namely, broadcast news (BNEWS), newspaper (NPAPER), and newswire (NWIRE).5 Statistics of these data sets are shown in Table 2. In our experiments, we use the training texts to acquire coreference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the B-CUBED scorer (Bagga and Baldwin, 1998). 4.2 Results Using the MUC Scorer Baseline systems. We employ as our baseline systems two existing coreference resolvers: our duplication of the Soon et al. (2001) system and the Ng and Cardie (2002b) system. Both resolvers adopt the standard machine learning approach and therefore can be characterized using the four elements discussed in Section 3.1. Specifically, Soon et al.’s system employs a decision tree learner to train a coreference classifier on instances created by Soon’s method and represented by Soon’s feature set, coordinating the c"
P05-1020,P03-1023,0,0.128646,"l for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 1 Introduction Recent research in coreference resolution — the problem of determining which noun phrases (NPs) in a text or dialogue refer to which real-world entity — has exhibited a shift from knowledgebased approaches to data-driven approaches, yielding learning-based coreference systems that rival their hand-crafted counterparts in performance (e.g., Soon et al. (2001), Ng and Cardie (2002b), Strube et al. (2002), Yang et al. (2003), Luo et al. (2004)). The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task. Specifically, a classifier is first trained to determine whether two NPs in a document are co-referring or not. A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on the given set of NPs, with one cluster for each set of coreferent NPs. Though reasonably successful, this “standard” approach is not as robust as one may think. First, design"
P05-1020,W04-0704,0,0.0186141,"arch. To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al. (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)). Of course, we can also expand our pre-selected set of coreference systems via incorporating additional learning algorithms, clustering algorithms, and feature sets. Once again, we may use previous work to guide our choices. For instance, Iida et al. (2003) and Zelenko et al. (2004) have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers. McCallum and Wellner (2003) and Zelenko et al. (2004) have employed graph-based partitioning algorithms such as correlation clustering (Bansal et al., 2002). Finally, Strube et al. (2002) and Iida et al. (2003) have proposed new edit-distance-based string-matching features and centering-based features, respectively. Test Set BNEWS NPAPER NWIRE Scoring Program MUC BCUBED MUC B-CUBED MUC B-CUBED Average Rank 7.2549 16.9020 1.4706 9.3529 7.7241 13.1379 Instance Creation Method McCarthy a"
P05-1020,C98-1012,0,\N,Missing
P06-2079,P97-1023,0,0.375928,"Missing"
P06-2079,C04-1200,0,0.259453,"ures interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification. isolation or in conjunction with unigrams. This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification. Manually-tagged term polarity. Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al. (2005), Esuli and Sebastiani (2005)). Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision. While manually constructed positive and negative word lists exist (e.g., General Inquirer1 ), they too suffer from the problem of having low coverage. This prompts us to manually construct our own polarity word lists 2 and study their use in polarity classification. Dependency relations. There have been several attempts at extracting features for polarity classification from depe"
P06-2079,W04-3253,0,0.638152,"m 2 Wilson et al. (2005) have also manually tagged a list of terms with their polarity, but this list is not publicly available. 612 movie domain from the Internet Movie Database website4 , randomly selecting any documents from this site that are on the movie topic but are not reviews themselves. With this criterion in mind, the 2000 non-review documents we end up with are either movie ads or plot summaries. Finally, previous work has also investigated features that do not fall into any of the above categories. For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turney’s (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration. 3 Review Identification Recall that the goal of review identification is to determine whether a given document is a review or not. Given this definition, two immediate questions come to mind. First, should this problem be addressed in a domain-specific or domainindependent manner? In other words, should a review identification system take as input documents coming f"
P06-2079,P04-1035,0,0.33812,"investigate the use of a larger set of dependency relations for classifying reviews. Objective information. The objective portions of a review do not contain the author’s opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). Motivated by the work of Koppel and Schler (2005), we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification. 2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found. Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective. Ther"
P06-2079,W02-1011,0,0.054563,"e phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found. Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective. There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al. (2004) for details). 2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g., Pang et al. (2002), Turney (2002)), a sentence (e.g., Liu et al. (2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al. (2005)), a phrase (e.g., Wilson et al. (2005)), and a specific object (such as a product) mentioned in a document (e.g., Morinaga et al. (2002), Yi et al. (2003), Popescu and Etzioni (2005)). Below we will center our discussion of related work around the four types of features we will explore for polarity classification. Higher-order n-grams. While n-grams offer a simple way of capturing context, previous work has rarely explored the use of n-grams as features in a polarity"
P06-2079,N03-1025,0,0.0297091,"Missing"
P06-2079,H05-1043,0,0.293851,"e manually constructed positive and negative word lists exist (e.g., General Inquirer1 ), they too suffer from the problem of having low coverage. This prompts us to manually construct our own polarity word lists 2 and study their use in polarity classification. Dependency relations. There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g., Dave et al. (2003), Yi et al. (2003)) or nouns that enjoy a dependency relation with a polarity term (e.g., Popescu and Etzioni (2005)). Wilson et al. (2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document. In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews. Objective information. The objective portions of a review do not contain the author’s opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. Indeed, recent work has shown that be"
P06-2079,P02-1053,0,0.195647,"s types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification. isolation or in conjunction with unigrams. This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification. Manually-tagged term polarity. Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al. (2005), Esuli and Sebastiani (2005)). Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision. While manually constructed positive and negative word lists exist (e.g., General Inquirer1 ), they too suffer from the problem of having low coverage. This prompts us to manually construct our own polarity word lists 2 and study their use in polarity classification. Dependency relations. There have been several attempts at extracting features for polarity cla"
P06-2079,J04-3002,0,0.0927149,"lassification. 2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found. Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective. There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al. (2004) for details). 2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g., Pang et al. (2002), Turney (2002)), a sentence (e.g., Liu et al. (2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al. (2005)), a phrase (e.g., Wilson et al. (2005)), and a specific object (such as a product) mentioned in a document (e.g., Morinaga et al. (2002), Yi et al. (2003), Popescu and Etzioni (2005)). Below we will center our discussion of related work around the four types of features we will explore for polarity classification. Higher-order n-g"
P06-2079,H05-1044,0,0.47933,"ve and negative word lists exist (e.g., General Inquirer1 ), they too suffer from the problem of having low coverage. This prompts us to manually construct our own polarity word lists 2 and study their use in polarity classification. Dependency relations. There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g., Dave et al. (2003), Yi et al. (2003)) or nouns that enjoy a dependency relation with a polarity term (e.g., Popescu and Etzioni (2005)). Wilson et al. (2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document. In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews. Objective information. The objective portions of a review do not contain the author’s opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. Indeed, recent work has shown that benefits can be made by f"
P06-2079,W03-1017,0,0.213386,"rses, but unlike us, their goal is to determine the polarity of a phrase, not a document. In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews. Objective information. The objective portions of a review do not contain the author’s opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). Motivated by the work of Koppel and Schler (2005), we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification. 2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found. Hence, review identification can be viewed as an instance of the"
P06-2079,H05-2017,0,\N,Missing
P07-1068,N04-1038,0,0.0269655,"rence relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et"
P07-1068,W99-0613,0,0.258197,"Missing"
P07-1068,H05-1013,0,0.0965953,"Missing"
P07-1068,P06-1060,0,0.0994906,"Missing"
P07-1068,C92-2082,0,0.0101491,"etermine whether the head noun of NPi is a hyponym of w in WordNet, using only the first WordNet sense of NPi .1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)). Given a large, unannotated corpus3 , we use IdentiFinder to label each NE with its NE type and MINIPAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier&gt;, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types4 or OTHERS5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrenc"
P07-1068,H05-1003,0,0.0276413,"lable to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a pro"
P07-1068,N04-1037,0,0.124655,"Missing"
P07-1068,P98-2127,0,0.12379,"Missing"
P07-1068,M98-1006,0,0.126549,"Missing"
P07-1068,P04-1018,0,0.068869,"reference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms. Joint probabilistic models of coreference. Recently, there has been a surge of interest in improving coreference resolution by jointly modeling coreference with a related task such as MD (e.g., Daum´e and Marcu (2005)). However, joint models typically need to be trained on data that is simultaneously annotated with information required by all of the underlying models. For instance, Daum´e and Marcu’s model assume"
P07-1068,J05-3004,0,0.0294022,"pe of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al. (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.’s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE coreferProceedings of the 45th Annual Meeting of the Association of Computational Linguistics,"
P07-1068,P98-2143,0,0.0497078,"Missing"
P07-1068,P02-1014,1,0.813704,"ner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , and its closest antecedent, NPi ; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1 , NPi+2 , . . ., NPj−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below. Lexical features. Nine features allow different types of string matching operations to be performed on the given pair of NPs, NPx and NPy 10 , including (1) exact string match for pronouns, proper nouns, and non-pronominal NPs (both before and after determiners are removed); (2) substring match for proper nouns and non-pronominal NPs; and (3) head noun match. In addition, one feature tests whether all the words that appear in one NP also appear in the other NP. Finally, a nationality matching feature is used to match, for instance, British with Brit"
P07-1068,P04-1019,0,0.0259677,"emantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many"
P07-1068,N06-1025,0,0.469303,"role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun t"
P07-1068,J01-4004,0,0.936394,"(2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al. (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.’s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE coreferProceedings of the 45th Annual Meeting of the Association of"
P07-1068,J01-4003,0,0.0441215,"Missing"
P07-1068,M95-1005,0,0.0205952,"and mention — and incorporate them into our learning-based coreference resolver in eight different ways, as described in the introduction. This section examines whether our coreference resolver can benefit from any of the eight ways of incorporating these KSs. 4.1 Experimental Setup As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determin"
P07-1068,P03-1023,0,0.0651913,"quire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj , and its closest antecedent, NPi ; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1 , NPi+2 , . . ., NPj−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below. Lexical features. Nine features allow different types of string matching operations to be performed on the given pair of NPs, NPx and NPy 10 , including (1) exact string match for pronouns, proper nouns, and non-pronominal NPs (both before and after determiners are removed); (2) substring match for proper nouns and non-pronominal NPs; and (3) head noun match. In addition, one feature tests whether all the words that appear in one NP also appear in the other NP. Finally, a nationality matching feature is used to match, for instance, British with Britain. Grammatical featur"
P07-1068,P95-1026,0,0.171185,"Missing"
P07-1068,C98-2122,0,\N,Missing
P07-1068,C98-2138,0,\N,Missing
P09-1079,W02-1011,0,0.0274367,"ration of our algorithm for removing ambiguous points (see Section 3.1). We then put point i into set Li mod 5 . This ensures that each set consists of not only an equal number of positive and negative points, but also a mix of very confidently labeled points and comparatively less confidently labeled points. Each classifier Ci will then be trained transductively, using the 100 manually labeled points and the points in Li as labeled data, and the remaining points (including all For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007). Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives). We divide the 2000 reviews into 10 equal-sized folds for cross-validation purposes, maintaining balanced class distributions in each fold. It is important to note that while the test fold is accessible to the transductive learner (Step 3), only the reviews in training folds (but not their labels) are used for the acquisition of se"
P09-1079,P07-1056,0,0.751258,"ic clusters are typically well-separated from each other, resulting from the fact that word usage differs considerably between two topically-different documents. On the other hand, many reviews are sentimentally 701 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701–709, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP classification systems are domain-specific. Hence, when given a new domain, a large amount of annotated data from the domain typically needs to be collected in order to train a high-performance polarity classification system. As Blitzer et al. (2007) point out, this data collection process can be “prohibitively expensive, especially since product features can change over time”. Unfortunately, to our knowledge, unsupervised polarity classification is largely an under-investigated task in NLP. Turney’s (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. However, while his system learns the semantic orientation of phrases in a review in an unsupervised manner, such information is used to heuristically predict the polarity of a review. perimental results on five sentiment classification datasets de"
P09-1079,P02-1053,0,0.0849051,"Missing"
P09-1079,P08-2059,0,0.0109081,"nable to assume that the reviews whose labels the classifier is most uncertain about (and therefore are most informative to the classifier) are those that are ambiguous. Following previous work on active learning for SVMs (e.g., Campbell et al. (2000), Schohn and Cohn (2000), Tong and Koller (2002)), we define the uncertainty of a data point as its distance from the separating hyperplane. In other words, Incorporating Active Learning Spectral clustering allows us to focus on a small number of dimensions that are relevant as far as creating well-separated clusters is concerned, but 4 Following Dredze and Crammer (2008), we perform cross-validation experiments on the 2000 labeled reviews in each evaluation dataset, choosing the active learning points from the training folds. Note that the seeds obtained in the previous step were also acquired using the training folds only. 3 Additional experiments indicate that the accuracy of our approach is not sensitive to small changes to these values. 705 points that are closer to the hyperplane are more uncertain than those that are farther away. We perform active learning for five iterations. In each iteration, we select the 10 most uncertain points from each side of"
P09-1079,P07-1055,0,0.0525028,"view is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart. Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers. For instance, Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification. Koppel and Schler (2006) use neutral reviews to help improve the classification of positive and negative reviews. More recently, McDonald et al. (2007) have investigated a model for jointly performing sentence- and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited. However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated data. For instance, Koppel and Schler needed to manually identify neutral reviews to train their polarity classifier, and McDonald et al.’s joint model requires that each sentence in a review be labeled with polarity information. Given the difficulties of supervised polarity classificatio"
P09-1079,P04-1035,0,0.124628,"sed both the positive and negative aspects of the movie, which is not uncommon in reviews. As another example, a large portion of a movie review may be devoted exclusively to the plot, with the author only briefly expressing her sentiment at the end of the review. In this case, the review is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart. Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers. For instance, Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification. Koppel and Schler (2006) use neutral reviews to help improve the classification of positive and negative reviews. More recently, McDonald et al. (2007) have investigated a model for jointly performing sentence- and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited. However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated d"
P09-1095,J96-2004,0,0.0133945,"ences of each shaping factor in the dataset. The “Total” column shows the number of narratives labeled with each shaper and the percentage of narratives tagged with each shaper in the 1,333 labeled narrative set. The “F” columns show the number narratives associated with each shaper in folds F1 – F5. x (# Shapers) Percentage 1 53.6 2 33.2 3 10.3 4 2.7 5 0.2 6 0.1 Table 3: Percentage of documents with x labels. with this research independently annotate them with shaping factors, based solely on the definitions presented in Table 1. To measure interannotator agreement, we compute Cohen’s Kappa (Carletta, 1996) from the two sets of annotations, obtaining a Kappa value of only 0.43. This not only suggests the difficulty of the cause identification task, but also reveals the vagueness inherent in the definition of the 14 shapers. As a result, we had the two annotators re-examine each report for which there was a disagreement and reach an agreement on its final set of labels. Statistics of the annotated dataset can be found in Table 2, where the “Total” column shows the size of each of the 14 classes, expressed both as the number of reports that are labeled with a particular shaper and as a percent (in"
P09-1095,W99-0908,0,0.0247614,"data using unlabeled documents collected from the Web. Minority classes can be expanded without the availability of unlabeled data as well. For example, Chawla et al. (2002) describe a method by which synthetic training examples of minority classes can be generated from other labeled training examples to address the problem of imbalanced data in a variety of domains. Nigam et al. (2000) propose an iterative semisupervised method that employs the EM algorithm in combination with the naive Bayes generative model to combine a small set of labeled documents and a large set of unlabeled documents. McCallum and Nigam (1999) suggest that the initial labeled examples can be obtained using a list of keywords rather than through annotated data, yielding an unsupervised algorithm. Similar bootstrapping methods are applicable outside text classification as well. One of the most notable examples is Yarowsky’s (1995) bootstrapping algorithm for word sense disambiguation. Beginning with a list of unlabeled contexts surrounding a word to be disambiguated and a list of seed words for each possible sense, the algorithm iteratively uses the seeds to label a training set from the unlabeled contexts, and then uses the training"
P09-1095,W02-1011,0,0.0143666,"involves multiple classes; (3) it involves multilabel categorization, where more than one label can be assigned to each document; (4) the class distributions are skewed, with some categories significantly outnumbering the others; and (5) the documents belong to the same domain (e.g., movie review classification). In particular, when the documents to be classified are from the same domain, 1 http://kdd.ics.uci.edu/databases/20newsgroups/ Of course, the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topic-based text classification (Pang et al., 2002). 2 843 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 843–851, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP et al., 2005), the method was evaluated on only 20 manually labeled reports, which are not made publicly available. Second, the fact that this is a 14-class classification problem makes it more challenging than a binary classification problem. Third, a report can be labeled with more than one category, as several shapers can contribute to the occurrence of an aviation incident. Fourth, the class distribution is very skewed: based on"
P09-1095,P95-1026,0,0.185641,"Missing"
P09-1095,W01-0501,0,\N,Missing
P10-1142,P95-1017,0,0.196944,"ding an annotation project that aims to collect large amounts of coreference data for English via a Web Collaboration game called Phrase Detectives2 . 1 2 http://www.itl.nist.gov/iad/mig/tests/ace/ http://www.phrasedetectives.org 3 Learning-Based Coreference Models In this section, we examine three important classes of coreference models that were developed in the past fifteen years, namely, the mention-pair model, the entity-mention model, and ranking models. 3.1 Mention-Pair Model The mention-pair model is a classifier that determines whether two NPs are coreferent. It was first proposed by Aone and Bennett (1995) and McCarthy and Lehnert (1995), and is one of the most influential learning-based coreference models. Despite its popularity, this binary classification approach to coreference is somewhat undesirable: the transitivity property inherent in the coreference relation cannot be enforced, as it is possible for the model to determine that A and B are coreferent, B and C are coreferent, but A and C are not coreferent. Hence, a separate clustering mechanism is needed to coordinate the pairwise classification decisions made by the model and construct a coreference partition. Another issue that surrou"
P10-1142,P08-1002,0,0.0239454,"a subset of the NPs are anaphoric and therefore need to be resolved. Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver. Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques. For example, pleonastic it has been identified using heuristic approaches (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)), supervised approaches (e.g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to earlier work on anaphoricity determination, recently proposed"
P10-1142,J01-4006,0,0.0187244,"ering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scor1403 ing (see Popescu-Belis et al. (2004)). In practice, these general-purpose methods are typically used to provide scores that complement those obtained via the three coreference scorers discussed above. It is worth mentioning that there is a trend towards evaluating a resolver against multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer. For further discussion on evaluation issues, see Byron (2001). 6 Concluding Remarks While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training (M¨uller et al., 2002), self-training (Kehler et al., 2004a), and EM (Cherry and Bergsma, 2005; Ng, 2008). Interestingly, recent results indicate that unsupervised approaches to coreference resolution (e.g., Haghighi and Klein (2007; 2010), Poon and Domingos (2"
P10-1142,W99-0611,0,0.227782,"abilistic model is used, we can define a threshold above which a pair of NPs is considered coreferent. 1398 lae (2006)). The Dempster-Shafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition. Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters. In these algorithms, not only are the NPs in a text processed in a left-to-right manner, the later coreference decisions are dependent on the earlier ones (Cardie and Wagstaff, 1999; Klenner and Ailloud, 2008).5 For example, to resolve an NP, NPk , Cardie and Wagstaff’s algorithm considers each preceding NP, NPj , as a candidate antecedent in a right-to-left order. If NPk and NPj are likely to be coreferent, the algorithm imposes an additional check that NPk does not violate any constraint on coreference (e.g., gender agreement) with any NP in the cluster containing NPj before positing that the two NPs are coreferent. Luo et al.’s (2004) Bell-tree-based algorithm is another clustering algorithm where the later coreference decisions are dependent on the earlier ones. A Be"
P10-1142,J96-2004,0,0.155606,"nalize partitions with overly large clusters. To address these problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition. To apply these scorers to automatically extracted NPs, different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al. (2009)). Since coreference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scor1403 ing (see Popescu-Belis et al. (2004)). In practice, these general-purpose methods are typically used to provide scores that complement those obtained via the three coreference scorers discussed above. It is worth mentioning that there is a trend towards evaluating a resolver against multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer. For further discussion on evaluation issues, see Byron (2001). 6 Concluding Remarks While we have focused our discussion on supervised approaches, coreference researchers have a"
P10-1142,N01-1008,0,0.510605,"NP is a pronoun. To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of training instances from NP pairs that are unlikely to be coreferent, such as NP pairs that violate gender and number agreement (e.g., Strube et al. (2002), Yang et al. (2003)). While many instance creation methods are heuristic in nature (see Uryupina (2004) and Hoste and Daelemans (2005)), some are learning-based. For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al. (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model. 3.1.2 Training a Coreference Classifier Once a training set is created, we can train a coreference model using an off-the-shelf learning algorithm. Decision tree induction systems (e.g., C5 (Quinlan, 1993)) are the first and one of the most widely used learning algorithms by coreference researchers, although rule learners (e.g., RIPPER (Cohen, 1995)) and memory-based learners (e.g., Ti"
P10-1142,N06-2015,0,0.0368097,"ra produced by the Automatic Content Extraction (ACE1 ) evaluations in the past decade: while the earlier ACE corpora (e.g., ACE-2) consist of solely English newswire and broadcast news articles, the later ones (e.g., ACE 2005) have also included Chinese and Arabic documents taken from additional sources such as broadcast conversations, webblog, usenet, and conversational telephone speech. Coreference annotations are also publicly available in treebanks. These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Mart´ı, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al., 2002), which contains 2000 MEDLINE abstracts. Other publicly available coreference corpora of"
P10-1142,N04-4009,0,0.0719761,"ent NPs have to agree in number and gender and cannot span one another (e.g., “Google” and “Google employees”). There are also features that encode general linguistic preferences either for or against coreference. For example, an indefinite NP (that is not in apposition to an anaphoric NP) is not likely to be coreferent with any NP that precedes it. There has been an increasing amount of work on investigating semantic features for coreference resolution. One of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given wor"
P10-1142,D09-1128,0,0.00826235,"antic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use. Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragraph of the Wiki page that has NPk/j as the title or in the list of categor"
P10-1142,W07-1522,0,0.112104,"s broadcast conversations, webblog, usenet, and conversational telephone speech. Coreference annotations are also publicly available in treebanks. These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Mart´ı, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al., 2002), which contains 2000 MEDLINE abstracts. Other publicly available coreference corpora of interest include two annotated by Ruslan Mitkov’s research group: (1) a 55,000-word corpus in the domain of security/terrorism (Hasler et al., 2006); and (2) training data released as part of the 2007 Anaphora Resolution Exercise (Or˘asan et al., 2008), a coreference resolution shared task. There are also two"
P10-1142,W97-0319,0,0.158521,"is used by McCallum and Wellner (2004), Zelenko et al. (2004), and Finley and Joachims (2005). Graph partitioning algorithms are applied on a weighted, undirected graph where a vertex corresponds to an NP and an edge is weighted by the pairwise coreference scores between two NPs (e.g., McCallum and Wellner (2004), Nicolae and Nico4 If a probabilistic model is used, we can define a threshold above which a pair of NPs is considered coreferent. 1398 lae (2006)). The Dempster-Shafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition. Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters. In these algorithms, not only are the NPs in a text processed in a left-to-right manner, the later coreference decisions are dependent on the earlier ones (Cardie and Wagstaff, 1999; Klenner and Ailloud, 2008).5 For example, to resolve an NP, NPk , Cardie and Wagstaff’s algorithm considers each preceding NP, NPj , as a candidate antecedent in a right-to-left order. If NPk and NPj are likely to be coreferent, the algorithm"
P10-1142,C96-1021,0,0.055552,"ning algorithm. 3.1.4 Determining NP Anaphoricity While coreference clustering algorithms attempt to resolve each NP encountered in a document, only a subset of the NPs are anaphoric and therefore need to be resolved. Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver. Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques. For example, pleonastic it has been identified using heuristic approaches (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)), supervised approaches (e.g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e"
P10-1142,D09-1103,0,0.0248684,"Missing"
P10-1142,J94-4002,0,0.168269,"m-cutbased graph partitioning algorithm. 3.1.4 Determining NP Anaphoricity While coreference clustering algorithms attempt to resolve each NP encountered in a document, only a subset of the NPs are anaphoric and therefore need to be resolved. Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver. Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques. For example, pleonastic it has been identified using heuristic approaches (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)), supervised approaches (e.g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve"
P10-1142,H05-1083,0,0.076145,", including minimum edit distance (Strube et al., 2002) and longest common subsequence (Casta˜no et al., 2002). Yang et al. (2004a) treat the two NPs involved as two bags of words, and compute their similarity using metrics commonly-used in information retrieval, such as the dot product, with each word weighted by their TF-IDF value. Syntactic features are computed based on a syntactic parse tree. Ge et al. (1998) implement 1401 a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs’s (1978) seminal syntax-based pronoun resolution algorithm. Luo and Zitouni (2005) extract features from a parse tree for implementing Binding Constraints (Chomsky, 1988). Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent. Rather than deriving features from parse trees, Iida et al. (2006) and Yang et al. (2006) employ these trees directly as"
P10-1142,P04-1018,0,0.617545,"th all of the NPs in Cj and NO otherwise. Other commonly-used logical predicates for creating cluster-level features include relaxed versions of the ALL predicate, such as MOST, which is true if NPk agrees in number with more than half of the NPs in Cj , and ANY, which is true as long as NPk agrees in number with just one of the NPs in Cj . The ability of the entity-mention model to employ cluster-level features makes it more expressive than its mention-pair counterpart. Despite its improved expressiveness, the entitymention model has not yielded particularly encouraging results. For example, Luo et al. (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. Yang et al. (2004b; 2008a) also investigate the entity-mention model, which produces results that are only marginally better than those of the mention-pair model. However, it appears that they are not fully exploiting the expressiveness of the entity-mention model, as cluster-level features only comprise a small fraction of their features. Variants of the entity-mention model have been investigated. For example, Culotta et al. (2007) present a fi"
P10-1142,H05-1004,0,0.136414,"ence resolvers employing different extraction methods should not be compared against each other. 5.2 Scoring a Coreference Partition The MUC scorer (Vilain et al., 1995) is the first program developed for scoring coreference partitions. It has two often-cited weaknesses. As a linkbased measure, it does not reward correctly identified singleton clusters since there is no coreference link in these clusters. Also, it tends to underpenalize partitions with overly large clusters. To address these problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition. To apply these scorers to automatically extracted NPs, different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al. (2009)). Since coreference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scor1403 ing (see Popescu-Belis et al. (2004)). In practice, these general-purpose methods are typically used to provide scores tha"
P10-1142,N07-1010,0,0.0231593,"s. In particular, preprocessing tools can have a large impact on the performance of a resolver (Barbu and Mitkov, 2001). Worse still, assumptions about whether gold or automatically extracted NPs are used are sometimes not explicitly stated, potentially causing results to be interpreted incorrectly. To our knowledge, however, the best results on the MUC-6 and MUC-7 data sets using automatically extracted NPs are reported by Yang et al. (2003) (71.3 MUC F-score) and Ng and Cardie (2002c) (63.4 MUC F-score), respectively;8 and the best results on the ACE data sets using gold NPs can be found in Luo (2007) (88.4 ACE-value). Second, what lessons can we learn from fifteen years of learning-based coreference research? The mention-pair model is weak because it makes coreference decisions based on local information (i.e., information extracted from two NPs). Expressive models (e.g., those that can exploit cluster-level features) generally offer better performance, and so are models that are “global” in nature. Global coreference models may refer to any kind of models that can exploit non-local information, including models that can consider multiple candidate antecedents simultaneously (e.g., rankin"
P10-1142,J93-2004,0,0.0364504,"n extensively used for training and evaluating coreference models. Equally popular are the corpora produced by the Automatic Content Extraction (ACE1 ) evaluations in the past decade: while the earlier ACE corpora (e.g., ACE-2) consist of solely English newswire and broadcast news articles, the later ones (e.g., ACE 2005) have also included Chinese and Arabic documents taken from additional sources such as broadcast conversations, webblog, usenet, and conversational telephone speech. Coreference annotations are also publicly available in treebanks. These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Mart´ı, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al.,"
P10-1142,J05-3004,0,0.0601273,"tterns have been used to capture the semantic relatedness between two NPs and hence the likelihood that they are coreferent. For instance, given the pattern X is a Y (which is highly indicative that X and Y are coreferent), we can instantiate it with a pair of NPs and search for the instantiated pattern in a large corpus or the Web (Daum´e III and Marcu, 2005; Haghighi and Klein, 2009). The more frequently the pattern occurs, the more likely they are coreferent. This technique has been applied to resolve different kinds of anaphoric references, including other-anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) and bridging references (Poesio et al., 2004a). While these patterns are typically hand-crafted (e.g., Garera and Yarowsky (2006)), they can also be learned from an annotated corpus (Yang and Su, 2007) or bootstrapped from an unannotated corpus (Bean and Riloff, 2004). Despite the large amount of work on discoursebased anaphora resolution in the 1970s and 1980s (see Hirst (1981)), learning-based resolvers have only exploited shallow discourse-based features, which primarily involve characterizing the salience of a candidate antecedent by measuring its distance from the anaphoric NP to be reso"
P10-1142,W03-1023,0,0.0152473,"on. Lexico-syntactic patterns have been used to capture the semantic relatedness between two NPs and hence the likelihood that they are coreferent. For instance, given the pattern X is a Y (which is highly indicative that X and Y are coreferent), we can instantiate it with a pair of NPs and search for the instantiated pattern in a large corpus or the Web (Daum´e III and Marcu, 2005; Haghighi and Klein, 2009). The more frequently the pattern occurs, the more likely they are coreferent. This technique has been applied to resolve different kinds of anaphoric references, including other-anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) and bridging references (Poesio et al., 2004a). While these patterns are typically hand-crafted (e.g., Garera and Yarowsky (2006)), they can also be learned from an annotated corpus (Yang and Su, 2007) or bootstrapped from an unannotated corpus (Bean and Riloff, 2004). Despite the large amount of work on discoursebased anaphora resolution in the 1970s and 1980s (see Hirst (1981)), learning-based resolvers have only exploited shallow discourse-based features, which primarily involve characterizing the salience of a candidate antecedent by measuring its distance from"
P10-1142,P02-1045,0,0.0900933,"Missing"
P10-1142,E06-1007,0,0.056921,"Missing"
P10-1142,W02-1008,1,0.428522,"rence partition. 3.1.1 Creating Training Instances As noted above, the primary purpose of training instance creation is to reduce class skewness. Many heuristic instance creation methods have been proposed, among which Soon et al.’s (1999; 2001) is arguably the most popular choice. Given 1397 an anaphoric noun phrase3 , NPk , Soon et al.’s method creates a positive instance between NPk and its closest preceding antecedent, NPj , and a negative instance by pairing NPk with each of the intervening NPs, NPj+1 , . . ., NPk−1 . With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead. This modification is motivated by the observation that it is not easy for a human, let alone a machine learner, to learn from a positive instance where the antecedent of a non-pronominal NP is a pronoun. To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of traini"
P10-1142,C02-1139,1,0.474223,"rence partition. 3.1.1 Creating Training Instances As noted above, the primary purpose of training instance creation is to reduce class skewness. Many heuristic instance creation methods have been proposed, among which Soon et al.’s (1999; 2001) is arguably the most popular choice. Given 1397 an anaphoric noun phrase3 , NPk , Soon et al.’s method creates a positive instance between NPk and its closest preceding antecedent, NPj , and a negative instance by pairing NPk with each of the intervening NPs, NPj+1 , . . ., NPk−1 . With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead. This modification is motivated by the observation that it is not easy for a human, let alone a machine learner, to learn from a positive instance where the antecedent of a non-pronominal NP is a pronoun. To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of traini"
P10-1142,P02-1014,1,0.447747,"rence partition. 3.1.1 Creating Training Instances As noted above, the primary purpose of training instance creation is to reduce class skewness. Many heuristic instance creation methods have been proposed, among which Soon et al.’s (1999; 2001) is arguably the most popular choice. Given 1397 an anaphoric noun phrase3 , NPk , Soon et al.’s method creates a positive instance between NPk and its closest preceding antecedent, NPj , and a negative instance by pairing NPk with each of the intervening NPs, NPj+1 , . . ., NPk−1 . With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead. This modification is motivated by the observation that it is not easy for a human, let alone a machine learner, to learn from a positive instance where the antecedent of a non-pronominal NP is a pronoun. To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of traini"
P10-1142,P04-1020,1,0.693911,"at do not fall into any of the preceding categories. For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008). Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al., 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). An anaphoricity feature indicates whether an NP to be resolved is anaphoric, and is typically computed using an anaphoricity classifier (Ng, 2004), hand-crafted patterns (Daum´e III and Marcu, 2005), and automatically acquired patterns (Bean and Riloff, 1999). Finally, the outputs of rule-based pronoun and coreference resolvers have also been used as features for learning-based coreference resolution (Ng and Cardie, 2002c). For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008). 5 Evaluation Issues Two important issues surround the evaluation of a coreference resolver. First, how do we obtain the set of NPs that a resolver will partition? Second, how do we sc"
P10-1142,P07-1068,1,0.784286,"inds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use. Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragraph of the Wiki page that has NPk/j as the title or"
P10-1142,D08-1067,1,0.585046,"nst multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer. For further discussion on evaluation issues, see Byron (2001). 6 Concluding Remarks While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training (M¨uller et al., 2002), self-training (Kehler et al., 2004a), and EM (Cherry and Bergsma, 2005; Ng, 2008). Interestingly, recent results indicate that unsupervised approaches to coreference resolution (e.g., Haghighi and Klein (2007; 2010), Poon and Domingos (2008)) rival their supervised counterparts, casting doubts on whether supervised resolvers are making effective use of the available labeled data. Another issue that we have not focused on but which is becoming increasingly important is multilinguality. While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Conv"
P10-1142,N09-1065,1,0.853237,"ssifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions. Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). 3.1.5 Combining Classification & Clustering From a learning perspective, a two-step approach to coreference — classification and clustering — is undesirable. Since the classification model is trained independently of the clustering algorithm, improvements in classification accuracy do not guarantee corresponding improvements in clustering-level accuracy. That is, overall performance on the coreference task might not improve. To address this problem, McCallum and Wellner (2004) and Finley and Joachims (2005) eliminate the classification step entirely, treating coref1399 erence as a supervised"
P10-1142,W09-3939,0,0.0589234,"Missing"
P10-1142,W06-1633,0,0.49835,"The search starts at the root, and a partitioning of the NPs is incrementally constructed as we move down the tree. Specifically, based on the coreference decisions it has made in the first i−1 levels of the tree, the algorithm determines at the ith level whether the ith NP should start a new cluster, or to which preceding cluster it should be assigned. While many coreference clustering algorithms have been developed, there have only been a few attempts to compare their effectiveness. For example, Ng and Cardie (2002c) report that bestfirst clustering is better than closest-first clustering. Nicolae and Nicolae (2006) show that bestfirst clustering performs similarly to Bell-treebased clustering, but neither of these algorithms 5 When applying closest-first and best-first clustering, Soon et al. (2001) and Ng and Cardie (2002c) also process the NPs in a sequential manner, but since the later decisions are not dependent on the earlier ones, the order in which the NPs are processed does not affect their clustering results. performs as well as their proposed minimum-cutbased graph partitioning algorithm. 3.1.4 Determining NP Anaphoricity While coreference clustering algorithms attempt to resolve each NP encou"
P10-1142,orasan-etal-2008-anaphora,0,0.0369857,"Missing"
P10-1142,poesio-kabadjov-2004-general,0,0.00648892,"ss” on improving machine learning approaches to the problem in the past fifteen years. To ensure further progress, researchers should compare their results against a baseline that is stronger than the commonly-used Soon et al. (2001) system, which relies on a weak model (i.e., the mention-pair model) and a small set of linguistic features. As recent systems are becoming more sophisticated, we suggest that researchers make their systems publicly available in order to facilitate performance comparisons. Publicly available coreference systems currently include JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), BART (Versley et al., 2008b), CoRTex (Denis and Baldridge, 2008), the Illinois Coreference Package (Bengtson and Roth, 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), and Charniak and Elsner’s (2009) pronoun resolver. We conclude with a discussion of two questions regarding supervised coreference research. First, what is the state of the art? This is not an easy question, as researchers have been evaluating their resolvers on different corpora using different evaluation metrics and preprocessing tools. In particular, preprocessing tools can have a large impact o"
P10-1142,P09-5006,0,0.0608001,"Missing"
P10-1142,N06-1025,0,0.501049,"Missing"
P10-1142,E06-2015,0,0.0310538,"b, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use. Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragraph of the Wiki page that has NPk/j as the title or in the list of categories to which this page belongs, and the degree of overlap between the two pages that have the two NPs as their titles (see Poesio et al. (2007) for other uses of encyclopedic knowledge for coreference resolution). Contextual roles ("
P10-1142,D08-1068,0,0.0958894,"es, see Byron (2001). 6 Concluding Remarks While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training (M¨uller et al., 2002), self-training (Kehler et al., 2004a), and EM (Cherry and Bergsma, 2005; Ng, 2008). Interestingly, recent results indicate that unsupervised approaches to coreference resolution (e.g., Haghighi and Klein (2007; 2010), Poon and Domingos (2008)) rival their supervised counterparts, casting doubts on whether supervised resolvers are making effective use of the available labeled data. Another issue that we have not focused on but which is becoming increasingly important is multilinguality. While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Converse (2006)), Japanese (e.g., Iida (2007)), Arabic (e.g., Luo and Zitouni (2005)), Dutch (e.g., Hoste (2005)), German (e.g., Wunsch (2010)), Swedish (e.g., Nils"
P10-1142,popescu-belis-etal-2004-online,0,0.0109152,"problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition. To apply these scorers to automatically extracted NPs, different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al. (2009)). Since coreference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scor1403 ing (see Popescu-Belis et al. (2004)). In practice, these general-purpose methods are typically used to provide scores that complement those obtained via the three coreference scorers discussed above. It is worth mentioning that there is a trend towards evaluating a resolver against multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer. For further discussion on evaluation issues, see Byron (2001). 6 Concluding Remarks While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combinin"
P10-1142,qiu-etal-2004-public,0,0.0172318,"ng “slow, but steady progress” on improving machine learning approaches to the problem in the past fifteen years. To ensure further progress, researchers should compare their results against a baseline that is stronger than the commonly-used Soon et al. (2001) system, which relies on a weak model (i.e., the mention-pair model) and a small set of linguistic features. As recent systems are becoming more sophisticated, we suggest that researchers make their systems publicly available in order to facilitate performance comparisons. Publicly available coreference systems currently include JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), BART (Versley et al., 2008b), CoRTex (Denis and Baldridge, 2008), the Illinois Coreference Package (Bengtson and Roth, 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), and Charniak and Elsner’s (2009) pronoun resolver. We conclude with a discussion of two questions regarding supervised coreference research. First, what is the state of the art? This is not an easy question, as researchers have been evaluating their resolvers on different corpora using different evaluation metrics and preprocessing tools. In particular, preproces"
P10-1142,P04-1019,0,0.0629412,"M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions. Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). 3.1.5 Combining Classification & Clustering From a learning perspective, a two-step approach to coreference — classification and clustering — is undesirable. Since the classi"
P10-1142,P03-2012,0,0.0243137,"g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions. Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). 3.1.5 Combining Classification & Clustering From a learning perspective, a two-step approach to coreference — classification and clustering — is undesira"
P10-1142,W09-2411,0,0.110071,"Missing"
P10-1142,W99-0634,0,0.0942989,"Missing"
P10-1142,J01-4004,0,0.98524,"in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3). 3.1.3 Generating an NP Partition After training, we can apply the resulting model to a test text, using a clustering algorithm to coordinate the pairwise classification decisions and impose an NP partition. Below we describe some commonly used coreference clustering algorithms. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002c) are arguably the most widely used coreference clustering algorithms. The closest-first clustering algorithm selects as the antecedent for an NP, NPk , the closest preceding noun phrase that is classified as coreferent with it.4 However, if no such preceding noun phrase exists, no antecedent is selected for NPk . The best-first clustering algorithm aims to improve the precision of closest-first clustering, specifically by selecting as the antecedent of NPk the most probable preceding NP that is classified as coreferent with it. One criticism of"
P10-1142,P09-1074,0,0.124563,"the NPs that belong to one of the ACE entity types (e.g., PER SON , ORGANIZATION , LOCATION ) are annotated. Owing in large part to the difference in the number of NPs extracted by these three methods, a coreference resolver can produce substantially different results when applied to the resulting three sets of NPs, with gold NPs yielding the best results and NPs extracted from a parser yielding the worst (Nicolae and Nicolae, 2006). While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their coreference algorithm, Stoyanov et al. (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. Whichever NP extraction method is employed, it is clear that the use of gold NPs can considerably simplify the coreference task, and hence resolvers employing different extraction methods should not be compared against each other. 5.2 Scoring a Coreference Partition The MUC scorer (Vilain et al., 1995) is the first program developed for scoring coreference partitions. It has two often-cited weaknesses. As a linkbased measure, it does not reward correctly identified sing"
P10-1142,P10-2029,0,0.032845,"Missing"
P10-1142,W02-1040,0,0.0173793,"Missing"
P10-1142,telljohann-etal-2004-tuba,0,0.047566,"Missing"
P10-1142,J00-4005,0,0.225245,"Missing"
P10-1142,C08-1121,0,0.0642144,"attempt to resolve each NP encountered in a document, only a subset of the NPs are anaphoric and therefore need to be resolved. Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver. Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques. For example, pleonastic it has been identified using heuristic approaches (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)), supervised approaches (e.g., Evans (2001), M¨uller (2006), Versley et al. (2008a)), and distributional methods (e.g., Bergsma et al. (2008)); and non-anaphoric definite descriptions have been identified using rule-based techniques (e.g., Vieira and Poesio (2000)) and unsupervised techniques (e.g., Bean and Riloff (1999)). Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to e"
P10-1142,D07-1052,0,0.035791,"atures for coreference resolution. One of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use. Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragrap"
P10-1142,M95-1005,0,0.894709,"2006). While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their coreference algorithm, Stoyanov et al. (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. Whichever NP extraction method is employed, it is clear that the use of gold NPs can considerably simplify the coreference task, and hence resolvers employing different extraction methods should not be compared against each other. 5.2 Scoring a Coreference Partition The MUC scorer (Vilain et al., 1995) is the first program developed for scoring coreference partitions. It has two often-cited weaknesses. As a linkbased measure, it does not reward correctly identified singleton clusters since there is no coreference link in these clusters. Also, it tends to underpenalize partitions with overly large clusters. To address these problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition. To apply these scorers"
P10-1142,P07-1067,0,0.0136249,"e coreferent), we can instantiate it with a pair of NPs and search for the instantiated pattern in a large corpus or the Web (Daum´e III and Marcu, 2005; Haghighi and Klein, 2009). The more frequently the pattern occurs, the more likely they are coreferent. This technique has been applied to resolve different kinds of anaphoric references, including other-anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) and bridging references (Poesio et al., 2004a). While these patterns are typically hand-crafted (e.g., Garera and Yarowsky (2006)), they can also be learned from an annotated corpus (Yang and Su, 2007) or bootstrapped from an unannotated corpus (Bean and Riloff, 2004). Despite the large amount of work on discoursebased anaphora resolution in the 1970s and 1980s (see Hirst (1981)), learning-based resolvers have only exploited shallow discourse-based features, which primarily involve characterizing the salience of a candidate antecedent by measuring its distance from the anaphoric NP to be resolved or determining whether it is in a prominent grammatical role (e.g., subject). A notable exception 1402 is Iida et al. (2009), who train a ranker to rank the candidate antecedents for an anaphoric p"
P10-1142,P03-1023,0,0.320891,"ld be formed between NPk and its closest preceding nonpronominal antecedent instead. This modification is motivated by the observation that it is not easy for a human, let alone a machine learner, to learn from a positive instance where the antecedent of a non-pronominal NP is a pronoun. To further reduce class skewness, some researchers employ a filtering mechanism on top of an instance creation method, thereby disallowing the creation of training instances from NP pairs that are unlikely to be coreferent, such as NP pairs that violate gender and number agreement (e.g., Strube et al. (2002), Yang et al. (2003)). While many instance creation methods are heuristic in nature (see Uryupina (2004) and Hoste and Daelemans (2005)), some are learning-based. For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al. (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model. 3.1.2 Training a Coreference Classifier Once a training set is created, we can train a coreference model using"
P10-1142,C04-1033,0,0.600149,"h as MOST, which is true if NPk agrees in number with more than half of the NPs in Cj , and ANY, which is true as long as NPk agrees in number with just one of the NPs in Cj . The ability of the entity-mention model to employ cluster-level features makes it more expressive than its mention-pair counterpart. Despite its improved expressiveness, the entitymention model has not yielded particularly encouraging results. For example, Luo et al. (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. Yang et al. (2004b; 2008a) also investigate the entity-mention model, which produces results that are only marginally better than those of the mention-pair model. However, it appears that they are not fully exploiting the expressiveness of the entity-mention model, as cluster-level features only comprise a small fraction of their features. Variants of the entity-mention model have been investigated. For example, Culotta et al. (2007) present a first-order logic model that determines 1400 the probability that an arbitrary set of NPs are all co-referring. Their model resembles the entitymention model in that it"
P10-1142,P05-1021,0,0.0142325,"in number and gender and cannot span one another (e.g., “Google” and “Google employees”). There are also features that encode general linguistic preferences either for or against coreference. For example, an indefinite NP (that is not in apposition to an anaphoric NP) is not likely to be coreferent with any NP that precedes it. There has been an increasing amount of work on investigating semantic features for coreference resolution. One of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al., 2004b; Yang et al., 2005; Haghighi and Klein, 2009): given a pronoun to be resolved, its governing verb, and its grammatical role, we prefer a candidate antecedent that can be governed by the same verb and be in the same role. Semantic knowledge has also been extracted from WordNet and unannotated corpora for computing the semantic compatibility/similarity between two common nouns (Harabagiu et al., 2001; Versley, 2007) as well as the semantic class of a noun (Ng, 2007a; Huang et al., 2009). One difficulty with deriving knowledge from WordNet is that one has to determine which sense of a given word to use. Some resea"
P10-1142,P06-1006,0,0.0412908,"onoun resolution algorithm. Luo and Zitouni (2005) extract features from a parse tree for implementing Binding Constraints (Chomsky, 1988). Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent. Rather than deriving features from parse trees, Iida et al. (2006) and Yang et al. (2006) employ these trees directly as structured features for pronoun resolution. Specifically, Yang et al. define tree kernels for efficiently computing the similarity between two parse trees, and Iida et al. use a boosting-based algorithm to compute the usefulness of a subtree. Grammatical features encode the grammatical properties of one or both NPs involved in an instance. For example, Ng and Cardie’s (2002c) resolver employs 34 grammatical features. Some features determine NP type (e.g., are both NPs definite or pronouns?). Some determine the grammatical role of one or both of the NPs. Some enc"
P10-1142,P08-1096,0,0.128364,"Missing"
P10-1142,J08-3002,0,0.0203007,"Missing"
P10-1142,W04-0704,0,0.0216261,"positive pairwise decisions are unjustifiably favored over their negative counterparts. For example, three NPs are likely to end up in the same cluster in the resulting partition even if there is strong evidence that A and C are not coreferent, as long as the other two pairs (i.e., (A,B) and (B,C)) are classified as positive. Several algorithms that address one or both of these problems have been used for coreference clustering. Correlation clustering (Bansal et al., 2002), which produces a partition that respects as many pairwise decisions as possible, is used by McCallum and Wellner (2004), Zelenko et al. (2004), and Finley and Joachims (2005). Graph partitioning algorithms are applied on a weighted, undirected graph where a vertex corresponds to an NP and an edge is weighted by the pairwise coreference scores between two NPs (e.g., McCallum and Wellner (2004), Nicolae and Nico4 If a probabilistic model is used, we can define a threshold above which a pair of NPs is considered coreferent. 1398 lae (2006)). The Dempster-Shafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most"
P10-1142,D07-1057,0,0.0122544,"lity. While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Converse (2006)), Japanese (e.g., Iida (2007)), Arabic (e.g., Luo and Zitouni (2005)), Dutch (e.g., Hoste (2005)), German (e.g., Wunsch (2010)), Swedish (e.g., Nilsson (2010)), and Czech (e.g., Ngu.y et al. (2009)). In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)). As Mitkov (2001) puts it, coreference resolution is a “difficult, but not intractable problem,” and we have been making “slow, but steady progress” on improving machine learning approaches to the problem in the past fifteen years. To ensure further progress, researchers should compare their results against a baseline that is stronger than the commonly-used Soon et al. (2001) system, which relies on a weak model (i.e., the mention-pair model) and a small set of linguistic features. As recent systems are becoming more sophisticated, we suggest that researchers make their systems publicly avai"
P10-1142,D09-1102,0,0.0172346,"howing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e.g., Ng and Cardie (2002b), Uryupina (2003), Poesio et al. (2004b)). Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions. Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). 3.1.5 Combining Classification & Clustering From a learning perspective, a two-step approach to coreference — classification and clustering — is undesirable. Since the classification model is trained independently of the clustering algorithm, improvements in classification accuracy do not guarantee corresponding improvements in clustering-level accuracy. That is, overall performance on the coreference task might not improve. To address this problem, McCallum and Wellner (2004) and Finley and Joachims (2005) eliminate the classification step entirely, treating cor"
P10-1142,J81-2001,0,\N,Missing
P10-1142,N07-1011,0,\N,Missing
P10-1142,N10-1061,0,\N,Missing
P10-1142,W98-1119,0,\N,Missing
P10-1142,P00-1053,0,\N,Missing
P10-1142,H05-1003,0,\N,Missing
P10-1142,H05-1013,0,\N,Missing
P10-1142,D08-1031,0,\N,Missing
P10-1142,D08-1069,0,\N,Missing
P10-1142,C90-3063,0,\N,Missing
P10-1142,J96-1002,0,\N,Missing
P10-1142,P06-1005,0,\N,Missing
P10-1142,P99-1048,0,\N,Missing
P10-1142,N04-1038,0,\N,Missing
P10-1142,P08-2012,0,\N,Missing
P10-1142,P09-1073,0,\N,Missing
P10-1142,P06-1079,0,\N,Missing
P10-1142,J95-2003,0,\N,Missing
P10-1142,W05-0612,0,\N,Missing
P10-1142,N07-1030,0,\N,Missing
P10-1142,D09-1120,0,\N,Missing
P10-1142,E09-1018,0,\N,Missing
P10-1142,P83-1007,0,\N,Missing
P10-1142,P07-1107,0,\N,Missing
P10-1142,W06-2906,0,\N,Missing
P10-1142,P01-1006,0,\N,Missing
P10-1142,J86-3001,0,\N,Missing
P10-1142,N04-1037,0,\N,Missing
P10-1142,hasler-etal-2006-nps,0,\N,Missing
P10-1142,W04-0707,0,\N,Missing
P11-1082,P98-1013,0,0.207442,"ges 814–824, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics edge sources have been shown to be useful when applied in isolation to a coreference system, do they offer complementary benefits and therefore can further improve a resolver when applied in combination? We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learning-based coreference resolution. Specifically, we (1) derive world knowledge from encyclopedic sources that are underinvestigated for coreference resolution, including FrameNet (Baker et al., 1998) and YAGO (Suchanek et al., 2007), in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed (Rahman and Ng, 2009); and (3) evaluate their utility using two coreference models, the traditional mention-pair model (Soon et al., 2001) and the recently developed cluster-ranking model (Rahman and Ng, 2009). Our evaluation corpus contains 410 documents, which are coreference-annotated using the ACE annotation scheme as well as the OntoNotes annotation scheme (Hovy et al., 2006). By evaluat"
P11-1082,N04-1038,0,0.352897,"Missing"
P11-1082,D08-1031,0,0.379098,"esolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daum´e III and Marcu (2005), Ng (2007)), and coreferenceannotated data (e.g., Bengtson and Roth (2008)). While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements were typically obtained by incorporating world knowledge (as features) into a baseline resolver composed of a rather weak coreference model (i.e., the mention-pair model) and a small set of features (i.e., the 12 features adopted by Soon et al.’s (2001) knowledge-lean approach). As a result, some questions naturally arise. First, can world knowledge still offer benefits when used in combination with a richer set of features? Second, since automatically extracted world know"
P11-1082,N07-1011,0,0.35444,"ng preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding cluster) can be employed. Details of the CR model can be found in Rahman and Ng (2009). Since the CR model ranks preceding clusters, a training instance i(cj , NPk ) represents a preceding cluster, cj , and an anaphoric NP, NPk . Each instance consists of features that are computed based solely on NPk as well as cluster-level features, which describe the relationship between cj and NPk . Motivated in part by Culotta et al. (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOSTFALSE, MOST- TRUE, and ALL. Specifically, for each relational feature X, we first convert X into an equivalent set of binary-valued features if it is multivalued. Then, for each resulting binary-valued feature Xb , we create four binary-valued cluster-level features: (1) NONE - Xb is true when Xb is false between NPk and each NP in cj ; (2) MOST- FALSE - Xb is true when Xb is true between NPk and less than half (but at least one) of the NPs in cj ; (3) MOST- TRUE X b is true when"
P11-1082,H05-1013,0,0.0983607,"Missing"
P11-1082,D08-1069,0,0.247155,"s to answer the critical question of which candidate antecedent is most probable. Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision. 2.3.2 Cluster-Ranking Model The cluster-ranking (CR) model addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et 1 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. 816 al. (2004), Yang et al. (2008)) and the mentionranking model (e.g., Denis and Baldridge (2008)). Specifically, the CR model ranks the preceding clusters for an active NP so that the highest-ranked cluster is the one to which the active NP should be linked. Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding cluster) can be employed. Details of the CR model can be found in Rahman and Ng (2009). Since the CR model ranks preceding clu"
P11-1082,P05-1045,0,0.0162305,"r an instance will have the value 1 if and only if the two NPs involved are in a T YPE or M EANS relation. On the other hand, if the CR model is used, the YAGO feature for an instance involving NPk and preceding cluster c will have the value 1 if and only if NPk has a T YPE or M EANS relation with any of the NPs in c. Since knowledge extraction from webbased encyclopedia is typically noisy (Ponzetto and Poesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun. 3.1.2 Extracting Knowledge from FrameNet mend, decry, denounce, praise, and slam. To better understand why FrameNet contains potentially useful knowledge for coreference resolution, consider the following text segment: Peter Anthony decries program trading as “limiting the game to a few,” but he is not sure whether he wants to denounce it because ... To establish the coreference relation between it and program trading, it may be helpful to know that decry and denounce appear in the same frame and the two NPs have the same semantic role. This example suggests"
P11-1082,P03-1001,0,0.00921271,"m Unannotated Data Previous work has shown that syntactic appositions, which can be extracted using heuristics from unannotated documents or parse trees, are a useful source of world knowledge for coreference resolution (e.g., Daum´e III and Marcu (2005), Ng (2007), Haghighi and Klein (2009)). Each extraction is an NP pair such as &lt;Barack Obama, the president> and &lt;Eastern Airlines, the carrier>, where the first NP in the pair is a proper name and the second NP is 820 a common NP. Low-frequency extractions are typically assumed to be noisy and discarded. We combine the extractions produced by Fleischman et al. (2003) and Ng (2007) to form a database consisting of 1.057 million NP pairs, and create a binary-valued feature for our coreference models using this database. If the MP model is used, this feature will have the value 1 if and only if the two NPs appear as a pair in the database. On the other hand, if the CR model is used, the feature for an instance involving NPk and preceding cluster c will have the value 1 if and only if NPk and at least one of the NPs in c appears as a pair in the database. 4 Evaluation 4.1 Experimental Setup As described in Section 2, we use as our evaluation corpus the 411 do"
P11-1082,D09-1120,0,0.70869,"e of a feature is 1 if and only if its value between NPk and one of the NPs in c is 1 under its original definition. The above discussion assumes that the two NPs under consideration serve as predicate arguments. If this assumption fails, we will not create any features based on verb pairs for these two NPs. 3.3 World Knowledge from Unannotated Data Previous work has shown that syntactic appositions, which can be extracted using heuristics from unannotated documents or parse trees, are a useful source of world knowledge for coreference resolution (e.g., Daum´e III and Marcu (2005), Ng (2007), Haghighi and Klein (2009)). Each extraction is an NP pair such as &lt;Barack Obama, the president> and &lt;Eastern Airlines, the carrier>, where the first NP in the pair is a proper name and the second NP is 820 a common NP. Low-frequency extractions are typically assumed to be noisy and discarded. We combine the extractions produced by Fleischman et al. (2003) and Ng (2007) to form a database consisting of 1.057 million NP pairs, and create a binary-valued feature for our coreference models using this database. If the MP model is used, this feature will have the value 1 if and only if the two NPs appear as a pair in the da"
P11-1082,N06-2015,0,0.00997372,"FrameNet (Baker et al., 1998) and YAGO (Suchanek et al., 2007), in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed (Rahman and Ng, 2009); and (3) evaluate their utility using two coreference models, the traditional mention-pair model (Soon et al., 2001) and the recently developed cluster-ranking model (Rahman and Ng, 2009). Our evaluation corpus contains 410 documents, which are coreference-annotated using the ACE annotation scheme as well as the OntoNotes annotation scheme (Hovy et al., 2006). By evaluating on two sets of coreference annotations for the same set of documents, we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to annotate the documents. 2 Preliminaries In this section, we describe the corpus, the NP extraction methods, the coreference models, and the evaluation measures we will use in our evaluation. 2.1 Data Set We evaluate on documents that are coreferenceannotated using both the ACE annotation scheme and the OntoNotes annotation scheme, so that we can examine whether"
P11-1082,P04-1018,0,0.114576,"Missing"
P11-1082,H05-1004,0,0.183988,"response that are singletons. The rationale is simple: since the resolver has successfully identified these NPs as singletons, it should not be penalized, and removing them avoids such penalty. Since B3 and CEAF align NPs/clusters, the lack of singleton clusters in the OntoNotes annotations implies that the resulting scores reflect solely how well a resolver identifies coreference links and do not take into account how well it identifies singleton clusters. 2.4 3 Extracting World Knowledge Evaluation Measures We employ two commonly-used scoring programs, B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005), both of which report results in terms of recall (R), precision (P), and F-measure (F) by comparing the gold-standard (i.e., key) partition, KP , against the system-generated (i.e., response) partition, RP . Briefly, B3 computes the R and P values of each NP and averages these values at the end. Specifically, for each NP, NPj , B3 first computes the number of common NPs in KPj and RPj , the clusters containing NPj in KP and RP , respectively, and then divides this number by |KPj |and |RPj |to obtain the R and P values of NPj , respectively. On the other hand, CEAF finds the best one-to-one al"
P11-1082,P02-1014,1,0.798969,"prefers resolving an NP to a candidate antecedent that has the same grammatical role) in this example would also allow us to correctly resolve the celebrity (Mitkov, 814 2002), thereby obviating the need for world knowledge. However, since these heuristics are not perfect, complementing them with world knowledge would be an important step towards bringing coreference systems to the next level of performance. Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daum´e III and Marcu (2005), Ng (2007)), and coreferenceannotated data (e.g., Bengtson and Roth (2008)). While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements w"
P11-1082,P09-5006,0,0.0203945,"Missing"
P11-1082,N06-1025,0,0.936325,"uld be an important step towards bringing coreference systems to the next level of performance. Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daum´e III and Marcu (2005), Ng (2007)), and coreferenceannotated data (e.g., Bengtson and Roth (2008)). While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements were typically obtained by incorporating world knowledge (as features) into a baseline resolver composed of a rather weak coreference model (i.e., the mention-pair model) and a small set of features (i.e., the 12 features adopted by Soon et al.’s (2001) knowledge-lean approach). As a result, some questions naturally arise."
P11-1082,N04-1030,0,0.0472495,"s that features encoding both the semantic roles of the two NPs under consideration and whether the associated predicates are “related” to each other in FrameNet (i.e., whether they appear in the same frame) could be useful for identifying coreference relations. Two points regarding our implementation of these features deserve mention. First, since we do not employ verb sense disambiguation, we consider two predicates related as long as there is at least one semantic frame in which they both appear. Second, since FrameNet-style semantic role labelers are not publicly available, we use ASSERT (Pradhan et al., 2004), a semantic role labeler that provides PropBank-style semantic roles such as A RG 0 (the P ROTOAGENT, which is typically the subject of a transitive verb) and A RG 1 (the P ROTO PATIENT, which is typically its direct object). Now, assuming that NPj and NPk are the arguments of two stemmed predicates, predj and predk , we create 15 features using the knowledge extracted from FrameNet and ASSERT as follows. First, we encode the knowledge extracted from FrameNet as one of three possible values: (1) predj and predk are in the same frame; (2) they are both predicates in FrameNet but never appear i"
P11-1082,D09-1101,1,0.882746,"ts and therefore can further improve a resolver when applied in combination? We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learning-based coreference resolution. Specifically, we (1) derive world knowledge from encyclopedic sources that are underinvestigated for coreference resolution, including FrameNet (Baker et al., 1998) and YAGO (Suchanek et al., 2007), in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed (Rahman and Ng, 2009); and (3) evaluate their utility using two coreference models, the traditional mention-pair model (Soon et al., 2001) and the recently developed cluster-ranking model (Rahman and Ng, 2009). Our evaluation corpus contains 410 documents, which are coreference-annotated using the ACE annotation scheme as well as the OntoNotes annotation scheme (Hovy et al., 2006). By evaluating on two sets of coreference annotations for the same set of documents, we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to a"
P11-1082,J01-4004,0,0.925141,"c parallelism (which prefers resolving an NP to a candidate antecedent that has the same grammatical role) in this example would also allow us to correctly resolve the celebrity (Mitkov, 814 2002), thereby obviating the need for world knowledge. However, since these heuristics are not perfect, complementing them with world knowledge would be an important step towards bringing coreference systems to the next level of performance. Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daum´e III and Marcu (2005), Ng (2007)), and coreferenceannotated data (e.g., Bengtson and Roth (2008)). While each of these three sources of world knowledge has been shown to improve coreference resoluti"
P11-1082,P09-1074,0,0.37581,"divides this number by |KPj |and |RPj |to obtain the R and P values of NPj , respectively. On the other hand, CEAF finds the best one-to-one alignment between the key clusters and the response clusters. A complication arises when B3 is used to score a response partition containing automatically extracted NPs. Recall that B3 constructs a mapping between the NPs in the response and those in the key. Hence, if the response is generated using goldstandard NPs, then every NP in the response is mapped to some NP in the key and vice versa. In other words, there are no twinless (i.e., unmapped) NPs (Stoyanov et al., 2009). This is not the case when automatically extracted NPs are used, but the original description of B3 does not specify how twinless NPs should be scored (Bagga and Baldwin, 1998). To address this problem, we set the recall and precision of a twinless NP to zero, regardless of whether the NP appears in the key or the response. Note that CEAF can compare partitions with twin817 In this section, we describe how we extract world knowledge for coreference resolution from three different sources: large-scale knowledge bases, coreference-annotated data and unannotated data. 3.1 World Knowledge from Kn"
P11-1082,P10-2029,0,0.315706,"Missing"
P11-1082,P03-1023,0,0.0139524,"NP to a candidate antecedent that has the same grammatical role) in this example would also allow us to correctly resolve the celebrity (Mitkov, 814 2002), thereby obviating the need for world knowledge. However, since these heuristics are not perfect, complementing them with world knowledge would be an important step towards bringing coreference systems to the next level of performance. Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daum´e III and Marcu (2005), Ng (2007)), and coreferenceannotated data (e.g., Bengtson and Roth (2008)). While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements were typically obtain"
P11-1082,P08-1096,0,0.015842,"antecedent is relative to other candidates. So, it fails to answer the critical question of which candidate antecedent is most probable. Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision. 2.3.2 Cluster-Ranking Model The cluster-ranking (CR) model addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et 1 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. 816 al. (2004), Yang et al. (2008)) and the mentionranking model (e.g., Denis and Baldridge (2008)). Specifically, the CR model ranks the preceding clusters for an active NP so that the highest-ranked cluster is the one to which the active NP should be linked. Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously. Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding cluster) can be employed. Details of the CR model can be found"
P11-1082,C98-1013,0,\N,Missing
P11-1082,D08-1067,1,\N,Missing
P13-1026,J96-2004,0,0.0896351,"These results are consistent with our intuition that each of the enumerated error classes has a negative impact on thesis clarity score. In particular, each has a demonstrable negative impact, costing essays an average of more than 0.59 points when it occurs. Moreover, this set of errors accounts for a large majority of all errors impacting thesis clarity because unenumerated errors cost essays an average of only one-tenth of one point on the four-point thesis clarity scale. the errors in the same 100 essays that were doublyannotated with thesis clarity scores. We then compute Cohen’s Kappa (Carletta, 1996) on each error from the two sets of annotations, obtaining an average Kappa value of 0.75, which indicates fair agreement. Table 5 shows the number of essays assigned to each of the five thesis clarity errors. As we can see, Confusing Phrasing, Incomplete Prompt Response, and Relevance to Prompt are the major error types. error essays CP 152 IPR 123 R 142 MD 47 WP 39 Table 5: Distribution of thesis clarity errors. 4 Error Classification In this section, we describe in detail our system for identifying thesis clarity errors. Relationship between clarity scores and error classes. To determine th"
P13-1026,N10-1138,0,0.0182876,"on features. We believe this kind of feature may help improve performance on Missing Details because the list of features aggregated to generate the Ap+i feature’s value includes POS n-gram features like CC “ NN ” (scare quotes). This feature type may also help with Confusing Phrasing because the list of POS tag n-grams our annotator generated for its Ap+i contains useful features like DT NNS VBZ VBN (e.g., “these signals has been”), which captures noun-verb disagreement. Semantic roles. Our last aggregated feature is generated using FrameNet-style semantic role labels obtained using SEMAFOR (Das et al., 2010). For each sentence in our data set, SEMAFOR identifies each semantic frame occurring in the sentence as well as each frame element that participates in it. For example, a semantic frame may describe an event that occurs in a sentence, and the event’s frame elements may be the people or objects that participate in the event. For a more concrete example, consider the sentence “They said they do not believe that the prison system is outdated”. This sentence contains a Statement frame because a statement is made in it. One of the frame elements participating in the frame is the Speaker “they”. Fr"
P13-1026,D10-1023,1,0.547089,"Missing"
P13-1026,N04-1024,0,0.436641,"e art in this task). A major weakness of many existing scoring engines such as the Intelligent Essay AssessorTM (Landauer et al., 2003) is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., style, coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, Relevance to Prompt (Higgins et al., 2004), and organization (Persing et al., 2010). Essay grading software that provides feedback along multiple dimensions of essay quality such as E-rater/Criterion (Attali and Burstein, 2006) has also begun to emerge. 1 An essay’s thesis is the overall message of the entire essay. This concept is unbound from the the concept of thesis sentences, as even an essay that never explicitly states its thesis in any of its sentences may still have an overall message that can be inferred from the arguments it makes. 260 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,"
P13-1026,P10-4006,0,0.0199143,"ind a local maximum by employing the simulated annealing algorithm (Kirkpatrick et al., 1983), altering one parameter at a time to optimize F-score by holding the remaining parameters fixed. After training the classifiers, we use them to classify the test set essays. The test instances are created in the same way as the training instances. 4.2 and Dutnais, 1997) which allows us to automatically generate a semantic similarity measure between any two words. We train our random indexing model on over 30 million words of the English Gigaword corpus (Parker et al., 2009) using the S-Space package (Jurgens and Stevens, 2010). We expect that features based on random indexing may be particularly useful for the Incomplete Prompt Response and Relevance to Prompt errors because they may help us find text related to the prompt even if some of its components have been rephrased (e.g., an essay may talk about “jail” rather than “prison”, which is mentioned in one of the prompts). For each essay, we therefore generate four random indexing features, one encoding the entire essay’s similarity to the prompt, another encoding the essay’s highest individual sentence’s similarity to the prompt, a third encoding the highest enti"
P13-2142,W11-1701,0,0.49011,"t out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification of online debates proposed by Anand et al. (2011), focusing in particular on ideological debates. Specifically, we hypothesize that there are two types of soft extra-linguistic constraints on the stance labels of debate posts that, 1 Introduction While a lot of work on document-level opinion mining has involved determining the polarity expressed in a customer review (e.g., whether a review is “thumbs up” or “thumbs down”) (see Pang and Lee (2008) and Liu (2012) for an overview of the field), researchers have begun exploring new opinion mining tasks in recent years. One such task is debate stance classification: given a post written for a two"
P13-2142,C10-2100,0,0.060545,"aints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of onli"
P13-2142,C08-2004,0,0.122287,"soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults"
P13-2142,P11-1151,0,0.349615,"ts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These pro"
P13-2142,C12-2045,1,0.922451,"um, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification"
P13-2142,W04-2401,0,0.0249875,"ce two types of inter-post constraints (namely, ACs and ICs), we will have to employ a more sophisticated inference mechanism. Previous work has focused on employing graph minimum cut (MinCut) as the inference algorithm. However, since MinCut suffers from the weakness of not being able to enforce negative constraints (i.e., two posts cannot receive the same label) (Bansal et al., 2008), we propose to use integer linear programming (ILP) as the underlying inference mechanism. Below we show how to implement ACs and ICs within the ILP framework. Owing to space limitations, we refer the reader to Roth and Yih (2004) for details of the ILP framework. Briefly, ILP seeks to optimize an objective function subject to a set of linear conIdeology Constraints Next, we introduce our second type of inter-post constraints, ideology constraints (ICs). ICs are cross-domain, author-based constraints: they are only applicable to debate posts written by the same author in different domains. ICs model the fact that for some authors, their stances on various issues are determined in part by their ideological 818 System Anand Anand+AC Anand+AC+UC Anand+AC+UC+IC straints. Below we focus on describing the ILP program and how"
P13-2142,W10-0214,0,0.247727,"ate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-th"
P13-2142,W06-1639,0,0.86768,"modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may inv"
P13-2142,N12-1072,0,0.215378,"reads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the stateof-the-art supervised learning approach to debate stance classification of online debates propose"
P13-2142,N10-1097,0,0.0513093,"Missing"
P13-2142,D10-1102,0,0.0239684,"tance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification. Figure 1: A sample post on abortion. a debate forum, debate posts form threads, where later posts often support or oppose the viewpoints raised in earlier posts in the same thread. Previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Yessenalina et al., 2010; Burfoot et al., 2011), companyinternal discussions (Murakami and Raymond, 2010), and online social, political, and ideological debates in public forums (Agrawal et al., 2003; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Hasan and Ng, 2012). As Walker et al. (2012) point out, debates in public forums differ from congressional debates and company-internal discussions in terms of language use. Specifically, online debaters use colorful and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions"
P14-1119,I11-1130,0,0.263711,"rom the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of Length The dif"
P14-1119,C10-2042,1,0.653626,"tems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of Length The difficulty of the task increases with the length of the input document as longer documents yield more candidate keyphrases (i.e., phrases that are eligible to be keyphrases (see Section 3.1)). For instance, each Inspec abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases. In contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space (Hasan and Ng, 2010). Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles. Structural consistency In a structured document, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper’s keyphrases should appear in the abstract and the introduction. While structural information has been exploited to extract keyphrases from scientific papers (e.g., title, section information) (Kim et al., 2013), web pages (e.g., metadata) (Yih et al., 2006), and chats (e.g.,"
P14-1119,I13-1062,0,0.650053,"raditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in the graph is defined recursively in terms of t"
P14-1119,P06-1068,0,0.922395,"lection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora f"
P14-1119,I11-1019,0,0.0330502,"nce on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyp"
P14-1119,S10-1041,0,0.454786,"orresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors"
P14-1119,W03-1028,0,0.957584,"ntroduction, conclusion, etc.). In contrast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source Dataset/Contributor Paper abstracts Inspec (Hulth, 2003)∗ NUS corpus (Nguyen and Kan, 2007)∗ citeulike.org (Medelyan et al., 2009)∗ SemEval-2010 (Kim et al., 2010b)∗ NZDL (Witten et al., 1999)∗ DUC-2001 (Wan and Xiao, 2008b)∗ Reuters corpus (Hulth and Megyesi, 2006) Yih et al. (2006) Hammouda et al. (2005)∗ Blogs (Grineva et al., 2009) ICSI (Liu et al., 2009a) Enron corpus (Dredze et al., 2008)∗ Library of Congress (Kim and Baldwin, 2012) Scientific papers Technical reports News articles Web pages Meeting transcripts Emails Live chats Documents 2,000 211 180 284 1,800 308 12,848 828 312 252 161 14,659 15 Statistics Tokens/doc &lt;200 ≈8K >5K ≈900 ≈500"
P14-1119,N04-4005,0,0.0962837,", 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2 , c1 should be preferred to c2 . Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervis"
P14-1119,C10-1065,0,0.684698,"ctured documents (e.g., web pages, which can be blogs, forums, or reviews) 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source Dataset/Contributor Paper abstracts Inspec (Hulth, 2003)∗ NUS corpus (Nguyen and Kan, 2007)∗ citeulike.org (Medelyan et al., 2009)∗ SemEval-2010 (Kim et al., 2010b)∗ NZDL (Witten et al., 1999)∗ DUC-2001 (Wan and Xiao, 2008b)∗ Reuters corpus (Hulth and Megyesi, 2006) Yih et al. (2006) Hammouda et al. (2005)∗ Blogs (Grineva et al., 2009) ICSI (Liu et al., 2009a) Enron corpus (Dredze et al., 2008)∗ Library of Congress (Kim and Baldwin, 2012) Scientific papers Technical reports News articles Web pages Meeting transcripts Emails Live chats Documents 2,000 211 180 284 1,800 308 12,848 828 312 252 161 14,659 15 Statistics Tokens/doc &lt;200 ≈8K >5K ≈900 ≈500 ≈1K ≈1.6K - Keys/doc 10 11 5 15 8 6 8 4 10 Table 1: Evaluation datasets. Publicly available datasets are"
P14-1119,D09-1137,0,0.648749,"ral consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source Dataset/Contributor Paper abstracts Inspec (Hulth, 2003)∗ NUS corpus (Nguyen and Kan, 2007)∗ citeulike.org (Medelyan et al., 2009)∗ SemEval-2010 (Kim et al., 2010b)∗ NZDL (Witten et al., 1999)∗ DUC-2001 (Wan and Xiao, 2008b)∗ Reuters corpus (Hulth and Megyesi, 2006) Yih et al. (2006) Hammouda et al. (2005)∗ Blogs (Grineva et al., 2009) ICSI (Liu et al., 2009a) Enron corpus (Dredze et al., 2008)∗ Library of Congress (Kim and Baldwin, 2012) Scientific papers Technical reports News articles Web pages Meeting transcripts Emails Live chats Documents 2,000 211 180 284 1,800 308 12,848 828 312 252 161 14,659 15 Statistics Tokens/doc &lt;200 ≈8K >5K ≈900 ≈500 ≈1K ≈1.6K - Keys/doc 10 11 5 15 8 6 8 4 10 Table 1: Evaluation datasets."
P14-1119,S10-1004,0,0.711207,"ctured documents (e.g., web pages, which can be blogs, forums, or reviews) 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source Dataset/Contributor Paper abstracts Inspec (Hulth, 2003)∗ NUS corpus (Nguyen and Kan, 2007)∗ citeulike.org (Medelyan et al., 2009)∗ SemEval-2010 (Kim et al., 2010b)∗ NZDL (Witten et al., 1999)∗ DUC-2001 (Wan and Xiao, 2008b)∗ Reuters corpus (Hulth and Megyesi, 2006) Yih et al. (2006) Hammouda et al. (2005)∗ Blogs (Grineva et al., 2009) ICSI (Liu et al., 2009a) Enron corpus (Dredze et al., 2008)∗ Library of Congress (Kim and Baldwin, 2012) Scientific papers Technical reports News articles Web pages Meeting transcripts Emails Live chats Documents 2,000 211 180 284 1,800 308 12,848 828 312 252 161 14,659 15 Statistics Tokens/doc &lt;200 ≈8K >5K ≈900 ≈500 ≈1K ≈1.6K - Keys/doc 10 11 5 15 8 6 8 4 10 Table 1: Evaluation datasets. Publicly available datasets are"
P14-1119,N09-1070,0,0.0458553,"-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art"
P14-1119,D09-1027,0,0.146937,"-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art"
P14-1119,D10-1036,0,0.936968,"e searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of Length The difficulty of the task increases with the length of the input document as longer documents yield more candidate keyphrases (i.e., phrases that are eligible to be keyphrases (see Section 3.1)). For instance, each Inspec abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases. In"
P14-1119,C12-1127,0,0.0239041,"et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3 3.2 Keyphrase Extraction Approaches A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some 3.1 Selecting Candidate Words and Phrases Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to trai"
P14-1119,P09-2046,0,0.0645737,"ndidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3 3.2 Keyphrase Extraction Approaches A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as candidate key"
P14-1119,W11-0316,0,0.186572,"Missing"
P14-1119,C08-2021,0,0.021601,"Missing"
P14-1119,C12-1105,0,0.00808932,"that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster) that clusters semantically similar candidates using Wikipedia and co-occurrence-based statistics. The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid"
P14-1119,W03-1805,0,0.0842927,"xamined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved"
P14-1119,C08-1122,0,0.1073,"ums, or reviews) 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source Dataset/Contributor Paper abstracts Inspec (Hulth, 2003)∗ NUS corpus (Nguyen and Kan, 2007)∗ citeulike.org (Medelyan et al., 2009)∗ SemEval-2010 (Kim et al., 2010b)∗ NZDL (Witten et al., 1999)∗ DUC-2001 (Wan and Xiao, 2008b)∗ Reuters corpus (Hulth and Megyesi, 2006) Yih et al. (2006) Hammouda et al. (2005)∗ Blogs (Grineva et al., 2009) ICSI (Liu et al., 2009a) Enron corpus (Dredze et al., 2008)∗ Library of Congress (Kim and Baldwin, 2012) Scientific papers Technical reports News articles Web pages Meeting transcripts Emails Live chats Documents 2,000 211 180 284 1,800 308 12,848 828 312 252 161 14,659 15 Statistics Tokens/doc &lt;200 ≈8K >5K ≈900 ≈500 ≈1K ≈1.6K - Keys/doc 10 11 5 15 8 6 8 4 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). heuristics (Section 3.1); and ("
P14-1119,P07-1070,0,0.00806847,"recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor. 3.3.3 Simultaneous Learning Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously. Zha (2002) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation: a sentence is important if it contains important words, and important words appear in important sentences. Wan et al. (2007) extend Zha’s work by adding two assumptions: (1) an important sentence is connected to other important sentences, and (2) an important word is linked to other important words, a TextRank-like assumption. Based on these assumptions, Wan et al. (2007) build three graphs to capture the association between the sentences (S) and the words (W) in an input document, namely, a S–S graph, a bipartite S–W graph, and a W–W graph. The weight of an edge connecting two sentence nodes in a S–S graph corresponds to their content similarity. An edge weight in a S–W graph denotes the word’s importance in the s"
P14-1119,R09-1086,0,0.0286976,"Missing"
P14-1119,W04-3252,0,\N,Missing
P14-1119,P11-1039,0,\N,Missing
P14-1119,S10-1055,0,\N,Missing
P14-1119,Y12-1021,0,\N,Missing
P14-1144,N04-1024,0,0.807157,"g multiple dimensions of essay quality such as E-rater/Criterion (Attali and Burstein, 2006) has also begun to emerge. Our goal in this paper is to develop a computational model for scoring an essay along an under-investigated dimension — prompt adherence. Prompt adherence refers to how related an essay’s content is to the prompt for which it was written. An essay with a high prompt adherence score consistently remains on the topic introduced by the prompt and is free of irrelevant digressions. To our knowledge, little work has been done on scoring the prompt adherence of student essays since Higgins et al. (2004). Nevertheless, there are major differences between Higgins et al.’s work and our work with respect to both the way the task is formulated and the approach. Regarding task formulation, while Higgins et al. focus on classifying each sentence as having either good or bad adherence to the prompt, we focus on assigning a prompt adherence score to the entire essay, allowing the score to range from one to four points at half-point increments. As far as the approach is concerned, Higgins et al. adopt a knowledgelean approach to the task, where almost all of the features they employ are computed based"
P14-1144,P10-4006,0,0.0105724,"est instances are created in the same way as the training instances. 4.2 Baseline Features Our baseline system for score prediction employs various features based on Random Indexing. 1. Random Indexing Random Indexing (RI) is “an efficient, scalable and incremental alternative” (Sahlgren, 2005) to Latent Semantic Indexing (Deerwester et al., 1990; Landauer and Dutnais, 1997) which allows us to automatically generate a semantic similarity measure between any two words. We train our RI model on over 30 million words of the English Gigaword corpus (Parker et al., 2009) using the S-Space package (Jurgens and Stevens, 2010). We expect that features based on RI will be useful for prompt adherence scoring because they may help us find text related to the prompt even if some of its concepts have have been rephrased (e.g., an essay may talk about “jail” rather than “prison”, which is mentioned in one of the prompts), and because they have already proven useful for the related task of determining which sentences in an essay are related to the prompt (Higgins et al., 2004). For each essay, we therefore attempt to adapt the RI features used by Higgins et al. (2004) to our problem of prompt adherence scoring. We do this"
P14-1144,W10-1013,0,0.145845,"e dimension. Our annotators were selected from over 30 applicants who were familiarized with the scoring rubric and given sample essays to score. The six who were most consistent with the expected scores were given additional essays to annotate. Annotators evaluated how well each essay adheres to its prompt using a numerical score from one to four at half-point increments (see Table 2 for a description of each score). This contrasts with previous work on prompt adherence essay scoring, where the corpus is annotated with a binary decision (i.e., good or bad) (e.g., Higgins et al. (2004; 2006), Louis and Higgins (2010)). Hence, our annotation scheme not only provides 4.1 Model Training and Application We cast the problem of predicting an essay’s prompt adherence score as 13 regression problems, one for each prompt. Each essay is represented as an instance whose label is the essay’s true score (one of the values shown in Table 3) with up to seven types of features including baseline (Section 4.2) and six other feature types proposed by us (Section 4.3). Our regressors may assign an essay any score in the range of 1.0−4.0. Using regression captures the fact that some pairs of scores are more similar than othe"
P14-1144,P13-1026,1,0.793853,"s of many existing scoring engines such as the Intelligent Essay AssessorTM (Landauer et al., 2003) is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., style, coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, organization (Persing et al., 2010), and thesis clarity (Persing and Ng, 2013). Essay grading software that provides feedback along multiple dimensions of essay quality such as E-rater/Criterion (Attali and Burstein, 2006) has also begun to emerge. Our goal in this paper is to develop a computational model for scoring an essay along an under-investigated dimension — prompt adherence. Prompt adherence refers to how related an essay’s content is to the prompt for which it was written. An essay with a high prompt adherence score consistently remains on the topic introduced by the prompt and is free of irrelevant digressions. To our knowledge, little work has been done on s"
P14-1144,D10-1023,1,0.456326,"Missing"
P14-2006,W12-4501,1,0.648168,"ity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations"
P14-2006,W10-4305,1,0.648818,"ainst them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially h"
P14-2006,D09-1101,1,0.646014,"e mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30 —to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them and penalizes recall for twinless predicted mentions. Rahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAF m , leaving the question of handling CEAF e as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube ("
P14-2006,I13-1193,1,0.853507,"very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recas"
P14-2006,P11-1082,1,0.341706,"he OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evaluVariations of Scoring Measures Two commonly used coreference scoring metrics —B3 and CEAF—are underspecified in their application for scoring predicted, as opposed to key mentions. The examples in the papers describing"
P14-2006,W09-2411,1,0.899331,"Missing"
P14-2006,doddington-etal-2004-automatic,0,0.0166341,"ith this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold st"
P14-2006,N13-1071,1,0.782346,"Missing"
P14-2006,P09-1074,0,0.498213,"arnegie Mellon University, Pittsburgh, PA 5 HLTRI, University of Texas at Dallas, Richardson, TX, 6 HITS, Heidelberg, Germany 3 sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com, hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org Abstract purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et"
P14-2006,N13-2001,0,0.0493029,"pora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 200"
P14-2006,Q14-1012,1,0.042996,"Missing"
P14-2006,P14-2005,1,0.929886,"ne-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 t"
P14-2006,M95-1005,0,0.887578,"proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified"
P14-2006,H05-1004,1,0.706057,"re informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or"
P14-2006,W11-1901,1,0.482275,"al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setbac"
P14-2006,J00-4006,0,\N,Missing
P14-2006,D08-1067,1,\N,Missing
P14-2006,S10-1001,1,\N,Missing
P15-1053,N10-1138,0,0.0126393,"ically label each essay with POS tags using the Stanford CoreNLP system (Manning et al., 2014), then count the number of times the POS tag sequence occurs in the essay. An example of a useful feature of this type is “CC NN ,”, as it is able to capture when a student writes either “for instance,” or “for example,”. We normalize each essay’s set of POS n-gram features to unit length. 2. Semantic Frames (SFR) While POS n-grams provide syntactic generalizations of word n-grams, FrameNet-style semantic role labels provide semantic generalizations. For each essay in our data set, we employ SEMAFOR (Das et al., 2010) to identify each semantic frame occurring in the essay as well as each frame element that participates in it. For example, a semantic frame may describe an event that occurs in a sentence, and the event’s frame elements may be the people or objects that participate in the event. For a more concrete example, consider the sentence “I said that I do not believe that it is a good idea”. This sentence contains a Statement frame because a statement is made in it. One of the frame elements participating in the frame is the Speaker “I”. From this frame, we would extract a feature pairing the frame to"
P15-1053,N04-1024,0,0.56714,"e art in this task). A major weakness of many existing scoring engines such as the Intelligent Essay AssessorTM (Landauer et al., 2003) is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., style, coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Persing and Ng, 2014), organization (Persing et al., 2010), and thesis clarity 2 Corpus Information We use as our corpus the 4.5 million word International Corpus of Learner English (ICLE) (Granger 543 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 543–552, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Topic Most university degrees are theoretical and do not prepare students for the real world. They are therefore of very little value."
P15-1053,W14-2104,0,0.241065,"Missing"
P15-1053,P13-1026,1,0.779691,"t reasons. The first binary feature is turned on whenever the maximum entropy classifier believes that the reason applies (i.e., when it assigns the reason a probability of over 0.5). The second feature’s value is the probability the classifier assigns for this reason. using MALLET (McCallum, 2002) for identifying which one of these seven categories an author’s opinion falls into. The feature set we use for this task includes POS n-gram and semantic frame features as described earlier in this section, lemmatized word 1-3 grams, the keyword and prompt adherence keyword features we described in Persing and Ng (2013) and Persing and Ng (2014), respectively, and a feature indicating which statement in the prompt we are attempting to classify the author’s agreement level with respect to. 6. Argument Component Predictions (ACP) Many of our features thus far do not result from an attempt to build a deep understanding of the structure of the arguments within our essays. To introduce such an understanding into our system, we follow Stab and Gurevych (2014a), who collected and annotated a corpus of 90 persuasive essays (not from the ICLE corpus) with the understanding that the arguments contained therein consist"
P15-1053,P14-1144,1,0.688031,"major weakness of many existing scoring engines such as the Intelligent Essay AssessorTM (Landauer et al., 2003) is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., style, coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Persing and Ng, 2014), organization (Persing et al., 2010), and thesis clarity 2 Corpus Information We use as our corpus the 4.5 million word International Corpus of Learner English (ICLE) (Granger 543 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 543–552, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Topic Most university degrees are theoretical and do not prepare students for the real world. They are therefore of very little value. The prison system is ou"
P15-1053,D10-1023,1,0.797069,"Missing"
P15-1053,W14-2110,0,0.257984,"Missing"
P15-1053,C14-1142,0,0.35475,"sions of essay quality such as E-rater/Criterion (Attali and Burstein, 2006) has also begun to emerge. Our goal in this paper is to develop a computational model for scoring the essay dimension of argument strength, which is arguably the most important aspect of argumentative essays. Argument strength refers to the strength of the argument an essay makes for its thesis. An essay with a high argument strength score presents a strong argument for its thesis and would convince most readers. While there has been work on designing argument schemes (e.g., Burstein et al. (2003), Song et al. (2014), Stab and Gurevych (2014a)) for annotating arguments manually (e.g., Song et al. (2014), Stab and Gurevych (2014b)) and automatically (e.g., Falakmasir et al. (2014), Song et al. (2014)) in student essays, little work has been done on scoring the argument strength of student essays. It is worth mentioning that some work has investigated the use of automatically determined argument labels for heuristic (Ong et al., 2014) and learning-based (Song et al., 2014) essay scoring, but their focus is holistic essay scoring, not argument strength essay scoring. In sum, our contributions in this paper are twofold. First, we dev"
P15-1053,P14-5010,0,0.00167801,"utomatically compiled from a given set of training essays would be contaminated with prompt-specific n-grams that may make the resulting regressor generalize less well to essays written for new prompts. To generalize our feature set in a way that does not risk introducing prompt-dependent features, we introduce POS n-gram features. Specifically, we construct one feature from each sequence of 1−5 part-of-speech tags appearing in our corpus. In order to obtain one of these features’ values for a particular essay, we automatically label each essay with POS tags using the Stanford CoreNLP system (Manning et al., 2014), then count the number of times the POS tag sequence occurs in the essay. An example of a useful feature of this type is “CC NN ,”, as it is able to capture when a student writes either “for instance,” or “for example,”. We normalize each essay’s set of POS n-gram features to unit length. 2. Semantic Frames (SFR) While POS n-grams provide syntactic generalizations of word n-grams, FrameNet-style semantic role labels provide semantic generalizations. For each essay in our data set, we employ SEMAFOR (Das et al., 2010) to identify each semantic frame occurring in the essay as well as each frame"
P15-1053,D14-1006,0,0.347945,"sions of essay quality such as E-rater/Criterion (Attali and Burstein, 2006) has also begun to emerge. Our goal in this paper is to develop a computational model for scoring the essay dimension of argument strength, which is arguably the most important aspect of argumentative essays. Argument strength refers to the strength of the argument an essay makes for its thesis. An essay with a high argument strength score presents a strong argument for its thesis and would convince most readers. While there has been work on designing argument schemes (e.g., Burstein et al. (2003), Song et al. (2014), Stab and Gurevych (2014a)) for annotating arguments manually (e.g., Song et al. (2014), Stab and Gurevych (2014b)) and automatically (e.g., Falakmasir et al. (2014), Song et al. (2014)) in student essays, little work has been done on scoring the argument strength of student essays. It is worth mentioning that some work has investigated the use of automatically determined argument labels for heuristic (Ong et al., 2014) and learning-based (Song et al., 2014) essay scoring, but their focus is holistic essay scoring, not argument strength essay scoring. In sum, our contributions in this paper are twofold. First, we dev"
P15-2049,S14-2147,0,0.175568,"he training data are suffixated. For example, “infectious source” becomes “source of infectious” in 3 A list of the disorder word synonyms is manually created by inspection of the training data. 299 with c. The normalized form of m will be the union of the concepts to which each of its split mentions is normalized.4 by stochastic gradient descent for all the concept terms, their matched training data mentions, and their mismatched training data mentions. After convergence, the weight matrix is then employed in the scoring function to normalize the test disorder mentions. Ghiasvand and Kate’s (Ghiasvand and Kate, 2014) system has produced the best results to date on ShARe. It first generates variations of a given disorder word/phrase based on a set of learned edit distance patterns for converting one word/phrase to another, and then attempts to normalize these query phrase variations by performing exact match with a training disorder mention or a concept term. Rao et al.’s (2013) open-domain entity-linking system adopts an approach that is similar in spirit to ours. It links organizations, geo-political entities, and persons to the entities in a Wikipediaderived knowledge base, utilizing heuristics for matc"
P15-2049,D10-1048,0,0.0296314,"oncepts taken from these two ontologies. 3.2 Normalization Sieves In this subsection, we describe the ten sieves we designed for normalization. For convenience, we use the word concept to refer to a concept in the ontology, and we say that a disorder mention has an exact match with a concept if it has an exact match with one of the terms associated with it. Sieve 1: Exact Match. This sieve normalizes a disorder mention m to a concept c if m has an exact match with c. Despite the simplicity and modularity of the multipass sieve approach and its successful application to coreference resolution (Raghunathan et al., 2010), it has not been extensively applied to other NLP tasks. In this section, we investigate its application to normalization. Sieve 2: Abbreviation Expansion. This sieve first expands all abbreviated disorder mentions using Schwartz and Hearst’s (2003) algorithm and the Wikipedia list of disorder abbreviations.2 Then, it normalizes a disorder mention m to a concept c if the unabbreviated version of m has an exact match with c. For each unnormalized mention, we pass both its original form and its new (i.e., unabbreviated) form, if applicable, to the next sieve. As we will see, we keep expanding t"
P15-2053,E09-1018,0,0.0157521,"ese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 3.2 Training Our model estimates P (z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motivated in part by previous work on English overt pronoun resolution (e.g., Cherry and Bergsma (2005) and Charniak and Elsner (2009)), we estimate the model parameters using the Expectation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Conver"
P15-2053,P09-2022,0,0.363118,"Missing"
P15-2053,D13-1135,1,0.663937,"e probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the corr"
P15-2053,W03-1024,0,0.62934,"the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of conditional probability an"
P15-2053,D14-1084,1,0.43525,"noun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advancing the state of the art in unsupervised AZP resolution. The design of our unsupervised model is motivated by a key question: can we resolve AZPs by using a probabilistic model trained on zero pro"
P15-2053,D13-1028,0,0.0213405,"supervised model for joint AZP identification and resolution.1 In addition, motivated by work on overt pronoun resolution, we hypothesize that AZP resolution can be improved by exploiting discourse information. Specifically, we design a model of salience and incorporate salience information into our model as a feature. Inspired by traditional work on discoursebased anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 3.2 Training Our model estimates P (z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motiva"
P15-2053,W05-0612,0,0.0244647,"on. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 3.2 Training Our model estimates P (z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motivated in part by previous work on English overt pronoun resolution (e.g., Cherry and Bergsma (2005) and Charniak and Elsner (2009)), we estimate the model parameters using the Expectation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolv"
P15-2053,D10-1086,0,0.804242,"As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advancing the state of the a"
P15-2053,J94-4002,0,0.10639,"every candidate antecedent c in C; kc is the context surrounding z and candidate antecedent c; and l is a binary variable indicating whether c is the correct antecedent of z. proposing here is the first unsupervised model for joint AZP identification and resolution.1 In addition, motivated by work on overt pronoun resolution, we hypothesize that AZP resolution can be improved by exploiting discourse information. Specifically, we design a model of salience and incorporate salience information into our model as a feature. Inspired by traditional work on discoursebased anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 3.2 Training Our model estimates P (z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent"
P15-2053,P00-1022,0,0.67773,"tation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterpar"
P15-2053,W12-4501,0,0.212474,"Missing"
P15-2053,J95-2003,0,0.904441,"Missing"
P15-2053,I11-1085,0,0.584821,"antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of conditional probability and the Theorem of Total Probability, we can rewrite P (l=1|z, k, c) as follows: P (z, k, c, l=1) P (z, k, c,"
P15-2053,P11-1081,0,0.660401,"th each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of conditional probability and the Theorem of Total Probability, we can rewrite P (l=1|z, k, c) as follows:"
P15-2053,C02-1078,0,0.497006,"dents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of"
P15-2053,W03-2604,0,0.256413,"-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 3.2.1 E-Step The goal of the E-step is to compute P (l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of conditional probability and the Theorem of To"
P15-2053,P06-1079,0,0.825588,"Missing"
P15-2053,D07-1057,0,0.894992,"e political crisis.) As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advanc"
P15-2053,I08-1004,0,0.0609549,"Missing"
P16-1074,chen-ng-2014-sinocoreferencer,1,0.936917,"ttributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models (henceforth the CN14 model and the CN15 model, respectively) that rival their supervised counterparts in performance. An appealing aspect of these unsupervised models is that their language-independent generative process enables them to be applied to languages where data annotated with ZP links are not readily available. Though achieving state-of-the-art performance, these models have several weaknesses. First, a lot of manual efforts need to be spent on engineering the features for generative probabilistic models, as thes"
P16-1074,P15-2053,1,0.751692,"Missing"
P16-1074,P15-1136,0,0.0150631,"Missing"
P16-1074,P00-1022,0,0.782841,"p it into a low-dimensional feature space. The resulting vector can be viewed as the low-dimensional semantic embedding of the corresponding input vector. Finally, the model computes a matching score between z and each of its candidate antecedents based on their lowdimensional representations. These scores are then normalized into probabilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a sa"
P16-1074,D13-1057,0,0.0110164,"t task. For the task of AZP resolution, this is desirable. Traditionally, it is difficult to correctly resolve an AZP if its context is lexically different from its antecedent's context. This is especially the case for unsupervised resolvers. In contrast, a deep network can handle difficult cases like this via learning representations that make lexically different contexts look similar. Second, we train our deep network in a supervised manner.1 In particular, motivated by recent successes of applying the mention-ranking model (Denis and Baldridge, 2008) to entity coreference resolution (e.g., Chang et al. (2013), Durrett and Klein (2013), Clark and Manning (2015), Martschat and Strube (2015), Wiseman et al. (2015)), we propose to employ a ranking-based deep network, which is trained to assign the highest probability to the correct antecedent of an AZP given a set of candidate antecedents. This contrasts with existing supervised AZP resolvers, all of which are classification-based. Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter. Finally, given that our network is trained in"
P16-1074,J95-2003,0,0.754556,"ll of which are classification-based. Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter. Finally, given that our network is trained in a supervised manner, we can extensively employ lex2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs' algorithm (Hobbs, 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are learning-based. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. Chen and Ng (2013) extended Zhao and Ng's feature set with novel features that encode the context surrounding a ZP and its candidate ant"
P16-1074,D13-1135,1,0.644203,"er could harm model performance. Second, in the absence of labeled data, it is difficult, though not impossible, for these models to profitably employ lexical features (e.g., word pairs, syntactic patterns involving words), as determining which lexical features are useful and how to combine the potentially large number of lexical features in an unsupervised manner is a very challenging task. In fact, the unsupervised models proposed by Chen and Ng (2014b; 2015) are unlexicalized, presumably owing to the aforementioned reasons. Unfortunately, as shown in previous work (e.g, Zhao and Ng (2007), Chen and Ng (2013)), the use of lexical features contributed significantly to the performance of state-of-the-art supervised AZP resolvers. Finally, owing to the lack of labeled data, the model parameters are learned to maximize data 778 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 778–788, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ical features and use them in combination with other types of features that have been shown to be useful for AZP resolution. However, rather than employing words directly as features, we empl"
P16-1074,D13-1095,0,0.284971,"ate antecedents, and exploited the coreference links between ZPs as bridges to 1 Note that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders (Salakhutdinov and Hinton, 2007), the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors. 779 Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector. hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve anaphoric ZPs, Rao et a"
P16-1074,I11-1023,0,0.0198929,"or other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z.3 To reduce the size of the list of preceding entities, we only consider a partial entity active if at least one of its mentions appears with"
P16-1074,N15-1052,0,0.0866493,". (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve anaphoric ZPs, Rao et al. (2015) resolved deictic non-anaphoric ZPs, which ""refer to salient entities in the environment such as the speaker, hearer or pragmatically accessible referent without requiring any introduction in the preceding text''. 3 Model In this section, we first introduce our network architecture (Section 3.1), and then describe how we train it (Section 3.2) and apply it (Section 3.3). 3.1 Network Architecture The network architecture is shown in Figure 1. Since we employ a ranking model to rank the candidate antecedents of an AZP z, the inputs to the network are (1) a feature vector representing the AZP, an"
P16-1074,P11-1081,0,0.625715,"that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders (Salakhutdinov and Hinton, 2007), the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors. 779 Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector. hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve anaphoric ZPs, Rao et al. (2015) resolved deictic non-anaphoric ZPs, which ""refer to salient entities in the e"
P16-1074,P06-1079,0,0.626793,"tions. These scores are then normalized into probabilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding"
P16-1074,I11-1085,0,0.746372,"Missing"
P16-1074,C08-1097,0,0.548856,"en normalized into probabilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z.3 To reduce the size of th"
P16-1074,D15-1260,0,0.575543,"e links between ZPs as bridges to 1 Note that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders (Salakhutdinov and Hinton, 2007), the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors. 779 Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector. hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve anaphoric ZPs, Rao et al. (2015) resolved deictic non-anaphoric ZP"
P16-1074,N09-1059,0,0.185089,"et xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z.3 To reduce the size of the list of preceding entities, we only consider a partial entity ac"
P16-1074,P09-2022,0,0.327035,"Missing"
P16-1074,W03-1024,0,0.789049,"lowdimensional representations. These scores are then normalized into probabilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partia"
P16-1074,C02-1078,0,0.62177,"dents based on their lowdimensional representations. These scores are then normalized into probabilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salie"
P16-1074,D08-1055,0,0.27146,"babilities using a softmax. More formally, let xe (z) and xh (z) be the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z.3 To reduce the size of the list of preceding e"
P16-1074,D10-1086,0,0.834997,"As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models (henceforth the CN14 model and the CN15 model, respectively) that rival their supervised counterparts in performance. An appealing aspect of these unsupervised models is that their language-independent generative process enables them to be applied to languages where data annotated with ZP links are not readily available. Though achieving state-of-the-art performance, these models have several weaknesses. First, a lot of manual efforts need to be spent on engineering the features for gener"
P16-1074,P10-2018,0,0.0680054,"e the vecZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro780 AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate antecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents. We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z.3 To reduce the size of the list of preceding entities, we only consider a partial entity active if at least one of"
P16-1074,Q15-1029,0,0.014291,"it is difficult to correctly resolve an AZP if its context is lexically different from its antecedent's context. This is especially the case for unsupervised resolvers. In contrast, a deep network can handle difficult cases like this via learning representations that make lexically different contexts look similar. Second, we train our deep network in a supervised manner.1 In particular, motivated by recent successes of applying the mention-ranking model (Denis and Baldridge, 2008) to entity coreference resolution (e.g., Chang et al. (2013), Durrett and Klein (2013), Clark and Manning (2015), Martschat and Strube (2015), Wiseman et al. (2015)), we propose to employ a ranking-based deep network, which is trained to assign the highest probability to the correct antecedent of an AZP given a set of candidate antecedents. This contrasts with existing supervised AZP resolvers, all of which are classification-based. Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter. Finally, given that our network is trained in a supervised manner, we can extensively employ lex2 Related Work Chinese ZP reso"
P16-1074,P15-1137,0,0.0169901,"resolve an AZP if its context is lexically different from its antecedent's context. This is especially the case for unsupervised resolvers. In contrast, a deep network can handle difficult cases like this via learning representations that make lexically different contexts look similar. Second, we train our deep network in a supervised manner.1 In particular, motivated by recent successes of applying the mention-ranking model (Denis and Baldridge, 2008) to entity coreference resolution (e.g., Chang et al. (2013), Durrett and Klein (2013), Clark and Manning (2015), Martschat and Strube (2015), Wiseman et al. (2015)), we propose to employ a ranking-based deep network, which is trained to assign the highest probability to the correct antecedent of an AZP given a set of candidate antecedents. This contrasts with existing supervised AZP resolvers, all of which are classification-based. Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter. Finally, given that our network is trained in a supervised manner, we can extensively employ lex2 Related Work Chinese ZP resolution. Early approache"
P16-1074,W12-4501,0,0.0168678,"ng to the following update rule: Λt = Λt−1 − α ∂J (Λt−1 ) ∂Λt−1 Test 172 6,083 110,034 1,713 Table 3: Statistics on the training and test sets. ci ∈C { Training 1,391 36,487 756,063 12,111 (10) where α is the learning rate, and Λt and Λt−1 are model parameters at the tth iteration and the (t − 1)th iteration respectively. To avoid overfitting, we determine the hyperparameters of the network using a held-out development set. 4 Evaluation 4.1 Experimental Setup 3.3 Inference Datasets. We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task (Pradhan et al., 2012). In the CoNLL-2012 data, the training set and the development set contain ZP coreference annotations, but the test set does not. Therefore, we train our models on the training set and perform evaluation on the development set. Statistics on the datasets are shown in Table 3. The documents in these datasets come from six sources, namely Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). After training, we can apply the resulting network to find an antecedent for each AZP. Each test instance corresponds to an AZP z and"
P16-1074,I11-1126,0,0.151027,"nding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to 1 Note that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders (Salakhutdinov and Hinton, 2007), the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors. 779 Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector. hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve a"
P16-1074,I13-1126,0,0.149338,"xploited the coreference links between ZPs as bridges to 1 Note that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders (Salakhutdinov and Hinton, 2007), the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors. 779 Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector. hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)). find textually distant antecedents for ZPs. As mentioned above, there have been attempts to perform unsupervised AZP resolution. For instance, using only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a supervised overt pronoun resolver and applied it to resolve AZPs. More recently, Chen and Ng (2014b; 2015) have proposed unsupervised probabilistic AZP resolution models that rivaled their supervised counterparts in performance. While we aim to resolve anaphoric ZPs, Rao et al. (2015) resolved deic"
P16-1205,P13-2144,0,0.0481314,"Missing"
P16-1205,C08-2004,0,0.0751002,"Missing"
P16-1205,W14-2107,0,0.041402,"Missing"
P16-1205,P11-1151,0,0.14714,"Missing"
P16-1205,N10-1138,0,0.0861444,"Missing"
P16-1205,I13-1191,1,0.790579,"Missing"
P16-1205,P14-5010,0,0.00482376,"Missing"
P16-1205,C10-2100,0,0.344833,"Missing"
P16-1205,P15-1053,1,0.875417,"gories such as grammar, usage, mechanics, style and organization. Hence, persuasiveness and other content-dependent dimensions of argumentative essay quality are largely ignored in existing automated essay scoring research. While full-fledged content-based essay scoring is still beyond the reach of state-of-the-art essay scoring engines, recent work has enabled us to move one step closer to this ambitious goal by analyzing essay content, attempting to determine the argumentative structure of student essays (Stab and Gurevych, 2014) and the persuasiveness of the arguments made in these essays (Persing and Ng, 2015). Stance classification is an important first step in determining how persuasive an argumentative student essay is because persuasiveness depends on how well the author argues w.r.t. the stance she takes using the supporting evidence she provides. For instance, if her stance is Agree Somewhat, a persuasive argument would involve explaining what reservations she has about the given proposition. As another example, an argumentative essay in which the author takes a neutral stance or the author presents evidence that does not support the stance she claims to take should receive a low persuasivene"
P16-1205,W15-0509,0,0.0363284,"Missing"
P16-1205,W10-0214,0,0.402459,"Missing"
P16-1205,D14-1006,0,0.0315227,"ot grade essay content, focusing instead on providing diagnostic trait feedback on categories such as grammar, usage, mechanics, style and organization. Hence, persuasiveness and other content-dependent dimensions of argumentative essay quality are largely ignored in existing automated essay scoring research. While full-fledged content-based essay scoring is still beyond the reach of state-of-the-art essay scoring engines, recent work has enabled us to move one step closer to this ambitious goal by analyzing essay content, attempting to determine the argumentative structure of student essays (Stab and Gurevych, 2014) and the persuasiveness of the arguments made in these essays (Persing and Ng, 2015). Stance classification is an important first step in determining how persuasive an argumentative student essay is because persuasiveness depends on how well the author argues w.r.t. the stance she takes using the supporting evidence she provides. For instance, if her stance is Agree Somewhat, a persuasive argument would involve explaining what reservations she has about the given proposition. As another example, an argumentative essay in which the author takes a neutral stance or the author presents evidence t"
P16-1205,W06-1639,0,0.283766,"Missing"
P16-1205,N12-1072,0,0.193384,"Missing"
P16-1205,N10-1097,0,0.0737861,"Missing"
P16-1205,D10-1102,0,0.052818,"Missing"
P16-1205,P15-1012,0,\N,Missing
P17-1009,W06-0901,0,0.20045,"articularly for noun triggers, owing to the absence of dependency relations that can be used to reliably extract their arguments. Moreover, using dependency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performe"
P17-1009,D13-1203,0,0.513731,"following features: 4 We train a CRF-based entity extraction model for jointly identifying the entity mentions and their types. Details can be found in Lu et al. (2016). 5 The dictionary is available from http://ir.hit.edu.cn/. An entry number in this dictionary conceptually resembles a synset id in WordNet (Fellbaum, 1998). 93 mi . These features are: (1) the pair of mi and mj ’s subtypes, (2) the pair of mj ’s subtype and mi ’s word, and (3) the pair of mi ’s subtype and mj ’s word. tional likelihood of collectively resolving the mentions to their correct antecedents in the training texts (Durrett and Klein, 2013). Below we describe the features used to represent the candidate antecedents for the mention to be resolved, mj . Features representing the N ULL candidate antecedent: Besides mj ’s word and mj ’s lemma, we employ feature conjunctions given their usefulness in entity coreference (Fernandes et al., 2014). Specifically, we create a conjunction between mj ’s lemma and the number of sentences preceding mj , as well as a conjunction between mj ’s lemma and the number of mentions preceding mj in the document. Features representing a non-N ULL candidate antecedent, mi : mi ’s word, mi ’s lemma, wheth"
P17-1009,araki-etal-2014-detecting,0,0.148181,"e sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint infere"
P17-1009,Q14-1037,0,0.0879473,"tically with the number of event mentions in a document. To improve scalability, we restrict the domains of the coreference variables. Rather than allow the domain of coreference variable cj to be of size j, we allow a preceding mention mi to be a candidate antecedent of mention mj if (1) the sentence distance between the two mentions is less than an empirically determined threshold and (2) either they are coreferent at least once in the training data or their head words have the same lemma. Doing so effectively enables us to prune the unlikely candidate antecedents for each event mention. As Durrett and Klein (2014) point out, such pruning has the additional benefit of reducing “the memory footprint and time needed to build a factor graph”, as we do not need to create any factor between mi and mj and its associated features if mi is pruned. To further reduce the memory footprint, we additionally restrict the domains of the event subtype variables. Given a candidate event mention created from word w, we allow the domain of its subtype variable to include only N ONE as well as those subtypes that w is labeled with at least once in the training data. For decoding, we employ minimum Bayes risk, which compute"
P17-1009,D15-1247,0,0.106742,"t coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint inference approach to event coreference. With the rarest exceptions (e.g., Lu et al. (2016)), existing resolvers have adopted a pipeline architecture in which trigger detection is performed prior"
P17-1009,N10-1112,0,0.0354655,"s the most probable assignment to each variable. In this objective, p′ is obtained by augmenting the distribution p (defined in Section 3.1) with task-specific parameterized loss functions: p′ (t, a, c|xi ; Θ) ∝ p(t, a, c|xi ; Θ) exp[αt lt (t, t∗ ) + αa la (a, a∗ ) + αc lc (c, C ∗ )] where lt , la and lc are task-specific loss functions, and αt , αa and αc are the associated weight parameters that specify the relative importance of the three tasks in the objective function. Softmax-margin, the technique of integrating task-specific loss functions into the objective function, was introduced by Gimpel and Smith (2010) and subsequently used by Durrett and Klein (2013, 2014). By encoding task-specific knowledge, these loss functions can help train a model that places less probability mass on less desirable output configurations. Our loss function for event coreference, lc , is motivated by the one Durrett and Klein (2013) developed for entity coreference. It is a weighted sum of the counts of three error types: lc (c, C ∗ ) = αc,F AF A(c, C ∗ )+αc,F N F N (c, C ∗ ) + αc,W L W L(c, C ∗ ) where F A(c, C ∗ ) is the number of non-anaphoric mentions misclassified as anaphoric, F N (c, C ∗ ) is the number of anaph"
P17-1009,J14-2004,0,0.118565,"portant indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint inference approach to event coreference. With the rarest exceptions (e.g., Lu et al. (2016)), e"
P17-1009,I11-1012,0,0.014063,"endency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to eval"
P17-1009,I13-1100,1,0.908129,"t coreference links to be discovered. 4.3 Model Ablations To evaluate the importance of each of the three types of joint factors in the joint model, we perform ablation experiments.7 Table 3 shows the results on the English and Chinese datasets when we add each type of joint factors to the independent model and remove each type of joint factors from the full joint model. The results of each task are expressed in terms of changes to the corresponding independent model’s F-score. 4.4 Error Analysis Next, we conduct an analysis of the major sources of error made by our joint coreference model. 7 Chen and Ng (2013) also performed ablation on their ACE-style Chinese event coreference resolver. However, given the differences in the tasks involved (e.g., they did not model event anaphoricity, but included tasks such as event argument extraction and role classification, entity coreference, and event mention attribute value computation) and the ablation setup (e.g., they ablated individual tasks/components in their pipeline-based system in an incremental fashion, whereas we ablate interaction factors rather than tasks), a direct comparison of their observations and ours is difficult. 4.4.1 Two Major Types of"
P17-1009,W97-1311,0,0.458048,"action. Doing so proves inadequate, particularly for noun triggers, owing to the absence of dependency relations that can be used to reliably extract their arguments. Moreover, using dependency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in"
P17-1009,N15-1116,1,0.81233,"n be used to reliably extract their arguments. Moreover, using dependency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016"
P17-1009,K16-1024,0,0.100863,"act their arguments. Moreover, using dependency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015"
P17-1009,D12-1045,0,0.0930297,"arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint inference approach to event coreference. With the rarest exceptions"
P17-1009,W09-3208,0,0.656084,"for noun triggers, owing to the absence of dependency relations that can be used to reliably extract their arguments. Moreover, using dependency relations does not allow the extraction of arguments that do not appear in the same sentence as their trigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016"
P17-1009,liu-etal-2014-supervised,0,0.355219,"rigger. Since the presence of incompatible arguments is an important indicator of noncoreference, our model’s failure to extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint inference approach to eve"
P17-1009,C16-1308,1,0.853677,"candidate antecedents. When applied in isolation, the model is trained to maximize the condi3.2.1.1 Trigger Detection When applied in isolation, our trigger detection model returns a distribution over possible subtypes given a candidate trigger. Each candidate trigger t is represented using t’s word, t’s lemma, word bigrams formed with a window size of three from t, as well as feature conjunctions created by pairing t’s lemma with each of the following features: 4 We train a CRF-based entity extraction model for jointly identifying the entity mentions and their types. Details can be found in Lu et al. (2016). 5 The dictionary is available from http://ir.hit.edu.cn/. An entry number in this dictionary conceptually resembles a synset id in WordNet (Fellbaum, 1998). 93 mi . These features are: (1) the pair of mi and mj ’s subtypes, (2) the pair of mj ’s subtype and mi ’s word, and (3) the pair of mi ’s subtype and mj ’s word. tional likelihood of collectively resolving the mentions to their correct antecedents in the training texts (Durrett and Klein, 2013). Below we describe the features used to represent the candidate antecedents for the mention to be resolved, mj . Features representing the N ULL"
P17-1009,H05-1004,0,0.643927,"J OINT model and the corresponding I NDEP. model. All results are expressed in terms of F-score. nearest-neighbor classifier, for event coreference. As we can see, this system achieves an AVG-F of 30.08 for event coreference and an F-score of 46.99 for trigger detection. KBP 2016 Chinese test set. Results of event coreference and trigger detection are obtained using version 1.7.2 of the official scorer provided by the KBP 2016 organizers. To evaluate event coreference performance, the scorer employs four scoring measures, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). The scorer reports event mention detection performance in terms of F-score, considering a mention correctly detected if it has an exact match with a gold mention in terms of boundary, event type, and event subtype. In addition, we report anaphoricity determination performance in terms of the F-score computed over anaphoric mentions, counting an extracted anaphoric mention as a true positive if it has an exact match with a gold anaphoric mention in terms of boundary. 4.2 Row 2 shows the performanc"
P17-1009,P14-5010,0,0.00560076,"ns. anaphoricity and trigger factors encourage anaphoric menwhen a model is run independently of other models. the head word of the entity syntactically closest to t, the head word of the entity textually closest to t, the entity type of the entity that is syntactically closest to t, and the entity type of the entity that is textually closest to t.4 In addition, for event mentions with verb triggers, we use the head words and the entity types of their subjects and objects as features, where the subjects and objects are extracted from the dependency parse trees obtained using Stanford CoreNLP (Manning et al., 2014). For event mentions with noun triggers, we create the same features that we did for verb triggers, except that we replace the subjects and verbs with heuristically extracted agents and patients. Finally, for the Chinese trigger detector, we additionally create two features from each character in t, one encoding the character itself and the other encoding the entry number of the corresponding character in a Chinese synonym dictionary.5 where θi ∈ Θ is the weight associated with feature function fi and x is the input document. 3.2 The binary Features Given that our model is a structured conditi"
P17-1009,W15-0812,0,0.0495521,"evious subsection, KBP 2016 addresses event argument detection as a separate shared task. In other words, the KBP 2016 Event Nugget Detection and Coreference task focuses solely on trigger detection and event coreference. It is worth mentioning that the KBP Event Nugget Detection and Coreference task, which started in 2015, aims to address a major weakness of the ACE 2005 event coreference task. Specifically, ACE 2005 adopts a strict notion of event identity, with which two event mentions were annotated as coreferent if and only if “they had the same agent(s), patient(s), time, and location” (Song et al., 2015), and their event attributes (polarity, modality, genericity, and tense) were not incompatible. In contrast, KBP adopts a more relaxed definition of event coreference, allowing two 2 Following the entity coreference literature, we overload the term anaphoricity, saying that an event mention is anaphoric if it is coreferent with a preceding mention in the associated text. 91 Georges Cipriani[P erson] , {left}ev1 the prison[Origin] in Ensisheim in northern France on parole on Wednesday[T ime] . He[P erson] {departed}ev2 Ensisheim[Origin] in a police vehicle[Instrument] bound for an open prison n"
P17-1009,M95-1005,0,0.826505,"016 evaluation. ∆ is the performance difference between the J OINT model and the corresponding I NDEP. model. All results are expressed in terms of F-score. nearest-neighbor classifier, for event coreference. As we can see, this system achieves an AVG-F of 30.08 for event coreference and an F-score of 46.99 for trigger detection. KBP 2016 Chinese test set. Results of event coreference and trigger detection are obtained using version 1.7.2 of the official scorer provided by the KBP 2016 organizers. To evaluate event coreference performance, the scorer employs four scoring measures, namely MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011), as well as the unweighted average of their F-scores (AVG-F). The scorer reports event mention detection performance in terms of F-score, considering a mention correctly detected if it has an exact match with a gold mention in terms of boundary, event type, and event subtype. In addition, we report anaphoricity determination performance in terms of the F-score computed over anaphoric mentions, counting an extracted anaphoric mention as a true positive if it has an exact match with a gold anaphoric mention in"
P17-1009,P15-1137,0,0.0248438,"the underlying triggers must have the same event subtype. While the use of anaphoricity information for entity coreference has been extensively studied (see Ng (2010)), to our knowledge there has thus far been no attempt to explicitly model event anaphoricity for event coreference.2 Although the mention-ranking model we employ for event coreference also allows an event mention to be posited as non-anaphoric (by resolving it to a null candidate antecedent), our decision to train a separate anaphoricity model and integrate it into our joint model is motivated in part by the recent successes of Wiseman et al. (2015), who showed that there are benefits in jointly training a noun phrase anaphoricity model and a mention-ranking model for entity coreference resolution. Finally, event anaphoricity and trigger detection can also mutually benefit each other. For instance, any verb posited as a non-trigger cannot be anaphoric, and any verb posited as anaphoric must be a trigger. Note that in our joint model, anaphoricity serves as an auxiliary task: its intended use is to improve trigger detection and event coreference, potentially mediating the interaction between trigger detection and event coreference. Being"
P17-1009,Q15-1037,0,0.247209,"o extract arguments has resulted in incorrect coreference links. 4.4.2 5 Related Work Existing event coreference resolvers have been evaluated on different corpora, such as MUC (e.g., Humphreys et al. (1997)), ACE (e.g., Ahn (2006), Chen and Ji (2009), McConky et al. (2012), Sangeetha and Arock (2012), Chen and Ng (2015, 2016), Krause et al. (2016)), OntoNotes (e.g., Chen et al. (2011)), the Intelligence Community corpus (e.g., Cybulska and Vossen (2012), Araki et al. (2014), Liu et al. (2014)), the ECB corpus (e.g., Lee et al. (2012), Bejan and Harabagiu (2014)) and its extension ECB+ (e.g., Yang et al. (2015)), and ProcessBank (e.g., Araki and Mitamura (2015)). The newest event coreference corpora are the ones used in the KBP 2015 and 2016 Event Nugget Detection and Coreference shared tasks, in which the best performers in 2015 and 2016 are RPI’s system (Hong et al., 2015) and UTD’s system (Lu and Ng, 2016), respectively. The KBP 2015 corpus has recently been used to evaluate Peng et al.’s (2016) minimally supervised approach and Lu et al.’s (2016) joint inference approach to event coreference. With the rarest exceptions (e.g., Lu et al. (2016)), existing resolvers have adopted a pipeline architec"
P17-1009,P10-1142,1,0.831311,"ncerned with realis classification, as it does not play any role in event coreference. 90 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 90–101 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1009 2 Definitions, Task, and Corpora trigger detection. For instance, if two event mentions are posited as coreferent, then the underlying triggers must have the same event subtype. While the use of anaphoricity information for entity coreference has been extensively studied (see Ng (2010)), to our knowledge there has thus far been no attempt to explicitly model event anaphoricity for event coreference.2 Although the mention-ranking model we employ for event coreference also allows an event mention to be posited as non-anaphoric (by resolving it to a null candidate antecedent), our decision to train a separate anaphoricity model and integrate it into our joint model is motivated in part by the recent successes of Wiseman et al. (2015), who showed that there are benefits in jointly training a noun phrase anaphoricity model and a mention-ranking model for entity coreference resol"
P17-1009,D16-1038,0,0.374072,"Missing"
P18-1058,C16-1324,0,0.0886251,"Missing"
P18-1058,P17-1002,0,0.0429836,"students, as the attribute values predicted by these systems can help a student understand why her essay receives a particular persuasiveness score. To our knowledge, this is the first corpus of essays that are simultaneously annotated with argument components, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whether one argument is more pe"
P18-1058,E17-1070,0,0.0200156,"argument’s persuasiveness: in ranking, a system simply determines whether one argument is more persuasive than the other, but not how much more persuasive one argument is than the other; in scoring, however, a system has to determine how persuasive an argument is on an absolute scale. Note that ranking is not an acceptable evaluation setting for studying argument persuasiveness in the essay domain, as feedback for an essay has to be provided independently of other essays. In contrast, there are studies that focus on factors affecting argument persuasiveness in internet debates. For instance, Lukin et al. (2017) examine how audience variables (e.g., personalities) interact with argument style (e.g., factual vs. emotional arguments) to affect argument persuasive3 Corpus The corpus we chose to annotate is composed of 102 essays randomly chosen from the Argument Annotated Essays corpus (Stab and Gurevych, 2014a). This collection of essays was taken from essayforum3 , a site offering feedback to students wishing to improve their ability to write persuasive essays for tests. Each essay is written in response to a topic such as “should high school make music lessons compulsory?” and has already been annota"
P18-1058,D10-1023,1,0.930837,"Missing"
P18-1058,P13-1026,1,0.913028,"on to how persuasive they are. 1 Introduction The vast majority of existing work on automated essay scoring has focused on holistic scoring, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer (see Shermis and Burstein (2013) for the state of the art). While recent attempts address this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Persing and Ng, 2014), organization (Persing et al., 2010), and thesis clarity (Persing and Ng, 2013), argument persuasiveness is largely ignored in existing automated essay scoring research despite being one of the most important dimensions of essay quality. Nevertheless, scoring the persuasiveness of arguments in student essays is by no means easy. 621 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 621–631 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness. Persing and Ng (2017) identify factors that negatively impact persuasiveness, so their factors, unlike ours, cannot explain what m"
P18-1058,D16-1129,0,0.0256033,"usly annotated with argument components, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whether one argument is more persuasive than the other, but not how much more persuasive one argument is than the other; in scoring, however, a system has to determine how persuasive an argument is on an absolute scale. Note that ranking is not an acceptable ev"
P18-1058,P14-1144,1,0.903148,"useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are. 1 Introduction The vast majority of existing work on automated essay scoring has focused on holistic scoring, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer (see Shermis and Burstein (2013) for the state of the art). While recent attempts address this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Persing and Ng, 2014), organization (Persing et al., 2010), and thesis clarity (Persing and Ng, 2013), argument persuasiveness is largely ignored in existing automated essay scoring research despite being one of the most important dimensions of essay quality. Nevertheless, scoring the persuasiveness of arguments in student essays is by no means easy. 621 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 621–631 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness. Persing and Ng (2017) identify factors that negati"
P18-1058,P16-1150,0,0.0294469,"usly annotated with argument components, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whether one argument is more persuasive than the other, but not how much more persuasive one argument is than the other; in scoring, however, a system has to determine how persuasive an argument is on an absolute scale. Note that ranking is not an acceptable ev"
P18-1058,P15-1053,1,0.866421,"tudent Essays Winston Carlile Nishant Gurrapadi Zixuan Ke Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {winston,zixuan,vince}@hlt.utdallas.edu,Nishant.Gurrapadi@utdallas.edu Abstract The difficulty stems in part from the scarcity of persuasiveness-annotated corpora of student essays. While persuasiveness-annotated corpora exist for other domains such as online debates (e.g., Habernal and Gurevych (2016a; 2016b)), to our knowledge only one corpus of persuasivenessannotated student essays has been made publicly available so far (Persing and Ng, 2015). Though a valuable resource, Persing and Ng’s (2015) (P&N) corpus has several weaknesses that limit its impact on automated essay scoring research. First, P&N assign only one persuasiveness score to each essay that indicates the persuasiveness of the argument an essay makes for its thesis. However, multiple arguments are typically made in a persuasive essay. Specifically, the arguments of an essay are typically structured as an argument tree, where the major claim, which is situated at the root of the tree, is supported by one or more claims (the children of the root node), each of which is i"
P18-1058,W17-5102,0,0.025398,"ay quality. Nevertheless, scoring the persuasiveness of arguments in student essays is by no means easy. 621 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 621–631 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness. Persing and Ng (2017) identify factors that negatively impact persuasiveness, so their factors, unlike ours, cannot explain what makes an argument persuasive. Other argument mining tasks. Some of the attributes that we annotate our corpus with have been studied. For instance, Hidey et al. (2017) examine the different semantic types of claims and premises, whereas Higgins and Walker (2012) investigate persuasion strategies (i.e., ethos, pathos, logos). Unlike ours, these studies use data from online debate forums and social/environment reports. Perhaps more importantly, they study these attributes independently of persuasiveness. Several argument mining tasks have recently been proposed. For instance, Stab and Gurevych (2017b) examine the task of whether an argument is sufficiently supported. Al Khatib et al. (2016) identify and annotate a news editorial corpus with fine-grained argum"
P18-1058,C14-1142,0,0.29892,"t can provide useful feedback to students, as the attribute values predicted by these systems can help a student understand why her essay receives a particular persuasiveness score. To our knowledge, this is the first corpus of essays that are simultaneously annotated with argument components, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whe"
P18-1058,N04-1024,0,0.111409,"asiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are. 1 Introduction The vast majority of existing work on automated essay scoring has focused on holistic scoring, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer (see Shermis and Burstein (2013) for the state of the art). While recent attempts address this problem by scoring a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Persing and Ng, 2014), organization (Persing et al., 2010), and thesis clarity (Persing and Ng, 2013), argument persuasiveness is largely ignored in existing automated essay scoring research despite being one of the most important dimensions of essay quality. Nevertheless, scoring the persuasiveness of arguments in student essays is by no means easy. 621 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 621–631 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness. Persing and Ng (2017) ident"
P18-1058,D14-1006,0,0.585315,"t can provide useful feedback to students, as the attribute values predicted by these systems can help a student understand why her essay receives a particular persuasiveness score. To our knowledge, this is the first corpus of essays that are simultaneously annotated with argument components, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whe"
P18-1058,J17-3005,0,0.067187,"annot explain what makes an argument persuasive. Other argument mining tasks. Some of the attributes that we annotate our corpus with have been studied. For instance, Hidey et al. (2017) examine the different semantic types of claims and premises, whereas Higgins and Walker (2012) investigate persuasion strategies (i.e., ethos, pathos, logos). Unlike ours, these studies use data from online debate forums and social/environment reports. Perhaps more importantly, they study these attributes independently of persuasiveness. Several argument mining tasks have recently been proposed. For instance, Stab and Gurevych (2017b) examine the task of whether an argument is sufficiently supported. Al Khatib et al. (2016) identify and annotate a news editorial corpus with fine-grained argumentative discourse units for the purpose of analyzing the argumentation strategies used to persuade readers. Wachsmuth et al. (2017) focus on identifying and annotating 15 logical, rhetorical, and dialectical dimensions that would be useful for automatically accessing the quality of an argument. Most recently, the Argument Reasoning Comprehension task organized as part of SemEval 2018 has focused on selecting the correct warrant that"
P18-1058,E17-1092,0,0.029376,"annot explain what makes an argument persuasive. Other argument mining tasks. Some of the attributes that we annotate our corpus with have been studied. For instance, Hidey et al. (2017) examine the different semantic types of claims and premises, whereas Higgins and Walker (2012) investigate persuasion strategies (i.e., ethos, pathos, logos). Unlike ours, these studies use data from online debate forums and social/environment reports. Perhaps more importantly, they study these attributes independently of persuasiveness. Several argument mining tasks have recently been proposed. For instance, Stab and Gurevych (2017b) examine the task of whether an argument is sufficiently supported. Al Khatib et al. (2016) identify and annotate a news editorial corpus with fine-grained argumentative discourse units for the purpose of analyzing the argumentation strategies used to persuade readers. Wachsmuth et al. (2017) focus on identifying and annotating 15 logical, rhetorical, and dialectical dimensions that would be useful for automatically accessing the quality of an argument. Most recently, the Argument Reasoning Comprehension task organized as part of SemEval 2018 has focused on selecting the correct warrant that"
P18-1058,E17-1017,0,0.0309345,"ersuasion strategies (i.e., ethos, pathos, logos). Unlike ours, these studies use data from online debate forums and social/environment reports. Perhaps more importantly, they study these attributes independently of persuasiveness. Several argument mining tasks have recently been proposed. For instance, Stab and Gurevych (2017b) examine the task of whether an argument is sufficiently supported. Al Khatib et al. (2016) identify and annotate a news editorial corpus with fine-grained argumentative discourse units for the purpose of analyzing the argumentation strategies used to persuade readers. Wachsmuth et al. (2017) focus on identifying and annotating 15 logical, rhetorical, and dialectical dimensions that would be useful for automatically accessing the quality of an argument. Most recently, the Argument Reasoning Comprehension task organized as part of SemEval 2018 has focused on selecting the correct warrant that explains reasoning of an argument that consists of a claim and a reason.2 gument in each essay (rather than simply the persuasiveness of the overall argument), but we also identify a set of attributes that can explain an argument’s persuasiveness and annotate each argument with the values of t"
P18-1058,P16-2032,0,0.0376427,"nents, persuasiveness scores, and related attributes.1 2 Related Work While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) (Stab and Gurevych, 2014b, 2017a; Eger et al., 2017), researchers have recently begun to study new argument mining tasks, as described below. Persuasiveness-related tasks. Most related to our study is work involving argument persuasiveness. For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t. their persuasiveness. As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument’s persuasiveness: in ranking, a system simply determines whether one argument is more persuasive than the other, but not how much more persuasive one argument is than the other; in scoring, however, a system has to determine how persuasive an argument is on an absolute scale. Note that ranking is not an acceptable evaluation setting for stu"
P18-1065,P07-1056,0,0.00882122,"Missing"
P18-1065,N18-2095,0,0.514723,"y ignored in current helpfulness prediction systems. 1 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 2 http://jmcauley.ucsd.edu/data/amazon/ 3 https://www.cse.msu.edu/˜tangjili/ trust.html 699 3 Helpfulness Prediction (Kim et al., 2006; Hong et al., 2012; Zeng et al., 2014; Krishnamoorthy, 2015), but thresholded linear regression models (Ghose and Ipeirotis, 2011), Naive Bayes, Random Forests, J48 and JRip have also been used (O’Mahony et al., 2010; Ghose and Ipeirotis, 2011; Krishnamoorthy, 2015). Recent work has also approached this task with neural networks (Malik and Hussain, 2017; Chen et al., 2018). Regarding ranking, some researchers have used ranking-specific methods such as SVM ranking (Tsur and Rappoport, 2009; Hong et al., 2012), but others have attempted to recover rankings from classification (O’Mahony and Smyth, 2009, 2010) or regression (Mukherjee et al., 2017) outputs. Helpfulness prediction tasks include score regression (predicting the helpfulness score h ∈ [0, 1] of a review), binary review classification (classifying a review as helpful or not), and review ranking (ordering a set of reviews by their helpfulness). In this section, we present the evaluation measures and appr"
P18-1065,D17-1142,0,0.302909,"se findings are relevant not only because they can be used to enhance helpfulness prediction, but because, when put together, they constitute arguments in favor of reconsidering the helpfulness prediction task and its focus. In this section, we will present a variety of moderating factors. The State of Helpfulness Prediction The classical approach to helpfulness prediction has consisted of finding new hand-crafted features that can improve system performance. Although many interesting features continue to be found (e.g., emotion (Martin and Pu, 2014), aspect (Yang et al., 2016), and argument (Liu et al., 2017) based features), advances have been hindered by the lack 4 We do not discuss features that are not helpful since, in general, they are not as thoroughly tested as those mentioned here. 702 4.1 The Voting Process and its Entities 3. As variance becomes large, reviews with star ratings both above and below the average are more helpful (positive reviews still deemed somewhat more helpful). To start our discussion on moderating factors, let us provide a brief, intuitive definition of the steps involved in the helpfulness voting process and outline the entities involved in it5 : 1. A reviewer, a,"
P18-1065,D07-1035,0,0.825848,"f testing on pre-collected datasets has made system comparisons difficult. The majority of researchers simply use helpfulness scores (the fraction of users who vote a review as helpful) as found in websites as ground truth for system training and evaluation. Given that these scores are volatile when reviews have few votes, researchers frequently filter out reviews Figure 1: Example Review that do not have a minimum number of votes. Some researchers have argued that helpfulness scores might not be good indicators of actual helpfulness, and have resorted to rating or ranking reviews themselves (Liu et al., 2007; Tsur and Rappoport, 2009; Yang et al., 2015), but these approaches are not the norm. Researchers have observed interesting patterns in review datasets. For instance, positive reviews are more likely to have high helpfulness scores (O’Mahony et al., 2010; Huang et al., 2015), top ranking reviews hold a disproportionate amount of votes when compared to lower-ranked reviews (Liu et al., 2007), and more recent reviews tend to get fewer votes than older reviews (Liu et al., 2007). Although some of these effects may be the consequence of website voting mechanisms (e.g., Amazon shows reviews based"
P18-1065,W06-1650,0,0.843584,"tains only a couple of judgments on its product, 97 out of 102 people voted it as helpful (0.95 score). The quality of this review does not seem to match its near-perfect score. As we will see in Section 4, these discrepancies could be explained as the consequence of several moderating factors, which have a direct influence on the helpfulness voting process but are largely ignored in current helpfulness prediction systems. 1 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 2 http://jmcauley.ucsd.edu/data/amazon/ 3 https://www.cse.msu.edu/˜tangjili/ trust.html 699 3 Helpfulness Prediction (Kim et al., 2006; Hong et al., 2012; Zeng et al., 2014; Krishnamoorthy, 2015), but thresholded linear regression models (Ghose and Ipeirotis, 2011), Naive Bayes, Random Forests, J48 and JRip have also been used (O’Mahony et al., 2010; Ghose and Ipeirotis, 2011; Krishnamoorthy, 2015). Recent work has also approached this task with neural networks (Malik and Hussain, 2017; Chen et al., 2018). Regarding ranking, some researchers have used ranking-specific methods such as SVM ranking (Tsur and Rappoport, 2009; Hong et al., 2012), but others have attempted to recover rankings from classification (O’Mahony and Smyt"
P18-1065,P15-2007,0,0.541258,"system comparisons difficult. The majority of researchers simply use helpfulness scores (the fraction of users who vote a review as helpful) as found in websites as ground truth for system training and evaluation. Given that these scores are volatile when reviews have few votes, researchers frequently filter out reviews Figure 1: Example Review that do not have a minimum number of votes. Some researchers have argued that helpfulness scores might not be good indicators of actual helpfulness, and have resorted to rating or ranking reviews themselves (Liu et al., 2007; Tsur and Rappoport, 2009; Yang et al., 2015), but these approaches are not the norm. Researchers have observed interesting patterns in review datasets. For instance, positive reviews are more likely to have high helpfulness scores (O’Mahony et al., 2010; Huang et al., 2015), top ranking reviews hold a disproportionate amount of votes when compared to lower-ranked reviews (Liu et al., 2007), and more recent reviews tend to get fewer votes than older reviews (Liu et al., 2007). Although some of these effects may be the consequence of website voting mechanisms (e.g., Amazon shows reviews based on their helpfulness), they should be taken in"
P18-1065,O14-3002,0,0.0898362,"ts product, 97 out of 102 people voted it as helpful (0.95 score). The quality of this review does not seem to match its near-perfect score. As we will see in Section 4, these discrepancies could be explained as the consequence of several moderating factors, which have a direct influence on the helpfulness voting process but are largely ignored in current helpfulness prediction systems. 1 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 2 http://jmcauley.ucsd.edu/data/amazon/ 3 https://www.cse.msu.edu/˜tangjili/ trust.html 699 3 Helpfulness Prediction (Kim et al., 2006; Hong et al., 2012; Zeng et al., 2014; Krishnamoorthy, 2015), but thresholded linear regression models (Ghose and Ipeirotis, 2011), Naive Bayes, Random Forests, J48 and JRip have also been used (O’Mahony et al., 2010; Ghose and Ipeirotis, 2011; Krishnamoorthy, 2015). Recent work has also approached this task with neural networks (Malik and Hussain, 2017; Chen et al., 2018). Regarding ranking, some researchers have used ranking-specific methods such as SVM ranking (Tsur and Rappoport, 2009; Hong et al., 2012), but others have attempted to recover rankings from classification (O’Mahony and Smyth, 2009, 2010) or regression (Mukherje"
P19-1390,Q17-1010,0,0.011075,"Missing"
P19-1390,P18-1058,1,0.835477,"s by high school students is also pub¨ licly available (Ostling et al., 2013). Dimension-specific scoring. The Argument Annotated Essays corpus contains 402 essays taken from essayforum2, a site offering feedback to students wishing to improve their ability to write persuasive essays for tests (Stab and Gurevych, 2014). Each essay in the corpus is annotated with its argumentative structure (i.e., argument components such as claims and premises as well as the relationships between them (e.g., support, attack)). The corpus has been used extensively to evaluate argument mining systems. Recently, Carlile et al. (2018) annotated each argument in 102 essays randomly selected from the corpus with its persuasiveness score. There are two corpora of essays that are scored along multiple dimensions of quality. Horbach et al. (2017) annotated a corpus of 2200 German essays written by prospective university students. Each essay is a summary of a given news article and is manually scored w.r.t. coherence, organiza2 Our annotated corpus is publicly available at the website http://www.hlt.utdallas.edu/˜zixuan/ EssayScoring. 3995 tion, argumentation, style, and grammar. Neither the essays nor the annotations are public"
P19-1390,K17-1017,0,0.0793128,"yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide feedback to students on why her essay receives a particular thesis strength score. 1 Introduction Recent work on automated essay scoring has largely focused on holistic scoring, which summarizes the quality of an essay with a single score (e.g., Taghipour and Ng (2016), Dong et al. (2017), Wang et al. (2018)). There are at least two reasons for this focus. First, corpora manually annotated with holistic scores such as the one used in the Kaggle-sponsored ASAP competition1 are publicly available, facilitating the training and evaluation of holistic essay scoring engines. Second, holistic scoring technologies are commercially valuable: being able to successfully automate the scoring of the millions of essays written for aptitude tests such as SAT, GRE, and GMAT every year can save a lot of manual grading effort. However, holistic essay scoring technologies are far from adequate"
P19-1390,W10-1013,0,0.0356315,"coring technologies are far from adequate for use in classroom settings, where providing students with feedback on how to improve their essays is of utmost importance. 1 https://www.kaggle.com/c/asap-aes Specifically, merely returning a low holistic score to an essay provides essentially no feedback to its author on which aspect(s) of the essay contributed to the low score and how it can be improved. Recently, researchers have attempted to score a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Louis and Higgins, 2010; Persing and Ng, 2014), organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), and argument persuasiveness (Persing and Ng, 2015; Ke et al., 2018). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Providing scores along different dimensions of essay quality could help an author identify which aspects of her essay need improvements. Unfortunately, progress in dimensionspecific essay scoring research is hampered in part by the lack of annotated corpora needed"
P19-1390,W13-1705,0,0.0484512,"Missing"
P19-1390,P17-1002,0,0.0287415,"Missing"
P19-1390,D10-1023,1,0.917466,"Missing"
P19-1390,P13-1026,1,0.871695,"h feedback on how to improve their essays is of utmost importance. 1 https://www.kaggle.com/c/asap-aes Specifically, merely returning a low holistic score to an essay provides essentially no feedback to its author on which aspect(s) of the essay contributed to the low score and how it can be improved. Recently, researchers have attempted to score a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Louis and Higgins, 2010; Persing and Ng, 2014), organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), and argument persuasiveness (Persing and Ng, 2015; Ke et al., 2018). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Providing scores along different dimensions of essay quality could help an author identify which aspects of her essay need improvements. Unfortunately, progress in dimensionspecific essay scoring research is hampered in part by the lack of annotated corpora needed to train and evaluate systems for scoring essays along specific dimensions of essay quality. Motiv"
P19-1390,N04-1024,0,0.107784,"ever, holistic essay scoring technologies are far from adequate for use in classroom settings, where providing students with feedback on how to improve their essays is of utmost importance. 1 https://www.kaggle.com/c/asap-aes Specifically, merely returning a low holistic score to an essay provides essentially no feedback to its author on which aspect(s) of the essay contributed to the low score and how it can be improved. Recently, researchers have attempted to score a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Louis and Higgins, 2010; Persing and Ng, 2014), organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), and argument persuasiveness (Persing and Ng, 2015; Ke et al., 2018). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Providing scores along different dimensions of essay quality could help an author identify which aspects of her essay need improvements. Unfortunately, progress in dimensionspecific essay scoring research is hampered in part by the lack of"
P19-1390,P14-1144,1,0.92968,"ar from adequate for use in classroom settings, where providing students with feedback on how to improve their essays is of utmost importance. 1 https://www.kaggle.com/c/asap-aes Specifically, merely returning a low holistic score to an essay provides essentially no feedback to its author on which aspect(s) of the essay contributed to the low score and how it can be improved. Recently, researchers have attempted to score a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Louis and Higgins, 2010; Persing and Ng, 2014), organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), and argument persuasiveness (Persing and Ng, 2015; Ke et al., 2018). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Providing scores along different dimensions of essay quality could help an author identify which aspects of her essay need improvements. Unfortunately, progress in dimensionspecific essay scoring research is hampered in part by the lack of annotated corpora needed to train and evaluate"
P19-1390,W17-5040,0,0.0157269,"k to students wishing to improve their ability to write persuasive essays for tests (Stab and Gurevych, 2014). Each essay in the corpus is annotated with its argumentative structure (i.e., argument components such as claims and premises as well as the relationships between them (e.g., support, attack)). The corpus has been used extensively to evaluate argument mining systems. Recently, Carlile et al. (2018) annotated each argument in 102 essays randomly selected from the corpus with its persuasiveness score. There are two corpora of essays that are scored along multiple dimensions of quality. Horbach et al. (2017) annotated a corpus of 2200 German essays written by prospective university students. Each essay is a summary of a given news article and is manually scored w.r.t. coherence, organiza2 Our annotated corpus is publicly available at the website http://www.hlt.utdallas.edu/˜zixuan/ EssayScoring. 3995 tion, argumentation, style, and grammar. Neither the essays nor the annotations are publicly available, however. The second corpus is the International Corpus of Learner English (ICLE) (Granger et al., 2009). ICLE is composed of essays written by university undergraduates. Approximately 1000 persuasi"
P19-1390,P15-1053,1,0.955676,"st importance. 1 https://www.kaggle.com/c/asap-aes Specifically, merely returning a low holistic score to an essay provides essentially no feedback to its author on which aspect(s) of the essay contributed to the low score and how it can be improved. Recently, researchers have attempted to score a particular dimension of essay quality such as coherence (Miltsakaki and Kukich, 2004), technical errors, relevance to prompt (Higgins et al., 2004; Louis and Higgins, 2010; Persing and Ng, 2014), organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), and argument persuasiveness (Persing and Ng, 2015; Ke et al., 2018). Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion (Burstein et al., 2004) have also begun to emerge. Providing scores along different dimensions of essay quality could help an author identify which aspects of her essay need improvements. Unfortunately, progress in dimensionspecific essay scoring research is hampered in part by the lack of annotated corpora needed to train and evaluate systems for scoring essays along specific dimensions of essay quality. Motivated by this observation, we aim to contribute to d"
P19-1390,C14-1142,0,0.0840762,"en for 10 prompts by ESOL test takers. Each essay is not only holistically scored but also annotated with different kinds of errors, and therefore the corpus can also be used for grammatical error detection and correction. A Swedish corpus containing 1702 holistically scored essays written for 19 prompts by high school students is also pub¨ licly available (Ostling et al., 2013). Dimension-specific scoring. The Argument Annotated Essays corpus contains 402 essays taken from essayforum2, a site offering feedback to students wishing to improve their ability to write persuasive essays for tests (Stab and Gurevych, 2014). Each essay in the corpus is annotated with its argumentative structure (i.e., argument components such as claims and premises as well as the relationships between them (e.g., support, attack)). The corpus has been used extensively to evaluate argument mining systems. Recently, Carlile et al. (2018) annotated each argument in 102 essays randomly selected from the corpus with its persuasiveness score. There are two corpora of essays that are scored along multiple dimensions of quality. Horbach et al. (2017) annotated a corpus of 2200 German essays written by prospective university students. Ea"
P19-1390,D16-1193,0,0.0576433,"for scoring an important, yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide feedback to students on why her essay receives a particular thesis strength score. 1 Introduction Recent work on automated essay scoring has largely focused on holistic scoring, which summarizes the quality of an essay with a single score (e.g., Taghipour and Ng (2016), Dong et al. (2017), Wang et al. (2018)). There are at least two reasons for this focus. First, corpora manually annotated with holistic scores such as the one used in the Kaggle-sponsored ASAP competition1 are publicly available, facilitating the training and evaluation of holistic essay scoring engines. Second, holistic scoring technologies are commercially valuable: being able to successfully automate the scoring of the millions of essays written for aptitude tests such as SAT, GRE, and GMAT every year can save a lot of manual grading effort. However, holistic essay scoring technologies ar"
P19-1390,D18-1090,0,0.0128722,"nsion of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide feedback to students on why her essay receives a particular thesis strength score. 1 Introduction Recent work on automated essay scoring has largely focused on holistic scoring, which summarizes the quality of an essay with a single score (e.g., Taghipour and Ng (2016), Dong et al. (2017), Wang et al. (2018)). There are at least two reasons for this focus. First, corpora manually annotated with holistic scores such as the one used in the Kaggle-sponsored ASAP competition1 are publicly available, facilitating the training and evaluation of holistic essay scoring engines. Second, holistic scoring technologies are commercially valuable: being able to successfully automate the scoring of the millions of essays written for aptitude tests such as SAT, GRE, and GMAT every year can save a lot of manual grading effort. However, holistic essay scoring technologies are far from adequate for use in classroom"
P19-1390,P11-1019,0,0.0398003,"interact and facilitate the design of joint models that capture such interactions. Unfortunately, existing essay annotations are spread over Related Work In this section, we provide an overview of the popularly used annotated essay corpora for scoring. Holistic scoring. As mentioned before, the ASAP corpus, which was produced as part of a Kaggle competition, has recently been used extensively to evaluate holistic essay scoring systems. It contains holistically scored essays written for eight prompts by American students from grades 7 through 10, with 1190−3000 essays for each prompt. CLC-FCE (Yannakoudakis et al., 2011) is a relatively small corpus that contains 1244 essays written for 10 prompts by ESOL test takers. Each essay is not only holistically scored but also annotated with different kinds of errors, and therefore the corpus can also be used for grammatical error detection and correction. A Swedish corpus containing 1702 holistically scored essays written for 19 prompts by high school students is also pub¨ licly available (Ostling et al., 2013). Dimension-specific scoring. The Argument Annotated Essays corpus contains 402 essays taken from essayforum2, a site offering feedback to students wishing to"
P19-1390,P17-1144,0,0.025752,"nder-emphasized issue is which corpus of essays we should annotate. We envision that in the long run, substantial progress in this area of research can only be made if different researchers on automated essay grading create their annotations on the same corpus of essays. For instance, having a corpus of essays that are scored along different dimensions of quality, such as organization, prompt adherence, and thesis strength will facilitate the study of how these dimensions interact with each other to produce a holistic score. As another example, researchers working on automated essay revision (Zhang et al., 2017), where the goal is to revise, for instance, a thesis statement or an argument in an essay to make it stronger, would benefit from having the thesis strength scores we annotate. Specifically, the first step in deciding how to revise a thesis statement to make it stronger is to understand why it is weak, and the aforementioned attributes that we propose to annotate will provide insights into what makes a thesis statement weak and subsequently how to revise it. So, having both the attributes and the revised thesis annotated on the same set of essays will allow researchers to study how they inter"
S15-2146,S13-2096,0,0.446603,"abeling and relation classification of spatial elements. Prior systems have adopted either a pipeline approach or a joint approach to these subtasks. Given an n-tuple of distinct spatial elements in a sentence, a pipeline spatial 2 In the ISO-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. 863 relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier w"
S15-2146,de-marneffe-etal-2006-generating,0,0.118268,"Missing"
S15-2146,kordjamshidi-etal-2010-spatial,0,0.217197,"Missing"
S15-2146,S12-1048,0,0.654891,"1 was organized as a shared task for the semantic evaluation of spatial information extraction (IE) systems. The goals of the shared task include identifying and classifying particular constructions in natural language for expressing spatial information that are conveyed through the spatial concepts of locations, entities participating in spatial relations, paths, topological relations, direction and orientation, motion, etc. It presents a wide spectrum of spatial IE related subtasks for interested participants to choose from, building on the two previous years shared tasks on the same topic (Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Our goal in this paper is to describe the version of our spatial relation extraction system that partic1 http://alt.qcri.org/semeval2015/task8/ ipated in subtask 3a of SpaceEval. Systems participating in this subtask assume as input the spatial elements in a text document. For example, in the sentence The flower is in the vase1 and the vase2 is on the table, the set of spatial elements {flower, in, vase1 , vase2 , on, table} are given and subsequently used as candidates for predicting spatial relations. Leveraging the successes of a joint role-labeling approach to s"
S15-2146,S12-1056,0,0.221709,"-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. 863 relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that"
S15-2146,W13-0119,0,0.403743,"Missing"
W02-1008,W97-0319,0,\N,Missing
W02-1008,M95-1005,0,\N,Missing
W02-1008,N01-1008,0,\N,Missing
W02-1008,P95-1017,0,\N,Missing
W02-1008,P02-1014,1,\N,Missing
W02-1008,J01-4004,0,\N,Missing
W02-1008,W99-0611,1,\N,Missing
W03-1015,P01-1005,0,0.0313355,"Missing"
W03-1015,W99-0613,0,0.240775,"ut a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different lea"
W03-1015,P02-1045,0,0.0479292,". Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou an"
W03-1015,W02-1008,1,0.57556,"bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method. 2 Noun Phrase Coreference Resolution Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.2 In this section, we give an overview of the coreference resolution system to which the boot2 Concrete examples of the coreference task can be found in MUC-6 (1995) and MUC-7 (1998). strapping algorithms will be applied. The framework underlying the coreference system is a standard combination of classification and clustering (see Ng and Cardie (2002) for details). Coreference resolution is first recast as a classification task, in which a pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. When the system operates within the weakly supervised setting, a weakly supervised algorithm bootstraps the coreference classifier from the given labeled and unlabeled data rather than from a much larger set of labeled instances. The clustering algor"
W03-1015,N03-1023,1,0.717456,"e-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. 100 Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 2 on the 20–30 “formal evaluation” texts for each of the MUC-6 and MUC-7 data sets. Two sets of experiments are conducted, one using naive Bayes as the underlying supervised learning algorithm and the other the decision list learner. All results reported are"
W03-1015,W01-0501,1,0.933763,"Naive Bayes Decision List R P F R P F 50.7 52.6 51.6 17.9 72.0 28.7 33.3 90.7 48.7 19.5 71.2 30.6 53.6 79.0 63.9 40.1 83.1 54.1 48.3 63.5 54.9 18.7 70.8 29.6 MUC-7 Naive Bayes Decision List R P F R P F 40.1 40.2 40.1 32.4 78.3 45.8 32.9 76.3 46.0 32.4 78.3 45.8 43.5 73.2 54.6 38.3 75.4 50.8 40.1 40.2 40.1 32.9 78.1 46.3 Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. 100 Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is perf"
W03-1015,N03-1031,0,0.122407,"algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled"
W03-1015,E03-1008,0,0.129526,"algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled"
W03-1015,M95-1005,0,0.28504,"Missing"
W03-1015,P95-1026,0,0.0569571,"d with instances from the unlabeled data and the process is repeated. During testing, each classifier makes an independent decision for a test instance. In this paper, the decision associated with the higher confidence is taken to be the final prediction for the instance. 4 4.2 Experimental Setup P (y |fi = vj ) = N (fi = vj , y) + α N (fi = vj ) + kα Multi-View Co-Training In this section, we describe the Blum and Mitchell (B&M) multi-view co-training algorithm and apply it to coreference resolution. 3 This justifies the use of a decision list as a potential classifier for bootstrapping. See Yarowsky (1995) for details. One of the goals of the experiments is to enable a fair comparison of the multi-view algorithm with our single-view bootstrapping algorithm. Since the B&M co-training algorithm is sensitive not only to the views employed but also to other input parameExperiments Baseline Multi-view Co-Training Single-view Bootstrapping Self-Training MUC-6 Naive Bayes Decision List R P F R P F 50.7 52.6 51.6 17.9 72.0 28.7 33.3 90.7 48.7 19.5 71.2 30.6 53.6 79.0 63.9 40.1 83.1 54.1 48.3 63.5 54.9 18.7 70.8 29.6 MUC-7 Naive Bayes Decision List R P F R P F 40.1 40.2 40.1 32.4 78.3 45.8 32.9 76.3 46."
W09-2211,P07-1056,0,0.0267025,"Missing"
W09-2211,W06-1647,0,0.0288487,"Language Processing, pages 84–85, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community needs to perform more research of this kind, which f"
W09-2211,P08-1103,0,0.0321878,"loyed in a semi-supervised setting since the early days of the statistical revolution in NLP, we advocate the use of discriminative models. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discrimin"
W09-2211,P06-1096,0,0.0289675,"dle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, cotraining (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others acros"
W09-2211,C08-1071,0,0.0268611,"sductive SVM. It would be interesting to see whether EM-SVM can beat existing semisupervised learners for other NLP tasks. 84 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008)"
W09-2211,N03-1023,1,0.828973,"ther EM-SVM can beat existing semisupervised learners for other NLP tasks. 84 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which"
W09-2211,W01-0501,0,0.0411159,"Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing. We believe that the NLP community need"
W09-2211,P04-1007,0,0.0335381,"s. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, cotraining (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, non"
W09-2211,W04-3201,0,0.0193391,"advocate the use of discriminative models. The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts. Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, cotraining (Blum and Mitchell, 1998), and transdu"
W09-2211,P05-1073,0,0.0803218,"Missing"
W09-2211,W06-1668,0,0.017248,"o offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006). While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a “no prior”, agnostic learning setting. See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models. 2 Discriminative EM? A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, cotraining (Blum and Mitchell, 1998), and transductive SVM (Joachims, 1999). However, none of them seems to outperform the others across different domains, and each has its pros and cons. Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data poi"
W09-2211,P07-1004,0,0.0308609,"arners for other NLP tasks. 84 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Effectiveness of Bootstrapping How effective are the aforementioned semisupervised learning systems in bootstrapping from small amounts of labeled data? While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueffing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)). Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration). To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem. Recently, McClosky et al. (2008) have characterized the conditions under which self-training would be effective for semi-super"
W12-4504,D09-1120,0,0.0908766,"eve posits two mentions as coreferent if two conditions are satisfied: (1) they are both pronouns; and (2) they are produced by the same speaker. Sieves are ordered by their precision, with the most precise sieve appearing first. To resolve a set of mentions in a document, the resolver makes multiple passes over them: in the i-th pass, it attempts to use only the rules in the i-th sieve to find an antecedent for each mention mk . Specifically, when searching for an antecedent for mk , its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). The partial clustering of the mentions created in the i-th pass is then passed to the i+1-th pass. Hence, later passes can exploit the information computed by previous passes, but a coreference link established earlier cannot be overridden later. 3.2 The Sieves 3.2.1 Sieves for English Our sieves for English are modeled after those employed by the Stanford resolver (Lee et al., 2011), which is composed of 12 sieves.1 Since we participated in the closed track, we re-implemented the 10 sieves that do not exploit external knowledge sources. These 10 sieves are listed under the &quot;English&quot; column"
W12-4504,W11-1902,0,0.308682,"Missing"
W12-4504,P04-1020,1,0.768353,"-based methods and learning-based methods each have their unique strengths. As shown by the Stanford coreference resolver (Lee et al., 2011), the winner of last year&apos;s shared task, many coreference relations in OntoNotes can be identified using a fairly small set of simple hand-crafted rules. On the other hand, our prior work on machine learning for coreference resolution suggests that coreference-annotated data can be profitably exploited to (1) induce lexical features (Rahman and Ng, 2011a, 2011b) and (2) optimize system parameters with respect to the desired coreference evaluation measure (Ng, 2004, 2009). Our system employs a fairly standard architecture, performing mention detection prior to coreference resolution. As we will see, however, the parameters of these two components are optimized jointly with respect to the desired evaluation measure. In the rest of this paper, we describe the mention detection component (Section 2) and the coreference resolution component (Section 3), show how their parameters are jointly optimized (Section 4), and present evaluation results on the development set and the official test set (Section 5). 2 Mention Detection To build a mention detector that"
W12-4504,N09-1065,1,0.918287,"Missing"
W12-4504,W12-4501,0,0.157485,"cchen,vince}@hlt.utdallas.edu Abstract We describe our system for the CoNLL-2012 shared task, which seeks to model coreference in OntoNotes for English, Chinese, and Arabic. We adopt a hybrid approach to coreference resolution, which combines the strengths of rule-based methods and learningbased methods. Our official combined score over all three languages is 56.35. In particular, our score on the Chinese test set is the best among the participating teams. 1 Introduction The CoNLL-2012 shared task extends last year&apos;s task on coreference resolution from a monolingual to a multilingual setting (Pradhan et al., 2012). Unlike the SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Recasens et al., 2010), which focuses on coreference resolution in European languages, the CoNLL shared task is arguably more challenging: it focuses on three languages that come from very different language families, namely English, Chinese, and Arabic. We designed a system for resolving references in all three languages. Specifically, we participated in four tracks: the closed track for all three languages, and the open track for Chinese. In comparison to last year&apos;s participating systems, our resolver has"
W12-4504,P11-1082,1,0.685871,"m&apos;s parameters with respect to each genre. Our decision to adopt a hybrid approach is motivated by the observation that rule-based methods and learning-based methods each have their unique strengths. As shown by the Stanford coreference resolver (Lee et al., 2011), the winner of last year&apos;s shared task, many coreference relations in OntoNotes can be identified using a fairly small set of simple hand-crafted rules. On the other hand, our prior work on machine learning for coreference resolution suggests that coreference-annotated data can be profitably exploited to (1) induce lexical features (Rahman and Ng, 2011a, 2011b) and (2) optimize system parameters with respect to the desired coreference evaluation measure (Ng, 2004, 2009). Our system employs a fairly standard architecture, performing mention detection prior to coreference resolution. As we will see, however, the parameters of these two components are optimized jointly with respect to the desired evaluation measure. In the rest of this paper, we describe the mention detection component (Section 2) and the coreference resolution component (Section 3), show how their parameters are jointly optimized (Section 4), and present evaluation results on"
W12-4504,W09-2411,0,0.0844685,"Missing"
W12-4504,J03-4003,0,\N,Missing
W12-4504,S10-1001,0,\N,Missing
W13-1720,C12-1027,0,0.0600561,"function words, which belong to stop words, such as ‘the’, ‘at’, ‘which’, have been proven to be effective to distinguish native language for writers (Koppel et al., 2005). There are two settings: either punctuation marks are removed or kept. When punctuation marks are kept, they are viewed the same as word in constructing n-grams. For example, in the sentence “NLI is fun.”, “fun .” is viewed as a bigram. 4.2 Features In our system, word level n-grams are used to represent an essay. Previous studies have shown that word level n-grams are useful in determining the native language of a writer (Bykh and Meurers, 2012). One reasonable hypothesis is that nonnative English writers with the same native languages tend to choose more similar words to express the same or similar concepts. In addition, the combination of a sequence of words might also be affected by the different native language of writers. Therefore, word n-gram is useful to distinguish the native language of a writer. Even though some previous studies have looked into using word level n-grams as features, how to use word level ngrams has not been explored too much yet on TOEFL11 corpus. To our knowledge, the most recent study by Blanchard et al."
W13-1720,W07-0602,0,0.0242527,"Missing"
W13-1720,U09-1008,0,0.0372193,"Missing"
W13-1720,D11-1148,0,0.0400838,"Missing"
W13-3514,W11-1701,0,0.252071,"and emotional language to express their points, which may involve sarcasm, insults, and questioning another debater’s assumptions and evidence. These properties can potentially make stance classification of online debates more challenging than that of the other two types of debates. Our goal in this paper is to improve the state of the art in stance classification of online debates, focusing in particular on ideological debates. Specifically, we present two extensions, one linguistic and the other extra-linguistic, to the state-of-the-art supervised learning approach to this task proposed by Anand et al. (2011). In our linguistic extension, we induce patterns from each sentence in the training set using syntactic dependencies and semantic frames that aim to capture the meaning of a sentence and provide a generalized representation of it. Note that while Anand et al.’s lexico-syntactic approach aims to generalize from a sentence using syntactic dependencies, we aim to generalize using semantic frames. As we will see in Section 4, not only is there no guarantee that syntactic dependencies can retain or sufficiently capture the meaning of a sentence during the generalization process, it is in fact hard"
W13-3514,C08-2004,0,0.142328,"Missing"
W13-3514,P11-1151,0,0.03618,"of each post represents the subject stance but not the domain stance, we need to automatically convert the former to the latter. For example, for the subject “Abortion should be banned”, the subject stance yes implies that the author opposes abortion, and hence the domain label for the corresponding label should be against. We construct one dataset for each domain. Statistics of these datasets are shown in Table 1. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Burfoot et al. (2011), Lu et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. 3 Baseline Systems We employ as baselines two stance classification systems, Anand et al.’s (2011) approach and an enhanced version of it, as described below. Our first baseline, Anand et al.’s approach, is a sup"
W13-3514,N10-1138,0,0.0757673,"nd (6) we don’t care (DC) whether the verb is sentiment-bearing. If the verb is sentiment-bearing (in this case, hate has a negative sentiment), we create another pattern that is the same as the first one, except that DC is replaced with its sentiment value (see Pattern 2). Next, note that since the subject of hate is the target of the frame People and its object is a topic, we need to create patterns in a similar manner, resulting in Patterns 3 and 4. Note that People in these two patterns (with ‘P’ capitalized) is the 3 We use the Stanford parser (de Marneffe and Manning, 2008) and SEMAFOR (Das et al., 2010) to obtain dependency relations and semantic frames, respectively. 4 A word w is the target of a frame f if f is assigned to w to generalize its meaning. For example, assassination, kill, and terminate are the targets of the frame Killing. Step 1: Pattern Induction This step is composed of two sub-steps. 2 We use as the confidence value the signed distance of the associated test point from the SVM hyperplane. 126 1 <SFO:people:EF:Weapon:POS:DC> 9 <SFO:people:EF:Weapon:NEG:DC> 2 <SFO:people:EF:Weapon:POS:−> 10 <SFO:people:EF:Weapon:POS:−> 3 <SFO:People:EF:guns:POS:DC> 11 <SFO:People:EF:guns:NEG"
W13-3514,W08-1301,0,0.112109,"Missing"
W13-3514,C12-2045,1,0.890919,"Missing"
W13-3514,C10-2100,0,0.0800812,"Missing"
W13-3514,W10-0214,0,0.202676,"Missing"
W13-3514,W06-1639,0,0.121196,"ject. Since the label of each post represents the subject stance but not the domain stance, we need to automatically convert the former to the latter. For example, for the subject “Abortion should be banned”, the subject stance yes implies that the author opposes abortion, and hence the domain label for the corresponding label should be against. We construct one dataset for each domain. Statistics of these datasets are shown in Table 1. Related work on stance classification of congressional debates has found that enforcing author constraints (ACs) can improve classification performance (e.g., Thomas et al. (2006), Burfoot et al. (2011), Lu et al. (2012)). ACs are a type of interpost constraints that specify that two posts written by the same author for the same debate domain should have the same stance. We hypothesize that ACs could similarly be used to improve stance classification of ideological debates, and therefore propose a second baseline where we enhance the first baseline with ACs. Enforcing ACs is simple. 3 Baseline Systems We employ as baselines two stance classification systems, Anand et al.’s (2011) approach and an enhanced version of it, as described below. Our first baseline, Anand et a"
W13-3514,N12-1072,0,0.272452,"Missing"
W13-3514,N10-1097,0,0.0369494,"Missing"
W13-3514,D10-1102,0,0.030969,"Missing"
