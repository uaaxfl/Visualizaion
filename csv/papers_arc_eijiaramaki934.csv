2021.wnut-1.21,Mitigation of Diachronic Bias in Fake News Detection Dataset,2021,-1,-1,3,0,163,taichi murayama,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Fake news causes significant damage to society. To deal with these fake news, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel fake news generated by political changes and social changes; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the bias, especially proper nouns including person names, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using Wikidata to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data."
2021.bionlp-1.18,End-to-end Biomedical Entity Linking with Span-based Dictionary Matching,2021,-1,-1,5,0,12167,shogo ujiie,Proceedings of the 20th Workshop on Biomedical Language Processing,0,"Disease name recognition and normalization is a fundamental process in biomedical text mining. Recently, neural joint learning of both tasks has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset cannot be accurately predicted. This study introduces a novel end-to-end approach that combines span representations with dictionary-matching features to address this problem. Our model handles unseen concepts by referring to a dictionary while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our model achieved competitive results with strong baselines, especially for unseen concepts during training."
2020.nlp4musa-1.16,Classification of Nostalgic Music Through {LDA} Topic Modeling and Sentiment Analysis of {Y}ou{T}ube Comments in {J}apanese Songs,2020,-1,-1,4,0,16368,kongmeng liew,Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA),0,None
2020.lrec-1.561,Towards a Versatile Medical-Annotation Guideline Feasible Without Heavy Medical Knowledge: Starting From Critical Lung Diseases,2020,-1,-1,5,0,12168,shuntaro yada,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Applying natural language processing (NLP) to medical and clinical texts can bring important social benefits by mining valuable information from unstructured text. A popular application for that purpose is named entity recognition (NER), but the annotation policies of existing clinical corpora have not been standardized across clinical texts of different types. This paper presents an annotation guideline aimed at covering medical documents of various types such as radiography interpretation reports and medical records. Furthermore, the annotation was designed to avoid burdensome requirements related to medical knowledge, thereby enabling corpus development without medical specialists. To achieve these design features, we specifically focus on critical lung diseases to stabilize linguistic patterns in corpora. After annotating around 1100 electronic medical records following the annotation scheme, we demonstrated its feasibility using an NER task. Results suggest that our guideline is applicable to large-scale clinical NLP projects."
2020.coling-main.175,Offensive Language Detection on Video Live Streaming Chat,2020,-1,-1,4,0,21266,zhiwei gao,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper presents a prototype of a chat room that detects offensive expressions in a video live streaming chat in real time. Focusing on Twitch, one of the most popular live streaming platforms, we created a dataset for the task of detecting offensive expressions. We collected 2,000 chat posts across four popular game titles with genre diversity (e.g., competitive, violent, peaceful). To make use of the similarity in offensive expressions among different social media platforms, we adopted state-of-the-art models trained on offensive expressions from Twitter for our Twitch data (i.e., transfer learning). We investigated two similarity measurements to predict the transferability, textual similarity, and game-genre similarity. Our results show that the transfer of features from social media to live streaming is effective. However, the two measurements show less correlation in the transferability prediction."
P19-1202,"Learning to Select, Track, and Generate for Data-to-Text",2019,0,1,5,1,7220,hayate iso,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization."
L18-1375,{J}-{M}e{D}ic: A {J}apanese Disease Name Dictionary based on Real Clinical Usage,2018,0,1,6,0,29935,kaoru ito,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5803,Multivariate Linear Regression of Symptoms-related Tweets for Infectious Gastroenteritis Scale Estimation,2017,8,0,5,0,31450,ryo takeuchi,Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 ({DDDSM}-2017),0,"To date, various Twitter-based event detection systems have been proposed. Most of their targets, however, share common characteristics. They are seasonal or global events such as earthquakes and flu pandemics. In contrast, this study targets unseasonal and local disease events. Our system investigates the frequencies of disease-related words such as {``}nausea{''},{``}chill{''},and {``}diarrhea{''} and estimates the number of patients using regression of these word frequencies. Experiments conducted using Japanese 47 areas from January 2017 to April 2017 revealed that the detection of small and unseasonal event is extremely difficult (overall performance: 0.13). However, we found that the event scale and the detection performance show high correlation in the specified cases (in the phase of patient increasing or decreasing). The results also suggest that when 150 and more patients appear in a high population area, we can expect that our social sensors detect this outbreak. Based on these results, we can infer that social sensors can reliably detect unseasonal and local disease events under certain conditions, just as they can for seasonal or global events."
W16-4203,{M}ed{NLPD}oc: {J}apanese Shared Task for Clinical {NLP},2016,3,0,1,1,165,eiji aramaki,Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP}),0,"Due to the recent replacements of physical documents with electronic medical records (EMR), the importance of information processing in medical fields has been increased. We have been organizing the MedNLP task series in NTCIR-10 and 11. These workshops were the first shared tasks which attempt to evaluate technologies that retrieve important information from medical reports written in Japanese. In this report, we describe the NTCIR-12 MedNLPDoc task which is designed for more advanced and practical use for the medical fields. This task is considered as a multi-labeling task to a patient record. This report presents results of the shared task, discusses and illustrates remained issues in the medical natural language processing field."
W16-4211,Detecting {J}apanese Patients with {A}lzheimer{'}s Disease based on Word Category Frequencies,2016,0,7,4,0,33608,daisaku shibata,Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP}),0,"In recent years, detecting Alzheimer disease (AD) in early stages based on natural language processing (NLP) has drawn much attention. To date, vocabulary size, grammatical complexity, and fluency have been studied using NLP metrics. However, the content analysis of AD narratives is still unreachable for NLP. This study investigates features of the words that AD patients use in their spoken language. After recruiting 18 examinees of 53{--}90 years old (mean: 76.89), they were divided into two groups based on MMSE scores. The AD group comprised 9 examinees with scores of 21 or lower. The healthy control group comprised 9 examinees with a score of 22 or higher. Linguistic Inquiry and Word Count (LIWC) classified words were used to categorize the words that the examinees used. The word frequency was found from observation. Significant differences were confirmed for the usage of impersonal pronouns in the AD group. This result demonstrated the basic feasibility of the proposed NLP-based detection approach."
C16-1008,Forecasting Word Model: {T}witter-based Influenza Surveillance and Prediction,2016,10,7,3,1,7220,hayate iso,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Because of the increasing popularity of social media, much information has been shared on the internet, enabling social media users to understand various real world events. Particularly, social media-based infectious disease surveillance has attracted increasing attention. In this work, we specifically examine influenza: a common topic of communication on social media. The fundamental theory of this work is that several words, such as symptom words (fever, headache, etc.), appear in advance of flu epidemic occurrence. Consequently, past word occurrence can contribute to estimation of the number of current patients. To employ such forecasting words, one can first estimate the optimal time lag for each word based on their cross correlation. Then one can build a linear model consisting of word frequencies at different time points for nowcasting and for forecasting influenza epidemics. Experimentally obtained results (using 7.7 million tweets of August 2012 {--} January 2016), the proposed model achieved the best nowcasting performance to date (correlation ratio 0.93) and practically sufficient forecasting performance (correlation ratio 0.91 in 1-week future prediction, and correlation ratio 0.77 in 3-weeks future prediction). This report is the first of the relevant literature to describe a model enabling prediction of future epidemics using Twitter."
W15-1701,Location Name Disambiguation Exploiting Spatial Proximity and Temporal Consistency,2015,17,3,3,0,37002,takashi awamura,Proceedings of the third International Workshop on Natural Language Processing for Social Media,0,"As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambiguation, and did not make use of location-specific clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We confirm the effectiveness of these clues through experiments on Twitter tweets with GPS information."
P15-3005,Disease Event Detection based on Deep Modality Analysis,2015,19,1,3,0,27505,yoshiaki kitagawa,Proceedings of the {ACL}-{IJCNLP} 2015 Student Research Workshop,0,"Social media has attracted attention because of its potential for extraction of information of various types. For example, information collected from Twitter enables us to build useful applications such as predicting an epidemic of influenza. However, using text information from social media poses challenges for event detection because of the unreliable nature of user-generated texts, which often include counter-factual statements. Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or xe2x80x9ctweetsxe2x80x9d. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points."
P15-1160,Who caught a cold ? - Identifying the subject of a symptom,2015,39,7,4,0,23269,shin kanouchi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The development and proliferation of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recognizing diseases and symptoms from text messages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a subject (carrier) of a disease or symptommentioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing corpus for public surveillance. In addition, we present a supervised approach for predicting the subject of a disease/symptom. The results of our experiments demonstrate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom."
W13-4601,Incorporating Knowledge Resources to Enhance Medical Information Extraction,2013,-1,-1,5,1,4466,yasuhide miura,The First Workshop on Natural Language Processing for Medical and Healthcare Fields,0,None
W13-4602,Clinical Vocabulary and Clinical Finding Concepts in Medical Literature,2013,-1,-1,2,0,27899,takashi okumura,The First Workshop on Natural Language Processing for Medical and Healthcare Fields,0,None
W13-4606,Proper and Efficient Treatment of Anaphora and Long-Distance Dependency in Context-Free Grammar: An Experiment with Medical Text,2013,2,0,4,0,40647,wailok tam,The First Workshop on Natural Language Processing for Medical and Healthcare Fields,0,"These characteristics put the text in pathological reports under the category of controlled natural language, making it a better object text for semantic analysis and knowledge representation. Readers unfamiliar with controlled natural language are recommended to check the survey by (Schwitter2010). The purpose of this paper is to present how to combine a CFG (Context-Free Grammar) with an ontology to account for both syntactic structures and semantic structures of sentences (and discourses) containing longdistance dependencies and anaphora found in pathological reports. The syntactic and semantic framework outlined in this paper are developed on the foundation of the Global Document Annotation (GDA) guidelines proposed by (Hasida2010). When constructing our grammar, we have an application in mind. This application is auto-completion and hence speed matters. We want to do a bit more than bigrams can achieve with auto-completion such that the effect of an antecedent or a relative clause on user input can be captured. It is true that an elaborated feature structurebased grammars with hundreds of features would have little problem with anaphora and long distance dependencies. But speed is a problem for such a grammar. This leaves us with CFGs but typical CFGs can handle neither of the phenomena we are interested in. So we make CFGs do the job."
I13-1110,Word in a Dictionary is used by Numerous Users,2013,4,3,1,1,165,eiji aramaki,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,None
D11-1145,{T}witter Catches The Flu: Detecting Influenza Epidemics using {T}witter,2011,22,336,1,1,165,eiji aramaki,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"With the recent rise in popularity and scale of social media, a growing need exists for systems that can extract useful information from huge amounts of data. We address the issue of detecting influenza epidemics. First, the proposed system extracts influenza related tweets using Twitter API. Then, only tweets that mention actual influenza patients are extracted by the support vector machine (SVM) based classifier. The experiment results demonstrate the feasibility of the proposed approach (0.89 correlation to the gold standard). Especially at the outbreak and early spread (early epidemic stage), the proposed method shows high correlation (0.97 correlation), which outperforms the state-of-the-art methods. This paper describes that Twitter texts reflect the real world, and that NLP techniques can be applied to extract only tweets that contain useful information."
Y10-1075,Using Various Features in Machine Learning to Obtain High Levels of Performance for Recognition of {J}apanese Notational Variants,2010,17,2,6,0,44118,masahiro kojima,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We proposed a method of using machine learning with various features for the recognition of Japanese notational variants. We increased 0.06 at the F-measure by specific features using existing dictionaries and character pairs useful for recognizing notational variants and obtained 0.91 at the F-measure for the recognition of notational variants. By using the method, we could extract 160 thousand word pairs with a precision rate of 0.9. We also constructed a method using patterns in addition to machine learning and observed that we could extract 4.2 million notational variant pairs with a precision rate of 0.78. We confirmed that our method was much better than an existing method through experiments."
W10-3911,Adverse-Effect Relations Extraction from Massive Clinical Records,2010,-1,-1,2,1,4466,yasuhide miura,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,None
W09-3513,Fast Decoding and Easy Implementation: Transliteration as Sequential Labeling,2009,13,8,1,1,165,eiji aramaki,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"Although most of previous transliteration methods are based on a generative model, this paper presents a discriminative transliteration model using conditional random fields. We regard character(s) as a kind of label, which enables us to consider a transliteration process as a sequential labeling process. This approach has two advantages: (1) fast decoding and (2) easy implementation. Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach."
W09-1324,{TEXT}2{TABLE}: Medical Text Summarization System Based on Named Entity Recognition and Modality Identification,2009,17,39,1,1,165,eiji aramaki,Proceedings of the {B}io{NLP} 2009 Workshop,0,"With the rapidly growing use of electronic health records, the possibility of large-scale clinical information extraction has drawn much attention. It is not, however, easy to extract information because these reports are written in natural language. To address this problem, this paper presents a system that converts a medical text into a table structure. This system's core technologies are (1) medical event recognition modules and (2) a negative event identification module that judges whether an event actually occurred or not. Regarding the latter module, this paper also proposes an SVM-based classifier using syntactic information. Experimental results demonstrate empirically that syntactic information can contribute to the method's accuracy."
I08-1007,Orthographic Disambiguation Incorporating Transliterated Probability,2008,15,20,1,1,165,eiji aramaki,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Orthographic variance is a fundamental problem for many natural language processing applications. The Japanese language, in particular, contains many orthographic variants for two main reasons: (1) transliterated words allow many possible spelling variations, and (2) many characters in Japanese nouns can be omitted or substituted. Previous studies have mainly focused on the former problem; in contrast, this study has addressed both problems using the same framework. First, we automatically collected both positive examples (sets of equivalent term pairs) and negative examples (sets of inequivalent term pairs). Then, by using both sets of examples, a support vector machine based classifier determined whether two terms (t1 and t2) were equivalent. To boost accuracy, we added a transliterated probability P (t1|s)P (t2|s), which is the probability that both terms (t1 and t2) were transliterated from the same source term (s), to the machine learning features. Experimental results yielded high levels of accuracy, demonstrating the feasibility of the proposed approach."
S07-1103,{UTH}: {SVM}-based Semantic Relation Classification using Physical Sizes,2007,9,8,1,1,165,eiji aramaki,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"Although researchers have shown increasing interest in extracting/classifying semantic relations, most previous studies have basically relied on lexical patterns between terms. This paper proposes a novel way to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods."
2007.tmi-papers.3,Support vector machine based orthographic disambiguation,2007,-1,-1,1,1,165,eiji aramaki,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
I05-7008,Toward Medical Ontology using Natural Language Processing,2005,11,4,1,1,165,eiji aramaki,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"In this paper, we introduce our project aiming to build a medical ontology, and also present a method to estimate term relations and term classification, which are the basic structure for the ontology. First, relations between medical terms are extracted from a medical electronic dictionary. Next, the terms are classified based on the co-occurrence verbs. Preliminary experimental results show the basic feasibility of our approach."
2005.mtsummit-papers.29,Probabilistic Model for Example-based Machine Translation,2005,-1,-1,1,1,165,eiji aramaki,Proceedings of Machine Translation Summit X: Papers,0,"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in retrieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the proposed model, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems."
2004.iwslt-evaluation.15,Example-based machine translation using structural translation examples,2004,6,6,1,1,165,eiji aramaki,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper proposes an example-based machine translation system which handles structural translation examples. The structural translation examples have the potential advantage of high-usability. However, technologies which build such translation examples are still being developed. In such a situation, the comparison of the proposed system and the other approach systems is meaningful. This paper presents the system algorithm and its performance on the IWSLT04 Japanese-English unrestricted task."
W03-0312,Word Selection for {EBMT} based on Monolingual Similarity and Translation Confidence,2003,12,9,1,1,165,eiji aramaki,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach."
2001.mtsummit-papers.5,Finding translation correspondences from parallel parsed corpus for example-based translation,2001,9,14,1,1,165,eiji aramaki,Proceedings of Machine Translation Summit VIII,0,"This paper describes a system for finding phrasal translation correspondences from parallel parsed corpus that are collections paired English and Japanese sentences. First, the system finds phrasal correspondences by Japanese-English translation dictionary consultation. Then, the system finds correspondences in remaining phrases by using sentences dependency structures and the balance of all correspondences. The method is based on an assumption that in parallel corpus most fragments in a source sentence have corresponding fragments in a target sentence."
C00-2131,Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation,2000,12,64,3,0,52368,hideo watanabe,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system."
