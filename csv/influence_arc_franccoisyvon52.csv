2003.jeptalnrecital-long.10,P01-1008,0,0.0521646,"Missing"
2003.jeptalnrecital-long.10,W99-0613,0,0.0761808,"Missing"
2003.jeptalnrecital-long.10,P99-1062,0,0.0357948,"Missing"
2004.jeptalnrecital-poster.22,W97-0813,0,0.0765581,"Missing"
2004.jeptalnrecital-poster.22,hathout-2002-wordnet,0,0.0304886,"Missing"
2004.jeptalnrecital-poster.22,C94-1097,0,0.0327313,"Missing"
2004.jeptalnrecital-poster.22,P98-1120,0,0.0561117,"Missing"
2004.jeptalnrecital-poster.22,2002.jeptalnrecital-long.22,0,0.103913,"Missing"
2004.jeptalnrecital-poster.22,P97-1055,1,0.865331,"Missing"
2008.jeptalnrecital-court.19,N06-1060,0,0.0649437,"Missing"
2008.jeptalnrecital-long.13,P06-2005,0,0.138809,"Missing"
2008.jeptalnrecital-long.13,J94-3001,0,0.124636,"Missing"
2008.jeptalnrecital-long.13,N03-1017,0,0.00990468,"Missing"
2008.jeptalnrecital-long.13,P96-1031,0,0.435683,"Missing"
2008.jeptalnrecital-long.13,J03-1002,0,0.00433238,"Missing"
2008.jeptalnrecital-long.13,P02-1040,0,0.0741734,"Missing"
2009.eamt-1.10,W99-0604,0,0.125254,"Missing"
2009.eamt-1.10,H05-1095,0,0.514304,"hierarchical system that derives translations in two steps, so as to mitigate the computational impact resulting from the intersection of a probabilistic synchronous CFG and and the n-gram language model. Firstly, a CYK-style decoding considering first-best chart item approximations is used to generate an hypergraph of target language derivations. In the second step, a detailed exploration of the previous hypergraph is performed. The language model is used to drive the second step search process and to recover from search errors made during the first step. Related Work We follow the work in (Simard et al., 2005), which, to the best of our knowledge is the first MT system that within a left-to-right decoding approach, introduces the idea of phrases with gaps. A main limitation of their work arised from the difficulties of left-to-right decoders to handle gaps in the target side, again because of the non-monotonic generation of the target. Such gaps are to be filled in further steps of the search, thus, increasing the complexity of decoding and at the same time that hindering the use of the target language model. Such translation units are more naturally used under systems employing parsing techniques"
2009.eamt-1.10,J93-2003,0,0.0326071,"we aim at capturing the benefits of the higher generalization power shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The co"
2009.eamt-1.10,takezawa-etal-2002-toward,0,0.0332215,"iments 4.2 In this section, we give details regarding the evaluation framework and report on the experimental work carried out to evaluate the improvements. 4.1 Accuracy results are reported for different configurations in table 2. System configurations consist of: base for which translation units do not introduce the ability to split source words into multiple tokens, and +split where the previous technique is used. The POS configuration employs POS tags in the source side of the reordering rules while +SYN employs both POS tag and syntactic rules. Evaluation Framework We have used the BTEC (Takezawa et al., 2002) corpus focusing on translations from Chinese to English. It consists of the data made available for the IWSLT 2007 evaluation campaign. Some statistics regarding the corpora used, namely number of sentences, words, vocabulary, average sentence length and number of references per language are shown in table 1. Sent Words Train en 377k 40k zh 354k Tune / Test (zh) tune 506 3,564 tst2 500 3,608 tst3 506 3,889 tst4 489 5,476 tst5 500 5,846 tst6 489 3,325 Voc Avg 11k 9,6k 9.5 8.9 871 921 916 1,094 1,292 864 7 7.22 7.69 11.2 11.69 6.8 Set tst2 tst3 tst4 tst5 tst6 base POS +SYN 47.25 48.15 55.82 56."
2009.eamt-1.10,J07-2003,0,0.882404,"utational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c 2009 European A"
2009.eamt-1.10,N07-1063,0,0.033169,"ose translation. More recently, (Watanabe et al., 2006) presents a hierarchical system in which the target sentence is generated in left-to-right order, thus enabling a straightforward integration of the n-gram language models during search. The authors employ a top-down strategy to parse the foreign language side, using a synchronous grammar having a GNF2 -like structure. This means that the target side body of each translation rule takes the form bβ, where b is a string of terminal symbols and β a (possibly empty) string of non-terminals. This ensures that the target is built monotonously. (Venugopal et al., 2007) present a hierarchical system that derives translations in two steps, so as to mitigate the computational impact resulting from the intersection of a probabilistic synchronous CFG and and the n-gram language model. Firstly, a CYK-style decoding considering first-best chart item approximations is used to generate an hypergraph of target language derivations. In the second step, a detailed exploration of the previous hypergraph is performed. The language model is used to drive the second step search process and to recover from search errors made during the first step. Related Work We follow the"
2009.eamt-1.10,P07-2054,1,0.904734,"Missing"
2009.eamt-1.10,2007.mtsummit-papers.16,1,0.9267,"Missing"
2009.eamt-1.10,P06-1098,0,0.0414791,"discontinuous constituents, a major difference between FST- and CFG-based approaches to translation, has to do with the size of the search space, or more precisely with the kind of pruning that takes place to make the search feasible. As previously outlined, when considering the use of translation units with gaps under the left-to-right decoding approach, the main difficulty arises motivated by the appearance of discontinuities in the output side. In this work, we make use of an input word lattice to naturally avoid this problem, allowing to monotonically compose translation. More recently, (Watanabe et al., 2006) presents a hierarchical system in which the target sentence is generated in left-to-right order, thus enabling a straightforward integration of the n-gram language models during search. The authors employ a top-down strategy to parse the foreign language side, using a synchronous grammar having a GNF2 -like structure. This means that the target side body of each translation rule takes the form bβ, where b is a string of terminal symbols and β a (possibly empty) string of non-terminals. This ensures that the target is built monotonously. (Venugopal et al., 2007) present a hierarchical system t"
2009.eamt-1.10,2007.mtsummit-papers.29,0,0.0323727,"ied on top of the sentence abcd (s). As it can be seen, the resulting word lattice contains the path of the original sentence s : abcd, as well as the additional paths appeared by the composition of reordering rules: τ1 (s) : cab, τ2 (s) : ba and τ2 (τ1 (s)) : cba. Part-of-speech (POS) and syntactic information are used to increase the generalization 3 Translation units with gaps In this section we give details of the gappy translation units introduced in this work. 3.1 Split rules and reordering Some phrase-based systems have been able to introduce some levels of syntactical information. In (Habash, 2007) the author employs automatically learned syntactic reordering rules to preprocess the input, aiming at solving the reordering problem, before passing the reordered input to a phrase-based decoder for Arabic-English translation. However, this kind of systems cannot produce the translation 68 ings and split words are introduced in the source sentence only, motivating the use of a word lattice. During training, the alignment is entirely monotonized before extracting tuples, only keeping those one-to-many and many-toone alignments where the tokens on the many are contiguous; when this is not the"
2009.eamt-1.10,2002.tmi-tutorials.2,0,0.03956,"r shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split"
2009.eamt-1.10,N03-1017,0,0.195799,"ical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, t"
2009.eamt-1.10,P07-2045,0,0.00718213,"ces per language are shown in table 1. Sent Words Train en 377k 40k zh 354k Tune / Test (zh) tune 506 3,564 tst2 500 3,608 tst3 506 3,889 tst4 489 5,476 tst5 500 5,846 tst6 489 3,325 Voc Avg 11k 9,6k 9.5 8.9 871 921 916 1,094 1,292 864 7 7.22 7.69 11.2 11.69 6.8 Set tst2 tst3 tst4 tst5 tst6 base POS +SYN 47.25 48.15 55.82 56.88 15.72 16.82 15.89 16.32 29.56 30.81 +split POS +SYN 47.42 48.39 56.44 57.17 16.48 17.08 16.34 16.89 29.81 31.67 Moses 48.14 55.95 18.06 15.91 31.76 Refs Table 2: Accuracy results measured using the BLEU score. 1 The last column shows accuracy results obtained by Moses (Koehn et al., 2007), a stateof-the-art phrase-based SMT system. It is worth saying that the Moses system was built using the same data sets and alignments that were used for our system (Moses performs lexicalized reordering with a maximum reordering distance of 8 words). In this case, we run a different optimization for each of the system configurations. BLEU confidence intervals range depending on the test set approximately from ±2.0 to ±3.0 points BLEU. As it can be seen, the system built using the +split technique obtains higher accuracy results than the baseline one (base), in all test 16 16 16 7 7 6 Table 1"
2009.eamt-1.10,W03-1730,0,0.0126589,"built using the same data sets and alignments that were used for our system (Moses performs lexicalized reordering with a maximum reordering distance of 8 words). In this case, we run a different optimization for each of the system configurations. BLEU confidence intervals range depending on the test set approximately from ±2.0 to ±3.0 points BLEU. As it can be seen, the system built using the +split technique obtains higher accuracy results than the baseline one (base), in all test 16 16 16 7 7 6 Table 1: BTEC Corpus (Chinese-to-English). Chinese words were segmented by means of the ICTCLAS (Zhang et al., 2003) tagger/segmenter. Word alignments were computed for the training data in the original word order, using GIZA++3 . The grow-final-diagand heuristic is used to refine the alignments 3 Results 4 5 www.fjoch.com/GIZA++ 71 nlp.stanford.edu/downloads/lex-parser.shtml www.speech.sri.com/projects/srilm 5 sets and for both reordering rule configurations (POS and +SYN). Conclusions and Further Work In this paper, we have presented an extension to a bilingual n-gram translation system in which we allow translation units with gaps. The use of word lattices allowed us to introduce the concept of gappy tra"
2009.eamt-1.10,W06-3119,0,0.0394058,"s not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 66–73, Barcelona, May 2009 66 tion they use. We mainly differentiate here between translation units that are formally syntax-based, like those appearing in (Chiang, 2007), which employ non-terminal categories without linguistic motivation, working as placeholders to be filled by words in further translation steps; and hierarchical units that are more linguistically motivated, as in (Zollmann and Venugopal, 2006). This ability to capture better generalization comes at a double price: translation as parsing is typically cubic with respect to the source sentence length; furthermore, in this formalism, target constituent are no longer produced monotonically from left-to-right, thus rendering the application of the language model score difficult (Chiang, 2007). This example also suggests that hierarchical rules tend to be less sparse, given that the holistic unit in the phrase-based (PB) model is divided into two smaller, more reusable, rules. Notice that, in this specific case, the rich morphology of Fre"
2009.eamt-1.10,J06-4004,1,0.931134,"Missing"
2009.eamt-1.10,C08-1144,0,0.102713,"Missing"
2009.eamt-1.10,P04-1083,0,0.0610688,"avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make difficult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 1 Introduction don′ t want : ne veux pas Work in SMT has evolved from the traditional word-based (Brown et al., 1993) to the current phrase-based (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) and hierarchicalbased (Melamed, 2004; Chiang, 2007) translation models. Phrase-based and hierarchical systems are also characterized by the underlying formal device employed to produce translations (Knight, 2008): finite-state transducers (FST) on the one hand, and tree transducers In contrast, under hierarchical systems, it is possible to obtain the right generalization, decomposing the previous pattern as: X → don′ t Y : ne Y pas Y → want : veux 1 This example is only used for illustrative purposes. The contracted form don’t is not a real issue as most tokenizers split the form as do not, thus solving the alignment problem. c"
2009.jeptalnrecital-court.28,P05-1048,0,0.295677,"Missing"
2009.jeptalnrecital-court.28,W06-2606,0,0.0423906,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.3,0,0.0259168,"Missing"
2009.jeptalnrecital-court.28,2005.mtsummit-papers.11,0,0.0623989,"Missing"
2009.jeptalnrecital-court.28,N03-1017,0,0.0219395,"Missing"
2009.jeptalnrecital-court.28,W09-0407,0,0.055301,"Missing"
2009.jeptalnrecital-court.28,J06-4004,1,0.880029,"Missing"
2009.jeptalnrecital-court.28,2009.jeptalnrecital-long.16,1,0.785303,"Missing"
2009.jeptalnrecital-court.28,P04-1063,0,0.0604008,"Missing"
2009.jeptalnrecital-court.28,2001.mtsummit-papers.46,0,0.14582,"Missing"
2009.jeptalnrecital-court.28,P02-1040,0,0.076177,"Missing"
2009.jeptalnrecital-court.28,N07-1029,0,0.0454675,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.6,0,0.022181,"Missing"
2009.jeptalnrecital-court.28,N04-1023,0,0.0770885,"Missing"
2009.jeptalnrecital-court.28,P07-1108,0,0.0239088,"Missing"
2009.jeptalnrecital-court.28,W08-0309,0,\N,Missing
2010.amta-papers.18,N06-1013,0,0.629907,"to the first problem is to use discriminative models, which are able to consider arbitrary features of the involved words. In this framework, the alignment task is casted as a classification problem: a binary classifier predicts, for each possible assignment, whether it should be included or not in the alignment. Discriminative models can also consider predictions provided by other alignment models as features, and therefore constitute a solution to the second problem: by applying these features to learn symmetrization decisions in light of a global view of the data. By applying these ideas (Ayan and Dorr, 2006) obtained promising results. However, their model remains unable to model interactions between alignment decisions which are, intuitively, of great help to correctly prevent or encourage certain configurations in the predicted alignment. To overcome this shortcoming, we propose to extend their model by introducing a stacked classification layer (Wolpert, 1992) that operates globally and, hence, enables arbitrary features, describing interactions between alignment decisions, to be taken into consideration. The main contribution of this work is a reexamination of (Ayan and Dorr, 2006) work which"
2010.amta-papers.18,P06-1009,0,0.0912765,"nt task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symme"
2010.amta-papers.18,J93-2003,0,0.0208821,"p. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext and is performed in an unsupervised way with the EM algorithm. This approach has two main caveats, leaving room for improving the alignment quality and, consequently, the translation quality. Firstly, the generative paradigm is not well suited to incorporate arbitrary and possibly interdependent information sources. Secondly, the"
2010.amta-papers.18,P03-1012,0,0.0307885,"tained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form o"
2010.amta-papers.18,P08-2007,0,0.0743953,"stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation di"
2010.amta-papers.18,N07-2007,0,0.396858,"nt matrix is typically sparse, with a majority of inactive links, the classification task we consider is unbalanced. To avoid learning a biased classifier with high tendency toward labeling all links as inactive, we use a set of input alignments to reduce the set of links to be predicted to a subset of the alignment matrix: a point that has not been proposed by at least one input alignment will be labeled as inactive; the others are labeled by the classifier. The union of all input alignments is hence used to reduce the search space and avoid biasing the classifier as in (Ayan and Dorr, 2006; Elming and Habash, 2007). Input alignments are pre-computed separately using GIZA++. During inference, the model assigns a probability to each proposed alignment link. The final output matrix consists of active links whose probability exceeds a threshold p (optimized on a development set using a grid search). This parameter is used to control the density of the resulting alignment and therefore the balance between its precision and recall. In this work, we used a maximum entropy (ME) classifier to estimate the probability of a link of A: Align 2 Align 1 AAlign Target ADist AJump Figure 1: Features extracted to label"
2010.amta-papers.18,J07-3002,0,0.0746646,"Missing"
2010.amta-papers.18,N06-2013,0,0.0585018,"Missing"
2010.amta-papers.18,H05-1012,0,0.020578,"native matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imp"
2010.amta-papers.18,P06-1141,0,0.0209513,"the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] to generate an extended representation of the training c"
2010.amta-papers.18,N06-1015,0,0.0149343,"ir and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference an"
2010.amta-papers.18,P05-1057,0,0.0168931,"ght of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of t"
2010.amta-papers.18,2006.amta-papers.11,0,0.0391831,"Missing"
2010.amta-papers.18,D08-1017,0,0.0309813,"s are assumed to be independent. While this keeps the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] t"
2010.amta-papers.18,H05-1011,0,0.0168663,"t have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment"
2010.amta-papers.18,W08-0303,0,0.0353835,"usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symmetrization step, at the expense of an approximate inference and a complicated two-step training. Many of these discriminative models do not entirely dispense with the generative models, but rather integrate their predictions as supplementary features. 3 Maximum Entropy for Alignment Matrix Modeling In this section, we present the task of word alignment as a binary classification problem, in which we model the alignment matrix directly. We also explain how to improve the expressivity o"
2010.amta-papers.18,J03-1002,0,0.0614447,"delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext an"
2010.amta-papers.18,P03-1021,0,0.0261178,"ng we used a freely available toolkit3 . The model parameters are estimated using L-BFGS (Byrd et al., 1994) to maximize the regularized log-likelihood on a training corpus. A Gaussian prior is used during optimization to prevent overfitting. GIZA++ (Och and Ney, 2003) is used to train our generative alignments, with the additional parallel data made available by NIST MT Eval’09 constrained training condition. We used Moses4 with SRILM5 with the same data in our translation experiments. A 4-gram back-of language model is estimated using all English available data. Minimum Error-Rate Training (Och, 2003) is carried on to tune the parameters of the translation system on the NIST MT’06 test set. Translations are evaluated on NIST MT’08 test set. Arabic pre-processing scheme and remappings Arabic is a morphologically complex, highlyinflected language. This makes normalization necessary to reduce the sparsity of the data. We use MADA+TOKAN6 for morphological analysis, disambiguation and tokenization for Arabic. Given previous experiments on the NIST MT’09 task, we use the D2 tokenization scheme that showed to perform best under large resource conditions (Habash 3 http://homepages.inf.ed.ac.uk/lzh"
2010.amta-papers.18,P02-1040,0,0.0816489,"e one hand, we present a careful study of the impact of several novel features on the performance; on the other hand, we investigate the use of the stacking technique to improve the alignment quality. By conjoining these techniques, we were able to greatly reduce the AER as compared to previously published work, and to achieve better BLEU results. In this paper, we also contrast alignments obtained by the symmetrization heuristic with those obtained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of"
2010.amta-papers.18,H05-1010,0,0.0234131,"ity and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypot"
2010.iwslt-evaluation.13,W10-1704,1,0.845666,"is first described in Section 2, while Section 3 reports our work on Turkish pre-processing and on the use of continuous space language models. 2. TALK task 2.1. n-code SMT system 1. Introduction LIMSI took part in the IWSLT 2010 evaluation for two different tasks: Talk and BTEC. The goal of the new Talk task is to translate public speeches on a variety of topics, from English to French. Since the allowed training data includes the parallel corpora distributed by the ACL 2010 Workshop on Statistical Machine Translation (WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our"
2010.iwslt-evaluation.13,P07-2045,0,0.00317713,"(WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on biling"
2010.iwslt-evaluation.13,J06-4004,1,0.810779,"standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on bilingual units called tuples. The probability assigned to a sentence pair by the translation model is estimated by using the n-gram assumption: p(sJ1 , tI1 ) = K Y k=1 p((s, t)k |(s, t)k"
2010.iwslt-evaluation.13,P03-1021,0,0.0080791,"Figure 1: Tuple extraction from a sentence pair. The resulting sequence of tuples (1) is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalizatio"
2010.iwslt-evaluation.13,N04-4026,0,0.0409215,"is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalization tools provides a significant reward in BLEU . The downside is the need to post-proc"
2010.iwslt-evaluation.13,C10-2023,1,0.883793,"Missing"
2010.iwslt-evaluation.13,W07-0704,1,0.827751,"Missing"
2010.iwslt-evaluation.13,P10-1047,0,0.0285202,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.2,0,0.0280005,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.6,0,0.0375883,"Missing"
2010.iwslt-evaluation.13,P06-1001,0,0.0377565,"Missing"
2010.iwslt-evaluation.13,popovic-ney-2004-towards,0,0.0520656,"Missing"
2010.iwslt-evaluation.13,H05-1085,0,0.0623309,"Missing"
2010.iwslt-evaluation.13,P08-1087,0,0.0340659,"Missing"
2010.iwslt-evaluation.13,P07-1017,0,0.0300031,"Missing"
2010.iwslt-evaluation.13,J04-2003,0,0.0867503,"Missing"
2010.iwslt-evaluation.13,corston-oliver-gamon-2004-normalizing,0,0.068379,"Missing"
2010.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.0224187,"Missing"
2010.iwslt-evaluation.13,E06-1006,0,0.0399082,"Missing"
2010.iwslt-evaluation.13,N04-4015,0,0.0798783,"Missing"
2010.iwslt-evaluation.13,2001.mtsummit-papers.45,0,0.0857026,"Missing"
2010.iwslt-evaluation.13,D10-1076,1,0.891217,"Missing"
2010.iwslt-evaluation.13,W06-3102,1,\N,Missing
2010.iwslt-evaluation.13,2009.iwslt-evaluation.5,0,\N,Missing
2010.jeptalnrecital-long.13,D09-1129,0,0.0627687,"Missing"
2010.jeptalnrecital-long.13,max-wisniewski-2010-mining,1,0.783004,"Missing"
2011.eamt-1.33,W10-1704,1,0.889517,"Missing"
2011.eamt-1.33,W08-0304,0,0.749297,"a directed acyclic graph (lattice) encoding a large number of potential translations. Because of the form of the inference rule (1), the learning criterion (2) is neither convex nor differentiable. Furthermore, its exact computation is made intractable by the typical size of E, hence the recourse to various heuristic optimization strategies. The most successful to date is the proposal of (Och, 2003), usually referred to as Minimum Error Rate Training (MERT). This proposal has however been repeatedly questioned for (i) its computational cost and (ii) the instability of the resulting solutions (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009). The most promis1 At this stage, any other metrics could be used instead of BLEU (see e.g., (Zaidan, 2009)). Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 241248 Leuven, Belgium, May 2011 ing improvement consists in extending the approximation of the search space used in (2) from nbest lists to lattices, which both improves speed and reduces the variability of the final outcome (Macherey et al., 2008). Our main contribution in this paper"
2011.eamt-1.33,P10-4002,0,0.0331999,"equation (1) are to be tuned over some development data. The whole procedure (Och, 2003) is sketched in algorithm 1. Algorithm 1: The MERT optimization cycle ¯ 0 for λ, ¯ development data F , Input: initial value λ required minimum improvement  ¯ ∗ for λ ¯ Output: optimal value λ repeat for (f ∈ F ) do Ht (f , λt ) ← Translate(f ) ¯ t+1 ← Optimize({Ht (f , λ ¯ t ), f ∈ F }, λ ¯t) λ t←t+1 ¯ t+1 − λ ¯ t |&lt; ) until (|λ ∗ ¯ ¯ λ ← λt MERT thus implies two different kinds of operations: decoding, which basically implements the 2 The notion of a MERT semiring has been alluded to in the literature (Dyer et al., 2010). To the best of our knowledge, this semiring has never been formally described, neither from the algebraic, nor from the implementation standpoint. This is a gap that we intend to fill in this work. 242 inference procedure and returns a set Ht (f , λt ) of hypotheses, and optimization, which we now describe. The Optimize() function relies on optimization techniques for non-differentiable functions, such as the Powell’s search algorithm (Powell, 1964). This requires to perform a series of min¯=λ ¯ 0 + γ r¯ for some imizations of (2) along lines λ directions r¯. Due to the log-linear form of th"
2011.eamt-1.33,W09-0439,0,0.219851,"ng a large number of potential translations. Because of the form of the inference rule (1), the learning criterion (2) is neither convex nor differentiable. Furthermore, its exact computation is made intractable by the typical size of E, hence the recourse to various heuristic optimization strategies. The most successful to date is the proposal of (Och, 2003), usually referred to as Minimum Error Rate Training (MERT). This proposal has however been repeatedly questioned for (i) its computational cost and (ii) the instability of the resulting solutions (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009). The most promis1 At this stage, any other metrics could be used instead of BLEU (see e.g., (Zaidan, 2009)). Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 241248 Leuven, Belgium, May 2011 ing improvement consists in extending the approximation of the search space used in (2) from nbest lists to lattices, which both improves speed and reduces the variability of the final outcome (Macherey et al., 2008). Our main contribution in this paper is to recast the algorithm of (Macherey et al.,"
2011.eamt-1.33,P09-1019,0,0.568591,"ost promis1 At this stage, any other metrics could be used instead of BLEU (see e.g., (Zaidan, 2009)). Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 241248 Leuven, Belgium, May 2011 ing improvement consists in extending the approximation of the search space used in (2) from nbest lists to lattices, which both improves speed and reduces the variability of the final outcome (Macherey et al., 2008). Our main contribution in this paper is to recast the algorithm of (Macherey et al., 2008; Kumar et al., 2009) (“lattice-MERT”) in a sound algebraic framework, using the MERT semiring2 . Using this reformulation, we produce an efficient implementation of lattice MERT based on a generic finite-state toolbox (Allauzen et al., 2007). Preliminary experimental results confirm the main conclusions of (Macherey et al., 2008). The rest of this paper is organized as follows. In Section 2, we recall the main principles of the basic algorithm of (Och, 2003), and some of the improvements that have been proposed in the literature. Section 3 is where we introduce the MERT semiring and its main properties. We then d"
2011.eamt-1.33,2006.iwslt-papers.5,0,0.0181944,"operation, because of the definition (3): as no line is contained in ¯0D the result of multiplication by ¯0D is always empty. 3.2 Envelope semiring Definition 3.2. The upper envelope of a set of lines d ∈ D is a subset env(d) ⊆ d consisting of lines di ∈ d, s.t. for each line di ∈ env(d), there exists an non-empty interval Ii ∈ R, s.t. if x ∈ Ii , then di.s ·x+di.y &gt; di0 .s ·x+di0 .y , for any line di0 6= di . Two lines di and dj in env(d) are said to be neighbors if their corresponding intervals Ii and Ij are adjacent. 3 Not to mention changes in the core optimization routines, as in e.g., (Lambert and Banchs, 2006) 4 They are too numerous to be efficiently enumerated. 243 5 Formally, ⊗ corresponds to the Minkowski sum of the two sets of lines. Let E be a subset of D such that env(d) = d for each d ∈ E, and define the operations ⊕E and ⊗E as the projections of the respective operations in D on the set E: d1 ⊕E d2 = env(d1 ⊕D d2 ), d1 ⊗E d2 = env(d1 ⊗D d2 ). Figure 1: The upper envelope of a set of lines For MERT, it is important to know the intersections of neighboring lines in the envelope. For this purpose, an envelope can be ordered as a list of lines with increasing slopes and each line encoded as a"
2011.eamt-1.33,W10-1717,0,0.0455222,"(Macherey et al., 2008), and more recently, to hypergraphs (Kumar et al., 2009). These generalizations take advantage of the ¯ f ), decomposability of the feature functions h(e, which are computed as a sum of local feature functions. When this property holds, rather than constructing upper envelopes for each hypothesis in the lattice4 , the envelopes are distributed over nodes in the lattice. Working with much better approximations of the complete search spaces not only allows to converge in less iterations, but also to achieve better generalization, a finding that was recently confirmed by (Larkin et al., 2010). Our work is a continuation of this line of research, driven by the intuition that recasting MERT in a clear algebraic framework, as we do in the next section, can help develop faster, and even more efficient, implementations of MERT for complex hypotheses set. 3 The MERT Semiring Recall that a semiring K over a set K is a system hK, ⊕, ⊗, ¯0, ¯ 1i, where hK, ⊕, ¯ 0i is a commutative monoid with identity element ¯0, meaning that a ⊕ (b⊕c) = (a⊕b)⊕c, a⊕b = b⊕a and ∀a, a⊕ ¯ 0= ¯0 ⊕ a = a. Additionally, hK, ⊗, ¯1i is a monoid with identity element ¯1; ⊗ distributes over ⊕ so that a ⊗ (b ⊕ c) = ("
2011.eamt-1.33,D08-1076,0,0.666505,"the instability of the resulting solutions (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009). The most promis1 At this stage, any other metrics could be used instead of BLEU (see e.g., (Zaidan, 2009)). Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 241248 Leuven, Belgium, May 2011 ing improvement consists in extending the approximation of the search space used in (2) from nbest lists to lattices, which both improves speed and reduces the variability of the final outcome (Macherey et al., 2008). Our main contribution in this paper is to recast the algorithm of (Macherey et al., 2008; Kumar et al., 2009) (“lattice-MERT”) in a sound algebraic framework, using the MERT semiring2 . Using this reformulation, we produce an efficient implementation of lattice MERT based on a generic finite-state toolbox (Allauzen et al., 2007). Preliminary experimental results confirm the main conclusions of (Macherey et al., 2008). The rest of this paper is organized as follows. In Section 2, we recall the main principles of the basic algorithm of (Och, 2003), and some of the improvements that have been p"
2011.eamt-1.33,J06-4004,0,0.0787263,"Missing"
2011.eamt-1.33,C08-1074,0,0.108407,"graph (lattice) encoding a large number of potential translations. Because of the form of the inference rule (1), the learning criterion (2) is neither convex nor differentiable. Furthermore, its exact computation is made intractable by the typical size of E, hence the recourse to various heuristic optimization strategies. The most successful to date is the proposal of (Och, 2003), usually referred to as Minimum Error Rate Training (MERT). This proposal has however been repeatedly questioned for (i) its computational cost and (ii) the instability of the resulting solutions (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009). The most promis1 At this stage, any other metrics could be used instead of BLEU (see e.g., (Zaidan, 2009)). Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 241248 Leuven, Belgium, May 2011 ing improvement consists in extending the approximation of the search space used in (2) from nbest lists to lattices, which both improves speed and reduces the variability of the final outcome (Macherey et al., 2008). Our main contribution in this paper is to recast the algori"
2011.eamt-1.33,P03-1021,0,0.752729,"SMT sys¯ ∗ that maximizes the emtem consists in finding λ pirical gain G on a development set F = {(f , rf )} made of pairs of a source sentence f and corresponding reference translation(s) rf : Modern Statistical Machine Translation (SMT) systems make their decisions based on multiple information sources, which assess various aspects of the match between a source sentence and its possible translation(s). Tuning a SMT system consists in finding the right balance between these sources so as to produce the best possible output, and is usually achieved through Minimum Error Rate Training (MERT) (Och, 2003). In this paper, we recast the operations implied in MERT in the terms of operations over a specific semiring, which, in particular, enables us to derive a simple and generic implementation of MERT over word lattices. 1 ¯ ∗ = arg max G(F ; λ) ¯ λ ¯ λ Introduction Inference (decoding) in phrase-based statistical machine translation (SMT) systems is typically based on a log-linear model of the probability ¯ · h(e, ¯ f )) of obtaining a p(e|f ) = Z(f )−1 exp(λ target sentence e given an input sentence f . For ˜f as : such model, the MAP decision rule selects e ¯ = arg max p(e|f ) ˜f (λ) e e∈E ¯ ·"
2011.eamt-1.33,P02-1040,0,0.0838101,"f )) of obtaining a p(e|f ) = Z(f )−1 exp(λ target sentence e given an input sentence f . For ˜f as : such model, the MAP decision rule selects e ¯ = arg max p(e|f ) ˜f (λ) e e∈E ¯ · h(e, ¯ f ), = arg max λ e∈E (1) ¯ f) where E is the set of reachable translations, h(e, is the vector of feature functions representing var¯ is ious compatibility measures of f and e, and λ a parameter vector, each component λi of which regulates the influence of the feature hi (e, f ). c 2011 European Association for Machine Translation. (2) where the computation of the gain function G, typically the BLEU score (Papineni et al., 2002) 1 , ¯ f ∈ F} depends on the actual translations {˜ ef (λ), ¯ achieved for a given value of λ according to (1). For the sake of performing this optimization efficiently, the search space of the decoder is often approximated using an explicit list of n-best hypotheses or a directed acyclic graph (lattice) encoding a large number of potential translations. Because of the form of the inference rule (1), the learning criterion (2) is neither convex nor differentiable. Furthermore, its exact computation is made intractable by the typical size of E, hence the recourse to various heuristic optimizati"
2011.eamt-1.33,P06-2101,0,0.102937,"resource than optimization. It is thus expected that the most significant speed improvements will be obtained by reducing the number of iterations. The other main issue is the stability of the results, which, in practice, is addressed by running the optimize several times, with different starting points. These inefficiencies have stimulated the development of alternative approaches3 . Attempts at improving MERT can be split into two categories: works that try to fix the optimization procedure and works that consider alternative, arguably easier to optimize or better suited training criteria (Smith and Eisner, 2006; Zens et al., 2007; Watanabe et al., 2007). In the sequel, we only discuss the former approaches, which are more relevant to this work. (Cer et al., 2008) provides a thorough analysis of the optimization procedure, and suggests that improvements can be attained by (i) considering multiple random search directions instead of the Powell algorithm, and (ii) ensuring, through regulariza¯ ∗ have the ability to genertion, that the optimal λ alize well. This analysis is completed by the works of (Foster and Kuhn, 2009), which also suggests to improve the exploration of the search space by using well"
2011.eamt-1.33,D07-1080,0,0.150934,"ected that the most significant speed improvements will be obtained by reducing the number of iterations. The other main issue is the stability of the results, which, in practice, is addressed by running the optimize several times, with different starting points. These inefficiencies have stimulated the development of alternative approaches3 . Attempts at improving MERT can be split into two categories: works that try to fix the optimization procedure and works that consider alternative, arguably easier to optimize or better suited training criteria (Smith and Eisner, 2006; Zens et al., 2007; Watanabe et al., 2007). In the sequel, we only discuss the former approaches, which are more relevant to this work. (Cer et al., 2008) provides a thorough analysis of the optimization procedure, and suggests that improvements can be attained by (i) considering multiple random search directions instead of the Powell algorithm, and (ii) ensuring, through regulariza¯ ∗ have the ability to genertion, that the optimal λ alize well. This analysis is completed by the works of (Foster and Kuhn, 2009), which also suggests to improve the exploration of the search space by using well chosen multiple restarting points at each"
2011.eamt-1.33,D07-1055,0,0.1312,"ion. It is thus expected that the most significant speed improvements will be obtained by reducing the number of iterations. The other main issue is the stability of the results, which, in practice, is addressed by running the optimize several times, with different starting points. These inefficiencies have stimulated the development of alternative approaches3 . Attempts at improving MERT can be split into two categories: works that try to fix the optimization procedure and works that consider alternative, arguably easier to optimize or better suited training criteria (Smith and Eisner, 2006; Zens et al., 2007; Watanabe et al., 2007). In the sequel, we only discuss the former approaches, which are more relevant to this work. (Cer et al., 2008) provides a thorough analysis of the optimization procedure, and suggests that improvements can be attained by (i) considering multiple random search directions instead of the Powell algorithm, and (ii) ensuring, through regulariza¯ ∗ have the ability to genertion, that the optimal λ alize well. This analysis is completed by the works of (Foster and Kuhn, 2009), which also suggests to improve the exploration of the search space by using well chosen multiple re"
2011.eamt-1.41,P05-1071,0,0.0328206,"d by the translation systems, we select a subset of the LDC resources made available by the NIST MT’09 constrained track2 . In order to validate the obtained results on training corpora of varying sizes, we consider two training conditions, one with 30K parallel sentence pairs, and another with 130K. For each condition, we report below the AER, the BLEU scores on the test set, along with the size of the obtained phrase tables. A 4-gram back-off language model, estimated with SRILM3 is trained on the NIST MT’09 constrained English data. All Arabic sentences are pre-processed using MADA+TOKAN4 (Habash and Rambow, 2005), and segmented according to the D2 tokenization scheme. The IBM Arabic-English Word Alignment Corpus (Ittycheriah et al., 2006) is used to train both CRF and MaxEnt aligners and evaluate them using Alignment Error Rate (AER). 5.1 Translation Models Construction In section 3, we have described a generic algorithm that constructs the translation model in three steps: word alignment, phrase-pairs extraction, and scoring. In this section, we compare different instantiations of these steps, and report the translation performance of the resulting models. In the word alignment step, we experiment tw"
2011.eamt-1.41,N03-1017,0,0.449227,"p of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probability distribution over all possible alignments. However, the most common practice (Koehn et al., 2003) is to use only the 1-best, Viterbi alignment, while discarding all the other informations contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best"
2011.eamt-1.41,D09-1106,0,0.125085,"Missing"
2011.eamt-1.41,N06-1013,0,0.0703931,"y many alignments. The authors of (Liu et al., 2009) estimate link probabilities by calculating relative frequencies over a list of N-best alignments produced by generative models, and show some improvements in translation quality. However, using small N-best lists as samples is known to yield poor estimates of the alignment posteriors, as these lists usually contain too few variations. In this paper, we argue that better estimation of alignment probabilities helps achieving clearer improvements. Our solution is to directly model the weighted alignment matrices using a discriminative aligner (Ayan and Dorr, 2006; Tomeh et al., 2010). The rest of this paper is organized as follows: we start in Section 2 by a recap of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignmen"
2011.eamt-1.41,W06-3123,0,0.0194356,"ranslations of one another, are first extracted. 2) Phrase pairs accumulated over the entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixe"
2011.eamt-1.41,J93-2003,0,0.0367641,"ley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In"
2011.eamt-1.41,D10-1053,0,0.0306266,"Missing"
2011.eamt-1.41,P08-2007,0,0.0178677,"titutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixed segmentation, based on word boundaries, is used, and the resulting model is simpler to train using EM. Then, for each word-aligned sentence in the training corpus, an additional step is required to identify the set of phrase-p"
2011.eamt-1.41,W06-3105,0,0.023653,"e entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixed segmentation, based on word boundaries, is used, and the resulting model is simpler"
2011.eamt-1.41,H05-1022,0,0.0207298,"rly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In (de Gispert et al., 2010), a similar procedure"
2011.eamt-1.41,P06-1121,0,0.0388355,"s contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 19"
2011.eamt-1.41,W08-0509,0,0.0307294,"1 , ei1 ) can be calculated and used as a fractional count. Only phrase-pairs with a fractional count above certain threshold tp 7 are extracted. The same fractional counts are used for scoring with relative fractional frequencies. In both configurations, only phrasepairs that do not exceed a length limit of 7, on the source or the target side, are retained and scored. 5.2 Results and Discussion In this section, we describe five alignment systems and compare their performance (see Table 5.2). MGIZA++8 These alignments are produced by the multi-threaded and optimized alignment toolkit MGIZA++ (Gao and Vogel, 2008), which implements the IBM models. This tool only outputs deterministic alignment matrices in configuration (i). These models also deliver features for the discriminative word aligners described below. MGIZA++ IBM4 represents the performance of the standard baseline: one IBM4 alignment in each direction, which are symmetrized with grow-diagfinal-and heuristic. This system deliver competitive BLEU scores of 35.9 and 40.2 on the 30K and 130K respectively, with a much smaller phrase table than all the other systems. N-best WAM9 These alignments build weighted matrices, by averaging link occurence"
2011.eamt-1.41,W02-1018,0,0.0676035,"rase-pairs, that are translations of one another, are first extracted. 2) Phrase pairs accumulated over the entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words inste"
2011.eamt-1.41,D08-1022,0,0.0172246,"ich relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probability distribution over all possible alignments. However, the most common practice (Koehn et al., 2003) is to use only the 1-best, Viterbi alignment, while discarding all the other informations contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translat"
2011.eamt-1.41,W08-0303,0,0.0203569,"N-best WAM by ≈ 0.8 BLEU point. The weighted matrix configuration performs even better than the standard one and increases BLEU scores by another ≈ 0.3 BLEU point. Improvements are persistent but less apparent on the larger task. We notice that the phrase table extracted from the weighted matrix is considerably larger than the standard one (by a factor of at least 3). PostCAT also slightly decreases the AER as compared to the MGIZA++ baseline. CRF12 The alignment matrix is modeled with a conditional random field (CRF), of which the graphical structure is quite complex and contains many loops (Niehues and Vogel, 2008). Therefore, neither training nor inference can be performed exactly, and the loopy belief propagation algorithm is used to approximate the posteriors. The CRF approach differs from our MaxEnt model (Section 4) in two aspects: first, MaxEnt training only optimizes the log-likelihood, whereas CRF training also aims at minimizing the AER. Second, while both models use the same set of features, MaxEnt 10 http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html http://code.google.com/p/geppetto/ 12 We thank J. Niehues (KIT) for sharing his implementation. 11 Translation task: 30K Translation model constru"
2011.eamt-1.41,P03-1021,0,0.0110619,"using a combination of `1 and `2 terms, allowing for efficient feature selection while maintaining numerical stability. 5 Experiments In our experiments, we aim (1) to compare the standard translation model training method with the method based on weighted alignment matrices; and (2) to contrast different approches to populate the matrices with link posterior probabilities. For this purpose we build several phrasebased, Arabic to English, translation systems us309 ing Moses1 in its default configuration. In order to tune the parameters of the translation systems, Minimum Error-Rate Training (Och, 2003) is applied on the development corpus, for which we used the NIST MT’06 evaluation’s test set, containing 1,797 Arabic sentences (46K words) with four English references (53K words). The performance of each system is assessed by calculating the multi-reference BLEU on NIST MT’08 evaluation’s test set, which contains 1,360 Arabic sentences (43K words), each with four references (53K words). For training the various models used by the translation systems, we select a subset of the LDC resources made available by the NIST MT’09 constrained track2 . In order to validate the obtained results on tra"
2011.eamt-1.41,2010.amta-papers.18,1,0.92306,"e authors of (Liu et al., 2009) estimate link probabilities by calculating relative frequencies over a list of N-best alignments produced by generative models, and show some improvements in translation quality. However, using small N-best lists as samples is known to yield poor estimates of the alignment posteriors, as these lists usually contain too few variations. In this paper, we argue that better estimation of alignment probabilities helps achieving clearer improvements. Our solution is to directly model the weighted alignment matrices using a discriminative aligner (Ayan and Dorr, 2006; Tomeh et al., 2010). The rest of this paper is organized as follows: we start in Section 2 by a recap of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probabili"
2011.eamt-1.41,P03-1041,0,0.151405,"ng et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In (de Gispert et al., 201"
2011.eamt-1.41,2008.amta-papers.18,0,0.078552,"al researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this prob"
2011.eamt-1.41,D07-1078,0,0.0278246,"istribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al."
2011.eamt-1.41,2006.amta-papers.2,0,\N,Missing
2011.eamt-1.41,2010.iwslt-papers.14,0,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.iwslt-evaluation.7,J04-2004,0,0.757535,"hods for adapting statistical models using both in-domain and out-of-domain data are actively sought and several proposals have been studied in the literature (see below). The IWSLT’11 “TED” task offers a nice test case for adaptation techniques, since the volume of talk data is, by far, outnumbered by the other sources of data, be they parallel or monolingual. LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French with the intent to improve our understanding of adaptation techniques for SMT. Our submission is based on the n-gram based approach to Machine Translation [1, 2], a framework in which it is relatively simple to re-implement and compare various adaptation strategies. Several proposal have been put forward to adapt SMT systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the l"
2011.iwslt-evaluation.7,D10-1044,0,0.0409521,"anslation [1, 2], a framework in which it is relatively simple to re-implement and compare various adaptation strategies. Several proposal have been put forward to adapt SMT systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered va"
2011.iwslt-evaluation.7,W07-0717,0,0.0565975,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,W07-0733,0,0.0438259,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,2010.eamt-1.30,0,0.014856,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,2008.iwslt-papers.6,0,0.0267923,"ith the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that we reproduced in various past experiments [8], is that performing an ad-hoc linear combination of models seems to be more effective than tuning the weights of a log-linear model combination with MERT [9]. This finding seems to contradict the findings of [5]. We have found again the same effect, and try to provide some analysis for this unexpected behavior. Another co"
2011.iwslt-evaluation.7,P03-1021,0,0.126132,"elf-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that we reproduced in various past experiments [8], is that performing an ad-hoc linear combination of models seems to be more effective than tuning the weights of a log-linear model combination with MERT [9]. This finding seems to contradict the findings of [5]. We have found again the same effect, and try to provide some analysis for this unexpected behavior. Another contribution of the paper is an empirical study of adaptation for Neural Network Language models, which was found here to improve the performance of the non-adapted models. The rest of the paper is organized as follows. In Sections 2 and 3, we describe our decoder, then the various sources of data that have been used to train our baseline systems. Section 4 presents the experimental results achieved during the development period whe"
2011.iwslt-evaluation.7,N04-4026,0,0.16466,"using the n-gram assumption: p(sJ1 , tI1 ) = K Y Figure 1: Tuple extraction from a sentence pair. p((s, t)k |(s, t)k−1 . . . (s, t)k−n+1 ), k=1 where s refers to a source symbol (resp. t for target) and (s, t)k to the k th tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 3.2 for details); four lexicon models; two lexicalized reordering models [10] aiming at predicting the orientation of the next translation unit; a “weak” distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework [9] (Minimum Error R"
2011.iwslt-evaluation.7,P02-1040,0,0.0841673,"Missing"
2011.iwslt-evaluation.7,J92-4003,0,0.064522,"ower order models [18, 19]. All LMs except the one trained on the news corpora from 2010-2011 were first linearly interpolated. The associated coefficients were estimated so as to minimize the perplexity evaluated on dev2010-2011. The resulting LM and the 20102011 LM were finaly interpolated with newstest2008 as development data. This procedure aims to avoid overestimating the weight associated to the 2010-2011 LM. 3.4. The SOUL Model We give here a brief overview of the SOUL LM; refer to [17] for the complete training procedure. Following the classical work on distributed word representation [20], we assume that the output vocabulary is structured by a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the i-th word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for the word wi in the clustering tree, with D the depth of the tree, cd (wi ) a class or sub-class assigned to wi , and cD (wi ) the leaf associated with wi (the word itself). The n-gram probability of wi given its history h can then be estimated as follows using the chain rule: P (wi |h) = P (c1 (wi )|h) D Y P (cd (wi )|h, c1:d−1 ) d=2 Figure 2"
2011.iwslt-evaluation.7,2011.iwslt-evaluation.1,0,0.0176011,"on and target language models that have already been adapted, when the SOUL model has only seen News data. Adapting the SOUL model with in-domain data does even slightly better: compared to the initial WMT baseline, the total accumulated improvement of adaptation is approximately +2.5 bleu points. Most of the results presented above have been obtained as the result of post-evaluation analyses. Our primary submission for the official TED task uses two separate bilingual models, as well as two separated target language models, and a non-adapted SOUL LM; the corresponding results are reported in [21]. 5. Conclusion In this paper, we presented LIMSI’s submission for IWSLT’2011 text translation task. These results were obtained using our in-house n-code system, which implements th n-gram based approach to SMT. One convenient feature of n-code is its ability to handle a arbitrary number of bilingual and target side language models, a facility which makes domain adaptation straightforward: it suffices to incorporate all the available in- and out-of-domain models in the loglinear combination and let the tuning procedure determine the best mixture weights. In particular, models computed for oth"
2011.iwslt-evaluation.7,W08-0310,1,0.93088,"Missing"
2011.iwslt-evaluation.7,J06-4004,0,\N,Missing
2011.iwslt-evaluation.7,W11-2135,1,\N,Missing
2011.iwslt-papers.10,N04-1021,0,0.430021,"mi 2749, 21027 Ispra, Italy marco.turchi@jrc.ec.europa.eu {nadi,wisniews,allauzen,yvon}@limsi.fr P (a, e|f ) = Z(f, λ)−1 exp Alexandre Allauzen † (1) i=1 where Z is a normalization constant and each hi is a feature function that decomposes over atomic phrase translations and λi is the corresponding feature weight which scales each feature’s contribution to the final model score. Typical features include language model, reordering and conditional phrase translation probabilities, word and phrase penalties and lexical weights. Additional feature functions are also investigated in the literature [1, 2]. For a PBSMT to produce a good translation, two conditions must be met: (i) good translations must exist in the search space of the decoder, and (ii) the model score must be (positively) correlated with translation quality. The first condition mainly depends on the coverage of the phrase translation candidates that are stored in the phrase table. A maximal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of p"
2011.iwslt-papers.10,N09-1025,0,0.0418301,"mi 2749, 21027 Ispra, Italy marco.turchi@jrc.ec.europa.eu {nadi,wisniews,allauzen,yvon}@limsi.fr P (a, e|f ) = Z(f, λ)−1 exp Alexandre Allauzen † (1) i=1 where Z is a normalization constant and each hi is a feature function that decomposes over atomic phrase translations and λi is the corresponding feature weight which scales each feature’s contribution to the final model score. Typical features include language model, reordering and conditional phrase translation probabilities, word and phrase penalties and lexical weights. Additional feature functions are also investigated in the literature [1, 2]. For a PBSMT to produce a good translation, two conditions must be met: (i) good translations must exist in the search space of the decoder, and (ii) the model score must be (positively) correlated with translation quality. The first condition mainly depends on the coverage of the phrase translation candidates that are stored in the phrase table. A maximal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of p"
2011.iwslt-papers.10,W07-0414,0,0.0186814,"s to find the best path, of which we employ two in our experiments. The first method is constrained decoding, as implemented in M OSES2 : the lattice is searched for the path with the highest model score that exactly matches the reference, and thus has a local BLEU score of one. However, if the reference is not attainable the sentence is discarded. The second method relaxes this constraint by using an oracle decoder that searches for the hypothesis that explicitly optimizes an approximation of the BLEU score at the sentence level as an objective. We implemented the lattice oracle decoder from [21], which, while being less conservative than constrained decoding (all source sentences are decoded), is agnostic about the model score which, therefore, needs to be optimized 1 δ (x) = w.Φ(x) − ρ, where w is the classifier h 2 http://www.statmt.org/moses/ weight vector. indirectly by pruning the lattice input of the decoder. 5. Feature Functions One of the main motivation of this work is to incorporate features into phrase pairs extraction, so as to smooth the conventional, alignment-based, phrase scores. We consider features from the literature [6, 16, 22, 4, 23], which evaluate various aspec"
2011.iwslt-papers.10,N03-1017,0,0.475537,"imal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of possible phrase pairs, the vast majority of which are in fact irrelevant, taking all possible phrase pairs into account is impractical, and all methods for constructing phrase tables comprise a first step where the quality of each phrase pair is estimated, and where phrase pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, t"
2011.iwslt-papers.10,P03-1041,0,0.542925,"implemented the lattice oracle decoder from [21], which, while being less conservative than constrained decoding (all source sentences are decoded), is agnostic about the model score which, therefore, needs to be optimized 1 δ (x) = w.Φ(x) − ρ, where w is the classifier h 2 http://www.statmt.org/moses/ weight vector. indirectly by pruning the lattice input of the decoder. 5. Feature Functions One of the main motivation of this work is to incorporate features into phrase pairs extraction, so as to smooth the conventional, alignment-based, phrase scores. We consider features from the literature [6, 16, 22, 4, 23], which evaluate various aspects of the association between a source and a target chunk. Most features are data-driven and language-independent, based on statistical word alignment and language models. A small set of language-dependent morpho-syntactic features is also used. Weighted Alignment Matrix (WAM) feature is a score computed using discriminative Weighted Alignment Matrices [6] similar to the model-based phrase pair posterior metric described in [16]. Each cell in a weighted matrix [5] contains the posterior probability of aligning the corresponding source and target words. A phrase pa"
2011.iwslt-papers.10,D07-1103,0,0.376938,"ssible phrase pairs into account is impractical, and all methods for constructing phrase tables comprise a first step where the quality of each phrase pair is estimated, and where phrase pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translati"
2011.iwslt-papers.10,D09-1106,0,0.0879548,"pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, wher"
2011.iwslt-papers.10,2011.eamt-1.41,1,0.911426,"pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, wher"
2011.iwslt-papers.10,J93-2003,0,0.0146232,"at are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these proced"
2011.iwslt-papers.10,W02-1018,0,0.0303579,"at are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these proced"
2011.iwslt-papers.10,P03-1021,0,0.0895713,"marily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these procedures, we study in this paper novel extraction and scoring procedures that : (1) can straightforwardly handle arbitrary feature functions; (2) have a direct relationship to translation quality; and (3) give the user a finer control over the size of the phrase table. This study has both practical and methodological implications. From a practical perspective, the scenario we consider is the use of a small set of parallel sentences, from which we would like to extract as much phrases as possible, so as to ensure the larger possible coverage. In this setting,"
2011.iwslt-papers.10,N07-2053,0,0.258182,"obtain optimal solution, each optimization iteration that involves training a standard phrase table with parameters {λk , τ }, should tune its weights with MERT, which is expensive and hence omitted in their experiments. A similar model is used in [22] to add features to extraction, without any parameter tuning. Our work is similar in respect of incorporating additional features to extraction, whereas our formulation of the problem in the supervised classification framework, unlike [16], allows much less expensive incorporation of the translation quality measure, which is ignored in [22]. In [27] the standard features in the log-linear translation model tuned with MERT is used to score phrase pairs already existing in the phrase table and employ a competitive linking algorithm to keep the best one-to-one phrase matching while discarding the rest. Contrarily tou our approach, feature weights selected by MERT, although directly optimizing translation quality, they are learned for a given phrase table and do not generalize to unseen phrase pairs. In [28], the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting on"
2011.iwslt-papers.10,P10-1147,0,0.0234293,"ation framework, unlike [16], allows much less expensive incorporation of the translation quality measure, which is ignored in [22]. In [27] the standard features in the log-linear translation model tuned with MERT is used to score phrase pairs already existing in the phrase table and employ a competitive linking algorithm to keep the best one-to-one phrase matching while discarding the rest. Contrarily tou our approach, feature weights selected by MERT, although directly optimizing translation quality, they are learned for a given phrase table and do not generalize to unseen phrase pairs. In [28], the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting one phrase pair at a time. Word and phrase level features are incorporated into a discriminative model for extraction. Manually annotated word alignments are used to automatically obtain training extraction sets, whereas we use the oracle decoder. Another related line of research is phrase table pruning, which is carried out by first assigning scores to phrase pairs, by ways of statistical significance tests [4, 17], or by computing the decoder usage statistics"
2011.iwslt-papers.10,N07-2006,0,0.249995,", the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting one phrase pair at a time. Word and phrase level features are incorporated into a discriminative model for extraction. Manually annotated word alignments are used to automatically obtain training extraction sets, whereas we use the oracle decoder. Another related line of research is phrase table pruning, which is carried out by first assigning scores to phrase pairs, by ways of statistical significance tests [4, 17], or by computing the decoder usage statistics [29]. Our method takes advantage of different pruning criteria and integrates them into the filtering procedure. The introduction of the translation process into the definition of useful phrase pairs has also been investigated in the literature. 8. Conclusions and Future Work In this paper we presented a novel translation quality informed procedure for both extraction and scoring of phrase pairs. The model at the center of our procedure combines arbitrary features to assess phrase quality. It is parametrized with a threshold that allows improved control over the size of the resulting phrase table,"
2011.iwslt-papers.10,2010.amta-papers.28,0,0.210183,"equire to examine all the (nonoptimal) derivations of our test data, which is clearly unrealistic. A nice walk-around is to use single-class classification [10] techniques, which aim at learning concepts in the absence of counter examples, by distinguishing one class of (positive) instances from all other possible instances. Such techniques can handle arbitrary feature functions to represent candidate phrase pairs, thus making the extraction procedure more robust to alignment errors. A useful by-product of the model is the computation of an accuracy-based feature, analogous to the proposal in [11]. In short, our main contribution can be viewed as a novel translation quality informed procedure for both extraction and scoring of phrase pairs. The rest of the paper is organized as follows. In Section 2, we motivate the formulation of phrase pair extraction as a single-class classification problem and describe a practical extraction pipeline. The One-Class SVM (OC-SVM) [12] and the Mapping Convergence (MC) [13] algorithms, which are used to train the single-class classifier are presented in Section 3. In Section 4, we describe the oracle decoder used to label positive examples. Our feature"
2011.iwslt-papers.10,P10-1049,0,0.0397474,"res. 266 Source phrase: األوضاع الراهنة 37.3 Standard Z-scores SCC BLEU 36.8 36.3 F: STD 35.8 STD WAM F: STD+SCC 35.3 0 0.01 0.02 0.03 0.04 0.05 Fraction of selected unlabeled data (U - P) Candidate target translations Figure 5: BLEU: SCC scores as a new feature Figure 6: Z-scores: comparison of different scoring methods construct high precision phrase tables with the best phrase pairs, recall oriented phrase tables require more sophisticated decision procedures to retrieve good translations in the large set of candidates that are difficult to distinguish and ignored by standard methods. In [30] an oracle decoder is used to compute forced phrasal alignment, that are then used in a leaving-one-out smoothing technique, which results in a better estimation of translation probabilities. In [11] an oracle decoder is used to identify the best hypothesis in the n-best list output of the decoder and use an average edit-distance between phrase pairs occurring in the oracle hypothesis and other phrase pairs in other hypothesis in the n-best list, to compute a translation quality-based feature that is added to the phrase table. Unlike these methods, our procedure applies the oracle decoder to t"
2011.iwslt-papers.10,D09-1039,0,0.0202296,"n to learning from positive examples, exploits unlabeled data to improve the accuracy of the classifier. In step (4), the best classifier, output of the previous step, is applied to the unlabeled phrase pairs (U − P ), estimating to what extent they resemble the positive samples, and which ones ought to be extracted. The distance to the decision boundary (the hyperplane in the SVM feature space) is interpreted as a confidence measure, and used for two purposes: it is thresholded to extract phrase pairs; and injected into the final phrase table as an accuracy-based feature function, similar to [11, 15]. Final phrase table contains all phrase pairs labeled as positive either by the oracle decoder or by the learned classifier. Any subset of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as poss"
2011.iwslt-papers.10,P08-1010,0,0.586115,"used for two purposes: it is thresholded to extract phrase pairs; and injected into the final phrase table as an accuracy-based feature function, similar to [11, 15]. Final phrase table contains all phrase pairs labeled as positive either by the oracle decoder or by the learned classifier. Any subset of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as possible. Precision of standard phrase tables can be improved by filtering out most of the entries, using some statistical significance test [4, 17]. On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors [6]. The algorithm presented here attempts to circumvent alignment errors and increase accuracy by integrating multiple features and combining them discriminatively. A"
2011.iwslt-papers.10,2009.mtsummit-papers.17,1,0.874158,"of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as possible. Precision of standard phrase tables can be improved by filtering out most of the entries, using some statistical significance test [4, 17]. On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors [6]. The algorithm presented here attempts to circumvent alignment errors and increase accuracy by integrating multiple features and combining them discriminatively. At the same time, the threshold on the classifier score presents a control point over the balance between precision and recall, and introduces an additional parameter that can be tuned via grid search, for an optimal performance on a specific translation task. 262 3. Learning the Single-Class Classifier 3"
2011.jeptalnrecital-long.36,C94-2178,0,0.340414,"Missing"
2011.jeptalnrecital-long.36,H91-1026,0,0.402571,"Missing"
2011.jeptalnrecital-long.36,W08-0509,0,0.0278544,"Missing"
2011.jeptalnrecital-long.36,2005.mtsummit-papers.11,0,0.0678284,"Missing"
2011.jeptalnrecital-long.36,P07-2045,0,0.00856459,"Missing"
2011.jeptalnrecital-long.36,N03-1017,0,0.0393429,"Missing"
2011.jeptalnrecital-long.36,2008.amta-papers.11,1,0.873477,"Missing"
2011.jeptalnrecital-long.36,R09-1040,1,0.810694,"Missing"
2011.jeptalnrecital-long.36,J00-2004,0,0.0878283,"Missing"
2011.jeptalnrecital-long.36,W05-0801,0,0.0463745,"Missing"
2011.jeptalnrecital-long.36,P02-1040,0,0.0798969,"Missing"
2011.jeptalnrecital-long.36,2006.amta-papers.25,0,0.0980577,"Missing"
2011.jeptalnrecital-long.36,takezawa-etal-2002-toward,0,0.0599455,"Missing"
2011.jeptalnrecital-long.36,W09-3830,0,0.0320763,"Missing"
2011.jeptalnrecital-long.37,P89-1018,0,0.0143928,"Missing"
2011.jeptalnrecital-long.37,J93-2003,0,0.0424327,"Missing"
2011.jeptalnrecital-long.37,J07-2003,0,0.147443,"Missing"
2011.jeptalnrecital-long.37,D10-1053,0,0.0327985,"Missing"
2011.jeptalnrecital-long.37,W06-3105,0,0.0435905,"Missing"
2011.jeptalnrecital-long.37,P08-2007,0,0.0275874,"Missing"
2011.jeptalnrecital-long.37,H05-1022,0,0.0442315,"Missing"
2011.jeptalnrecital-long.37,N07-2007,0,0.0538593,"Missing"
2011.jeptalnrecital-long.37,P06-1121,0,0.104305,"Missing"
2011.jeptalnrecital-long.37,W08-0509,0,0.0323651,"Missing"
2011.jeptalnrecital-long.37,J10-3007,0,0.0303273,"Missing"
2011.jeptalnrecital-long.37,P07-2045,0,0.00753602,"Missing"
2011.jeptalnrecital-long.37,N03-1017,0,0.0408729,"Missing"
2011.jeptalnrecital-long.37,P10-1052,1,0.864066,"Missing"
2011.jeptalnrecital-long.37,2010.iwslt-papers.14,0,0.0214156,"Missing"
2011.jeptalnrecital-long.37,D09-1106,0,0.0297969,"Missing"
2011.jeptalnrecital-long.37,D08-1022,0,0.0391956,"Missing"
2011.jeptalnrecital-long.37,J03-1002,0,0.0174968,"Missing"
2011.jeptalnrecital-long.37,P02-1040,0,0.0830726,"Missing"
2011.jeptalnrecital-long.37,2010.amta-papers.18,1,0.848277,"Missing"
2011.jeptalnrecital-long.37,P03-1041,0,0.0578449,"Missing"
2011.jeptalnrecital-long.37,2008.amta-papers.18,0,0.031129,"Missing"
2011.jeptalnrecital-long.37,C96-2141,0,0.461889,"Missing"
2011.jeptalnrecital-long.37,D07-1078,0,0.0573657,"Missing"
2011.jeptalnrecital-long.37,P10-1049,0,0.028198,"Missing"
2011.jeptalnrecital-long.37,2002.tmi-tutorials.2,0,0.067176,"Missing"
2012.amta-papers.17,W05-0909,0,0.0369848,"boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even impossibility to adequately define the latter, quality measures generally depend on multiple inter-constrained characteristics describing the source sentence and the translation hypotheses. For instance, popular evaluation metrics, like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005), consider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performa"
2012.amta-papers.17,N09-1025,0,0.0559689,"on is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ"
2012.amta-papers.17,P08-2010,0,0.294298,"nstrated performance remains basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not"
2012.amta-papers.17,D11-1004,0,0.328202,"hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performance (Kumar et al., 2009), even when an almost exact optimization method is used (Galley and Quirk, 2011). Modeling inadequacy, and, in particular, the use of over-simplistic linear scoring functions in low-dimensional space can be held responsible for this disappointing performance. This paper can be seen as an attempt to verify whether the mere replacement of a linear with a nonlinear scoring function in a conventional phrasebased SMT system that uses only a few dozens features can actually improve performance, by capturing more precisely the complex boundaries between good and bad translations. The rest of the paper is organized as follows. In the next section, we review related work. In Secti"
2012.amta-papers.17,W11-2130,0,0.0893878,"ghted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly. Lagarda and Casacuberta (2008) apply AdaBoost by reweighting on each boosting iteration a separate “translation model” introduced into the linear model. Related Work 3 Recently, new approaches to tuning SMT systems have received attention, namely the ranking methods (Hopkins and May, 2011; Haddow et al., 2011). The motivation for these is as follows. Although BLEU is defined for a pair of corpora, one can use the same formula to calculate a sentence-level approximation of BLEU that evaluates the similarity between a single hypothesis e and its reference r, and to order hypotheses according to it. This natural ordering is used by ranking approaches in SMT to learn system parameters, taking advantage of the fact that one can deduce information about parameters even from the comparison between mediocre or bad hypotheses. Non-Linear Hypotheses Reranking Motivated by the inability of linear models to im"
2012.amta-papers.17,D11-1125,0,0.438729,"n, we review related work. In Section 3, we describe a ranking approach to tuning SMT systems, together with our method of learning a non-linear scoring function in the learning-to-rank paradigm. Next, we explain feature transformations (Section 4) used in the experiments reported in Section 5. Discussions in Section 6 close the paper. 2 Ranking approaches, however, were until now used only from the perspective of redefining the target loss in optimization. Scoring functions remained simple linear combinations, and the demonstrated performance remains basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learne"
2012.amta-papers.17,W04-3250,0,0.14744,"Missing"
2012.amta-papers.17,P09-1019,0,0.0500749,"sider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performance (Kumar et al., 2009), even when an almost exact optimization method is used (Galley and Quirk, 2011). Modeling inadequacy, and, in particular, the use of over-simplistic linear scoring functions in low-dimensional space can be held responsible for this disappointing performance. This paper can be seen as an attempt to verify whether the mere replacement of a linear with a nonlinear scoring function in a conventional phrasebased SMT system that uses only a few dozens features can actually improve performance, by capturing more precisely the complex boundaries between good and bad translations. The rest of the pape"
2012.amta-papers.17,2008.eamt-1.14,0,0.49314,"use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly. Lagarda and Casacuberta (2008) apply AdaBoost by reweighting on each boosting iteration a separate “translation model” introduced into the linear model. Related Work 3 Recently, new approaches to tuning SMT systems have received attention, namely the ranking methods (Hopkins and May, 2011; Haddow et al., 2011). The motivation for these is as follows. Although BLEU is defined for a pair of corpora, one can use the same formula to calculate a sentence-level approximation of BLEU that evaluates the similarity between a single hypothesis e and its reference r, and to order hypotheses according to it. This natural ordering is u"
2012.amta-papers.17,W11-2168,1,0.824934,"’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ e and f , g¯(a, e, f ) is the featur"
2012.amta-papers.17,N12-1005,1,0.905364,"be done using the approximate “3-rd method” described in (Freund et al., 2003), the other two learners using a straightforward generalization of the same method. 4 4.1 Features Baseline Configurations We test our proposal on two decoder configurations that differ by the number of features considered. First, the basic configuration uses only the 11 features routinely found in any SMT decoder;2 the extended configuration contains an enriched set of 23 features and corresponds to the state-of-the-art translation system – the best system for the FrenchEnglish pair in the recent WMT’12 evaluation (Le et al., 2012b). The additional features considered are mainly based on neural network language models and translation models (Le et al., 2011; Le et al., 2012a). These features are integrated within a reranking step optimized with MERT.3 A summary 2 Target language model, translation model, 2 CFB lexicalized reordering models, 4 lexical translation weights, distortion and 2 penalties for words and phrases: 11 features in total. 3 To construct the extended feature set, the basic feature set was first augmented with two supplementary translation models on bilingual tuples and four lexicalized reordering fea"
2012.amta-papers.17,C04-1072,0,0.0834367,"om independent directions supplemental to the default axis-aligned direction. For the extended configuration, n-best lists of the last MERT iteration are augmented with 5 neuralnetwork models (Section 4.1) and reoptimized with MERT before applying features transformations. RankBoost training is performed on the WMT’09 evaluation set on 100-best and 300-best lists, respectively, for the basic and extended configurations, using the final n-best lists after the complete MERT optimization, separately for each MERT rerun. In all our experiments, we consider the sentence level BLEU+1 approximation (Lin and Och, 2004) to evaluate translation quality. Similarly to (Hopkins and May, 2011), to reduce the number of pairs and to speed up learning we sampled the n-best lists leaving only 2, 000 randomly selected pairs with quality difference superior to 0.05 BLEU points. Tests with different number of sampled pairs showed little sensitivity to this parameter in the range between 50 and 5, 000 pairs (outside of this interval the performance considerably decreases). Conversion to a bipartite ranking problem (that enables a more efficient and simpler algorithm (Freund et al., 2003)) with gaps did marginally help on"
2012.amta-papers.17,P02-1038,0,0.28591,"ent: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ e and f , g¯(a, e, f ) is the feature vector representing various compatibility measures between a, e and ¯ is a parameter vector. Using this model, f , and λ searching for the most probable translation boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even im"
2012.amta-papers.17,P02-1040,0,0.0835126,"for the most probable translation boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even impossibility to adequately define the latter, quality measures generally depend on multiple inter-constrained characteristics describing the source sentence and the translation hypotheses. For instance, popular evaluation metrics, like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005), consider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores"
2012.amta-papers.17,W05-0908,0,0.0651604,"Missing"
2012.amta-papers.17,E12-1013,1,0.799177,"s approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT’10, WMT’11 and WMT’12 test sets. Several papers have recently pointed out that the scoring function is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (betw"
2012.amta-papers.17,D10-1091,1,0.847116,"the applicability of this approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT’10, WMT’11 and WMT’12 test sets. Several papers have recently pointed out that the scoring function is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small nu"
2012.amta-papers.17,P10-1076,0,0.095874,"basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly."
2012.eamt-1.62,P06-1002,0,0.0892244,"Missing"
2012.eamt-1.62,C88-1016,0,0.340863,"a certain number of scores loosely reflecting the likelihood that source translates to target. The problem of identifying sub-sentential mappings from parallel texts, e.g. between isolated words or n-grams of words, is well-known, and numerous proposals have been put forward to perform this task. Those methods roughly fall into two main c 2012 European Association for Machine Translation. this context, a phrase is a sequence of words and does not necessarily correspond to a syntactic phrase. 1 In 279 yves.lepage@waseda.jp categories. On the one hand, the probabilistic approach, introduced by Brown et al. (1988), considers the problem of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an imp"
2012.eamt-1.62,J93-2003,0,0.245932,". On the one hand, the probabilistic approach, introduced by Brown et al. (1988), considers the problem of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the s"
2012.eamt-1.62,A94-1006,0,0.331398,"Missing"
2012.eamt-1.62,P07-1003,0,0.0204054,"the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are gene"
2012.eamt-1.62,H05-1022,0,0.0233815,"rallel sentences. This approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heu"
2012.eamt-1.62,J93-1003,0,0.078444,"tric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is processed independently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integration with SMT, of which it constitu"
2012.eamt-1.62,D07-1006,0,0.016161,"n defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduc"
2012.eamt-1.62,C94-2178,0,0.381878,"ly produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is processed independently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integ"
2012.eamt-1.62,P98-1069,0,0.166484,"Missing"
2012.eamt-1.62,H91-1026,0,0.581153,"hev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is proces"
2012.eamt-1.62,P08-1112,0,0.040818,"Missing"
2012.eamt-1.62,W08-0509,0,0.0890915,"roparl corpus (Koehn, 2005), in three languages: Finnish–English (agglutinating language–isolating language), French–Spanish, and Portuguese-Spanish (very close languages). For each pair, we use a training set made up of 350,000 sentence pairs (avg.: 30 words/sentence in English), and development and test sets made up of 2,000 sentence pairs each. The systems are optimized with MERT (Och, 2003). Unless otherwise specified, a lexicalized reordering model is used. Translations are evaluated using BLEU (Papineni et al., 2002) and TER2 (Snover et al., 2006). Five approaches are compared: MGIZA++ (Gao and Vogel, 2008), implements the IBM models (Brown et al., 1993) and the HMM of Vogel et al. (1996). Integrated to Moses, it remains the reference in the domain. It is run with default settings: 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). The alignments are then made symmetric and a translation table is produced from the alignments using Moses tools (grow-diag-finaland heuristic for phrase pair extraction). Anymalign (Lardilleux et al., 2011a), used to directly build the translation tables. As this tool can be stopped at any time, its running time is"
2012.eamt-1.62,D07-1103,0,0.0190696,"ters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is processed independently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integration with SMT, of which it constitutes a cornerstone since the introduction of IBM models (Brown et al., 1993). The two approaches have shown complementary strengths and weaknesses, as acknowledged by e.g. Johnson et al. (2007), where phrase associations extracted from word alignments are filtered out according to statistical association measures. Anymalign, introduced in (Lardilleux and Lepage, 2009; Lardilleux et al., 2011a), aims at extracting sub-sentential associations, addressing a number of issues that are often overlooked. It can process any number of languages simultaneously, it does not make any distinction between source and target, is amenable to massive parallelism, scales easily, and is very simple to implement. Anymalign’s association scores have proven to produce better results than state-of-the-art"
2012.eamt-1.62,N03-1017,0,0.0492979,"n this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (200"
2012.eamt-1.62,P07-2045,0,0.0101203,"arallel corpus to align, because each sentence pair is processed independently. Aligning a corpus can thus easily be made parallel: the total running time is divided by the number of available processors. Another advantage is that the alignments produced are symmetric during the whole process, contrary to more widely spread models such as IBM models that produce better result when run in both translation directions and their outputs combined using heuristics. 282 Evaluation Description of Experiments Our alignment method is evaluated within a phrase-based SMT system. We use the Moses toolkit (Koehn et al., 2007), and data extracted from the Europarl corpus (Koehn, 2005), in three languages: Finnish–English (agglutinating language–isolating language), French–Spanish, and Portuguese-Spanish (very close languages). For each pair, we use a training set made up of 350,000 sentence pairs (avg.: 30 words/sentence in English), and development and test sets made up of 2,000 sentence pairs each. The systems are optimized with MERT (Och, 2003). Unless otherwise specified, a lexicalized reordering model is used. Translations are evaluated using BLEU (Papineni et al., 2002) and TER2 (Snover et al., 2006). Five ap"
2012.eamt-1.62,2005.mtsummit-papers.11,0,0.359582,"7 122 t1 A A¯ s1 ... sx−1 sx .. . sI B ... ty−1 B¯ ... ty W (A, B) ¯ W (A, B) ¯ B) W (A, ¯ B) ¯ W (A, tJ Figure 2: Schematic representation of the segmen¯ tation of a pair of sentences S = A. A¯ and T = B. B. w(pays, country) = p(pays|country) × p(country|pays) + 4,057 + 2,007 = 151,190 + 17,717 +17,717 10,865 + 6,284 + 4,057 + 3,742 + 2,007 17,717 + 4,057 + 2,007 × 17,717 + 4,057 + 2,007 + 122 ' 0.121 Figure 1: Computing a score between source word pays and target word country from a subset of a translation table produced by Anymalign with the French and English parts of the Europarl corpus (Koehn, 2005). an indicator of the quality of the entry; it is just the number of times the translation pair has been produced by Anymalign (see (Lardilleux et al., 2011a) for details). This computation is illustrated on Figure 1. What we do here is tantamount to a very simplified version of the algorithm that is used to train standard translation models: starting with lexical associations, we derive by heuristic means an optimal (Viterbi) alignment, from which the translation tables are finally computed. Our procedure is much simpler, though, as we do not iterate the procedure (like in EM training) and di"
2012.eamt-1.62,R09-1040,1,0.878331,"dependently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integration with SMT, of which it constitutes a cornerstone since the introduction of IBM models (Brown et al., 1993). The two approaches have shown complementary strengths and weaknesses, as acknowledged by e.g. Johnson et al. (2007), where phrase associations extracted from word alignments are filtered out according to statistical association measures. Anymalign, introduced in (Lardilleux and Lepage, 2009; Lardilleux et al., 2011a), aims at extracting sub-sentential associations, addressing a number of issues that are often overlooked. It can process any number of languages simultaneously, it does not make any distinction between source and target, is amenable to massive parallelism, scales easily, and is very simple to implement. Anymalign’s association scores have proven to produce better results than state-of-the-art methods on bilingual lexicon constitution tasks (evaluation performed by comparing word associations with reference dictionaries). However, Anymalign’s phrase tables are not as"
2012.eamt-1.62,N06-1014,0,0.0326161,"approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ne"
2012.eamt-1.62,Y11-1016,1,0.702092,"performed by comparing word associations with reference dictionaries). However, Anymalign’s phrase tables are not as good as those obtained with standard methods (evaluation performed with standard MT metrics) (Lardilleux et al., 2011b). One possible explanation for these contrasted results is that, Anymalign does not compute any alignment at the word or at the phrase level; instead, it directly computes translation tables along with their associated scores. Those tables have very different profiles than those obtained with probabilistic methods, mainly in terms of their n-gram distribution (Luo et al., 2011). In particular, despite recent improvements (Lardilleux et al., 2011b), the quantity of long n-grams produced remains relatively small compared with Moses’s translation tables. In this paper, we complement Anymalign with a simple alignment algorithm, so as to better understand its current limitations. The resulting alignments improve Anymalign’s phrase tables to a point where they can be used to obtain state-of-the art results. In passing, we also propose a computationally cheap way to compute ITG alignments based on arbitrary word level association scores. The rest of this paper is organized"
2012.eamt-1.62,W02-1018,0,0.0453506,"he corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficien"
2012.eamt-1.62,J00-2004,0,0.0244408,"h directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is processed independently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integration with SMT, of which it constitutes a cornerstone since the introduction of"
2012.eamt-1.62,W04-3243,0,0.0817405,"Missing"
2012.eamt-1.62,W05-0801,0,0.0788276,"al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation process is used, where each sentence is processed independently. Alignment links can then be computed, using for instance the greedy algorithm proposed by Melamed (2000) (competitive linking). The probabilistic approach is the most widely used, mainly due to its tight integration with SMT, of which it constitutes a cornerstone since the introduction of IBM models (Brown et"
2012.eamt-1.62,J03-1002,0,0.0566834,"al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual information (Gale and Church, 1991; Fung and Church, 1994), or likelihood ratio (Dunning, 1993)—see also more recent work by Melamed (2000) and by Moore (2005). Computations are generally limited to a list of association candidates precomputed using patterns and filters, for instance, by focusing exclusively on the most frequent word n-grams. In this approach, a local maximisation pr"
2012.eamt-1.62,P03-1021,0,0.0164216,"ombined using heuristics. 282 Evaluation Description of Experiments Our alignment method is evaluated within a phrase-based SMT system. We use the Moses toolkit (Koehn et al., 2007), and data extracted from the Europarl corpus (Koehn, 2005), in three languages: Finnish–English (agglutinating language–isolating language), French–Spanish, and Portuguese-Spanish (very close languages). For each pair, we use a training set made up of 350,000 sentence pairs (avg.: 30 words/sentence in English), and development and test sets made up of 2,000 sentence pairs each. The systems are optimized with MERT (Och, 2003). Unless otherwise specified, a lexicalized reordering model is used. Translations are evaluated using BLEU (Papineni et al., 2002) and TER2 (Snover et al., 2006). Five approaches are compared: MGIZA++ (Gao and Vogel, 2008), implements the IBM models (Brown et al., 1993) and the HMM of Vogel et al. (1996). Integrated to Moses, it remains the reference in the domain. It is run with default settings: 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). The alignments are then made symmetric and a translation table is produced from the alignments"
2012.eamt-1.62,P02-1040,0,0.0913641,"ased SMT system. We use the Moses toolkit (Koehn et al., 2007), and data extracted from the Europarl corpus (Koehn, 2005), in three languages: Finnish–English (agglutinating language–isolating language), French–Spanish, and Portuguese-Spanish (very close languages). For each pair, we use a training set made up of 350,000 sentence pairs (avg.: 30 words/sentence in English), and development and test sets made up of 2,000 sentence pairs each. The systems are optimized with MERT (Och, 2003). Unless otherwise specified, a lexicalized reordering model is used. Translations are evaluated using BLEU (Papineni et al., 2002) and TER2 (Snover et al., 2006). Five approaches are compared: MGIZA++ (Gao and Vogel, 2008), implements the IBM models (Brown et al., 1993) and the HMM of Vogel et al. (1996). Integrated to Moses, it remains the reference in the domain. It is run with default settings: 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). The alignments are then made symmetric and a translation table is produced from the alignments using Moses tools (grow-diag-finaland heuristic for phrase pair extraction). Anymalign (Lardilleux et al., 2011a), used to directly"
2012.eamt-1.62,J96-1001,0,0.2525,"Missing"
2012.eamt-1.62,2006.amta-papers.25,0,0.0463637,"toolkit (Koehn et al., 2007), and data extracted from the Europarl corpus (Koehn, 2005), in three languages: Finnish–English (agglutinating language–isolating language), French–Spanish, and Portuguese-Spanish (very close languages). For each pair, we use a training set made up of 350,000 sentence pairs (avg.: 30 words/sentence in English), and development and test sets made up of 2,000 sentence pairs each. The systems are optimized with MERT (Och, 2003). Unless otherwise specified, a lexicalized reordering model is used. Translations are evaluated using BLEU (Papineni et al., 2002) and TER2 (Snover et al., 2006). Five approaches are compared: MGIZA++ (Gao and Vogel, 2008), implements the IBM models (Brown et al., 1993) and the HMM of Vogel et al. (1996). Integrated to Moses, it remains the reference in the domain. It is run with default settings: 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). The alignments are then made symmetric and a translation table is produced from the alignments using Moses tools (grow-diag-finaland heuristic for phrase pair extraction). Anymalign (Lardilleux et al., 2011a), used to directly build the translation tables."
2012.eamt-1.62,C96-2141,0,0.89756,"words or groups of words in parallel sentences. This approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associa"
2012.eamt-1.62,2005.mtsummit-papers.33,0,0.0267901,"determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative approaches (also called heuristic by Och and Ney (2003)), were introduced by Gale and Church (1991). They do not rely on an alignment model: in order to detect translations, they rely on independence statistical measures such as, for instance, Dice coefficient, mutual informa"
2012.eamt-1.62,J97-3002,0,0.921712,"ords in parallel sentences. This approach consists in defining a probabilistic model of the parallel corpus, the parameters of which are estimated by a global maximization process which simultaneously considers all possible associations in the corpus. The goal is to determine the best set of alignment links between all source and target words of every parallel sentence pair. The most famous representatives in this category are the IBM models (Brown et al., 1993) for aligning isolated words, which have given rise to an impressive series of variants and amendments (see e.g. (Vogel et al., 1996; Wu, 1997; Deng and Byrne, 2005; Liang et al., 2006; Fraser and Marcu, 2007; Ganchev et al., 2008), to cite a few). Generalizing word alignment models to phrase alignment proves to be a much more difficult problem, and in the view of work of Marcu and Wong (2002) and Vogel (2005), such alignments are generally produced by heuristically combining asymmetric 1–n word alignments (“oriented”) in both directions (Koehn et al., 2003; DeNero and Klein, 2007). Once the set of alignment links is constituted, it is possible to assign scores to each pair of segments extracted. On the other hand, associative appro"
2012.eamt-1.62,C98-1066,0,\N,Missing
2012.iwslt-papers.20,C08-1064,0,0.0951023,"enario is that there is no guarantee that appropriate data will be available for the input text as regards e.g. genre, phraseology, theme vocabulary, or even effects of original language. Thus, adaptation will be performed with the objective of modeling some a priori conﬁdence into the system’s ability to translate short translation units. Another consequence of our setting is that online adaptation is necessary and is in fact the only solution. We therefore propose an on-the-ﬂy pipeline consisting of the following stages : sampling at the level of translation units is performed (similarly to [8, 9]) for selecting sentences from the training data, and instance weighting is applied for scoring phrase pairs (e.g. [6]). Based on these computations, two additional scores are then produced: the ﬁrst estimates the goodness of each collected source phrase as a translation unit for the language pair at hand; the second estimates how much conﬁdence should be put in the adaptated translation distribution for each source phrase1 . An important result of the paper will be the description of a document-level contrastive evaluation scheme that enables a more interpretable analysis of the differences b"
2012.iwslt-papers.20,D11-1033,0,0.114003,"m: given an in-domain training corpus and out-of-domain corpora, a ﬁxed number of sentences are selected in the out-of-domain corpora on the basis of their similarity to the in-domain cor1 Note that in the present work, the effect of this score will only be to act as a segmentation model, so that some segmentation may be preferred over some other. Future work will include searching for more translation examples for those unreliable phrases, as hinted by [5], and having recourse to automatic paraphrasing (e.g. [10]) of those phrases. pus. These sentences may be denoted as pseudo in-domain data [11], where it is hoped that, given the selected number of sentences to draw, performance will be improved. This approach is in fact ﬂawed in a particular respect, as it does not provide any guarantee that instances of rare units will be selected, speciﬁcally if they do not occur in sentences resembling the in-domain data. This has been sometimes solved by ad-hoc strategies to recover infrequent units [12]. We would like instead to make use of all available training corpora. Sampling at the level of phrases is an efﬁcient solution to achieve this goal [8, 9]. Indeed, sufﬁx arrays [13] offer fast a"
2012.iwslt-papers.20,E12-1016,0,0.0908219,"examples for those unreliable phrases, as hinted by [5], and having recourse to automatic paraphrasing (e.g. [10]) of those phrases. pus. These sentences may be denoted as pseudo in-domain data [11], where it is hoped that, given the selected number of sentences to draw, performance will be improved. This approach is in fact ﬂawed in a particular respect, as it does not provide any guarantee that instances of rare units will be selected, speciﬁcally if they do not occur in sentences resembling the in-domain data. This has been sometimes solved by ad-hoc strategies to recover infrequent units [12]. We would like instead to make use of all available training corpora. Sampling at the level of phrases is an efﬁcient solution to achieve this goal [8, 9]. Indeed, sufﬁx arrays [13] offer fast access to phrase instances in large corpora, and can be used to select a given number of instances of phrases, rather than sentences, thereby ensuring that all the phrases present in a corpus are appropriately covered.2 Previous approaches to sampling have resorted to random deterministic sampling, which picks a given number of examples by scanning the sufﬁx array index at ﬁxed intervals (hence the appa"
2012.iwslt-papers.20,N03-1017,0,0.0250979,"able. Although no deﬁnitive criterion as to what constitutes a good phrase translation unit has emerged3 , the two following criteria have been proposed: 2 Callison-Burch et al. [8] found that a sample size of 100 was sufﬁcient for German-to-English phrase-based SMT, while Lopez [9] determined that 300 was an appropriate value for Chinese-English hierarchical SMT. We will use a larger sample size of 1,000 in our experiments in an attempt to let instance weighting ﬁnd the most appropriate examples from a larger sample. 3 For instance, limiting phrases to constituents was found to be suboptimal [14]. The very deﬁnition of what a phrase is with respect to the SMT problem poses many interesting research questions, see e.g. [15]. 293 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 • Given some word alignment between a source and target parallel corpora, the absence of an aligned target phrase for a given source phrase may suggest that the corresponding failure of the extraction process should be accounted for in the translation model. Lopez [9] therefore proposes the following coherent estimate of the translation conditional probability: pcohe"
2012.iwslt-papers.20,2011.iwslt-papers.10,1,0.817563,"eria have been proposed: 2 Callison-Burch et al. [8] found that a sample size of 100 was sufﬁcient for German-to-English phrase-based SMT, while Lopez [9] determined that 300 was an appropriate value for Chinese-English hierarchical SMT. We will use a larger sample size of 1,000 in our experiments in an attempt to let instance weighting ﬁnd the most appropriate examples from a larger sample. 3 For instance, limiting phrases to constituents was found to be suboptimal [14]. The very deﬁnition of what a phrase is with respect to the SMT problem poses many interesting research questions, see e.g. [15]. 293 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 • Given some word alignment between a source and target parallel corpora, the absence of an aligned target phrase for a given source phrase may suggest that the corresponding failure of the extraction process should be accounted for in the translation model. Lopez [9] therefore proposes the following coherent estimate of the translation conditional probability: pcoherent (e|f ) = c(f, e) c(f ) (2) where c(f ), the number of occurrences of the source phrase, corresponds to the total number of a"
2012.iwslt-papers.20,P10-1049,0,0.0544116,"= c(f, e) c(f ) (2) where c(f ), the number of occurrences of the source phrase, corresponds to the total number of attempted extractions, in lieu of the traditional summation over  all extracted translations for f , e c(e , f ). • It has been observed that the traditional heuristic approach to phrase pair extraction does not offer a consistent view over the training and the actual use of phrases by decoders. It is thus possible to have recourse to a forced alignment which results in the decoder producing what it believes is the best alignment for a given training sentence. Wuebker et al. [16] implement this idea using leaving-one-out, so that the phrase examples for each training bi-sentence are not used to decode it, and subsequently estimate their system’s models on the resulting alignment. Even though this intuition does not guarantee that the retained phrases are intrinsically good translation units, they were selected as pertaining to best derivations allowing to reproduce the reference target sentence. We exploit the two above ideas as follows. First, we use some pre-trained standard phrase-based system to translate its own training corpus. Instead of sticking strictly to le"
2012.iwslt-papers.20,W07-0717,0,0.130643,"of any kind of ”in-domain” data, hence the name ”any-text translation”. In this context, we present a new approach to contextually adapt a translation model onthe-ﬂy, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists. 1. Introduction It is now a well-established fact in Statistical Machine Translation that systems must be adapted to each particular input text. Adaptation has been tackled in a variety of ways (see e.g. [1, 2, 3]), most notably by adapting the translation model, by adapting the target language model, and by adaptating the tuning set. In most of these works, it is assumed that the bilingual training corpus can be partitioned into “indomain” and “out-of-domain” subsets relative to the input text, and that there exists some smaller “in-domain” held-out corpus to tune the system. In typical settings, large bilingual corpora are collected opportunistically; as a result, the amount of data that do not resemble closely the input text largely outweights the data that appear to be the most relevant. Using as m"
2012.iwslt-papers.20,2007.mtsummit-papers.11,0,0.0605489,"difference of entropy values between the previous situation and the more informative situation of a given model provides some account of how much conﬁdence should be put in the collective contribution of all weighted examples. We thus used the following as a new feature in our experiments involving adapted translation models: Hunif (f ) = − hconf idence (f ) = = p(e|f ) log(p(e|f )) = log( Hunif (f ) − H(f ) (5)  1 )+ − log( piw (e|f ) log(piw (e|f )) c(f ) e 5 Inverse translation models and lexical weighting are in a way meant to compensate for this. 6 Context-dependent phrase tables (e.g. [17]) is a way to address this. 294 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Corpus newsco (in) ted (out) test newsco tuning #lines 934 934 1,859 #tok.en 22.4K 19.6K 44.2K #tok.fr 25.3K 20.3K 48.8K ppl.en 316.19 265.63 307.14 ppl.fr 211.07 164.57 222.79 oov.en 629 238 1,700 oov.fr 273 273 1,558 Table 1: Tuning and test documents statistics This value increases when either the number of examples for f is high or when the entropy of the adapted translation distribution is low. 5. Experiments We now describe experiments intended to show whether o"
2012.iwslt-papers.20,E12-1055,0,0.0227216,"ough better coverage: in particular, it seems to improve the alignment of some rare translation units, which would otherwise be misaligned, and yield inappropriate phrase pairs. On the other hand, adding more bilingual data increases the possibility of encountering new translations, and makes the translation of phrases more ambiguous, sometimes in a detrimental way, since not all corresponding translations (or senses) are appropriate for the input text. The data sparseness and the ambiguity problem thus entertain a repulsion relationship that is at the core of the adaptation problem (see e.g. [4]), even though the recent work of Haddow and Koehn [5] concludes that good coverage is more important than appropriate scoring: adding out-of-domain corpora containing examples of rare units beneﬁts more to translation than the inclusion of inappropriate examples of frequent units harms it. A practical solution is to use all the available training data, but to consider differently translation examples depending on their relevance to the input text, possibly at the corpus [1], sentence [6] or phrase [3] level. As noted e.g. by Haddow and Koehn [5], although the in-domain vs. out-ofdomain distin"
2012.iwslt-papers.20,D09-1074,0,0.0394755,"ty problem thus entertain a repulsion relationship that is at the core of the adaptation problem (see e.g. [4]), even though the recent work of Haddow and Koehn [5] concludes that good coverage is more important than appropriate scoring: adding out-of-domain corpora containing examples of rare units beneﬁts more to translation than the inclusion of inappropriate examples of frequent units harms it. A practical solution is to use all the available training data, but to consider differently translation examples depending on their relevance to the input text, possibly at the corpus [1], sentence [6] or phrase [3] level. As noted e.g. by Haddow and Koehn [5], although the in-domain vs. out-ofdomain distinction is frequently used, precise deﬁnitions are still lacking; in their words, “it is normally understood that data from the same domain is in some sense similar (for example in the words and grammatical constructions used)” and, in their experiments, they characterize domain differences in terms of word distributions and out-of-vocabulary rates. While some domain distinctions are clearly undebatable, such as when opposing e.g. News commentaries and parliamentary speeches, other distinct"
2012.iwslt-papers.20,2010.iwslt-papers.5,0,0.172375,"se of tuning techniques relying on a development bitext from the same data source or domain. • Training data was collected opportunistically and no speciﬁc document metadata (e.g. genre, document boundaries) are available for the full data set. Note that the issues of adapting alignments and target language models will not be considered in this work. As to the former, it has previously been shown that using all the available corpora during word alignment tends to improve 292 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 translation performance [7, 5], so our word alignment models will be built ofﬂine using all available parallel data. As to the latter, there is a large body of works addressing language model adaptation which all report improvements over non-adapted language models (e.g. [1]). We leave it to our future work to evaluate whether the effects of all types of adaptations can be compounded. This paper is to our knowledge the ﬁrst attempt at studying the scenario of what we call here “any-text translation”, with the notable absence of some predeﬁned identiﬁable indomain training and tuning corpora. An important aspect of our scen"
2012.iwslt-papers.20,J82-2005,0,0.72969,"enario is that there is no guarantee that appropriate data will be available for the input text as regards e.g. genre, phraseology, theme vocabulary, or even effects of original language. Thus, adaptation will be performed with the objective of modeling some a priori conﬁdence into the system’s ability to translate short translation units. Another consequence of our setting is that online adaptation is necessary and is in fact the only solution. We therefore propose an on-the-ﬂy pipeline consisting of the following stages : sampling at the level of translation units is performed (similarly to [8, 9]) for selecting sentences from the training data, and instance weighting is applied for scoring phrase pairs (e.g. [6]). Based on these computations, two additional scores are then produced: the ﬁrst estimates the goodness of each collected source phrase as a translation unit for the language pair at hand; the second estimates how much conﬁdence should be put in the adaptated translation distribution for each source phrase1 . An important result of the paper will be the description of a document-level contrastive evaluation scheme that enables a more interpretable analysis of the differences b"
2012.iwslt-papers.20,D12-1037,0,0.0342671,"n and is taken from presentations from TED talks9 (ted). These conditions allow us to compare situations where tuning corpora of various degrees of appropriateness are available and can be identiﬁed as more appropriate; we will also simulate the availability of a “perfect” tuning set by performing self-tuning. Lastly, our training corpus, described in Table 2, contains two sub-corpora of in-domain News commentaries (newsco) and out-of-domain parliamentary debates (epps). These sub-corpora will be either used separately or jointly. 7 Performing tuning set adaptation at the document-level as in [18] will be part of our future work. 8 http://www.statmt.org/wmt12 9 Available from IWSLT’11: http://iwslt2011.org Corpus newsco epps newsco+epps domain w.r.t. test in out mixed # lines 137K 1,982M 2,119M # tokens.en 3,381M 54,170M 57,551M # tokens.fr 4,017M 59,702M 63,790M Table 2: Training corpora statistics 5.2. Systems 5.2.1. Off-line baseline systems We build standard phrase-based systems using moses10 , and use MERT for tuning parameters. We compare the following conditions: training on all available data (newsco+epps), as well as using two separate phrase tables built from newsco and epps"
2012.iwslt-papers.20,2005.eamt-1.19,0,0.031576,"tables built from newsco and epps (i.e. multiple alternative decoding paths) as is standard practice in domain adaptation where corpus boundaries are known [1]. 5.2.2. On-the-ﬂy adapted systems We build various adapted systems on-the-ﬂy. All use the word alignments produced by Giza++11 on the full newsco+epps corpus, as out-of-domain data may improve alignment quality in our situation [7]. We test the three following sampling and instance-weighting strategies for estimating translation model: (a) random sampling and uniform weighting [8, 9] (RND), (b) using tf.idf values of training sentences [19] (IR), and (c) perplexity values of training sentences relative to each test document (PPL).12 An important difference with our baseline systems is that we do not estimate a back-translation model (p(f |e)) as this proves costly using sampling; [9] reported that this model does not have a signiﬁcant impact on translation performance for large training corpora. Furthermore, we believe that such a model should in fact not be needed, were the translation model appropriately estimated (i.e. contextually appropriate), as there would be no need to compensate for the “ambiguity” in this model by cons"
2012.iwslt-papers.20,2006.amta-papers.25,0,0.0655461,"exical weighting models, which are meant to model intra-biphrase cohesion. 295 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 perfect tuning set for each input document, each document is tuned independently using the reference corpus and the best optimization point is used for testing; this is obviously an oracle situation, and will be denoted as such in our results for moses and our adapted systems. 5.3. Evaluation setting and contrastive document-level evaluation We will compare our various settings using the wellestablished BLEU [20] and TER [21] metrics, using initially the full test corpus made up of the full collection of documents. Absolute values being always difﬁcult to interpret, we propose to resort to contrastive evaluation between two systems. Our contrastive document-level evaluation is performed as follows: given two systems we wish to compare, a single conﬁguration, and a target evaluation metrics, we look on a per document basis which system outperformed the other for some interval (e.g. “1-2 BLEU increase”, “0.50.75 TER decrease”). We then compute statistics over the entire document set. Considering a particular signiﬁc"
2012.iwslt-papers.20,cartoni-meyer-2012-extracting,0,0.0406631,"s the opposite, and darker colors indicates larger differences. 100 fr2en en2fr percentage of correctly translated source phrases 90 80 70 60 50 40 30 20 10 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 confidence model value intervals 5 5.5 6 6.5 Figure 3: Percentage of correctly translated source phrases in the trace of the decoder for the PPL+all systems against score value intervals of the conﬁdence model (conf). which correspond to the large majority of our training data, previous studies have shown that French as an original language is signiﬁcantly more represented than English as an original language [22]. Experimenting with other corpora in which original language is known may help us to conﬁrm this hypothesis. Our adapted systems have recourse to sampling, and consequently do not use a reverse translation model [9], thus resulting in systems that may be built very efﬁciently, even for large data set conditions. Most previously published domain adaptation techniques cannot be applied directly to our studied scenario, as the availability of an in-domain training corpus is almost always assumed. Note that the newsco part of our training corpus was in fact “in-domain” w.r.t. our test documents."
2012.iwslt-papers.20,D10-1044,0,\N,Missing
2012.iwslt-papers.20,P02-1040,0,\N,Missing
2012.iwslt-papers.20,W07-0733,0,\N,Missing
2012.iwslt-papers.20,P10-2001,0,\N,Missing
2012.iwslt-papers.20,P05-1032,0,\N,Missing
2012.iwslt-papers.20,W12-3154,0,\N,Missing
2013.iwslt-papers.7,C88-1016,0,0.425317,"a sentence-aligned parallel corpus, which is a crucial component of state-of-the-art Statistical Machine Translation (SMT) technology. One of the most prominent approaches nowadays is Phrase-based Statistical Machine Translation, which is built upon the word alignment output. The problem of learning sub-sentential alignment from parallel texts is well-known, and numerous proposals have been put forward to perform this task. Those methods roughly fall into two main categories, broadly described here as the probabilistic and the associative approaches. The probabilistic approach, introduced in [1], considers the problems of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model (e.g. IBM models [2]) of the parallel corpus, the parameters of which are estimated by a global optimization process which simultaneously considers all possible associations in the entire corpus. Due to its tight integration within the SMT framework, this approach is by far the most widely used. However, it is characterized by a number of shortcomings, in particular: • Its parameters have to be estimated and optimized based on the entire"
2013.iwslt-papers.7,J93-2003,0,0.0606461,"ys is Phrase-based Statistical Machine Translation, which is built upon the word alignment output. The problem of learning sub-sentential alignment from parallel texts is well-known, and numerous proposals have been put forward to perform this task. Those methods roughly fall into two main categories, broadly described here as the probabilistic and the associative approaches. The probabilistic approach, introduced in [1], considers the problems of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model (e.g. IBM models [2]) of the parallel corpus, the parameters of which are estimated by a global optimization process which simultaneously considers all possible associations in the entire corpus. Due to its tight integration within the SMT framework, this approach is by far the most widely used. However, it is characterized by a number of shortcomings, in particular: • Its parameters have to be estimated and optimized based on the entire parallel corpus, hence all units in the parallel corpus have to be aligned simultaneously. This makes it a time-consuming process, especially when working on large parallel corpo"
2013.iwslt-papers.7,N10-1062,0,0.0603724,"n particular: • Its parameters have to be estimated and optimized based on the entire parallel corpus, hence all units in the parallel corpus have to be aligned simultaneously. This makes it a time-consuming process, especially when working on large parallel corpora. In addition, many aligned parallel sentence pairs are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distri"
2013.iwslt-papers.7,C94-2178,0,0.299307,"gned parallel sentence pairs are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations."
2013.iwslt-papers.7,J93-1003,0,0.124091,"are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to"
2013.iwslt-papers.7,R09-1040,0,0.0179237,"ionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to produce better results than state-ofthe-art methods on bilingual lexicon constitution tasks, when the evaluation is performed by comparing word associations with reference dictionaries, but faile"
2013.iwslt-papers.7,2012.eamt-1.62,1,0.828471,"ed sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to produce better results than state-ofthe-art methods on bilingual lexicon constitution tasks, when the evaluation is performed by comparing word associations with reference dictionaries, but failed to perform on par with state-of-the-art methods for building SMT phrase tables. It was subsequently improved in [10], in which a recursive binary segmentation algorithm is used to process the output of Anymalign so as to obtain better sub-sentential alignments at the sentence level. While this improvement yields a performance that is comparable with the statistical approach, it can do so by processing large numbers of randomly sampled sub-corpora in order to obtain an accurate association measure and a good coverage for the entire corpus. In this work, we propose a method to adapt Anymalign in order to align the parallel sentences on a per-need basis, meaning that it can also be used to accurately align new"
2013.iwslt-papers.7,J97-3002,0,0.649942,"Missing"
2013.iwslt-papers.7,P07-2045,0,0.0103763,"to build an association table T (the same kind of table as the table in step 5 in Figure 1) Using T as the input of the binary segmentation algorithm (cf. Section 2.2), a new alignment A0 is computed if distance(A − A0 ) &lt;  then return A end if NumIter+=1 end while return A 3. Experiments 3.1. Experimental settings In this section, we describe experiments intended to test the performance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, a"
2013.iwslt-papers.7,D12-1089,0,0.0162464,"ion runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase table may in fact only artificially inflate oracle results. Two sets of experiments will be carried out in this work. The first set of experiments is designed to validate the quality of the alignment generated by our method (henceforth sba, for sampling-based alignment) on some predefined bilingual corpus against a state-of-art alignment pipeline, based on giza++ [20], using default parameters from Moses. This approach is refered to as giza++ . The second set of experiments aims to assess the ability to align new bilingual data. For this experiment, we will focus"
2013.iwslt-papers.7,J03-1002,0,0.0164095,"rase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase table may in fact only artificially inflate oracle results. Two sets of experiments will be carried out in this work. The first set of experiments is designed to validate the quality of the alignment generated by our method (henceforth sba, for sampling-based alignment) on some predefined bilingual corpus against a state-of-art alignment pipeline, based on giza++ [20], using default parameters from Moses. This approach is refered to as giza++ . The second set of experiments aims to assess the ability to align new bilingual data. For this experiment, we will focus on adding sentence pairs from a very large (unaligned) bilingual corpus, chosen on the basis that they contain translations for previously out-ofvocabulary tokens. Our approach will be compared against the same alignment pipeline using the augmented parallel corpus. This strategy is however costly as it requires to retrain the complete models, so we also performed a comparison with alignments obta"
2013.iwslt-papers.7,takezawa-etal-2002-toward,0,0.0266256,"very large (unaligned) bilingual corpus, chosen on the basis that they contain translations for previously out-ofvocabulary tokens. Our approach will be compared against the same alignment pipeline using the augmented parallel corpus. This strategy is however costly as it requires to retrain the complete models, so we also performed a comparison with alignments obtained using the orginal alignment models, without any retraining. 3.2. Data sets Experiments were performed on two parallel corpora, described in Table 1: BTEC is a small English-French subpart of the Basic Travel Expression Corpus [21]; and HIT is a corpus of basic expressions built for the Beijing 2008 Olympics, used here in English, French and Chinese. We used the BTEC development set of 2003 (devel03) and BTEC test set of 2009 (test09) as our development and test set, which are described in Table 2. Note that the former has 16 reference translations available for English, and the latter has 7, allowing for a somehow more interpretable measure of performance for language pairs with English as the target language. We will describe in Section 3.4 experiments that make Corpus BTEC HIT EPPS supp WMT # lines 20K 62K 1,982K 3.3"
2013.iwslt-papers.7,W08-0509,0,0.0164413,"parliamentary debates, as well as a substantially larger corpus from the translation task of the Workshop on Statistical Machine Translation (WMT)2 : both are described in Table 1. Our development and test sets will remain the same for all experiments. English and French texts are normalized and tokenized by our in-house tools, and Chinese texts are segmented by a CRF-based Chinese word segmenter3 . 3.3. Basic alignment task This experiment aims to assess the quality of the subsentential alignment generated by our method on a full bilingual parallel corpus. We use the giza++ implementation of [22] as a competitive baseline, with default settings : 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). As for our alignment method, its alignment quality depends on the number of sub-corpora (N ) that are drawn for each sentence pair. In this work, we choose a constant value of N = 1000 for all sentence pairs. The self-convergency normalization process is repeated for a maximum of 10 iterations. The results for the two alignment methods are reported in Table 3, where we compare them on 2 parallel corpus (BTEC and HIT) and their simple concate"
2013.iwslt-papers.7,2010.iwslt-papers.5,0,0.0519606,"r, composite training corpus evaluation The previous hypothesis seems to hold when considering the larger task corresponding to the concatenation of the two parallel corpora (BTEC+HIT), where HIT data outnumber BTEC data by more than 3:1. Results are however less clearcut here: for instance, our approach still performs better on French→English (average of +0.75 BLEU on one-best hypotheses), but fares worse in terms of oracle performance (average of -0.43 BLEU). These results include a reflection of the fact that giza++ improves its alignment with more data, even when adding out-of-domain data [23]. At this stage of our work, we do not control which particular sentence pairs are drawn in our samples, so assessing the impact of a larger overall sentence pool cannot be done. 3.3.5. Difficult language pair evaluation Lastly, we turn to the more difficult Chinese→English condition, which is significantly more difficult than its French→English counterpart (27.88 BLEU vs. 45.52 BLEU for the giza++ baselines). A similar pattern emerges for the two language pairs: one-best translation performance is comparable, but oracle results indicate a clear advantage for our sampling-based alignment (aver"
2013.iwslt-papers.7,2011.mtsummit-papers.10,0,0.0168133,"9 30.61 29.81 33.79 29.94 Table 4: Results of experiments where a supplementary corpus is pooled and aligned by several methods. only be performed on demand. Indeed, considering that all input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we us"
2013.iwslt-papers.7,C12-1010,0,0.011303,"9 30.61 29.81 33.79 29.94 Table 4: Results of experiments where a supplementary corpus is pooled and aligned by several methods. only be performed on demand. Indeed, considering that all input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we us"
2013.iwslt-papers.7,E12-1016,0,0.0528085,"input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we used the HIT corpus as our main corpus, relatively to which our test set contains 79 unique OOVs (436 occurrences). Our additional training data (EPPS) provided matches for 65 of them."
2013.iwslt-papers.7,P03-1021,0,0.00902301,"nts 3.1. Experimental settings In this section, we describe experiments intended to test the performance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phra"
2013.iwslt-papers.7,P02-1040,0,0.0876146,"ance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be r"
2013.iwslt-papers.7,C08-1064,0,0.354307,"lignment of rare words and its cascading effects. Figure 4 illustrates a case where the rare French word d´eguis´es (here: in costumes) was only correctly aligned by our technique, and where the negative consequences for the two giza/moses baselines could be important (at least, for our experiments, no translation for d´eguis´es alone could be extracted from this sentence pair by giza++ here). The framework that we have described for targeted additional data selection from parallel corpora will be the basis for our future work. We can, by principle, work at the level of tera-scale translation [28], by accessing efficiently (using suffix arrays) large quantities of unaligned parallel corpora, and perform transpotting and phrase table construction on a per-need basis. However, considering the diversity in nature, origin and quality of all possibly additional training examples, some adaptation should be performed so as to introduce preferences for the most promising examples, and hence extracted translations. In this context, the most realistic scenario will be a follow-up to our previous work on any-text translation [29], where notably little or no a priori knowledge exists about (additi"
2013.iwslt-papers.7,2006.amta-papers.25,0,0.033098,"ssociative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a"
2013.iwslt-papers.7,2012.iwslt-papers.20,1,0.824339,". We can, by principle, work at the level of tera-scale translation [28], by accessing efficiently (using suffix arrays) large quantities of unaligned parallel corpora, and perform transpotting and phrase table construction on a per-need basis. However, considering the diversity in nature, origin and quality of all possibly additional training examples, some adaptation should be performed so as to introduce preferences for the most promising examples, and hence extracted translations. In this context, the most realistic scenario will be a follow-up to our previous work on any-text translation [29], where notably little or no a priori knowledge exists about (additional) training examples, and adaptation should be performed on-the-fly. Finally, it seems obvious that the search for new translations, and in particular for unknown words and phrases as well as poorly adapted phrases, should also be pursued in less parallel corpora (see e.g. [30]). It is then an interesting question to consider how our technique would fare and how it could be adapted to work indifferently on parallel or reasonably comparable sentence pairs. 5. Acknowledgements This work was partially funded by the French Stat"
2013.iwslt-papers.7,P11-2031,0,0.0216776,"gnment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase ta"
2013.iwslt-papers.7,2012.amta-papers.2,0,0.0388714,"Missing"
2013.mtsummit-papers.15,W06-3114,0,0.0170076,"on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation campaigns. The second contribution of this work is a study of the variability of post-edition, a question that the growing role of the TER score, both in MT evaluation and as a measure of the post-edition effort,2 makes more and more important. Since it has long been recognized that MT evaluation (especially at the sentence level) is plagued with a low inter-rater agreement (Koehn and Monz, 2006), it seems appropriate to raise the same issues in relationship to the QE task. Our analysis relies on a subpart of the T RACE corpus containing automatic translations that have been post-edited independently by two translators. To the best of our knowledge, this is the first time that several post-editions of the same sentences have been collected, allowing us to perform both a qualitative comparison of the differences between the post-editions of two translators as well as a quantitative analysis of the inter-rater agreement for the hTER score. The rest of the paper is organized as follows."
2013.mtsummit-papers.15,E06-1032,0,0.0322995,"o compute standard MT metrics on the automatic output. As reflected in Table 1 for the English to French direction, the metric values are much higher than what is usually observed in MT evaluation campaigns. This shows that the post-edited references are indeed much closer to the translations than the references used in these campaigns. For instance, when S YS S TAT is evaluated against the references of the WMT campaign, its TER score is 56.27, nearly twice as worse as when evaluated using post-edited translations as reference. It should also be noted that, as mentioned in many past studies (Callison-Burch et al., 2006), rule-based systems are highly disfavored by automatic metrics. 3 Failure Analysis of MT systems We show, in this section, how comparing translation hypotheses with their post-editions can help identify and analyze failures of MT systems. For space reasons, only results for the English to French direction are presented. 3.1 Error Patterns By computing the edit distance at the word-level between translation hypotheses and their postedition, it is possible to automatically detect the modifications required to make MT output both fluent and adequate. The careful analysis of the most frequent cor"
2013.mtsummit-papers.15,W12-3102,0,0.572737,"in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to identify and analyze the limits of a MT system and to train a quality estimation (QE) system. For these tasks we present results achieved on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation"
2013.mtsummit-papers.15,2012.eamt-1.60,0,0.0137318,"ish to French direction. For the two directions, 1, 000 additional sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the correctio"
2013.mtsummit-papers.15,2004.tmi-1.8,0,0.0127131,"le a a` dans que en un des Deletion 799 335 329 278 277 256 242 215 212 167 de a` la le que les en et des pour Table 3: Most frequent editions. 3.2 Differences between Automatic Translations and their Post-Edition To characterize the differences between automatic translations and their post-edition, we propose to learn a classifier that could distinguish between these two kinds of translations. We hope that finding which features are relevant for making this distinction will provide us some insight about the limits of MT systems. This approach is directly inspired by earlier work in QE like (Kulesza and Shieber, 2004), where the authors try to learn the difference between a good and a bad translation. In the experiments described in this section, each translation is represented by 336 numerical features, most of which are inspired by works in QE for MT (Callison-Burch et al., 2012).4 These features can be classified into four categories: • Association Features: Measures of the quality of the ‘association’ between the source and the target sentences like, for instance, features derived from the IBM 1 model scores; • Fluency Features: Measures of the ‘fluency’ or the ‘grammaticality’ of the target and source"
2013.mtsummit-papers.15,S10-1003,0,0.0430789,"sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the corrections of the automatic translations were minimal: they were asked to prod"
2013.mtsummit-papers.15,D12-1077,0,0.0204608,"Missing"
2013.mtsummit-papers.15,potet-etal-2012-collection,0,0.0767283,"Missing"
2013.mtsummit-papers.15,2006.amta-papers.25,0,0.0359803,"these data, notably the development of an automatic Quality Estimation (QE) system and the detection of frequent errors in automatic translations. Both applications require a careful assessment of the variability in post-editions, that we study here. 1 Introduction Post-editing, the process of editing the outputs of a Machine Translation (MT) system in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to"
2013.mtsummit-papers.15,specia-etal-2010-dataset,0,0.0962939,"Missing"
2013.mtsummit-papers.15,2013.tc-1.10,0,0.0399314,"Missing"
2014.amta-researchers.17,W14-3313,1,0.878749,"Missing"
2014.amta-researchers.17,P07-2045,0,0.00783789,"25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate th"
2014.amta-researchers.17,D10-1076,1,0.734365,"the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in Le et al. (2010). In Niehues and Waibel (2012a), another RBM-based language model was introduced. This approach differs from the one intrdoduced in Mnih and Hinton (2007) by a simpler layout that allows us for a fast probability computation. This yields the integration of the model during the decoding step feasible. However, the training complexity heavily depends on the vocabulary size and this model can be trained on a limited amount of training data. In Dahl et al. (2012), a sampling method was presented to efficiently train restricted Boltzmann machines on word observations. This approach enables us to tr"
2014.amta-researchers.17,N12-1005,1,0.845844,"language models is the size of the output vocabulary in large vocabulary continuous speech recognition. A first way to overcome this is to use a short list. Recently, Le et al. (2011) presented a structured output layer neural network which is able to handle large output vocabularies by using a clustering tree to represent the output vocabulary. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In Schwenk et al. (2007) as well as in Le et al. (2012) the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in"
2014.amta-researchers.17,C90-3038,0,0.341794,"advantage of the RBM-based language model to use the language model during decoding. The remaining paper is structured as follows. First we review related work and then provide in section 3 a brief overview of RBM-based language models. Section 4 describes the tailored sampling strategies while we describe how the shared word representation is integrated into the RBM layout in section 5. Afterwards we describe and discuss experimental results measured in terms of translation quality in section 6. 2 Related Work A first approach to predict word categories using neural networks was presented in Nakamura et al. (1990). Later, Bengio et al. (2003) introduced neural networks for statistical language modeling. The authors described in detail an approach based on multi-layer neural networks and reported a significative perplexity reduction compared to conventional and class-based language models. In addition, they gave a short outlook to energy minimization networks. An approach using multi-layer neural networks has successfully been applied to speech recognition by Schwenk and Gauvain (2002), Schwenk (2007) and Mikolov et al. (2010). One main problem of continuous space language models is the size of the outp"
2014.amta-researchers.17,W11-2124,1,0.832421,"the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the"
2014.amta-researchers.17,W09-0435,1,0.867497,"and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we opti"
2014.amta-researchers.17,W08-0303,1,0.877148,"AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 226 Sampling size No RBMLM No Sampling 5 10 50 100 1000 BLEU Score 25.16 25.42 25.15 25.01 25.23 25.25 25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kols"
2014.amta-researchers.17,2012.iwslt-papers.3,1,0.361369,"ently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by techniques developed for other NN-based language model, we tackle in this work these two issues of RBM-based"
2014.amta-researchers.17,2007.tmi-papers.21,0,0.488897,"iminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used"
2014.amta-researchers.17,D07-1045,0,0.0703798,"Missing"
2014.amta-researchers.17,D13-1140,0,0.128933,"borhood can be modeled. In state of the art language models contexts of up to 10 words are used. Currently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by tech"
2014.amta-researchers.17,W05-0836,1,0.901514,"(Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we optimized and tested the systems on a set of computer science lectures collected at a university. The language models were tested on three different conditions. First, we used the baseline system, then we used a system, which has been adapted to the TED task by using an additional"
2014.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.822181,"to adapt standard MT systems to ASR output. Finally, the impact of re-scoring n-best translation hypotheses using SOUL models is presented in the closing section. 1 https://www.ted.com/ 106 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 dataset dev2010 tst2010 WER (del., ins.) 15.0 (4.0, 3.5) 12.7 (3.3, 2.7) Table 1: Case-insensitive recognition results on the 2010 dev and tst data, scored using sclite. LM. The first decoding pass is carried out with a modified version of our 2011 Quaero system for broadcast data in English [8, 9] in which a language model trained on the provided ASR texts including the IWSLT14 TED LM transcriptions (3.2M words) was interpolated with the baseline 78k-word language model. The first decoding pass is done in 1xRT. The acoustic models in the first pass were trained on the data distributed in Quaero as well as on data from other sources from previous European or national projects and from the LDC. All acoustic and other language model training data predate December 31, 2010. The Euronews data provided by the organizers was not used. The second pass decoding used the same interpolated langua"
2014.iwslt-evaluation.15,J04-2004,0,0.0584651,"model training data predate December 31, 2010. The Euronews data provided by the organizers was not used. The second pass decoding used the same interpolated language model with acoustic models trained only on 180 hours of transcribed TED talks predating December 31, 2010 to better target the TED data. The case-insensitive recognition results on the 2010 dev and tst data are given in Table 1 scoring with the NIST sclite scoring using the provided stm and no glm. 3. MT systems: adaptation to speech data 3.1. Machine Translation with N-code N CODE implements the bilingual n-gram approach to SMT [10, 11, 12] that is closely related to the standard phrase-based approach [13]. In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bil"
2014.iwslt-evaluation.15,2002.tmi-tutorials.2,0,0.045489,"y the organizers was not used. The second pass decoding used the same interpolated language model with acoustic models trained only on 180 hours of transcribed TED talks predating December 31, 2010 to better target the TED data. The case-insensitive recognition results on the 2010 dev and tst data are given in Table 1 scoring with the NIST sclite scoring using the provided stm and no glm. 3. MT systems: adaptation to speech data 3.1. Machine Translation with N-code N CODE implements the bilingual n-gram approach to SMT [10, 11, 12] that is closely related to the standard phrase-based approach [13]. In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. The best translation is selected by max"
2014.iwslt-evaluation.15,N04-4026,0,0.0177956,"tuples. The best translation is selected by maximizing a linear combination of feature functions using the following inference rule: e∗ = argmax e,a K X λk fk (f , e, a), (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ) and where a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target n-gram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models [14, 15] aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match the target word order by unfolding the word alignments [12]. Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved [11] and n-gram translation models are then estimated over the training corpus composed of tupl"
2014.iwslt-evaluation.15,W08-0310,1,0.802984,"peech (POS), rather than surface word forms, to increase their generalization power [12]. 3.2. MT baseline This section describes the MT systems trained on written material that served as a benchmark for the succeeding experiments aiming at improving the translation quality for speech transcriptions. All the parallel corpora used in our translation systems have been preprocessed to remove excessively long sentences as well as sentences with an important length difference between the source and the target. The common preprocessing also included tokenization using the in-house tool described in [16] and word alignments using MGIZA++ [17] and Moses’s grow-diag-final-and heuristic for alignment symmetrization. All the MT systems developed in this study make use of the N-code system described above for translation model training and for decoding. Since the N-code system uses factored models, the training corpora have been tagged with part-of-speech (POS) labels using TreeTagger [18]. The target language model used discriminative log-linear interpolation approach to combine the model trained on TED monolingual data provided by the organizers and the bigger LM trained on WMT data (SRILM [19]"
2014.iwslt-evaluation.15,W08-0509,0,0.0156267,"orms, to increase their generalization power [12]. 3.2. MT baseline This section describes the MT systems trained on written material that served as a benchmark for the succeeding experiments aiming at improving the translation quality for speech transcriptions. All the parallel corpora used in our translation systems have been preprocessed to remove excessively long sentences as well as sentences with an important length difference between the source and the target. The common preprocessing also included tokenization using the in-house tool described in [16] and word alignments using MGIZA++ [17] and Moses’s grow-diag-final-and heuristic for alignment symmetrization. All the MT systems developed in this study make use of the N-code system described above for translation model training and for decoding. Since the N-code system uses factored models, the training corpora have been tagged with part-of-speech (POS) labels using TreeTagger [18]. The target language model used discriminative log-linear interpolation approach to combine the model trained on TED monolingual data provided by the organizers and the bigger LM trained on WMT data (SRILM [19] toolkit was used for both models). Our"
2014.iwslt-evaluation.15,W14-3302,0,0.031887,"some lines, removing comments between square brackets and between parentheses, etc. Those notes are added by transcribers in order to facilitate the understanding of the text by human readers, but are useless and even confusing in the context of automatic speech translation. 107 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 3.2.1. Impact of the out-of-domain corpora We tried to improve the performance of the baseline system trained on in-domain data only, by adding various bilingual corpora from the WMT Evaluation Campaign [20]: NewsCommentary (NC), Europarl (EPPS) and Gigaword filtered as in [21] (GIGA). All those models were tuned on the same manually transcribed development set (dev2010). As can be seen in Table 2, only the filtered Gigaword corpus actually helped improve the performance of the baseline system. In accordance with these results, we used only this corpus as the additional out-of-domain corpus for our final system. Table 2: Baseline MT experiments with written corpora. training corpora TED TED + NC + EPPS TED + NC + EPPS + GIGA TED + GIGA BLEU dev2010 test2010 28.8 33.2 29.5 33.0 29.6 34.0 29.7 34.4"
2014.iwslt-evaluation.15,2011.iwslt-papers.7,0,0.028841,"em, on the other hand, is expected to produce fully punctuated text as its output and is typically trained on punctuated sources. The performance on the manually transcribed test data, that does not contain any recognition errors, is nevertheless degraded dramatically if the punctuation is removed from the source side of the test (BLEU=25.5, as compared to BLEU=33.0 for the punctuated test, see Table 3). 108 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Possible solutions to this problem have been explored, for example, in [22]. One solution is to build a new MT system based on the training corpora with unpunctuated source side: the system is thus trained to implicitly insert punctuation as part of the general translation process (implicit punctuation). Another solution is to produce automatic punctuation for the source language and to insert some punctuation marks to speech recognition output before translation (explicit punctuation in source): this approach has the advantage of allowing to keep the MT system unchanged. Our experiments with both approaches are shown in Table 4. We trained a new MT system unpunctuat"
2014.iwslt-evaluation.15,P06-2093,1,0.819533,"Missing"
2014.iwslt-evaluation.15,N12-1005,1,0.872164,"Missing"
2014.iwslt-evaluation.15,N12-1047,0,0.0643549,"Missing"
2014.iwslt-evaluation.15,2011.iwslt-evaluation.7,1,0.899941,"Missing"
2014.iwslt-papers.6,W07-0717,0,0.0553536,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,W09-0432,0,0.0614921,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,D11-1033,0,0.151975,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,E12-1055,0,0.102234,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,D07-1045,0,0.28546,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,N12-1005,1,0.601782,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,E14-1003,0,0.0172429,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,D14-1179,0,0.0491922,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,P14-1066,0,0.182463,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,P13-1045,0,0.351451,"an be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their"
2014.iwslt-papers.6,D13-1176,0,0.0769434,"even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their ability to adapt to other domains. By c"
2014.iwslt-papers.6,P14-1129,0,0.0712479,"even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their ability to adapt to other domains. By c"
2014.iwslt-papers.6,D07-1080,0,0.853065,"licitly capture some similarity relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and anal"
2014.iwslt-papers.6,N12-1047,0,0.597667,"licitly capture some similarity relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and anal"
2014.iwslt-papers.6,D11-1125,0,0.701771,"relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We final"
2014.iwslt-papers.6,P12-1002,0,0.24795,"relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We final"
2014.iwslt-papers.6,federico-etal-2012-iwslt,0,0.316459,"art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We finally provide a short review of similar works both on Discriminative Machine Translation and on Continuous Space Translation Models, before concluding with some perspectives for future work. 2. Continuous space translation models This sectio"
2014.iwslt-papers.6,J06-4004,0,0.318812,"e pair is: P (s, t) = L Y i=1 P (ui |ui−1 i−n+1 ), (1) where ui−1 i−n+1 denotes the tuple sequence ui−n+1 , . . . , ui−1 . The complete model for a sentence pair thus involves latent variables that specify the reordering applied to the source sentence, as well as its segmentation into translation units. These latent variables define the derivation of the source sentence that generates the target sentence. They are omitted for the sake of clarity. During the training step, the segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments (see [25, 26] for details). During the inference step, the SMT decoder will compute and output the best derivation. In this model, the elementary units are bilingual pairs, which means that the underlying vocabulary, hence the number of parameters, can be quite large, even for small translation tasks. Due to data sparsity issues, such models face severe estimation problems. Equation (1) can therefore be factored by decomposing tuples in two (source and target) parts and in two equivalent ways: P (ui |ui−1 i−n+1 ) i−1 i−1 i−1 i−1 = P (ti |sii−n+1 , ti−n+1 )P (si |si−1 i−n+1 , ti−n+1 ) (2) i = P (si |ti−n+1"
2014.iwslt-papers.6,J92-4003,0,0.0936148,"otes an aligned sentence pair, where the source words are reordered. Each decomposition involves two bilingual conditional distributions that can also be decomposed at the level of words, using again the n-gram assumption. 2.2. Continuous translation modeling with SOUL The n-gram distributions described in Section 2.1 are defined over potentially large vocabularies. As proposed in [9], these distributions can be estimated using the SOUL model introduced in [27]. Following [28], the SOUL model combines the feed-forward neural network approach for n-gram models [7] with a class-based prediction [29]. Structuring the output layer with word-class information makes the estimation of distributions over the entire vocabulary computationally feasible. Neural network architectures are also interesting as they can easily handle larger contexts than typical n-gram models. In the SOUL architecture, enlarging the context mainly consists in increasing the size of the projection layer, which corresponds to a simple look-up operation. Increasing the context length at the input layer thus causes only a linear growth in complexity in the worst case [14]. 2.3. Training and initialization issues The word-"
2014.iwslt-papers.6,2012.iwslt-papers.3,0,0.145403,"Missing"
2014.iwslt-papers.6,D13-1140,0,0.182472,"Missing"
2014.iwslt-papers.6,P05-1012,0,0.155117,"nce. The basic feature functions used in this study are very similar to those used by standard phrase-based SMT systems (see [30] for instance). When reranking with a continuous space model, Fλ (.) is augmented to also include an additional feature denoted fθ (s, h). As explained in Section 2.2, fθ (s, h) typically As explained above, each hypothesis hi produced by the decoder is scored according to (4). Its quality can also be evaluated by the sentence-level approximation of the BLEU score sBLEU (hi ). Let h∗ denote the hypothesis with the best sentence BLEU score. A max-margin loss function [33, 34, 20] for estimating θ can then be formulated as follows: Lmm (θ, s) = −Gλ,θ (s, h∗ ) + max (Gλ,θ (s, hj ) + costα (hj )) , 1≤j≤N 2 The following parameters can be initialized given a source and target language monolingual models: the source and target word embeddings respectively, and the structured output layer’s structure. 3 See however [31, 32, 19] for early attempts to integrate Neural Network Translation Models within the decoder. (5)  where costα (hj ) = α sBLEU (h∗ ) − sBLEU (hj ) . The parameter α mitigates the contribution of the cost function 4 http://www.statmt.org/moses/ 194 Proceedin"
2014.iwslt-papers.6,W02-1001,0,0.126019,"he structured output layer’s structure. 3 See however [31, 32, 19] for early attempts to integrate Neural Network Translation Models within the decoder. (5)  where costα (hj ) = α sBLEU (h∗ ) − sBLEU (hj ) . The parameter α mitigates the contribution of the cost function 4 http://www.statmt.org/moses/ 194 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 to the objective function. When alpha &gt; 0, the objective defined in (5) is a general max-margin training criterion; taking α = 0 corresponds to the structured perceptron loss [35]. This objective function aims to discriminatively learn to give the highest model score to the hypothesis h∗ having the best sentence level BLEU. Moreover, the margin term enforces the scoring difference between h∗ and the rest of the N -best list to be greater than the margin. However, a source sentence can have, among the N -best list, several good translations that differ only slightly from the best hypothesis. The max-margin objective function defined above nevertheless considers that all hypotheses, except the best one, are wrong. The ranking-based approach defined below tries to correct"
2014.iwslt-papers.6,P02-1040,0,0.0899927,"continuous space translation models, i.e to adapt the parameters θ. The baseline and out-of-domain system is trained in the condition of the shared translation task of WMT 2013 evaluation campaign.5 This system includes CSTMs that will be used as starting points for adaptation. The official development and test sets respectively contain 934 and 1, 664 sentence pairs. Following [9], these sets are swapped, the tuning of the feature weights λ is carried out on 1, 664 sentences of the latter, while the final test is on 934 sentences of the former. Translations are evaluated using the BLEU score [36]. For a fair comparison, all BLEU scores reported are obtained after a tuning phase on the dev set, including the baseline system. For Algorithm 1, (θ, λ) are selected by maximizing the BLEU score on the dev set (line 7). 4.2. Baseline system and models The n-gram-based system used here is based on an open source implementation6 of the bilingual n-gram approach to Statistical Machine Translation [37]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using an n-gram model of (source, target) pairs as described in section 2.1. Training this mode"
2014.iwslt-papers.6,J04-2004,0,0.550335,"apped, the tuning of the feature weights λ is carried out on 1, 664 sentences of the latter, while the final test is on 934 sentences of the former. Translations are evaluated using the BLEU score [36]. For a fair comparison, all BLEU scores reported are obtained after a tuning phase on the dev set, including the baseline system. For Algorithm 1, (θ, λ) are selected by maximizing the BLEU score on the dev set (line 7). 4.2. Baseline system and models The n-gram-based system used here is based on an open source implementation6 of the bilingual n-gram approach to Statistical Machine Translation [37]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using an n-gram model of (source, target) pairs as described in section 2.1. Training this model requires to reorder source sentences so as to match the target word order. This is performed by a non-deterministic finite-state reordering model, which uses part-of-speech information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities. In addition to the TM, fourteen feature functions are included that are similar to the standard phrase-based system: target-l"
2014.iwslt-papers.6,P13-1126,0,0.0203937,"d in-domain data, while in all other cases the baseline system only uses out-of-domain data. SMT system is entirely re-trained from scratch to integrate in-domain data (from word alignments to the large scale target language model), and all four CSTMs defined by (2) are adapted using the CLL criterion. This experiment shows that we can achieve slightly better performance by only adapting two CSTMs with the proposed objective function. 5. Related work Most recent works in domain adaptation for SMT focuses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually"
2014.iwslt-papers.6,P06-1096,0,0.0387168,"rge scale target language model), and all four CSTMs defined by (2) are adapted using the CLL criterion. This experiment shows that we can achieve slightly better performance by only adapting two CSTMs with the proposed objective function. 5. Related work Most recent works in domain adaptation for SMT focuses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually used for N -best reranking. In this work, the proposed objective functions borrow from these two lines of research to both adapt the CSTM (θ) and tune its contribution (λ) to the whole SMT system."
2014.iwslt-papers.6,N04-1023,0,0.0489942,"uses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually used for N -best reranking. In this work, the proposed objective functions borrow from these two lines of research to both adapt the CSTM (θ) and tune its contribution (λ) to the whole SMT system. To the best of our knowledge, the most similar work on discriminative training or adaptation of neural network models is [12]. In this article, the authors propose to estimate the parameters of a neural network towards the expected BLEU score, while tuning λ by standard tools. Algorithm 1 is very similar to the o"
2014.iwslt-papers.6,P14-2023,0,0.100364,"this article, the authors propose to estimate the parameters of a neural network towards the expected BLEU score, while tuning λ by standard tools. Algorithm 1 is very similar to the optimization algorithm they describe, except that in our case, the feature weights λ are regularly updated for a better and tighter integration of the CSTM into the SMT system. Moreover, their proposed model only considers phrase pairs in isolation, while we use a probabilistic model of the joint distribution of sentence pairs. Expected BLEU training was also applied to recurrent neural network language model in [42]. In [13], the authors also introduce a ranking-type objective function that only aims to estimate word embeddings in a multitask-learning framework. Furthermore, [17] investigates the use of a large-margin criterion to train a recursive neural network for syntactic parsing. Interestingly, their model is also used to rerank N -best derivations generated by a conventional probabilistic context-free grammar. However, as showed by experimental results, the max-margin criterion alone is less adapted to machine translation. One explanation is that the N -best lists generated by the SMT system are n"
2014.iwslt-papers.6,N13-1048,0,0.21861,"Missing"
2014.iwslt-papers.6,D13-1106,0,0.12517,"Missing"
2014.iwslt-papers.9,N13-1073,0,0.0357672,"1. Introduction Statistical Machine Translation (SMT) has considerably matured in the past decade and is nowadays a competitive option in most practical machine-assisted translation scenarios. A notable fact about SMT technology is that the construction of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of inp"
2014.iwslt-papers.9,P13-1031,0,0.0201309,"istical Machine Translation (SMT) has considerably matured in the past decade and is nowadays a competitive option in most practical machine-assisted translation scenarios. A notable fact about SMT technology is that the construction of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of input text. However, new"
2014.iwslt-papers.9,E12-1016,0,0.0115943,"n of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of input text. However, new data are constantly made available, and the state-of-the-art SMT approaches cannot seamlessly take advantage of them to improve their performance. Incorporating newly available data can help to increase the n-gram coverage and to i"
2014.iwslt-papers.9,P05-1032,0,0.185327,"rove their performance. Incorporating newly available data can help to increase the n-gram coverage and to improve the parameter estimates of an existing system. These observations provide motivation for incorporating newly available data into existing systems, in particular when the new data is known to be directly relevant to the application documents. Previous works have empirically shown that not all phrase translation examples are necessary to reach top performance, so that phrase tables can be built on a per-need basis for a given input text using random sampling of translation examples [4, 5]. The main strength of these approaches is that they reduce the computation time of translation models and make it possible to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly availa"
2014.iwslt-papers.9,C08-1064,0,0.109183,"rove their performance. Incorporating newly available data can help to increase the n-gram coverage and to improve the parameter estimates of an existing system. These observations provide motivation for incorporating newly available data into existing systems, in particular when the new data is known to be directly relevant to the application documents. Previous works have empirically shown that not all phrase translation examples are necessary to reach top performance, so that phrase tables can be built on a per-need basis for a given input text using random sampling of translation examples [4, 5]. The main strength of these approaches is that they reduce the computation time of translation models and make it possible to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly availa"
2014.iwslt-papers.9,N10-1062,0,0.0179721,"e to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly available parallel data, as well as online learning of system parameters. This is similar to the objectives of stream-based SMT [6], but crucially does not require the actual alignment of all available data. This means that we are able to develop systems even faster: as our experiments show, immediate integration of newly translated documents, combined with online tuning, make it possible to dispense altogether with the development step. This pragmatic solution offers both the capacity to deliver translations to users much earlier, but also to quickly improve subsequent automatic translations. The rest of the paper is organized as follows. In Section 2, we describe our framework for efficient on-demand development of SMT"
2014.iwslt-papers.9,2013.iwslt-papers.7,1,0.728572,"pproximation can be used instead: p(¯ s|t¯) = min(1.0, p(t¯|¯ s) × f req(¯ s) ) f req(t¯) (2) where f req(·) is the relative frequency of the given phrase in the entire corpus. The numerator (p(t¯|¯ s) × f req(¯ s)) represents the predicted joint probability of s¯ and t¯. 2.2. On-demand word alignment The second main peculiarity of our architecture is the ability to perform word alignment on demand for a subset of selected bi-sentences. Word and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores fo"
2014.iwslt-papers.9,2012.eamt-1.62,1,0.84098,"0, p(t¯|¯ s) × f req(¯ s) ) f req(t¯) (2) where f req(·) is the relative frequency of the given phrase in the entire corpus. The numerator (p(t¯|¯ s) × f req(¯ s)) represents the predicted joint probability of s¯ and t¯. 2.2. On-demand word alignment The second main peculiarity of our architecture is the ability to perform word alignment on demand for a subset of selected bi-sentences. Word and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores for the words in the source part of the bi-sentences o"
2014.iwslt-papers.9,J97-3002,0,0.156911,"ord and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores for the words in the source part of the bi-sentences of interest are generated by a sampling-based transpotting method, which 1 Querying a suffix array for a phrase of k words can be performed in (k + log(|C|)) operations, where |C |is the corpus size. A suffix array can be constructed in O(|C |log(|C|)) time. 2 Although this model has been shown to be non essential, we use it for the stability of our systems, especially when untuned systems"
2014.iwslt-papers.9,P07-2045,0,0.00453915,"in a translation sample of s¯, S[¯ s]. The sentence pairs in S[¯ s] are then aligned by our on-demand word alignment, where the generated alignments are denoted as AS[¯s] , and are then used to extract the translations and to compute model parameters θ s¯ for the source phrase s¯. This process is repeated for all source phrases in Σ[d], and the resulting translation table can then be used by a phrase-based decoder to translate the input text into the target language. Besides the translation models, the other models in our system are the same as in the default configuration of the moses system [11], including the lexical weighting and lexicalized reordering models. These models are also computed on-demand based on the computed word alignments. Algorithm 1 On-demand development procedure Data: training corpus C, Input: an input document d, sample size M compute Σ[d] for all s¯ ∈ Σ[d] do S[¯ s] = rnd(M, C, s¯) // Sampling AS[¯s] = owa(S[¯ s]) // Alignment estimate(θ s¯, S[¯ s], AS[¯s] ) // Estimation end for 3. Experiments In this section, we have chosen to illustrate two favorable use cases of our framework in order to demonstrate its capabilities and flexibility. The data used in this w"
2014.iwslt-papers.9,N12-1047,0,0.0265654,"ign the full bi-corpus and the moses scripts to extract a huge phrase table and a reordering table for the entire parallel corpus (respectively 20Gb and 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we"
2014.iwslt-papers.9,P02-1040,0,0.0951362,"ively 20Gb and 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we take the 100 documents constituting the test set (see Table 1) to simulate the document stream. In the following, we describe a series of"
2014.iwslt-papers.9,2006.amta-papers.25,0,0.00978038,"d 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we take the 100 documents constituting the test set (see Table 1) to simulate the document stream. In the following, we describe a series of increasingly"
2014.iwslt-papers.9,D09-1079,0,0.0234682,"be mostly attributed to the absence of tuning. However, translations for the test set are delivered much faster, where our system is x36 times wall clock faster than moses. 7 We used the moses decoder in our experiments, whose default parameters are: 0.3 for all 7 reordering features, including 6 lexical reordering features and 1 distance-based reordering feature; 0.2 for all 5 translation features; 0.5 for the language model and −1 for the word penalty. 8 In this work, the language model is still pre-trained. Future work will include the incremental / on-demand estimation of language models [15]. 216 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2500 8 6 2000 Time(ms)/Token 4 ∆BLEU 1500 1000 on-demand on-demand+spec 2 0 2 4 500 6 0 0 20 40 60 Document id 80 8 0 100 Figure 1: Evolution of the average per token processing time for a sequence of documents. As mentioned before, the computed word alignments are cached and are available for translating subsequent documents. To further analyze the effect of the cache, Figure 1 shows how the average per token processing time decreases as more and more documents from the s"
2014.iwslt-papers.9,E14-1042,0,0.0108161,"of data used. However, for those domain-specific terms, phraseology or long phrases which usually only exist in the in-domain data, we could use the spec phrase table to translate them. 9 In fact, the Cochrane dataset used in this study is made of two parts: a large portion of the data was translated by human translator from scratch, while a smaller amount a document where actually produced through postedition. We still use this data as a post-edited corpus in our experiments, although these two kinds of data are slightly different. We believe this does not affect our experimental conclusions [16]. 20 40 60 Document id 80 100 Figure 2: Document-level comparison with moses system in English-to-French translation direction. The y-axis represents the difference in BLEU score (∆BLEU) between our systems and the vanilla moses system for each document in the sequence. Results in Table 2 show that the additional table (+spec) helps to significantly improve translation quality over the raw on-demand configuration (+3.7 BP), for a modest additional processing time of half an hour for aligning the content of the first 99 documents. Since the spec table for document dt is estimated based on the p"
2014.iwslt-papers.9,W07-0733,0,0.026144,"system development. In practice, the system’s weights are retuned after each document has been translated (and post-edited) as follows: Taking the previous weights as the initial point, we run the parameter tuning process (here KBMIRA) on the just translated and post-edited document; the resulting parameter values are then averaged with the parameter values of the 10 previous documents10 , and then used for translating the next document. Additionally, in order to leverage the spec table, we also allow here the spec phrase table to compete with the phrase table estimated from the static corpus [17] instead of having the latter take precedence. Results for this last configuration are given in Table 2 (+online). Our simple online tuning yields a significant improvement (+4.1 BP) over the untuned on-demand+spec configuration. Even though the two configurations cannot be directly compared at the corpuslevel, since our system integrates a growing set of in-domain data, while moses on its part greatly benefits from the indomain development data, we still note that our framework now outperforms the moses baseline (+2.3 BP). More interestingly, comparison at the document-level (see Figure 3) de"
2014.iwslt-papers.9,W00-0507,0,0.0887616,"e translation quality at the beginning is not very competitive. Also, its incremental adaptation scheme quickly improves its performance, especially on long and repetitive documents. 4. Related Work Our framework provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output"
2014.iwslt-papers.9,J09-1002,0,0.013332,"rk provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusio"
2014.iwslt-papers.9,P09-4005,0,0.0134991,"rk provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusio"
2014.iwslt-papers.9,N10-1079,0,0.0223225,"build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusion This work has addressed the issue of how the computationally expensive cost of the development of high-performance 220 Proceedings of the 11th International Wor"
2014.iwslt-papers.9,2013.mtsummit-papers.24,0,0.0123177,"nt within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusion This work has addressed the issue of how the computationally expensive cost of the development of high-performance 220 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 SMT systems, which typic"
2015.iwslt-papers.10,W07-0735,0,\N,Missing
2015.iwslt-papers.10,N04-4015,0,\N,Missing
2015.iwslt-papers.10,W10-1729,0,\N,Missing
2015.iwslt-papers.10,W01-1407,0,\N,Missing
2015.iwslt-papers.10,C04-1045,0,\N,Missing
2015.iwslt-papers.10,J90-2002,0,\N,Missing
2015.iwslt-papers.10,D07-1091,0,\N,Missing
2015.iwslt-papers.10,P07-2045,0,\N,Missing
2015.iwslt-papers.10,W08-0336,0,\N,Missing
2015.iwslt-papers.10,2007.mtsummit-papers.65,0,\N,Missing
2015.iwslt-papers.10,D09-1075,0,\N,Missing
2015.iwslt-papers.10,P05-1033,0,\N,Missing
2015.iwslt-papers.10,J03-1002,0,\N,Missing
2015.iwslt-papers.10,P06-1001,0,\N,Missing
2015.iwslt-papers.10,C10-1092,0,\N,Missing
2015.iwslt-papers.10,W07-0704,0,\N,Missing
2015.iwslt-papers.10,H05-1085,0,\N,Missing
2015.iwslt-papers.10,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-papers.10,W11-2123,0,\N,Missing
2015.iwslt-papers.10,W07-0729,0,\N,Missing
2015.jeptalnrecital-long.1,J07-2003,0,0.0550672,"Missing"
2015.jeptalnrecital-long.1,P04-1015,0,0.388473,"Missing"
2015.jeptalnrecital-long.1,W06-1673,0,0.0835146,"Missing"
2015.jeptalnrecital-long.1,E14-1002,0,0.0208889,"Missing"
2015.jeptalnrecital-long.1,N10-1115,0,0.0628036,"Missing"
2015.jeptalnrecital-long.1,C12-1059,0,0.155152,"Missing"
2015.jeptalnrecital-long.1,Q13-1033,0,0.159446,"Missing"
2015.jeptalnrecital-long.1,P10-1052,1,0.852088,"Missing"
2015.jeptalnrecital-long.1,C12-1106,0,0.0334341,"Missing"
2015.jeptalnrecital-long.1,P07-1096,0,0.0800876,"Missing"
2015.jeptalnrecital-long.1,W03-3023,0,0.0673149,"Missing"
2015.jeptalnrecital-long.23,J04-2004,0,0.634087,"Missing"
2015.jeptalnrecital-long.23,N12-1047,0,0.150827,"Missing"
2015.jeptalnrecital-long.23,D14-1179,0,0.0687613,"Missing"
2015.jeptalnrecital-long.23,W02-1001,0,0.147491,"Missing"
2015.jeptalnrecital-long.23,P14-1129,0,0.116526,"Missing"
2015.jeptalnrecital-long.23,2014.iwslt-papers.6,1,0.781554,"Missing"
2015.jeptalnrecital-long.23,W14-3307,1,0.895942,"Missing"
2015.jeptalnrecital-long.23,federico-etal-2012-iwslt,0,0.0530621,"Missing"
2015.jeptalnrecital-long.23,D11-1125,0,0.196228,"Missing"
2015.jeptalnrecital-long.23,P12-1092,0,0.129908,"Missing"
2015.jeptalnrecital-long.23,D13-1176,0,0.10623,"Missing"
2015.jeptalnrecital-long.23,J10-4005,0,0.0560098,"Missing"
2015.jeptalnrecital-long.23,N12-1005,1,0.932129,"Missing"
2015.jeptalnrecital-long.23,J06-4004,0,0.523008,"Missing"
2015.jeptalnrecital-long.23,P05-1012,0,0.228448,"Missing"
2015.jeptalnrecital-long.23,C90-3038,0,0.54057,"Missing"
2015.jeptalnrecital-long.23,2012.iwslt-papers.3,0,0.3952,"Missing"
2015.jeptalnrecital-long.23,P02-1040,0,0.104932,"Missing"
2015.jeptalnrecital-long.23,C12-2104,0,0.0539265,"Missing"
2015.jeptalnrecital-long.23,D07-1045,0,0.562968,"Missing"
2015.jeptalnrecital-long.23,P13-1045,0,0.17715,"Missing"
2015.jeptalnrecital-long.23,P10-1040,0,0.0114571,"Missing"
2015.jeptalnrecital-long.23,D13-1140,0,0.16983,"Missing"
2015.jeptalnrecital-long.23,D07-1080,0,0.334531,"Missing"
2015.jeptalnrecital-long.23,P13-1017,0,0.27926,"Missing"
2015.jeptalnrecital-long.23,2002.tmi-tutorials.2,0,0.48302,"Missing"
2015.jeptalnrecital-long.4,N10-1066,0,0.0751533,"Missing"
2015.jeptalnrecital-long.4,P05-1022,0,0.0847008,"Missing"
2015.jeptalnrecital-long.4,J13-1005,0,0.0499864,"Missing"
2015.jeptalnrecital-long.4,P08-1085,0,0.0604285,"Missing"
2015.jeptalnrecital-long.4,A00-2013,0,0.166506,"Missing"
2015.jeptalnrecital-long.4,J00-4006,0,0.0394026,"Missing"
2015.jeptalnrecital-long.4,D12-1127,0,0.0294932,"Missing"
2015.jeptalnrecital-long.4,J94-2001,0,0.681755,"Missing"
2015.jeptalnrecital-long.4,C14-1110,0,0.0433613,"Missing"
2015.jeptalnrecital-long.4,D13-1032,0,0.0356679,"Missing"
2015.jeptalnrecital-long.4,W96-0213,0,0.79269,"Missing"
2015.jeptalnrecital-long.4,P09-1057,0,0.0433144,"Missing"
2015.jeptalnrecital-long.4,P14-2043,0,0.0337427,"Missing"
2015.jeptalnrecital-long.4,P05-1044,0,0.0758103,"Missing"
2015.jeptalnrecital-long.4,H05-1060,0,0.0593043,"Missing"
2015.jeptalnrecital-long.4,Q13-1001,0,0.0354823,"Missing"
2015.jeptalnrecital-long.4,C12-1170,0,0.0540179,"Missing"
2015.jeptalnrecital-long.4,D14-1187,1,0.887622,"Missing"
2015.lilt-12.6,P93-1002,0,0.493111,"azon are in the ’fiction’ category (source: http://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is pr"
2015.lilt-12.6,P91-1023,0,0.496955,"ap” of the bitext, trying to include as many anchors as possible, while also remaining close to the bitext “diagonal”. A post-processing step will take sentence boundaries into account to deliver the final sentence alignment. Note that GMA uses no length cues, and also that it has been shown to perform particularly well at spotting large omissions in a bitext (Melamed, 1996). Moore’s (2002) approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of length-based approaches (Brown et al., 1991, Gale and Church, 1991). This initial alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores. These scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high-confidence, 1:1 sentence alignments to be used as training material for data-intensive MT. Sentences that cannot be reliably aligned are discarded from the resulting alignment. Hunalign is described in (Varga et al., 2005). It also implements a two-pass strategy which resembles Moore’s approach."
2015.lilt-12.6,J93-1004,0,0.912338,"CTOBER 2015 ilar to (Munteanu and Marcu, 2005, Smith et al., 2010, Éva Mújdricza-Maydt et al., 2013). These steps are discussed below in detail. 3.1 A MaxEnt Model for Parallel Sentences Any sentence alignment method needs, at some point, to assess the level of parallelism of a sentence pair, based on a surface description of these sentences. As discussed above, two kinds of clues are widely employed in existing systems to perform such assessment: sentence lengths and lexical information. Most dynamic programming-based approaches further impose a prior probability distribution on link types (Gale and Church, 1993). Our system combines all the available clues in a principled, rather than heuristic, way, using a MaxEnt model.13 For any pair of sentences l = (e, f ), the model computes a link posterior probability p(Y = y|e, f ), where Y is a binary variable for the existence of an alignment link. The rationale for using MaxEnt is (a) that it is possible to efficiently integrate as many features as desired into the model, and (b) that we expect the resulting posterior probabilities to be less peaked towards extreme values than what we have observed with generative alignment models such as Moore’s model. W"
2015.lilt-12.6,2005.mtsummit-papers.11,0,0.515001,"rmance of the baseline sentence alignment tools on these four corpora, using the standard link-level F-measure. As explained above, the two larger corpora are aligned at the paragraph level, meaning that such resources cannot be readily used to compute alignment quality scores. Our solution has been to refine this coarse alignment by running the Gale and Church (1991) alignment program to compute within-paragraph sentence alignments, keeping the paragraph alignments unchanged from the reference. This approach is similar to the procedure used to align the Europarl corpus at the sentence level (Koehn, 2005), where reliable paragraph boundaries are readily derived from speaker turns or session changes. As a result, these semiautomatic references only contain a restricted number of link types as computed by Gale and Church (1991) program: 1:0, 0:1, 1:1, 1:2, and 2:1. We then take these partially correct alignments as pseudo-references for the purpose of evaluating alignment tools – keeping in mind that the corresponding results will only be approximate. Our main evaluation results are in Table 2. For more details, see the Appendix, Tables I, II and III. Regarding the gold corpus (manual en-fr), th"
2015.lilt-12.6,2013.mtsummit-papers.10,0,0.149492,"a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable"
2015.lilt-12.6,P98-1117,0,0.173728,"s. Section 3 presents our two-pass alignment algorithm; one of its distinguishing feature is its use of external resources to improve its decisions. Experiments on two language pairs (English-French and English-Spanish) are presented and discussed in Section 4, before we recap our main findings and discuss further prospects in Section 5. 2 Aligning literary texts: solved or unsolved? Commenting on the unsatisfactory results achieved by all sentence alignment systems during the Arcade evaluation campaign (Véronis and Langlais, 2000) on the single test book, Jules Verne’s De la terre à la lune, Langlais et al. (1998) hint that: these poor results are linked to the literary nature of the corpus, where translation is freer and more interpretative, They express a general feeling that literary texts should be more difficult to align than, say, technical documents. However, assessing the real difficulty of the task is in itself challenging, for lack of a large set of books annotated with a reference (gold) alignment. For instance, the recent study of Éva MújdriczaMaydt et al. (2013) on English-German alignment used only three books for evaluation. In this section, we aim to provide a more precise answer to thi"
2015.lilt-12.6,ma-2006-champollion,0,0.0290568,"://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sent"
2015.lilt-12.6,C96-2129,0,0.175554,"roach included in this sample, and yet one of the most effective: assuming “sure” lexical anchor points in the bitext map, obtained e.g. using bilingual dictionaries or cognatebased heuristics, GMA greedily builds a so-called “sentence map” of the bitext, trying to include as many anchors as possible, while also remaining close to the bitext “diagonal”. A post-processing step will take sentence boundaries into account to deliver the final sentence alignment. Note that GMA uses no length cues, and also that it has been shown to perform particularly well at spotting large omissions in a bitext (Melamed, 1996). Moore’s (2002) approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of length-based approaches (Brown et al., 1991, Gale and Church, 1991). This initial alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores. These scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high-confidence, 1:1 sentence alignments to be used as t"
2015.lilt-12.6,J99-1003,0,0.636382,"y (source: http://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision pa"
2015.lilt-12.6,moore-2002-fast,0,0.600086,"sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy p"
2015.lilt-12.6,J05-4003,0,0.543729,"of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable bilingual corpora (Munteanu and Marcu, 2005, Smith et al., 2010), using supervised learning techniques. While these restrictions are reasonable for the purpose of training SMT systems,4 for other applications, such as bitext visualization, translator training, automatic translation checking, the alignment for the entire bitext should be computed. This is especially the case for multilingual works of fiction: the parts that are more difficult for automatic alignment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For"
2015.lilt-12.6,2010.amta-papers.14,0,0.381855,"Missing"
2015.lilt-12.6,N10-1063,0,0.17645,"of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable bilingual corpora (Munteanu and Marcu, 2005, Smith et al., 2010), using supervised learning techniques. While these restrictions are reasonable for the purpose of training SMT systems,4 for other applications, such as bitext visualization, translator training, automatic translation checking, the alignment for the entire bitext should be computed. This is especially the case for multilingual works of fiction: the parts that are more difficult for automatic alignment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For such tasks, it seems"
2015.lilt-12.6,C10-1124,0,0.0197394,"f state-of-the-art methods for literary texts, both in terms of their precision and recall, using large collections of publicly available novels; (b) develop, analyse and improve an algorithm initially introduced in (Yu et al., 2012a,b), which was shown to outperform a significant sample of sentence alignment tools on a set of manually aligned corpora. The rest of this paper is organized as follows: we briefly review, in Section 2, several state-of-the-art sentence alignment tools and evaluate their performance, first on two small reference datasets of gold alignments, then on a 4 The work by Uszkoreit et al. (2010), however, shows that this procedure actually discards a lot of useful data. S ENTENCE ALIGNMENT FOR LITERARY TEXTS / 5 much larger set of approximately correct alignments. Section 3 presents our two-pass alignment algorithm; one of its distinguishing feature is its use of external resources to improve its decisions. Experiments on two language pairs (English-French and English-Spanish) are presented and discussed in Section 4, before we recap our main findings and discuss further prospects in Section 5. 2 Aligning literary texts: solved or unsolved? Commenting on the unsatisfactory results ac"
2015.lilt-12.6,W12-2505,1,0.148175,"nment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For such tasks, it seems that both precision and recall are to be maximized. This paper therefore reconsiders the task of full-text sentence alignment with two goals: (a) re-evaluate the actual performance of state-of-the-art methods for literary texts, both in terms of their precision and recall, using large collections of publicly available novels; (b) develop, analyse and improve an algorithm initially introduced in (Yu et al., 2012a,b), which was shown to outperform a significant sample of sentence alignment tools on a set of manually aligned corpora. The rest of this paper is organized as follows: we briefly review, in Section 2, several state-of-the-art sentence alignment tools and evaluate their performance, first on two small reference datasets of gold alignments, then on a 4 The work by Uszkoreit et al. (2010), however, shows that this procedure actually discards a lot of useful data. S ENTENCE ALIGNMENT FOR LITERARY TEXTS / 5 much larger set of approximately correct alignments. Section 3 presents our two-pass alig"
2016.jeptalnrecital-demo.12,L16-1099,1,0.848406,"Missing"
2016.jeptalnrecital-long.1,P81-1022,0,0.645685,"Missing"
2016.jeptalnrecital-long.1,D11-1005,0,0.0583432,"Missing"
2016.jeptalnrecital-long.1,N13-1073,0,0.0577735,"Missing"
2016.jeptalnrecital-long.1,C12-1059,0,0.0665175,"Missing"
2016.jeptalnrecital-long.1,2005.mtsummit-papers.11,0,0.0535649,"Missing"
2016.jeptalnrecital-long.1,J10-4005,0,0.0632718,"Missing"
2016.jeptalnrecital-long.1,C14-1075,0,0.029758,"Missing"
2016.jeptalnrecital-long.1,P14-1126,0,0.0382447,"Missing"
2016.jeptalnrecital-long.1,P13-2017,0,0.0311513,"Missing"
2016.jeptalnrecital-long.1,D11-1006,0,0.0608004,"Missing"
2016.jeptalnrecital-long.1,D10-1120,0,0.0828629,"Missing"
2016.jeptalnrecital-long.1,petrov-etal-2012-universal,0,0.0748813,"Missing"
2016.jeptalnrecital-long.1,D15-1039,0,0.0211998,"Missing"
2016.jeptalnrecital-long.1,P11-2120,0,0.0439935,"Missing"
2016.jeptalnrecital-long.1,W09-1104,0,0.0711999,"Missing"
2016.jeptalnrecital-long.1,N13-1126,0,0.0307885,"Missing"
2016.jeptalnrecital-long.1,C14-1175,0,0.0251049,"Missing"
2016.jeptalnrecital-long.1,W14-1614,0,0.024948,"Missing"
2016.jeptalnrecital-long.1,I08-3008,0,0.0900704,"Missing"
2016.jeptalnrecital-long.1,P11-2033,0,0.0870376,"Missing"
2016.jeptalnrecital-long.19,2015.jeptalnrecital-long.25,0,0.0863147,"Missing"
2016.jeptalnrecital-long.19,W02-1001,0,0.384938,"Missing"
2016.jeptalnrecital-long.19,P04-1015,0,0.196224,"Missing"
2016.jeptalnrecital-long.19,C12-1059,0,0.0473173,"Missing"
2016.jeptalnrecital-long.19,Q13-1033,0,0.0339019,"Missing"
2016.jeptalnrecital-long.19,D11-1125,0,0.0383654,"Missing"
2016.jeptalnrecital-long.19,N12-1015,0,0.0451411,"Missing"
2016.jeptalnrecital-long.19,2015.jeptalnrecital-long.1,1,0.834506,"Missing"
2016.jeptalnrecital-long.19,J08-4003,0,0.0276826,"Missing"
2016.jeptalnrecital-long.19,P02-1038,0,0.0406177,"Missing"
2016.jeptalnrecital-long.19,W14-6111,0,0.0604365,"Missing"
2016.jeptalnrecital-long.19,P11-2033,0,0.0713007,"Missing"
2016.jeptalnrecital-long.19,C12-2136,0,0.0394061,"Missing"
2017.jeptalnrecital-court.17,W02-1002,0,0.116147,"Missing"
2017.jeptalnrecital-court.17,W16-3905,0,0.0323013,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-long.4,1,0.852163,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-court.29,0,0.103588,"Missing"
2017.jeptalnrecital-court.17,C12-1149,0,0.111228,"Missing"
2017.jeptalnrecital-court.17,W13-4917,0,0.0214041,"Missing"
2017.jeptalnrecital-court.17,L16-1680,0,0.0514621,"Missing"
2017.jeptalnrecital-court.17,W11-0328,0,0.0411679,"Missing"
2017.jeptalnrecital-court.17,D13-1117,0,0.0567552,"Missing"
2017.jeptalnrecital-court.17,D14-1187,1,0.727723,"Missing"
2017.jeptalnrecital-court.17,F14-1016,1,0.734185,"Missing"
2017.jeptalnrecital-long.2,2015.iwslt-papers.10,1,0.885315,"Missing"
2017.jeptalnrecital-long.2,N12-1047,0,0.057461,"Missing"
2017.jeptalnrecital-long.2,W08-0310,1,0.844247,"Missing"
2017.jeptalnrecital-long.2,2010.iwslt-papers.6,0,0.04797,"Missing"
2017.jeptalnrecital-long.2,N13-1073,0,0.0645201,"Missing"
2017.jeptalnrecital-long.2,E12-1068,0,0.0566367,"Missing"
2017.jeptalnrecital-long.2,H05-1085,0,0.149199,"Missing"
2017.jeptalnrecital-long.2,W11-2123,0,0.0671681,"Missing"
2017.jeptalnrecital-long.2,2005.mtsummit-papers.11,0,0.101877,"Missing"
2017.jeptalnrecital-long.2,P07-2045,0,0.00469492,"Missing"
2017.jeptalnrecital-long.2,W15-3016,1,0.891992,"Missing"
2017.jeptalnrecital-long.2,P07-1017,0,0.0695655,"Missing"
2017.jeptalnrecital-long.2,C04-1045,0,0.12944,"Missing"
2017.jeptalnrecital-long.2,E17-3017,0,0.0349127,"Missing"
2017.jeptalnrecital-long.2,W16-2209,0,0.0453092,"Missing"
2017.jeptalnrecital-long.2,P16-1162,0,0.0543926,"Missing"
2017.jeptalnrecital-long.2,P14-5003,0,0.069008,"Missing"
2017.jeptalnrecital-long.2,P06-1122,0,0.0874536,"Missing"
2017.jeptalnrecital-long.2,P08-1059,0,0.0852121,"Missing"
2018.jeptalnrecital-court.41,2017.jeptalnrecital-court.17,1,0.830804,"Missing"
2018.jeptalnrecital-court.41,H92-1026,0,0.174031,"Missing"
2018.jeptalnrecital-court.41,W13-2308,0,0.0624263,"Missing"
2018.jeptalnrecital-court.41,F12-2024,0,0.0720861,"Missing"
2018.jeptalnrecital-court.41,E03-1068,0,0.162753,"Missing"
2018.jeptalnrecital-court.41,E14-4028,0,0.0507499,"Missing"
2018.jeptalnrecital-court.41,E17-5001,0,0.0485342,"Missing"
2018.jeptalnrecital-court.41,D14-1104,0,0.067159,"Missing"
2018.jeptalnrecital-court.41,C12-1149,0,0.0350381,"Missing"
2018.jeptalnrecital-court.41,K17-3016,0,0.0436645,"Missing"
2018.jeptalnrecital-court.41,D14-1187,1,0.858128,"Missing"
2018.jeptalnrecital-court.41,P11-2033,0,0.0239797,"Missing"
2018.jeptalnrecital-long.5,P17-1080,0,0.0450973,"Missing"
2018.jeptalnrecital-long.5,D16-1025,0,0.0364664,"Missing"
2018.jeptalnrecital-long.5,W17-4705,1,0.828605,"Missing"
2018.jeptalnrecital-long.5,W11-2123,0,0.0593294,"Missing"
2018.jeptalnrecital-long.5,D17-1263,0,0.02122,"Missing"
2018.jeptalnrecital-long.5,C90-2037,0,0.719431,"Missing"
2018.jeptalnrecital-long.5,2005.mtsummit-papers.11,0,0.0674661,"Missing"
2018.jeptalnrecital-long.5,J10-4005,0,0.0721088,"Missing"
2018.jeptalnrecital-long.5,P07-2045,0,0.0132636,"Missing"
2018.jeptalnrecital-long.5,Q16-1037,0,0.0316816,"Missing"
2018.jeptalnrecital-long.5,2014.eamt-1.38,0,0.0254457,"Missing"
2018.jeptalnrecital-long.5,P14-5010,0,0.0047111,"Missing"
2018.jeptalnrecital-long.5,P02-1040,0,0.10706,"Missing"
2018.jeptalnrecital-long.5,J11-4002,0,0.0570921,"Missing"
2018.jeptalnrecital-long.5,sagot-2010-lefff,0,0.0776293,"Missing"
2018.jeptalnrecital-long.5,E17-2060,0,0.0256647,"Missing"
2018.jeptalnrecital-long.5,E17-3017,0,0.0548732,"Missing"
2018.jeptalnrecital-long.5,P16-1009,0,0.0207476,"Missing"
2018.jeptalnrecital-long.5,P16-1162,0,0.0578375,"Missing"
2018.jeptalnrecital-long.5,E17-1100,0,0.0218339,"Missing"
2018.jeptalnrecital-long.5,P17-1184,0,0.0562338,"Missing"
2018.jeptalnrecital-long.5,vilar-etal-2006-error,0,0.106934,"Missing"
2020.amta-research.6,W16-2206,0,0.0151712,"egrate some biases that are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between successive attention vectors. The work of Peters et al. (2019) also aims to enhance the attention component of a sequence-to-sequence, by enforcing sparsity via the sparse-max operator. The work reported in (Alkhouli et al., 2016; Wang et al., 2017) explores ways to explicitely introduce alignments in NMT. They study various neuralizations of the standard generative alignment models, and also consider ways to exploit weak supervision from count-based models. This line of research is pursued by (Kim et al., 2017; Deng et al., 2018), where attention vectors are handled as structured latent variables in NMT; in this study, variational autoencoders are used represent the alignment structure. Finally, Garg et al. (2019) propose to jointly learn alignment and translation in a multi-task setting, thereby improving a Transfor"
2020.amta-research.6,K16-1002,0,0.256667,"n model, and which does not improve with joint learning. Model HMM Fastalign HMM Giza++ HMM+NN HMM+NN+BPE HMM+VAE+BPE +SP +AC English-French En-Fr Fr-En GDF 15.1 16.2 14.2 11.9 11.9 8.5 11.8 11.1 9.7 9.8 10.4 9.1 18.9 12.9 13.9 12.9 12.2 11.7 11.4 10.8 9.6 English-Romanian En-Ro Ro-En GDF 33.3 32.9 30.4 33.3 36.3 32.4 30.6 40.1 34.3 34.4 29.3 29.4 50.2 38.6 42.7 37.5 38.0 37.0 35.5 38.8 35.1 Table 3: AER scores for variants of the HMM model and for Fastalign. 4 4.1 Error Analysis Balancing the terms in the VAE objective One well-known issue of VAEs for text applications is posterior collapse (Bowman et al., 2016; Higgins et al., 2017), where the variational distribution collapses towards the prior distribution. This is because the KL term can get arbitrarily small, with a moderate effect on the reconstruction cost, assuming a strong reconstruction model (a recurrent network in typical applications). We also encountered this problem in our setting, but the interpretation is a bit different: when the KL term goes to zero, all words in the dictionary become indistinguishable and the reconstruction costs reaches its maximum, corresponding to the entropy of the uniform distribution of the target vocabular"
2020.amta-research.6,J93-2003,0,0.167931,"irections for further developments. 2 Neural word alignment variational models The standard approach to probabilistic alignment (Och and Ney, 2003) is to consider asymmetric models associating each word in a source sentence f1J = f1 . . . fJ of J words with exactly one word from the target sentence eI0 = e0 . . . eI of I + 1 words.1 This association is governed by unobserved alignment variables aJ1 = a1 ...aJ , yielding the following model: p(f1J , aJ1 |eI0 ) = J Y j−1 I p(aj |aj−1 , e0 )p(fj |aj1 , f1j−1 , eI0 ) 1 , f1 (1) j Two versions of this model are considered here: in the IBM model 1 (Brown et al., 1993), the j−1 I alignment model p(aj |aj−1 , e0 ) is uniform; in the HMM model of Vogel et al. (1996), 1 , f1 Markovian dependencies between alignment variables are assumed and aj is independant from all the preceding alignment variables given aj−1 . In both models, fj is conditionally independent to any other variable given aj and eI1 . Under these assumptions, both parameter estimation and optimal alignment can be performed efficiently with dynamic programming algorithms. In this approach, eI1 is not modeled. 2.1 A fully generative model We now present the fully generative approach introduced by"
2020.amta-research.6,W14-4012,0,0.170554,"Missing"
2020.amta-research.6,N16-1102,0,0.0474234,"Missing"
2020.amta-research.6,N19-1423,0,0.0101475,"ise) on English-Romanian data. R-Acc is the accuracy of the reconstruction model. 5 Related work The majority of recent approaches to neural word alignment fall into two categories: heuristic and probabilistic. A representative heuristic approach is (Legrand et al., 2016), which learns association scores between source and target word embeddings without any underlying probabilistic model. This simple approach is used to clean up translation memories in (Pham et al., 2018). More recently (Sabet et al., 2020) directly takes pre-trained non-contextual and contextual multilingual representations (Devlin et al., 2019) as their association scores, deriving individual word alignments by solving an optimal matching problem. Early work on probabilistic neural alignment is (Yang et al., 2013), where a feed-forward neural network is used to replace the count-based translation model of a HMM-based aligner. This approach is further developed in (Tamura et al., 2014) where a recurrent network helps to capture contextual dependencies between alignment links. This early work aims to improve the alignment quality for phrase-based MT. As discussed above, the work of (Rios et al., 2018) also considers neural versions of"
2020.amta-research.6,N13-1073,0,0.0848599,"Missing"
2020.amta-research.6,J07-3002,0,0.0710579,"as what we see for HMM+NN: as it predicts much more links than the others, this model also as an clear edge when it comes to post-hoc symmetrization, since the “grow-diag-final” heuristics heavily depends on the size of the intersection. Note that this problem has a much stronger overall effect in English-Romanian than in English-French. This is because the English-Romanian test set only contains sure links, which means that a low recall for aligned words directly impacts the AER. We do not see this for the French-English data, which contain many possible links that have no impact on recall (Fraser and Marcu, 2007). Incidentally, we also observe a null-word problem for HMM+NN+BPE; presumably splitting words in small units that are unrelated across languages can also make the model prefer the null alignment over links between actual words. These results clearly point out one deficiency of the current approach: for lack of having a proper model for the latent representation of the NULL token, the VAE-based approach tends to leave too many words unaligned. 4.3 Symmetrization and agreement We now study the effects of sharing parameters across alignment directions. We consider the English-Romanian test, for"
2020.amta-research.6,D19-1453,0,0.217505,"corpus used in WMT’16 evaluation,3 which correspond to a more challenging scenario where the training data is limited in size. Additional experiments with monolingual data use the Romanian data from News Crawl 2019 (∼ 6M sentences)4 . Basic statistics for these corpora are in Table 1. Corpus # sent. in train # sent. in test En-Fr En-Ro ∼1.9M ∼260K 447 246 # tokens in test Eng. For. 7 020 7 761 5 455 5 315 # non-null links 17 438 5 988 Table 1: Basic statistics for the data These corpora are preprocessed, lowercased and tokenized with standard tools from the Moses toolkit.5 Following notably (Garg et al., 2019), we perform the alignment between subword units generated by Byte-Pair-Encoding (Sennrich et al., 2015), implemented with the SentencePiece model (Kudo and Richardson, 2018) and computed independently6 in each language with 32K merge operations. This makes the training less computationally demanding and greatly mitigates the rare-word problem, which is a major weakness of historical count-based model. Our results and analyses are however based on word-level alignments. Subword-level alignments are converted into word-level alignments as follows: a link between a source and a target word exist"
2020.amta-research.6,J10-3007,0,0.0748838,"Missing"
2020.amta-research.6,2005.mtsummit-papers.11,0,0.13948,"tes the M N alignment variables betwen e˙ 1 and y¨0 . In our experiments, we only use IBM Model 1 as our alignment model. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 67 3 Experiments 3.1 Datasets Our experiments use two standard benchmarks from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003), respectively for aligning English with French and Romanian. We consider two different settings: for French, we use a large training corpus of parallel sentences from the Europarl corpus Koehn (2005). In the case of Romanian, we use the SETIMES corpus used in WMT’16 evaluation,3 which correspond to a more challenging scenario where the training data is limited in size. Additional experiments with monolingual data use the Romanian data from News Crawl 2019 (∼ 6M sentences)4 . Basic statistics for these corpora are in Table 1. Corpus # sent. in train # sent. in test En-Fr En-Ro ∼1.9M ∼260K 447 246 # tokens in test Eng. For. 7 020 7 761 5 455 5 315 # non-null links 17 438 5 988 Table 1: Basic statistics for the data These corpora are preprocessed, lowercased and tokenized with standard tools"
2020.amta-research.6,J10-4005,0,0.0333747,"Missing"
2020.amta-research.6,2005.iwslt-1.8,0,0.100605,"Missing"
2020.amta-research.6,W17-3204,0,0.0157183,"et al., 2018) also considers neural versions of IBM models, with the goal to improve word representations through cross-lingual transfer in low-resource contexts. Alignment is also the main focus of (Ngo-Ho and Yvon, 2019) which reviews a whole set of alternative parameterizations for neural IBM-1 and HMM models, varying the word embeddings (word and character based), the context-size in the translation model and the parameterization of the distortion model. A much more active line of research tries to improve neural MT by exploiting the conceptual similarity between alignments and attention (Koehn and Knowles, 2017). Cohn et al. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 73 (2016) modify the attention component to integrate some biases that are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between"
2020.amta-research.6,D18-2012,0,0.0226004,"l data use the Romanian data from News Crawl 2019 (∼ 6M sentences)4 . Basic statistics for these corpora are in Table 1. Corpus # sent. in train # sent. in test En-Fr En-Ro ∼1.9M ∼260K 447 246 # tokens in test Eng. For. 7 020 7 761 5 455 5 315 # non-null links 17 438 5 988 Table 1: Basic statistics for the data These corpora are preprocessed, lowercased and tokenized with standard tools from the Moses toolkit.5 Following notably (Garg et al., 2019), we perform the alignment between subword units generated by Byte-Pair-Encoding (Sennrich et al., 2015), implemented with the SentencePiece model (Kudo and Richardson, 2018) and computed independently6 in each language with 32K merge operations. This makes the training less computationally demanding and greatly mitigates the rare-word problem, which is a major weakness of historical count-based model. Our results and analyses are however based on word-level alignments. Subword-level alignments are converted into word-level alignments as follows: a link between a source and a target word exists if there is at least one link alignment between their subwords. 3.2 Implementation Our models are close in structure to the model proposed by Rios et al. (2018), and are ma"
2020.amta-research.6,W16-2207,0,0.0126515,"compared to the parameter sharing approach. Model +VAE+BPE+SP IBM-1 +Mono +Noise HMM +Mono +Noise English-Romanian R-ACC AER 84.6 49.3 98.1 49.1 98.4 48.8 95.5 37.5 98.5 37.9 98.8 36.3 Romanian-English R-ACC AER 93.0 51.4 98.1 47.8 97.9 47.6 97.5 38.0 98.1 38.0 97.5 36.5 Table 7: Training with a monolingual corpus (+Mono) and the noise model (+Noise) on English-Romanian data. R-Acc is the accuracy of the reconstruction model. 5 Related work The majority of recent approaches to neural word alignment fall into two categories: heuristic and probabilistic. A representative heuristic approach is (Legrand et al., 2016), which learns association scores between source and target word embeddings without any underlying probabilistic model. This simple approach is used to clean up translation memories in (Pham et al., 2018). More recently (Sabet et al., 2020) directly takes pre-trained non-contextual and contextual multilingual representations (Devlin et al., 2019) as their association scores, deriving individual word alignments by solving an optimal matching problem. Early work on probabilistic neural alignment is (Yang et al., 2013), where a feed-forward neural network is used to replace the count-based transl"
2020.amta-research.6,N06-1014,0,0.554044,"ve variants of generative word alignment models. Our main source of inspiration is the model of Rios et al. (2018), who consider variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) to approach the unsupervised estimation of neural alignment models. We revisit here this model, trying to analyze the reasons for its unsatisfactory performance and we extend it in several ways, taking advantage of its fully generative nature. We first generalize the approach, initially devised for IBM model 1, to the HMM model; we then explore ways to effectively enforce symmetry constraints (Liang et al., 2006); we finally study how these models could benefit from monolingual data. Our experiments with the English-Romanian and English-French language pairs show that our best model with symmetry constraints is on par with a conventional neural HMM model; they also highlight the remaining deficiencies of these approaches and suggest directions for further developments. 2 Neural word alignment variational models The standard approach to probabilistic alignment (Och and Ney, 2003) is to consider asymmetric models associating each word in a source sentence f1J = f1 . . . fJ of J words with exactly one wo"
2020.amta-research.6,D15-1210,0,0.0132979,"Garg et al. (2019) propose to jointly learn alignment and translation in a multi-task setting, thereby improving a Transformer-based model. When compared to heuristic approaches, an obvious defect of IBM models is their directionality, which means that they deliver asymmetric alignments. Attempts to remedy this problem, while preserving the sound probabilistic underlying models have been many. Liang et al. (2006) propose to jointly train EM in both directions, enforcing directional link posteriors to agree as much as possible through an additional agreement term; this work is generalized in (Liu et al., 2015). Grac¸a et al. (2010) use a different technique and enforce symmetry via additional constraints on the posterior link distribution. Since their introduction in (Bowman et al., 2016), VAE models of text generation have been developed in multiple ways, and applied to many NLP tasks, in particular to Machine Translation (Zhang et al., 2016). This approach generalizes the basic VAE approach by making the latent variable and the target sentence conditionally dependent from the observed source. One major difference with our work in that the model includes one latent variable per sentence, where we"
2020.amta-research.6,D15-1166,0,0.0437145,"loiting the conceptual similarity between alignments and attention (Koehn and Knowles, 2017). Cohn et al. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 73 (2016) modify the attention component to integrate some biases that are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between successive attention vectors. The work of Peters et al. (2019) also aims to enhance the attention component of a sequence-to-sequence, by enforcing sparsity via the sparse-max operator. The work reported in (Alkhouli et al., 2016; Wang et al., 2017) explores ways to explicitely introduce alignments in NMT. They study various neuralizations of the standard generative alignment models, and also consider ways to exploit weak supervision from count-based models. This line of research is pursued by (Kim et al., 2017; Deng et"
2020.amta-research.6,W05-0809,0,0.155486,"Missing"
2020.amta-research.6,W18-0311,0,0.025575,"Missing"
2020.amta-research.6,W03-0301,0,0.0640847,"o (θ, φ) = −Eqφ (¨y0N ) ([log pθ (¨ aM y0 , a ¨M y0N |¨ eN y0N )] (8) 1 )pθ (e˙ 1 |¨ 1 )) + KL[qφ (¨ 1 )||p(¨ a ¨M 1 M where e¨N ¨1N is the latent variable for e¨N ¨M 1 is a noisy version of e˙ 1 , y 1 , and a 1 denotes the M N alignment variables betwen e˙ 1 and y¨0 . In our experiments, we only use IBM Model 1 as our alignment model. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 67 3 Experiments 3.1 Datasets Our experiments use two standard benchmarks from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003), respectively for aligning English with French and Romanian. We consider two different settings: for French, we use a large training corpus of parallel sentences from the Europarl corpus Koehn (2005). In the case of Romanian, we use the SETIMES corpus used in WMT’16 evaluation,3 which correspond to a more challenging scenario where the training data is limited in size. Additional experiments with monolingual data use the Romanian data from News Crawl 2019 (∼ 6M sentences)4 . Basic statistics for these corpora are in Table 1. Corpus # sent. in train # sent. in test En-Fr En-Ro ∼1.9M ∼260K 447"
2020.amta-research.6,P03-1021,0,0.0781816,"all training sentences of length lower than 50. All parameters of the Giza++ and Fastalign baselines are set to their default values. IBM-1+NN and HMM+NN correspond to basic neuralizations of the IBM models as in (Rios et al., 2018; Ngo-Ho and Yvon, 2019) for both word-level and BPE-level. These models are trained by maximizing the likelihood with the expectation-maximization algorithm. We train all models for 10 iterations. Results with symmetric alignments use the grow-diag-final (GDF) heuristic proposed in (Koehn et al., 2005). 3.3 Evaluation protocol We use the alignment error rate (AER) (Och, 2003), accuracy, F-score, precision and recall as measures of performance. AER is based on a comparison of predicted alignment links (A) with a human reference including sure (S) and possible (P) links, and is defined as an average of the recall and precision taking into account the sets P and S. AER is defined as: AER = 1 − |A ∩ S |+ |A ∩ P | |A |+ |S| where A is the set of predicted alignments. Note that the Romanian-English reference data only contains sure links; in this case AER and F-measure are deterministically related. 3.4 Results The top part of Table 2 reports the AER score of the IBM-1"
2020.amta-research.6,J03-1002,0,0.211414,"roduction Word alignment is one of the basic tasks in multilingual Natural Language Processing (NLP) and is used to learn bilingual dictionaries, to train statistical machine translation (SMT) systems (Koehn, 2010), to filter out noise from translation memories (Pham et al., 2018) or in quality estimation applications (Specia et al., 2017). Word alignments can also be viewed as a form of possible explanation of the often opaque behavior of a Neural Machine Translation (Stahlberg et al., 2018). Word alignment aims to identify translational equivalences at the level of individual lexical units (Och and Ney, 2003; Tiedemann, 2011) in parallel sentences. Successful alignment models either rely on bilingual association measures parameterizing a combinatorial problem (eg. an optimal matching in a bipartite graph); or on probabilistic models, as represented by the IBM Models of Brown et al. (1993) and the HMM model of Vogel et al. (1996). All these models use unsupervised learning to estimate the likelihood of alignment links at the word level from large collections of parallel sentences. Such approaches are typically challenged by low-frequency words, whose co-occurrences are poorly estimated; they also"
2020.amta-research.6,P19-1146,0,0.0124856,"rence of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 73 (2016) modify the attention component to integrate some biases that are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between successive attention vectors. The work of Peters et al. (2019) also aims to enhance the attention component of a sequence-to-sequence, by enforcing sparsity via the sparse-max operator. The work reported in (Alkhouli et al., 2016; Wang et al., 2017) explores ways to explicitely introduce alignments in NMT. They study various neuralizations of the standard generative alignment models, and also consider ways to exploit weak supervision from count-based models. This line of research is pursued by (Kim et al., 2017; Deng et al., 2018), where attention vectors are handled as structured latent variables in NMT; in this study, variational autoencoders are used"
2020.amta-research.6,N18-1092,0,0.0693499,"2011), and notably for machine translation (Cho et al., 2014; Bahdanau et al., 2015), neural-based approaches offer new opportunities to Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 64 reconsider some of these issues. Following up on the work of eg. (Yang et al., 2013; Alkhouli et al., 2016; Wang et al., 2018), we study ways to take advantage of the flexibility of neural networks to design effective variants of generative word alignment models. Our main source of inspiration is the model of Rios et al. (2018), who consider variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) to approach the unsupervised estimation of neural alignment models. We revisit here this model, trying to analyze the reasons for its unsatisfactory performance and we extend it in several ways, taking advantage of its fully generative nature. We first generalize the approach, initially devised for IBM model 1, to the HMM model; we then explore ways to effectively enforce symmetry constraints (Liang et al., 2006); we finally study how these models could benefit from monolingual data. Our experiments with t"
2020.amta-research.6,2020.findings-emnlp.147,1,0.81907,"97.5 38.0 98.1 38.0 97.5 36.5 Table 7: Training with a monolingual corpus (+Mono) and the noise model (+Noise) on English-Romanian data. R-Acc is the accuracy of the reconstruction model. 5 Related work The majority of recent approaches to neural word alignment fall into two categories: heuristic and probabilistic. A representative heuristic approach is (Legrand et al., 2016), which learns association scores between source and target word embeddings without any underlying probabilistic model. This simple approach is used to clean up translation memories in (Pham et al., 2018). More recently (Sabet et al., 2020) directly takes pre-trained non-contextual and contextual multilingual representations (Devlin et al., 2019) as their association scores, deriving individual word alignments by solving an optimal matching problem. Early work on probabilistic neural alignment is (Yang et al., 2013), where a feed-forward neural network is used to replace the count-based translation model of a HMM-based aligner. This approach is further developed in (Tamura et al., 2014) where a recurrent network helps to capture contextual dependencies between alignment links. This early work aims to improve the alignment qualit"
2020.amta-research.6,W18-5420,0,0.0457183,"Missing"
2020.amta-research.6,P14-1138,0,0.0193797,"without any underlying probabilistic model. This simple approach is used to clean up translation memories in (Pham et al., 2018). More recently (Sabet et al., 2020) directly takes pre-trained non-contextual and contextual multilingual representations (Devlin et al., 2019) as their association scores, deriving individual word alignments by solving an optimal matching problem. Early work on probabilistic neural alignment is (Yang et al., 2013), where a feed-forward neural network is used to replace the count-based translation model of a HMM-based aligner. This approach is further developed in (Tamura et al., 2014) where a recurrent network helps to capture contextual dependencies between alignment links. This early work aims to improve the alignment quality for phrase-based MT. As discussed above, the work of (Rios et al., 2018) also considers neural versions of IBM models, with the goal to improve word representations through cross-lingual transfer in low-resource contexts. Alignment is also the main focus of (Ngo-Ho and Yvon, 2019) which reviews a whole set of alternative parameterizations for neural IBM-1 and HMM models, varying the word embeddings (word and character based), the context-size in the"
2020.amta-research.6,C96-2141,0,0.851526,"Missing"
2020.amta-research.6,P17-2020,0,0.0179797,"are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between successive attention vectors. The work of Peters et al. (2019) also aims to enhance the attention component of a sequence-to-sequence, by enforcing sparsity via the sparse-max operator. The work reported in (Alkhouli et al., 2016; Wang et al., 2017) explores ways to explicitely introduce alignments in NMT. They study various neuralizations of the standard generative alignment models, and also consider ways to exploit weak supervision from count-based models. This line of research is pursued by (Kim et al., 2017; Deng et al., 2018), where attention vectors are handled as structured latent variables in NMT; in this study, variational autoencoders are used represent the alignment structure. Finally, Garg et al. (2019) propose to jointly learn alignment and translation in a multi-task setting, thereby improving a Transformer-based model. Whe"
2020.amta-research.6,P18-2060,0,0.0389414,"Missing"
2020.amta-research.6,P10-2005,0,0.0189269,"Missing"
2020.amta-research.6,P13-1017,0,0.0279283,"ories: heuristic and probabilistic. A representative heuristic approach is (Legrand et al., 2016), which learns association scores between source and target word embeddings without any underlying probabilistic model. This simple approach is used to clean up translation memories in (Pham et al., 2018). More recently (Sabet et al., 2020) directly takes pre-trained non-contextual and contextual multilingual representations (Devlin et al., 2019) as their association scores, deriving individual word alignments by solving an optimal matching problem. Early work on probabilistic neural alignment is (Yang et al., 2013), where a feed-forward neural network is used to replace the count-based translation model of a HMM-based aligner. This approach is further developed in (Tamura et al., 2014) where a recurrent network helps to capture contextual dependencies between alignment links. This early work aims to improve the alignment quality for phrase-based MT. As discussed above, the work of (Rios et al., 2018) also considers neural versions of IBM models, with the goal to improve word representations through cross-lingual transfer in low-resource contexts. Alignment is also the main focus of (Ngo-Ho and Yvon, 201"
2020.amta-research.6,E17-2061,0,0.0252046,"al similarity between alignments and attention (Koehn and Knowles, 2017). Cohn et al. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 73 (2016) modify the attention component to integrate some biases that are useful in alignements: a preference for monotonic alignements, for reduced fertility values, etc. They also propose, following (Liang et al., 2006), to enforce symmetrization constraints, an idea also explored in (Cheng et al., 2016); The same methodology is studied in (Luong et al., 2015; Yang et al., 2017), with the objective to introduce dependencies between successive attention vectors. The work of Peters et al. (2019) also aims to enhance the attention component of a sequence-to-sequence, by enforcing sparsity via the sparse-max operator. The work reported in (Alkhouli et al., 2016; Wang et al., 2017) explores ways to explicitely introduce alignments in NMT. They study various neuralizations of the standard generative alignment models, and also consider ways to exploit weak supervision from count-based models. This line of research is pursued by (Kim et al., 2017; Deng et al., 2018), where a"
2020.amta-research.6,D16-1050,0,0.0152819,"g the sound probabilistic underlying models have been many. Liang et al. (2006) propose to jointly train EM in both directions, enforcing directional link posteriors to agree as much as possible through an additional agreement term; this work is generalized in (Liu et al., 2015). Grac¸a et al. (2010) use a different technique and enforce symmetry via additional constraints on the posterior link distribution. Since their introduction in (Bowman et al., 2016), VAE models of text generation have been developed in multiple ways, and applied to many NLP tasks, in particular to Machine Translation (Zhang et al., 2016). This approach generalizes the basic VAE approach by making the latent variable and the target sentence conditionally dependent from the observed source. One major difference with our work in that the model includes one latent variable per sentence, where we consider one for each target word. 6 Conclusion and outlook In this paper, we have revisited the proposal of Rios et al. (2018) and explored variants of the variational autoencoder models for the unsupervised estimation of neural word alignment models. Our study has confirmed the previous findings and highlighted two promising aspects of"
2020.findings-emnlp.147,W18-6318,0,0.082199,"stical aligner, trained on 100k parallel sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer"
2020.findings-emnlp.147,W16-2206,0,0.0299395,"sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013), as well as newer models"
2020.findings-emnlp.147,N13-1073,0,0.807961,"2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013), as well as newer models such as eflo¨ mal (Ostling and Tiedemann, 2016) are widely used for alignment. With the rise of NMT (Bahdanau et al., 2014), attempts have been made to interpret attention matrices as soft word alignments (Cohn et al., 2016; Koehn and Knowles, 2017; Ghader and Monz, 2017). Several methods create alignments from attention matrices (Peter et al., 2017; Zenkel et al., 2019) or pursue a multitask approach for alignment and translation (Garg et al., 2019). However, most systems require parallel data (in sufficient amount to train high quality NMT systems) and their perform"
2020.findings-emnlp.147,C16-1302,1,0.748781,"13), Giza++ (Och and Ney, ¨ 2003) and eflomal (Ostling and Tiedemann, 2016). ¨ (Ostling, 2015a) showed that Bayesian Alignment Models perform well. Neural network based extensions of these models have been considered (Ayan et al., 2005; Ho and Yvon, 2019). All of these models are trained on parallel text. Our method instead aligns based on embeddings that are induced from monolingual data only. We compare with prior methods and observe comparable performance. Prior work on using learned representations for alignment includes (Smadja et al., 1996; Och and Ney, 2003) (Dice coefficient), (Jalili Sabet et al., 2016) (incorporation of embeddings into IBM models), (Legrand et al., 2016) (neural network alignment model) and (Pourdamghani et al., 2018) (embeddings are used to encourage words to align to similar words). Tamura et al. (2014) use recurrent neural networks to learn alignments. They use noise contrastive estimation to avoid supervision. Yang et al. (2013) train a neural network that uses pretrained word embeddings in the initial layer. All of this work requires parallel data. mBERT is used for word alignments in concurrent work: Libovick´y et al. (2019) use the high quality of mBERT alignments as"
2020.findings-emnlp.147,W19-6721,0,0.0640835,"Missing"
2020.findings-emnlp.147,2005.mtsummit-papers.11,0,0.432713,"Missing"
2020.findings-emnlp.147,P00-1056,0,0.572969,"Missing"
2020.findings-emnlp.147,P19-1124,0,0.0189498,"at standard attention does not have access to the target word. To address this, Peter et al. (2017) tailor attention matrices to obtain higher quality alignments. Li et al. (2018)’s and Zenkel et al. (2019)’s models perform similarly to and Zenkel et al. (2020) outperform Giza++. Ding et al. (2019) propose better decoding algorithms to deduce word alignments from NMT predictions. Chen et al. (2016), Mi et al. (2016) and Garg et al. (2019) obtain alignments and translations in a multitask setup. Garg et al. (2019) find that operating at the subword level can be beneficial for alignment models. Li et al. (2019) propose two methods to extract alignments from NMT 1634 models, however they do not outperform fast-align. Stengel-Eskin et al. (2019) compute similarity matrices of encoder-decoder representations that are leveraged for word alignments, together with supervised learning, which requires manually annotated alignment. We find our proposed methods to be competitive with these approaches. In contrast to our work, they all require parallel data. 6 Conclusion We presented word aligners based on contextualized embeddings that outperform in four and match the performance of state-of-the-art aligners"
2020.findings-emnlp.147,N18-1125,0,0.0232238,"vick´y et al. (2019) use the high quality of mBERT alignments as evidence for the “language-neutrality” of mBERT. Nagata et al. (2020) phrase word alignment as crosslingual span prediction and finetune mBERT using gold alignments. Attention in NMT (Bahdanau et al., 2014) is related to a notion of soft alignment, but often deviates from conventional word alignments (Ghader and Monz, 2017; Koehn and Knowles, 2017). One difference is that standard attention does not have access to the target word. To address this, Peter et al. (2017) tailor attention matrices to obtain higher quality alignments. Li et al. (2018)’s and Zenkel et al. (2019)’s models perform similarly to and Zenkel et al. (2020) outperform Giza++. Ding et al. (2019) propose better decoding algorithms to deduce word alignments from NMT predictions. Chen et al. (2016), Mi et al. (2016) and Garg et al. (2019) obtain alignments and translations in a multitask setup. Garg et al. (2019) find that operating at the subword level can be beneficial for alignment models. Li et al. (2019) propose two methods to extract alignments from NMT 1634 models, however they do not outperform fast-align. Stengel-Eskin et al. (2019) compute similarity matrices"
2020.findings-emnlp.147,P19-1493,0,0.0788748,"Missing"
2020.findings-emnlp.147,C16-1291,0,0.0318143,"5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and"
2020.findings-emnlp.147,2000.bcs-1.11,0,0.662232,"Missing"
2020.findings-emnlp.147,2020.acl-demos.14,0,0.0236288,"erheblich zu dieser Entwicklung beigetragen . The Commission , for its part , will continue to play an active part in the intergovernmental conference . Die Kommission wird bei der Regierungskonferenz auch weiterhin eine aktive Rolle spielen . Figure 7: Example alignment of auxiliary verbs. Same setting as in Table 6. Solid lines: mBERT’s alignment, identical to the gold standard. Dashed lines: eflomal’s incorrect alignment. 4.5 Part-Of-Speech Analysis To analyze the performance with respect to different part-of-speech (POS) tags, the ENG-DEU gold standard was tagged with the Stanza toolkit (Qi et al., 2020). We evaluate the alignment performance for each POS tag by only considering the alignment edges where at least one of their member words has this tag. Table 6 shows results for frequent POS tags. Compared to eflomal, mBERT aligns auxiliaries, pronouns and verbs better. The relative position of auxiliaries and verbs in German can diverge strongly from that in English because they occur at the end of the sentence (verb-end position) in many clause types. Positions of pronouns can also diverge due to a more flexible word order in German. It is difficult for an HMM-based aligner like eflomal to m"
2020.findings-emnlp.147,J96-1001,0,0.726218,"gners, often based on IBM models, include fastalign (Dyer et al., 2013), Giza++ (Och and Ney, ¨ 2003) and eflomal (Ostling and Tiedemann, 2016). ¨ (Ostling, 2015a) showed that Bayesian Alignment Models perform well. Neural network based extensions of these models have been considered (Ayan et al., 2005; Ho and Yvon, 2019). All of these models are trained on parallel text. Our method instead aligns based on embeddings that are induced from monolingual data only. We compare with prior methods and observe comparable performance. Prior work on using learned representations for alignment includes (Smadja et al., 1996; Och and Ney, 2003) (Dice coefficient), (Jalili Sabet et al., 2016) (incorporation of embeddings into IBM models), (Legrand et al., 2016) (neural network alignment model) and (Pourdamghani et al., 2018) (embeddings are used to encourage words to align to similar words). Tamura et al. (2014) use recurrent neural networks to learn alignments. They use noise contrastive estimation to avoid supervision. Yang et al. (2013) train a neural network that uses pretrained word embeddings in the initial layer. All of this work requires parallel data. mBERT is used for word alignments in concurrent work:"
2020.findings-emnlp.147,J93-2003,0,\N,Missing
2020.findings-emnlp.147,H01-1035,0,\N,Missing
2020.findings-emnlp.147,P11-1042,0,\N,Missing
2020.findings-emnlp.147,P02-1050,0,\N,Missing
2020.findings-emnlp.147,W03-0301,0,\N,Missing
2020.findings-emnlp.147,P15-2034,0,\N,Missing
2020.findings-emnlp.147,J03-1002,0,\N,Missing
2020.findings-emnlp.147,2016.amta-researchers.10,0,\N,Missing
2020.findings-emnlp.147,Q17-1010,0,\N,Missing
2020.findings-emnlp.147,P16-2028,0,\N,Missing
2020.findings-emnlp.147,bojar-prokopova-2006-czech,0,\N,Missing
2020.findings-emnlp.147,P18-1141,1,\N,Missing
2020.findings-emnlp.147,N18-2083,0,\N,Missing
2020.findings-emnlp.147,P19-1452,0,\N,Missing
2020.findings-emnlp.147,P16-1162,0,\N,Missing
2020.findings-emnlp.147,N19-1423,0,\N,Missing
2020.findings-emnlp.147,D19-1453,0,\N,Missing
2020.findings-emnlp.147,D19-1084,0,\N,Missing
2020.findings-emnlp.147,D19-1448,0,\N,Missing
2020.findings-emnlp.147,I08-2093,0,\N,Missing
2020.findings-emnlp.147,H05-1009,0,\N,Missing
2020.findings-emnlp.147,W08-0303,0,\N,Missing
2020.findings-emnlp.147,P14-1138,0,\N,Missing
2020.findings-emnlp.147,W17-4711,0,\N,Missing
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.wmt-1.63,N19-1423,0,0.00803713,"f concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018) retrieve similar sentence to dynamically"
2020.wmt-1.63,P19-1294,0,0.0266447,"ed: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tez"
2020.wmt-1.63,N19-1191,0,0.0170632,"based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from t"
2020.wmt-1.63,N18-1118,0,0.0172585,"xplored in Schwenk et al. (2019b), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. Previously, Niehues et al. (2016) augmented input sentences with pre-translations generated by a phrase-based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common f"
2020.wmt-1.63,P19-1175,0,0.129039,"improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) b"
2020.wmt-1.63,W17-4713,0,0.366859,"NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and number of epochs are dynamically adapted considering sentence similarity. Adaptation is run on a sentenceby-sentence ba"
2020.wmt-1.63,P17-4012,1,0.0302657,"he French-side of the WikiMatrix data (Schwenk et al., 2019a). We randomly split the parallel corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the OpenNMT tokenizer6 (conservative mode). Sentence Retrieval To identify similar translations using distributed representations, we use the faiss8 search toolkit (Johnson et al., 2019) through its Python API with exact FlatIP index. Translation Our NMT models rely on the Transformer base architecture of Vaswani et al. (2017), implemented in the OpenNMT-tf9 toolkit (Klein et al., 2017). We use the standard setting of Transformers for all experiments: size of word embedding: 512; size of hidden layers: 512; size of inner feed-forward layer: 2, 048; number of heads: 8; number of layers in the encoder or in the decoder: 6. In the tgt1 +STU scheme, token (508 cells) and STU (4 4 Freely available from http://opus.nlpl.eu http://data.statmt.org/news-crawl/ 6 https://github.com/OpenNMT/Tokenizer 5 519 7 Optimization experiments on a held-out development set are carried out for both models. 8 https://github.com/facebookresearch/ faiss 9 https://github.com/OpenNMT/OpenNMT-tf cells)"
2020.wmt-1.63,2016.amta-researchers.9,0,0.008848,"-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear"
2020.wmt-1.63,kobus-etal-2017-domain,1,0.851148,"s been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous"
2020.wmt-1.63,2010.jec-1.4,1,0.473968,"arding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu et al., 2020) and keep the n-best matches when FM(s1 , s2 ) ≥ 0.5 with no approximation. Concerning S2V, the model is trained with default options during 20 epochs using all training data. We use an embedding dimension of 300 cells. Regarding CBON, we learn models using also the entire training data during one epoch (∼50,000 iterations). Similarly to S2V we use 10 negative samples per positive word to approximate the softmax, a batch size of 2k examples, and embedding size of 300 cells. We build CBON models using 3-grams and 4-grams to enable a comparison with sent"
2020.wmt-1.63,2020.lrec-1.302,0,0.0229122,"Missing"
2020.wmt-1.63,L18-1146,0,0.064128,"Missing"
2020.wmt-1.63,C16-1172,0,0.0170125,"Missing"
2020.wmt-1.63,N18-1049,0,0.0112408,"helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar"
2020.wmt-1.63,P02-1040,0,0.121238,"000 and update the learning rate for every 8 iterations. Models are optimised during 300K iterations, using a single NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and nu"
2020.wmt-1.63,D14-1162,0,0.118332,"many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar translations from a translation memory (TM) to boost translation accuracy. In this work, TMs are parallel corpora containing translations falling in the same domain as test sentences. We first describe the methods employed in this work to compute sentence similarity."
2020.wmt-1.63,N18-1202,0,0.0171998,"monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018"
2020.wmt-1.63,D19-3018,0,0.0124582,"by analyzing how on-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficie"
2020.wmt-1.63,2021.eacl-main.115,0,0.0345079,"Missing"
2020.wmt-1.63,W17-2619,0,0.0145288,"language (French in this work) and translate each sentence back into English to obtain synthetic parallel data. Similar to back-translation experiments in Sennrich et al. (2016b), we only use original (human-crafted) target-language data. We expect this to add less noise than incorporating synthetic target-language data into the NMT input. Once translated into English, the various priming approaches identify similar synthetic sentences and injects both the synthetic source and original target in the NMT input stream. Note that crosslingual sentence embedding models exist (Sabet et al., 2019; Schwenk and Douze, 2017; Conneau and Lample, 2019) but our preliminary experiments using these tools did not show satisfactory results. Thus, we exploit large collections of French texts for the News and Wikipedia domains (as detailed in Table 1) that we translate into English to enable similarity retrieval. Table 4 reports BLEU scores obtained by our best performing network CBON following the s+t5 scheme. The supplementary number of similar sentences (468 input sentences have similar translations) collected for the WIKI domain over parallel and mono521 lingual12 corpora (par+mon) yields an improvement of 2 BLEU poi"
2020.wmt-1.63,N16-1005,0,0.152513,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1009,0,0.160789,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1162,0,0.215406,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,tiedemann-2012-parallel,0,0.0135836,"is section gives learning/inference details of the various systems used in this work. Corpora We experiment with the English-French language pair and data originating from eight domains, corresponding to texts from three European institutions: the European Parliament (EPPS), the European Medicines Agency (EMEA) and the European Central Bank (ECB); Legislative texts of the European Union (JRC); IT-domain corpora corresponding to KDE4 and GNOME; News Commentaries (NEWS); and parallel sentences extracted from Wikipedia (WIKI). Table 1 contains statistics regarding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu e"
2020.wmt-1.63,W17-4811,0,0.174236,"sed in this paper also addresses the unrelated word problem, at a much reduced computational cost. It considers both sides of similar translations (sk and tk ). Training streams take the form: src: tgt: sk ◦ ... ◦ s2 ◦ s1 ◦ s tk ◦ ... ◦ t2 ◦ t1 ◦ t In inference, target-side similar translations tk are used by the model as a target prefix. The initial steps of the beam search use the given prefix tk ◦ ... ◦ t2 ◦ t1 ◦ in forced decoding mode, returning to a regular beam search after the last ◦ token is generated. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) in the context of handling discourse phenomena. However, since we use true translation as prefixes, our strategy does not suffer from exposure bias (Ranzato et al., 2016) and the subsequent error propagation problem. Continuing on our running example, during inference the model receives: input: prefix: measles vaccin ◦ pertussis vaccin vaccin contre la rougeole ◦ the encoder embeds the input stream, and forcedecodes the target prefix, before starting the translation generation. Note that during beam search, the decoder has thus access both to all input tokens (sk and s) as well as to similar"
2020.wmt-1.63,2020.acl-main.144,1,0.145304,"n performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) by proposing a nove"
2020.wmt-1.72,2020.acl-main.688,0,0.0315866,"c domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and res"
2020.wmt-1.72,2020.acl-main.421,0,0.0420588,"g NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly bec"
2020.wmt-1.72,2010.amta-papers.16,0,0.0611561,"Missing"
2020.wmt-1.72,D19-1165,0,0.370734,"baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the size of the baseline mo"
2020.wmt-1.72,W17-4712,0,0.0824316,"e them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. DM is a multi-task learner where a domain classifier is trained on top of the MT encoder, so as to make it aware of domain differences; ADM is the adversarial version of DM, pushing the encoder towards learning domain-independent source representations. These methods only use domain labels in training. Model / Domain Mixed FT-Full DC-Tag DC-Feat LDR TTM DM ADM Res-Adap Res-A"
2020.wmt-1.72,W17-4713,0,0.0169783,"cation or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation se"
2020.wmt-1.72,N09-1068,0,0.0896083,"Missing"
2020.wmt-1.72,2012.eamt-1.60,0,0.0125413,"tal settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Ma"
2020.wmt-1.72,2016.amta-researchers.10,0,0.019201,"lark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6) Res-Adap(6) Res-Adap(4) Res-Adap(2) Res-Adap-WD Res-Adap-LR MED LAW BANK TALK IT REL AVG PARAMS 37.3 37.3 37.7 37.7 37.9 37.8 37.2 37.4 54.6 57.9 57 55.8 55.6 55.5 56.0 56.1 50.1 53.9 53 51.5 51.7 51.4 52.9 51.8 33.5 33.8 33.3 33.9 33.7 34 33.4 33.3 43.2 46.7 45 43.6 44.4 43.8 46.0 45.0 77.5 90.2 90 89.2 88.7 86.7 90.6 89.7 49.4 53.3 52.7 51.9 52 51.5 52.7 52.2 65M/0 65M/12M 65M/6M 65M/2M 65M/2M 65M/2M 65M/12M 65M/12M Table 4: Translation performance of various fine-"
2020.wmt-1.72,C18-1111,0,0.0172823,"and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model con"
2020.wmt-1.72,1983.tc-1.13,0,0.339315,"Missing"
2020.wmt-1.72,D08-1072,0,0.169703,"er regularization, which penalizes the output of the adapters, corresponding to the following objective: X 1 (− log(P (y|x)) #(x, y) x,y X +λ kADAP(i) (hi (x, y))k2 ) ¯= L i∈{1,..,6}⊗{enc,dec} Finally, another independent design choice relates to the training strategy for adapters. A first option is to generalize supervised domain adaptation to multi-domain adaptation and to proceed in two steps: (a) train a generic model with all the available data; (b) train each adapter layer with domain-specific data, keeping the generic model parameters unchanged. Another strategy is to adopt the view of Dredze and Crammer (2008), where the multi-domain setting is viewed as an instance of multi-task learning (Caruana, 1997) with each domain corresponding to a specific task. This suggests training all the parameters from scratch, as we would do in a multi-task mode. The generic parameters will still depend on all the available data, while each adapter will only be trained with the corresponding in-domain data. Figure 1: Highway residual adapter network 2.3 Gated Residual Adapters The basic architecture presented above rests on a rather simplistic view of “domains” as made of well-separated and unrelated pieces of texts"
2020.wmt-1.72,P13-2119,0,0.0200228,"apter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parall"
2020.wmt-1.72,P12-2023,0,0.0258425,"Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed R"
2020.wmt-1.72,E14-1035,0,0.0224033,"and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6"
2020.wmt-1.72,2015.mtsummit-papers.19,0,0.0200802,"common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, us"
2020.wmt-1.72,D13-1176,0,0.0376761,"odel intact and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea using two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (201"
2020.wmt-1.72,P17-4012,1,0.831901,"Missing"
2020.wmt-1.72,kobus-etal-2017-domain,1,0.929226,"to its size; we then sample a batch of 12,288 tokens that is used to update the shared parameters and the parameters of the corresponding adapter. Models for En→De are larger and rely on embeddings as well as hidden layers of size 1024; each 621 3.3 Multi-domain systems In this section, we evaluate several proposals from the literature on multi-domain adaptation and compare them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. D"
2020.wmt-1.72,W17-3204,0,0.0198096,"lso make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of"
2020.wmt-1.72,L18-1146,0,0.0348477,"zation on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation setting. 5 tional co"
2020.wmt-1.72,2015.iwslt-evaluation.11,0,0.0287615,"4; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a rand"
2020.wmt-1.72,2020.ngt-1.5,0,0.0261148,"Missing"
2020.wmt-1.72,steinberger-etal-2006-jrc,0,0.0761424,"ility P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Co"
2020.wmt-1.72,P18-2050,0,0.0197571,"all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the"
2020.wmt-1.72,D18-1103,0,0.0223531,"Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially whe"
2020.wmt-1.72,P02-1040,0,0.109675,"luding: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Matrix ( W E B ). Statistics are in Table 2. We randomly select in each corpus a development and a test set of 1,000 lines each and keep the rest for training.8 Development sets help choose the best model according to the average BLEU score (Papineni et al., 2002).9 1. learn a generic model with mixed corpora from multiple domains. 3.2 2. train a domain classifier on top of the encoder and decoder; during this step, the parameters of the generic model are frozen. This model computes the posterior domain probability P (k|hL [t]) for each word wt , based on the representation computed by the last layer. 3. train the parameters of adapters with indomain data separately for each domain, while freezing all the other parameters. 2 The term “word” is employed here by mere convenience, as systems only manipulate sub-lexical BPE units; furthermore, the values o"
2020.wmt-1.72,tiedemann-2012-parallel,0,0.0211147,"the encoder (resp. decoder) as input and use the posterior probability P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European"
2020.wmt-1.72,W18-5431,0,0.0197505,"ng. Likewise, it might be meaningful to explore ways to share subsets of adapters across domains. This, in turn, raises the issue of which layer(s) to adapt, a question that can be approached in the light of recent analyses of Transformers models, which conjecture that the higher layers encode 618 1 In the decoder, the stack of self-attention and cross encoder-decoder attention only counts as one attention layer and only corresponds to one residual adapter. global patterns with a more “semantic” interpretation, while the lower layers encode local patterns akin to morpho-syntactic information (Raganato and Tiedemann, 2018). A related question concerns the regularization of adapter layers to mitigate overfitting. Reducing the number of adapters, or their dimensions, is simple, but such choices are difficult to optimize numerically – an issue that becomes important as the number of domain grows. Less naive alternatives can also be entertained, such as applying weight decay or layer regularization to the adapter. Implementing these requires to modify the objective function in a way that still allows for a smooth optimization problem. For instance, weight decay applies a penalization on the weights of the adapters,"
2020.wmt-1.72,N18-2080,0,0.0270309,"ents. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Ling"
2020.wmt-1.72,P13-1082,0,0.0169293,"erogeneous sources is a common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on"
2020.wmt-1.72,D18-1041,0,0.0177965,"the work by Jiang et al. (2019), who consider a Transformer model containing both domain-specific and domain-agnostic heads. Britz et al. (2017) study three general techniques to take domain information into account in training: they rely on either domain classification or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these tec"
2020.wmt-1.72,D16-1163,0,0.0230272,"en perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are av"
2020.wmt-1.86,E09-1003,0,0.0364631,"meno Yepes et al., 2017), Scielo (Neves et al., 2016) and the Ufal Medical corpus2 consisting of Cesta, Ecdc, Emea (OpenSubtitles), PatTR Medical and OpenSubtitles. In addition, we used the Cochrane bilingual parallel corpus (Ive et al., 2016)3 and the Taus Corona Crisis corpus.4 . We finally experimented with additional in-domain data selected using Information Retrieval (IR) techniques from general domain corpora including News-Commentary, Books and Wikipedia corpus obtained from Open Parallel Corpus (OPUS) (Tiedemann, 2012). These were selected using the data selection scheme described in (Abdul-Rauf and Schwenk, 2009). Medline titles were used as queries to find the related sentences. We used 3-best sentences returned from the IR pipeline as additional corpus to build the models (these are shown as X7 in table2). For development purposes, we used Khresmoi, Edp and Scielo test corpora. The Medline test sets of WMT’18 and 195 were used as internal test data. 7.70 0.33 M 16.2 0.06 M Table 1: Data sources for the English-French biomedical task (before tokenization) We gathered parallel and monolingual corpora 804 2 https://ufal.mff.cuni.cz/ufal_ medical_corpus 3 https://github.com/fyvo/ CochraneTranslations/ 4"
2020.wmt-1.86,W16-3905,0,0.0422037,"Missing"
2020.wmt-1.86,D19-1165,0,0.0755232,"al., 2017), or and on the related BERT-fused transformer model of Zhu et al. (2020). If our baselines were actually strong, we only managed to get relatively small gains from our auxiliary resources, for reasons that by and large remain to be analyzed in depth. Our biomedical systems are presented in Section 2. We also participated in the Robustness translation task, developing a multi-domain, noise-robust and amenable to fast adaptation translation system for the translation direction English-German. Our main focus was to study in more depth the adaptor architecture initially introduced in (Bapna and Firat, 2019) in a large-scale setting, where multiple heterogeneous corpora of unbalanced size are available for training, and explore ways to make the system robust to spelling noise in the test data. The zero-shot system is a generic system which 1 http://www.statmt.org/moses/ 803 Proceedings of the 5th Conference on Machine Translation (WMT), pages 803–812 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics does not use any adaptation layer; for our few-shot adaptation submission, we did not use the supplementary data provided by the organizers, which turned out to be only mi"
2020.wmt-1.86,W18-6315,1,0.845313,"dline 18 Medline 19 Medline 20 Monolingual Corpus 2 Lissa Med Fr 8.79 16.3 Monolingual sources Supplementary French data from two monolingual sources were collected from public archives: abstracts of medical papers published by Elsevier from the Lissa portal6 and a collection of research articles collected from various sources7 henceforth referred to as Med Fr (Maniez, 2009). The former corpus contains 41K abstract and totals approximately 7.7M running words; the latter contains 65K sentences, for a little more than 1.5M running words. These texts were back-translated (Sennrich et al., 2016a; Burlot and Yvon, 2018) into French using a relatively basic neural French-English engine trained with the official WMT data sources for the biomedical task, using the HuggingFace pipeline (see details below). This system had a BLEU score of 31.2 on Medline 18 test set. Note that back-translation has also been effecDevelopment Scielo Edp Khresmoi available for English-French in the biomedical domain. These first included the biomedical texts provided by the WMT’20 organizers: Edp, Medline abstracts and titles (Jimeno Yepes et al., 2017), Scielo (Neves et al., 2016) and the Ufal Medical corpus2 consisting of Cesta, E"
2020.wmt-1.86,W19-5418,0,0.0340225,"Missing"
2020.wmt-1.86,N19-1423,0,0.0543127,"Missing"
2020.wmt-1.86,P19-1294,0,0.0223939,"stems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. The target side (French) remained unchanged. Figure 1 displays a sentence extracted from Medline 18 before and after pretranslation (in the latter, French segments are underlined). Terms are extracted from the F"
2020.wmt-1.86,W19-5419,1,0.806799,"vergence, based on BLEU score on dev sets. These are then further fine-tuned using a selected part of the training corpus containing only the Medline abstracts and the three Cochrane corpora, again until convergence. The corresponding systems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translat"
2020.wmt-1.86,W18-6447,0,0.0187202,"ne-tuned using a selected part of the training corpus containing only the Medline abstracts and the three Cochrane corpora, again until convergence. The corresponding systems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. The target side (French) remained unchange"
2020.wmt-1.86,moore-2002-fast,0,0.138028,"Missing"
2020.wmt-1.86,L16-1470,0,0.0370122,"Missing"
2020.wmt-1.86,N19-4009,0,0.0339538,"Missing"
2020.wmt-1.86,W19-5420,0,0.0185669,"lected part of the training corpus containing only the Medline abstracts and the three Cochrane corpora, again until convergence. The corresponding systems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. The target side (French) remained unchanged. Figure 1 display"
2020.wmt-1.86,W18-6319,0,0.0121077,"and parameters as the baseline transformer models and to establish fair comparisons. In BERT-fused NMT model, the contextual representations are first computed by the BERT model for each token (in the source and target), these are then combined at each encoder and decoder layer using the attention mechanism. Full details are in Zhu et al. (2020). Given the size of our training data, the ”lazy” output dataset implementation was used to enable data loading in the RAM. Systems were trained until convergence based on the BLEU score on the development sets. Evaluation is performed using sacrebleu (Post, 2018). Scores are chosen based on the best score on the development set (Khres+Edp+Scielo) and the corresponding scores for that checkpoint are reported on Medline 18 and 806 4 14 https://fairseq.readthedocs.io/en/ latest/models.html 15 https://github.com/bert-nmt/bert-nmt Medline 19 test sets. For systems using terminology pre-translation, Khresmoi and Edp were used as development sets. 2.4 Results Results are in Table 2, where we report BLEU scores for the three tracks explored in this work. M ∗ denotes the Moses tokenization pipeline, H∗ represents the HuggingFace pipeline and B∗ denotes the BER"
2020.wmt-1.86,W19-6101,1,0.891565,"Missing"
2020.wmt-1.86,W19-5421,0,0.0160447,"training corpus containing only the Medline abstracts and the three Cochrane corpora, again until convergence. The corresponding systems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. The target side (French) remained unchanged. Figure 1 displays a sentence extracted f"
2020.wmt-1.86,P16-1009,0,0.259111,"t to Biomedical parallel corpora, allowing us to train our Neural Machine Translation (NMT) (Sutskever et al., 2014) with only in-domain corpora and dispense with the processing of large out-of-domain data that exist for this language pair. Our main focus for this year’s participation was to develop strong baselines by making the best of auxiliary resources: back translation of monolingual data; partial pre-translation of terms; pre-trained multilingual contextual embeddings and IR retrieved in domain corpora. Two pre-prossessing pipelines, one using the standard Moses tools1 and subword-nmt (Sennrich et al., 2016b) and other using HuggingFace BERT API were developed and compared. All systems are based on the transformer architecture (Vaswani et al., 2017), or and on the related BERT-fused transformer model of Zhu et al. (2020). If our baselines were actually strong, we only managed to get relatively small gains from our auxiliary resources, for reasons that by and large remain to be analyzed in depth. Our biomedical systems are presented in Section 2. We also participated in the Robustness translation task, developing a multi-domain, noise-robust and amenable to fast adaptation translation system for"
2020.wmt-1.86,P16-1162,0,0.287626,"t to Biomedical parallel corpora, allowing us to train our Neural Machine Translation (NMT) (Sutskever et al., 2014) with only in-domain corpora and dispense with the processing of large out-of-domain data that exist for this language pair. Our main focus for this year’s participation was to develop strong baselines by making the best of auxiliary resources: back translation of monolingual data; partial pre-translation of terms; pre-trained multilingual contextual embeddings and IR retrieved in domain corpora. Two pre-prossessing pipelines, one using the standard Moses tools1 and subword-nmt (Sennrich et al., 2016b) and other using HuggingFace BERT API were developed and compared. All systems are based on the transformer architecture (Vaswani et al., 2017), or and on the related BERT-fused transformer model of Zhu et al. (2020). If our baselines were actually strong, we only managed to get relatively small gains from our auxiliary resources, for reasons that by and large remain to be analyzed in depth. Our biomedical systems are presented in Section 2. We also participated in the Robustness translation task, developing a multi-domain, noise-robust and amenable to fast adaptation translation system for"
2020.wmt-1.86,W19-5422,0,0.0345313,"Missing"
2020.wmt-1.86,N19-1044,0,0.0154975,"d with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. The target side (French) remained unchanged. Figure 1 displays a sentence extracted from Medline 18 before and after pretranslation (in the latter, French segments are underlined). Terms are extracted from the French-English versio"
2020.wmt-1.86,W19-5344,0,0.0158649,"BLEU score on dev sets. These are then further fine-tuned using a selected part of the training corpus containing only the Medline abstracts and the three Cochrane corpora, again until convergence. The corresponding systems are post-fixed with *-ft. 2.2.2 Pre-translating terms Medical terms, made of monolexical or polylexical units, are abound in medical terms, and getting their translation right is a very difficult task. Approaches to Biomedical MT have tried to deal this in various ways including explicitly using terminology list (Carrino et al., 2019), domain adaptation (Hira et al., 2019; Stojanovski et al., 2019) and transfer learning (Khan et al., 2018; Peng et al., 2019; Saunders et al., 2019). We developed systems aimed at improving the translation of terms mainly following the recent proposals of (Dinu et al., 2019; Song et al., 2019). They mostly imply to pre-translate English terms into French, merely replacing the English version with a desired translation in a preprocessing step. The translation system thus inputs mixed-language sentences comprising both English and French words. In our implementation, we followed (Song et al., 2019) and did not mark the pre-translated segments in the input. T"
2020.wmt-1.86,tiedemann-2012-parallel,0,0.0326842,"dical texts provided by the WMT’20 organizers: Edp, Medline abstracts and titles (Jimeno Yepes et al., 2017), Scielo (Neves et al., 2016) and the Ufal Medical corpus2 consisting of Cesta, Ecdc, Emea (OpenSubtitles), PatTR Medical and OpenSubtitles. In addition, we used the Cochrane bilingual parallel corpus (Ive et al., 2016)3 and the Taus Corona Crisis corpus.4 . We finally experimented with additional in-domain data selected using Information Retrieval (IR) techniques from general domain corpora including News-Commentary, Books and Wikipedia corpus obtained from Open Parallel Corpus (OPUS) (Tiedemann, 2012). These were selected using the data selection scheme described in (Abdul-Rauf and Schwenk, 2009). Medline titles were used as queries to find the related sentences. We used 3-best sentences returned from the IR pipeline as additional corpus to build the models (these are shown as X7 in table2). For development purposes, we used Khresmoi, Edp and Scielo test corpora. The Medline test sets of WMT’18 and 195 were used as internal test data. 7.70 0.33 M 16.2 0.06 M Table 1: Data sources for the English-French biomedical task (before tokenization) We gathered parallel and monolingual corpora 804 2"
2021.blackboxnlp-1.24,2020.cl-1.1,0,0.0264855,"tuitive result is consistent with several observations made in the literature: the fact that a ‘linguistic’ information is encoded in the neural representations does not imply that it will be used by the neural network (see, for instance, (Belinkov and Glass, 2019)). This suggests that the information flow along the path denoted (c) in Figure 3 should be small and the choice of the English possessive pronoun is based on other information than the representation of son. 7 Related Work been used in several works to study the information flow within an encoder-decoder architecture: for instance, Belinkov et al. (2020) rely on probes to find which components of a NMT system encode linguistic information when translating morphologically rich languages. However, to the best of our knowledge, this work is the first to use the differences between gender expression in French and English to get insights into the inner representations used in NMT systems based on the Transformer architecture. Experiments reported in Section 6 are inspired by causal analysis, a type of analysis that has been used by Vig et al. (2020) to analyze gender bias in neural monolingual NLP models. Several studies have investigated gender b"
2021.blackboxnlp-1.24,2020.findings-emnlp.180,0,0.0349815,"Missing"
2021.blackboxnlp-1.24,D19-1530,0,0.011392,"optimised for speed. Using a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several techniques, probing and manipulating. We extended the scope of the investigation of the locus of gender transfer beyond the determiner/noun analysis of Costa-jussà et al. (2020a) and qu"
2021.blackboxnlp-1.24,D19-1275,0,0.0375186,"Missing"
2021.blackboxnlp-1.24,D19-3019,0,0.0210302,"‘him’) were created when the gender of the profession for each of these cases. can not be inferred from the French sentence. For instance, Conversely, in English sentences, gender infor- the French sentence ‘l’artiste a terminé son travail’ appears twice in the parallel corpora: the first time as the translation mation is always overtly expressed in the English of ‘the artist has finished herF work’, the second time as the pronoun, and in rare cases, also in the English noun translation of ‘the artist has finished hisM work’. 313 3 Experimental Setting In all our experiments, we use JoeyNMT9 (Kreutzer et al., 2019), an educational implementation of a translation system based on the Transformer model of Vaswani et al. (2017). The simplicity of the codebase, which nonetheless allowed us to achieve near SOTA performance on our data, made it a perfect choice for our endeavor. In our system, encoder and decoder are composed of 6 layers, each with 8 attention heads; the feedforward layers have 2,048 parameters and the dimension of lexical embeddings is 512. Our model comprises a grand total of 76,596,736 parameters. The system was trained with data from the ‘News’ task of the WMT’2015 evaluation campaign.10 I"
2021.blackboxnlp-1.24,P18-1007,0,0.0214885,"n our system, encoder and decoder are composed of 6 layers, each with 8 attention heads; the feedforward layers have 2,048 parameters and the dimension of lexical embeddings is 512. Our model comprises a grand total of 76,596,736 parameters. The system was trained with data from the ‘News’ task of the WMT’2015 evaluation campaign.10 It includes the Europarl, NewsCommentary and CommonCrawl corpora, and altogether contains 4,813,682 sentences and nearly 141 million French running words. All the corpora were tokenized and segmented into sub-lexical units using the unigram model of SentencePiece (Kudo, 2018); the resulting vocabularies contain 32,000 units in each language. The model is trained by optimizing the cross-entropy using the A DAM strategy. This system achieves a BLEU score of 34.0 for the French-English direction. 4 Evaluation of Gender Translation 4.1 Experimental Results We evaluate the ability of our system to predict the gender of occupational nouns using the corpus described in Section 2 and consider, as a point of comparison, the translations generated by e-translation, a translation system developed by the European Commission that is freely accessible for academic research.11 W"
2021.blackboxnlp-1.24,J94-4004,0,0.668093,"a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer. 1 Introduction The existence of translation divergences (i.e. crosslinguistic distinctions) raises many challenges for machine translation (MT) (Dorr, 1994): when translating a sentence, some information or constructions are specific to the target language and, consequently, can only be inferred by the decoder from the target context; some are only found in the source language and have to be ignored; finally, some information has to be adapted and transferred from the encoder to the decoder. Contrary to previous generations of MT engines where transfer rules were quite transparent, understanding this information flow within state-of-the-art neural MT systems is a challenging task, and a key step for their interpretability. To illustrate these alt"
2021.blackboxnlp-1.24,W19-3821,0,0.0153134,"ual models seem to rely more on the determiner. In the language-specific case, the embeddings are reported to encode more gender information. der bias is amplified when the system is optimised for speed. Using a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several technique"
2021.blackboxnlp-1.24,2021.acl-short.15,0,0.0413944,"Missing"
2021.blackboxnlp-1.24,N18-2002,0,0.04674,"Missing"
2021.blackboxnlp-1.24,N18-2003,0,0.0233495,"42 ESCO occupational nouns, she evidenced a gender gap by comparing the translations from Google Translate, DeepL and Microsoft Translator in the two directions for the French/Italian language pair. She built a dataset with respectively “competence” (i.e. intelligent) and “appearance” (i.e. beautiful) adjectives (ADJ) in the following pattern &lt;A very [ADJ] [N] entered the room&gt;. The data was manually analyzed. Adjectives seem to have no influence for the translation of masculine nouns, but competence adjectives affect the translation of feminine nouns more severely than appearance adjectives. Zhao et al. (2018) studies gender bias in ELMo embeddings using probing techniques. In this study, biases in the embeddings also implied biases in a pronoun reference resolution task using the WinoGender dataset. Balancing data, and using averaged representations, to a certain extend, helped remove this bias. Analyzing misclassified occupations in terms of gender, Costa-jussà et al. (2020a) investigated the architectural bias for the translation of occupational nouns, suggesting that using language-specific encoders and decoder yields less bias than a shared encoder-decoder architecture. Considering the attenti"
2021.blackboxnlp-1.24,P19-1161,0,0.0176233,"sing a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several techniques, probing and manipulating. We extended the scope of the investigation of the locus of gender transfer beyond the determiner/noun analysis of Costa-jussà et al. (2020a) and questioned the role of pr"
2021.blackboxnlp-1.24,2020.acl-main.690,0,0.198927,"nouns used to create the corpus can be found in the training set. This is also reflected in Figure 2, where we see that most occupational nouns are tokenized into multiple BPE units. These sentences were automatically translated and manually verified to produce the corresponding English list. The motivations for using these fixed syntactic patterns are many. First, they limit the only source of variability between sentences to the [N] slots, allowing us to perform controlled experiments. Second, they simplify the analysis and manipulation 3 Using a simplified list from (Prates et al., 2020), Saunders and Byrne (2020) created a “handcrafted” dataset of 388 parallel sentences of the type The [PROFESSION] finished [his|her] work. for three translation directions (EnglishSpanish, English-German and English-Hebrew). In this paper, we adapted this approach for a new translation direction (French to English) using a much larger list of occupational nouns: our corpus contains 3,394 sentences. 4 The French determiner is l’ for both genres if the job noun begins with a vowel. 5 This dataset can be found at https://github.com/ neuroviz/neuroviz/tree/main/blackbox2021. 40 28.7 0, 00 0 000 20 ≤ 10 The [N] has finished"
2021.blackboxnlp-1.24,P19-1164,0,0.0150602,"rmer architecture. Experiments reported in Section 6 are inspired by causal analysis, a type of analysis that has been used by Vig et al. (2020) to analyze gender bias in neural monolingual NLP models. Several studies have investigated gender bias using dedicated datasets, some of them presented at the ACL Workshop on Gender Bias in Natural Language Processing (Costa-jussà et al., 2019, 2020b; Costa-jussa et al., 2021). Savoldi et al. (2021) synthesizes the studies and datasets on gender bias for translation. In particular, the controlled test set considered in our work builds on the works of Stanovsky et al. (2019) and Saunders and Byrne (2020), who both propose challenge test sets to evaluate gender bias in MT systems. The corresponding datasets consider the translation of occupational nouns with an anaphoric reference that makes gender explicit: the former contains instances of difficult translation patterns inspired by the WinoGender dataset of Rudinger et al. (2018); similar to our work, the latter contains a smaller set of simple sentences following a fixed template. Working with a slightly more varied set of sentence templates chosen to unambiguously express the gender of the occupational noun, (R"
2021.calcs-1.11,W18-3219,0,0.0291993,"Missing"
2021.calcs-1.11,2020.lrec-1.223,0,0.0116782,"rrect. Table 4: Results of SemEval 2014 Task 5 for En-Es. UEdin-run1 UEdin-run2 UEdin-run3 CNRC-run1 multi-csw free-dec token-cst presuf-cst joint-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology cons"
2021.calcs-1.11,2020.acl-main.716,0,0.0357545,"Missing"
2021.calcs-1.11,W14-3914,0,0.0215276,"Missing"
2021.calcs-1.11,D18-1347,0,0.0965413,"& CNRS, LISN Orsay, France jitao.xu@limsi.fr Abstract code-switched. This phenomenon is also becoming more pervasive in short text messages, chats, blogs, and the like (Samih et al., 2016). Code-switching however remains understudied in natural language processing (NLP) (Aguilar and Solorio, 2020), and most work to date has focused on token-level language identification (LID) (Samih et al., 2016) and on language models for Automatic Speech Recognition (Winata et al., 2019). More tasks are being considered lately, such as Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018) or Sentiment Analysis (Patwa et al., 2020). Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These resul"
2021.calcs-1.11,Q17-1010,0,0.0141121,"at they are endowed with some language identification abilities. Using these models, we are also able to obtain competitive results on the SemEval 2014 Task 5: L2 Writing Assistant, which we see as one potential application area of CSW translation. 84 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 84–94 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_011 2 Building translation systems for code-switched data 2.1 which do not possess the correct language by using the fasttext LID model3 (Bojanowski et al., 2017). We use Moses tools (Koehn et al., 2007) to normalize punctuations, remove non-printing characters and discard sentence pairs with a source / target ratio higher than 1.5, with a maximum sentence length of 250. We tokenize all WMT data using Moses tokenizer.4 Our procedure for artificial CSW data generation uses WMT13 En-Es parallel data with 14.5M sentences. For En-Fr, we use all WMT14 parallel data, for a grand total of 33.9M sentences. Our development sets are respectively newstest2011 and newstest2012 for EnEs, and newstest2012 and newstest2013 as development sets for En-Fr; the correspon"
2021.calcs-1.11,P19-1175,0,0.0172393,"ge vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretrained translation models by predicting original source segments from generated CSW sentences and claimed better results compared to other preTo better study the performance gap between these language pairs, we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting t"
2021.calcs-1.11,N19-1090,0,0.0500259,"Missing"
2021.calcs-1.11,2005.mtsummit-papers.37,0,0.0932997,"Missing"
2021.calcs-1.11,E14-1001,0,0.0247394,"ent (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretra"
2021.calcs-1.11,W14-3348,0,0.0367923,"poses, we also use our parallel data to train two baselines: (a) regular NMT systems for the considered language pairs (base), similar to base-csw; (b) bilingual NMT systems, capable of translating from and into both two languages (bilingual). The selection of the desired target language relies on the same tagging mechanism as multi-csw, which means that both types of models see exactly the same examples. All resulting baseline Transformer models have the exact same hyperparameters and use the same training scheme as Code-Switch. Performance is computed with SacreBLEU (Post, 2018) and METEOR (Denkowski and Lavie, 2014). 3 3.1 Moreover, we note the marked differences between BLEU scores obtained by these models when the matrix language for the CSW source is the target and when the embedded language is the target. In the former case, translation is near perfect; in the latter case they nonetheless use the little information available to improve over the monolingual scores (about 1-1.5 BLEU points), nearly matching the performance of the baseline systems. This is illustrated for Fr-En, for which joint-csw improved from 33.7 to 35.0; in the same condition, the bilingual system only improves by 0.1 point. Among"
2021.calcs-1.11,P19-1294,0,0.0160116,"al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretrained translation models by predicting original source segments from generated CSW"
2021.calcs-1.11,2020.acl-main.329,0,0.0129802,"s of SemEval 2014 Task 5 for En-Es. UEdin-run1 UEdin-run2 UEdin-run3 CNRC-run1 multi-csw free-dec token-cst presuf-cst joint-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the correspo"
2021.calcs-1.11,N13-1073,0,0.0823978,"En-Fr, we use all WMT14 parallel data, for a grand total of 33.9M sentences. Our development sets are respectively newstest2011 and newstest2012 for EnEs, and newstest2012 and newstest2013 as development sets for En-Fr; the corresponding test sets are newstest2013 (En-Es) and newstest2014 (En-Fr). Code-switched data generation Parallel corpora with natural CSW data are very scarce (Menacer et al., 2019) and, similar to Song et al. (2019a), we generate artificial CSW parallel sentences from regular translation data. We first compute word alignments between parallel sentences using fast align1 (Dyer et al., 2013). We then extract so-called minimal alignment units following the approach of Crego et al. (2005): these correspond to small bilingual phrase pairs (e, f ) extracted from (symmetrized) word alignments such that all alignment links outgoing from words in e reach a word in f , and vice-versa. For each pair of parallel sentence, we first randomly select the matrix language;2 then the number of replacements r to appear in a derived CSW sentence with an exponential distribution as: P (r = k) = 1 2k+1 ∀k = 1, . . . , rep 2.2.2 Machine Translation systems We use the fairseq5 (Ott et al., 2019) implem"
2021.calcs-1.11,P07-2045,0,0.00663272,"ication abilities. Using these models, we are also able to obtain competitive results on the SemEval 2014 Task 5: L2 Writing Assistant, which we see as one potential application area of CSW translation. 84 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 84–94 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_011 2 Building translation systems for code-switched data 2.1 which do not possess the correct language by using the fasttext LID model3 (Bojanowski et al., 2017). We use Moses tools (Koehn et al., 2007) to normalize punctuations, remove non-printing characters and discard sentence pairs with a source / target ratio higher than 1.5, with a maximum sentence length of 250. We tokenize all WMT data using Moses tokenizer.4 Our procedure for artificial CSW data generation uses WMT13 En-Es parallel data with 14.5M sentences. For En-Fr, we use all WMT14 parallel data, for a grand total of 33.9M sentences. Our development sets are respectively newstest2011 and newstest2012 for EnEs, and newstest2012 and newstest2013 as development sets for En-Fr; the corresponding test sets are newstest2013 (En-Es) a"
2021.calcs-1.11,N16-1101,0,0.173607,"Missing"
2021.calcs-1.11,2020.acl-main.80,0,0.0142093,"l data. METEOR scores for the Fr-En SemEval test are much worse than for En-Es. This is mostly due to the high “fragmentation penalty” computed by METEOR for English; the corresponding average Fmean is about 0.99, showing that translations are mostly correct. Table 4: Results of SemEval 2014 Task 5 for En-Es. UEdin-run1 UEdin-run2 UEdin-run3 CNRC-run1 multi-csw free-dec token-cst presuf-cst joint-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past"
2021.calcs-1.11,D18-1346,0,0.017853,"we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting that the observed difference in the SemEval metric is likely due to a mismatch between references and system outputs. The official metric is a word accuracy which may exclude acceptable translations by exact token match. 5 Related work Research in the area of NLP for CSW has mostly focused on CSW Language Modeling, especially for Automatic Speech Recognition (Pratapa et al., 2018; Garg et al., 2018; Gonen and Goldberg, 2019; 90 7 training methods (Conneau and Lample, 2019; Song et al., 2019b). Nevertheless, there barely exists work aimed at translating CSW sentences. Johnson et al. (2017) mentioned using a multilingual NMT system to translate CSW sentence to a third target language by showing only one example. To the best of our knowledge, only one parallel Arabic-English CSW corpus was specifically released for MT applications (Menacer et al., 2019). This CSW data was extracted from the UN data with Arabic as the matrix language: while translations into English were readily available,"
2021.calcs-1.11,1983.tc-1.13,0,0.739494,"Missing"
2021.calcs-1.11,P18-1143,0,0.0120281,"these language pairs, we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting that the observed difference in the SemEval metric is likely due to a mismatch between references and system outputs. The official metric is a word accuracy which may exclude acceptable translations by exact token match. 5 Related work Research in the area of NLP for CSW has mostly focused on CSW Language Modeling, especially for Automatic Speech Recognition (Pratapa et al., 2018; Garg et al., 2018; Gonen and Goldberg, 2019; 90 7 training methods (Conneau and Lample, 2019; Song et al., 2019b). Nevertheless, there barely exists work aimed at translating CSW sentences. Johnson et al. (2017) mentioned using a multilingual NMT system to translate CSW sentence to a third target language by showing only one example. To the best of our knowledge, only one parallel Arabic-English CSW corpus was specifically released for MT applications (Menacer et al., 2019). This CSW data was extracted from the UN data with Arabic as the matrix language: while translations into English were"
2021.calcs-1.11,W16-5805,0,0.0141222,"” computed by METEOR for English; the corresponding average Fmean is about 0.99, showing that translations are mostly correct. Table 4: Results of SemEval 2014 Task 5 for En-Es. UEdin-run1 UEdin-run2 UEdin-run3 CNRC-run1 multi-csw free-dec token-cst presuf-cst joint-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to impro"
2021.calcs-1.11,W19-4439,0,0.0171674,"t-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases w"
2021.calcs-1.11,N19-4009,0,0.0209719,"ign1 (Dyer et al., 2013). We then extract so-called minimal alignment units following the approach of Crego et al. (2005): these correspond to small bilingual phrase pairs (e, f ) extracted from (symmetrized) word alignments such that all alignment links outgoing from words in e reach a word in f , and vice-versa. For each pair of parallel sentence, we first randomly select the matrix language;2 then the number of replacements r to appear in a derived CSW sentence with an exponential distribution as: P (r = k) = 1 2k+1 ∀k = 1, . . . , rep 2.2.2 Machine Translation systems We use the fairseq5 (Ott et al., 2019) implementation of Transformer base (Vaswani et al., 2017) for our models with a hidden size of 512 and a feedforward size of 2048. We optimize with Adam, set up with an initial learning rate of 0.0007 and an inverse square root weight decay schedule, as well as 4000 warmup steps. All models were trained with mixed precision and a batch size of 8192 tokens for 300k iterations on 4 V100 GPUs. For each language pair, we use a shared source-target inventory built with Byte Pair Encoding (BPE) of 32K merge operations, using the implementation published by Sennrich et al. (2016).6 Note that we do n"
2021.calcs-1.11,D19-1679,0,0.0178868,"t-csw free-dec Accuracy 0.733 0.731 0.723 0.556 Word Accuracy 0.824 0.821 0.816 0.694 Recall 1.0 1.0 1.0 1.0 0.554 0.531 0.519 0.685 0.665 0.658 0.996 0.990 0.982 0.626 0.744 0.994 Winata et al., 2019; Lee and Li, 2020). Evaluation tasks, benchmarks have also been prepared for LID in user generated CSW content (Zubiaga et al., 2016; Molina et al., 2016), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018; Aguilar et al., 2020; Khanuja et al., 2020) and Sentiment Analysis (Patwa et al., 2020). CSW was also found useful in foreign language teaching: Renduchintala et al. (2019a,b) showed that replacing words by their counterparts in foreign language helps to learn foreign language vocabulary. Table 5: Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases w"
2021.calcs-1.11,P17-1180,0,0.0234458,"Missing"
2021.calcs-1.11,W16-5806,0,0.332946,"Missing"
2021.calcs-1.11,2020.wmt-1.63,1,0.84299,"l 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretrained translation models by predicting original source segments from generated CSW sentences and claimed better results compared to other preTo better study the performance gap between these language pairs, we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting that the observed difference in the Se"
2021.calcs-1.11,P16-1162,0,0.0222774,"ems We use the fairseq5 (Ott et al., 2019) implementation of Transformer base (Vaswani et al., 2017) for our models with a hidden size of 512 and a feedforward size of 2048. We optimize with Adam, set up with an initial learning rate of 0.0007 and an inverse square root weight decay schedule, as well as 4000 warmup steps. All models were trained with mixed precision and a batch size of 8192 tokens for 300k iterations on 4 V100 GPUs. For each language pair, we use a shared source-target inventory built with Byte Pair Encoding (BPE) of 32K merge operations, using the implementation published by Sennrich et al. (2016).6 Note that we do not share the embedding matrices. Our experiments with sharing the decoder’s input and output embeddings or sharing all encoder+decoder embeddings did not yield further gains. We compare three settings for Code-Switch models: (1) where rep is a predefined maximum number of replacements. We also make sure that the number of replacements does not exceed half of either the original source or target sentences length, adjusting the actual number of replacements as: S T n = min( , , r) 2 2 (2) where S and T are respectively the length of the source and target sentences. We finally"
2021.calcs-1.11,W18-6319,0,0.0188619,"tion. For comparison purposes, we also use our parallel data to train two baselines: (a) regular NMT systems for the considered language pairs (base), similar to base-csw; (b) bilingual NMT systems, capable of translating from and into both two languages (bilingual). The selection of the desired target language relies on the same tagging mechanism as multi-csw, which means that both types of models see exactly the same examples. All resulting baseline Transformer models have the exact same hyperparameters and use the same training scheme as Code-Switch. Performance is computed with SacreBLEU (Post, 2018) and METEOR (Denkowski and Lavie, 2014). 3 3.1 Moreover, we note the marked differences between BLEU scores obtained by these models when the matrix language for the CSW source is the target and when the embedded language is the target. In the former case, translation is near perfect; in the latter case they nonetheless use the little information available to improve over the monolingual scores (about 1-1.5 BLEU points), nearly matching the performance of the baseline systems. This is illustrated for Fr-En, for which joint-csw improved from 33.7 to 35.0; in the same condition, the bilingual sy"
2021.calcs-1.11,N19-1044,0,0.53737,"ntence length of 250. We tokenize all WMT data using Moses tokenizer.4 Our procedure for artificial CSW data generation uses WMT13 En-Es parallel data with 14.5M sentences. For En-Fr, we use all WMT14 parallel data, for a grand total of 33.9M sentences. Our development sets are respectively newstest2011 and newstest2012 for EnEs, and newstest2012 and newstest2013 as development sets for En-Fr; the corresponding test sets are newstest2013 (En-Es) and newstest2014 (En-Fr). Code-switched data generation Parallel corpora with natural CSW data are very scarce (Menacer et al., 2019) and, similar to Song et al. (2019a), we generate artificial CSW parallel sentences from regular translation data. We first compute word alignments between parallel sentences using fast align1 (Dyer et al., 2013). We then extract so-called minimal alignment units following the approach of Crego et al. (2005): these correspond to small bilingual phrase pairs (e, f ) extracted from (symmetrized) word alignments such that all alignment links outgoing from words in e reach a word in f , and vice-versa. For each pair of parallel sentence, we first randomly select the matrix language;2 then the number of replacements r to appear in"
2021.calcs-1.11,K19-1026,0,0.449633,"tion for Code-Switched Input Franc¸ois Yvon Univ. Paris-Saclay, & CNRS, LISN Orsay, France francois.yvon@limsi.fr Jitao Xu Univ. Paris-Saclay, & CNRS, LISN Orsay, France jitao.xu@limsi.fr Abstract code-switched. This phenomenon is also becoming more pervasive in short text messages, chats, blogs, and the like (Samih et al., 2016). Code-switching however remains understudied in natural language processing (NLP) (Aguilar and Solorio, 2020), and most work to date has focused on token-level language identification (LID) (Samih et al., 2016) and on language models for Automatic Speech Recognition (Winata et al., 2019). More tasks are being considered lately, such as Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018) or Sentiment Analysis (Patwa et al., 2020). Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regul"
2021.calcs-1.11,2020.acl-main.144,1,0.743754,"Results of SemEval 2014 Task 5 for Fr-En. Regarding MT, most past work has focused on using artificial CSW data to help conventional translation systems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretrained translation models by predicting original source segments from generated CSW sentences and claimed better results compared to other preTo better study the performance gap between these language pairs, we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting that the observed"
2021.calcs-1.11,2020.emnlp-main.208,0,0.0573718,"ystems. Huang and Yates (2014) used CSW corpus to improve word alignment and statistical MT. Dinu et al. (2019) experienced replacing and concatenating source terminology constraints by the corresponding translation(s) to boost the accuracy of term translations. Song et al. (2019a) shared the same idea by replacing phrases with prespecified translation to perform “soft” constraint decoding. A different line of research is in (Bulte and Tezcan, 2019; Xu et al., 2020; Pham et al., 2020), who explore ways to combine a source sentence with similar translations extracted from translation memories. Yang et al. (2020) also pretrained translation models by predicting original source segments from generated CSW sentences and claimed better results compared to other preTo better study the performance gap between these language pairs, we additionally score the development and test data with BLEU and METEOR. Results in Table 6 show that for these metrics, we achieve performance that are in that same ballpark for the two language pairs, suggesting that the observed difference in the SemEval metric is likely due to a mismatch between references and system outputs. The official metric is a word accuracy which may"
2021.emnlp-main.665,W16-2206,0,0.0269224,"ormation latent in a multiparallel corpus to achieve better word alignments than aligning pairs of languages in isolation. Starting from translations of a sentence in multiple languages in a multiparallel corpus, 1 Introduction MPWA generates bilingual word alignments for all Word alignment is a challenging NLP task that language pairs using any available bilingual word plays an essential role in statistical machine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially"
2021.emnlp-main.665,W17-4711,0,0.0253646,"ithms to exploit the information latent in a multiparallel corpus to achieve better word alignments than aligning pairs of languages in isolation. Starting from translations of a sentence in multiple languages in a multiparallel corpus, 1 Introduction MPWA generates bilingual word alignments for all Word alignment is a challenging NLP task that language pairs using any available bilingual word plays an essential role in statistical machine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in w"
2021.emnlp-main.665,D17-1011,1,0.827965,"alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense d"
2021.emnlp-main.665,2008.iwslt-papers.1,0,0.0357154,"sidered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense disambiguation (Schütze, 1998). In 2009, Netflix’s recommender system competition"
2021.emnlp-main.665,J93-2003,0,0.207669,"Missing"
2021.emnlp-main.665,N19-1423,0,0.00475273,"2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Ef"
2021.emnlp-main.665,2021.eacl-main.181,0,0.207864,"d typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random order. alignment. 8457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8457–8469 c November 7–11, 2021. 2021 Association for Computational Linguistics Link prediction methods are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstructi"
2021.emnlp-main.665,P18-1141,1,0.851847,"ds are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstruction (Zhang and Chen, 2018). We use the Adamic-Adar index (Adamic and Adar, 2003); it is a second-order link prediction algorithm, i.e., it exploits the information of neighbors that are up to two hops aways from the starting target nodes (Zhou et al., 2009). We use a second-order algorithm since a set of aligned words in multiple languages (representing a concept) tends to establish a clique (Dufter et al., 2018). This means that exploring the influence of nodes at a distance of two in the graph provides informative signals while at the same time keeping runtime complexity low. lel corpus. This is an alternative approach that is especially important for low resource languages for which monolingual data often are not available. The PBC corpus does not contain a word alignment gold standard. To conduct the comparative evaluation of our new method, we port three existing word alignment gold standards of Bible translations to PBC, for the language pairs EnglishFrench, Finnish-Hebrew and Finnish-Greek. We"
2021.emnlp-main.665,N13-1073,0,0.273162,"to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin"
2021.emnlp-main.665,P07-1092,0,0.0633369,"of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepag"
2021.emnlp-main.665,D19-1453,0,0.0196733,"our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of"
2021.emnlp-main.665,2020.acl-main.747,0,0.0314769,"stling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Mul"
2021.emnlp-main.665,2020.findings-emnlp.147,1,0.899592,"ilable and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant lan"
2021.emnlp-main.665,2020.acl-main.560,0,0.0185496,"e to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and"
2021.emnlp-main.665,N03-1017,0,0.0405072,"Missing"
2021.emnlp-main.665,D07-1005,0,0.0791796,"llelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and"
2021.emnlp-main.665,2008.amta-papers.11,0,0.0610193,"hn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and lin"
2021.emnlp-main.665,C08-2014,0,0.0615272,"hn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and lin"
2021.emnlp-main.665,R09-1040,0,0.0450543,"llel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent fac"
2021.emnlp-main.665,I08-2093,0,0.0597577,"ion and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random o"
2021.emnlp-main.665,W12-0209,0,0.0276456,"cedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 201"
2021.emnlp-main.665,mayer-cysouw-2014-creating,0,0.181368,"wo translations of a sentence with lengths M and N , among all possible alignment links (M × N ), only a few (O(M + N )) are correct. This is partly due to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based"
2021.emnlp-main.665,W17-4804,0,0.0191402,"hine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively"
2021.emnlp-main.665,J03-1002,0,0.282036,"t languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language"
2021.emnlp-main.665,E14-4024,0,0.0222088,"he availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since t"
2021.emnlp-main.665,P15-2034,0,0.0181217,"to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and N"
2021.emnlp-main.665,P14-1131,1,0.796545,"onnecting links (Zhou et al., 2009). Link prediction algorithms compute the likelihood of links based on different heuristics. One can categorize available methods based on the maximum number of hops they consider in their computations for each node (Zhang and Chen, 2018). First order algorithms, such as common neighbors (CN), only consider one hop neighborhoods, e.g., (Barabási and Albert, 1999). Second order methods consider two hops, e.g., (Zhou et al., 2009). Finally, higher order methods take the whole network into account for making predictions (Brin and Page, 1998; Jeh and Widom, 2002; Rothe and Schütze, 2014). In this paper, we use a two-hop method since it offers a good tradeoff between effectiveness and efficiency. 3 3.1 Methods The MPWA framework While a bilingual aligner considers each language pair separately, MPWA utilizes the synergy between all language pairs to improve word alignment performance. In Figure 1, Eflomal alignments of a sentence from PBC in four different languages are depicted. Although Eflomal has failed to find the link between German “Schritt” and French “pas”, we can easily find this relation by observing that the four nodes “step”, “Schritt”, “paso”, and “pas” are fully"
2021.emnlp-main.665,J98-1004,1,0.16735,"et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense disambiguation (Schütze, 1998). In 2009, Netflix’s recommender system competition revealed that this 2 https://github.com/robertostling/ eflomal technique effectively works for collaborative filtering (Koren et al., 2009). Since then it has been a state of the art method in recommender systems. Link prediction algorithms are widely used in different areas of science since many social, biological, and information systems can be described as networks with nodes and connecting links (Zhou et al., 2009). Link prediction algorithms compute the likelihood of links based on different heuristics. One can categorize available metho"
2021.emnlp-main.665,2021.acl-long.67,0,0.034569,"Missing"
2021.emnlp-main.665,2020.lrec-1.522,0,0.124181,"multilingual word alignment matrix W for each sentence as shown in Figure 2. Each cell contains 0 or 1 for Adamic-Adar or the alignment score for Weighted Adamic-Adar. We again apply Argmax to extract new alignment edges and then add them to the original alignment. 4 4.1 Experimental setup PBC New Testament and Hebrew Bible while others contain only one. Table 2 gives corpus statistics. 4.2 Word alignment datasets PBC does not provide gold word alignments. To evaluate MPWA, we port two word alignment gold datasets of the Bible to PBC: Blinker (Melamed, 1998) and the recently published HELFI (Yli-Jyrä et al., 2020). We further experiment with bilingual datasets, using Machine Translation (MT) to create multiparallel corpora. Table 1 gives dataset statistics. The HELFI dataset consists of the Greek New Testament, the Hebrew Bible and translations of both into Finnish. In addition, morpheme alignments are provided for Finnish-Greek and FinnishHebrew. We reformatted this dataset to the format used by PBC. In more detail, we added three new editions for the three languages to PBC. We identified the PBC verse identifier for each verse of HELFI to ensure proper verse alignment of these three new editions. The"
2021.emnlp-main.665,2020.acl-main.146,0,0.0138534,"applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. C"
2021.emnlp-main.665,D10-1058,0,0.0395324,". Matrix factorization is a collaborative filtering algorithm that is most prominently used in recommender systems where it provides users with product recommendations based on their interactions with other products. This method is especially useful if the matrix is sparse (Koren et al., 2009). This is true for our application: Given two translations of a sentence with lengths M and N , among all possible alignment links (M × N ), only a few (O(M + N )) are correct. This is partly due to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresource"
2021.emnlp-main.665,2020.emnlp-main.362,0,0.0346647,"sooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random order. alignment. 8457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8457–8469 c November 7–11, 2021. 2021 Association for Computational Linguistics Link prediction methods are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstruction (Zhang and Chen, 2018). We use the Adamic-Adar index (Adamic and Adar, 2003); it is a second"
2021.emnlp-main.671,2020.coling-tutorials.3,0,0.0207015,"be better controlled with sequential decoding. 7 Related Work The variety of applications considered here makes it difficult to give a thorough analysis of all the related work, and we only mention the most significant landmarks. Multi-source / Multi-target Machine Translation Multi-source MT was studied in the framework of SMT, considering with a tight integration (in the decoder), or a late integration (by combining multiple hypotheses obtained with different sources). This idea was revisited in the Neural framework (Zoph and Knight, 2016; Liu et al., 2020a). Setting multilingual MT aside (Dabre et al., 2020), studies of the multi-target case are comparatively rarer (Neubig et al., 2015). Notable references are (Dong et al., 2015), which introduces a multi-task framework; (Wang et al., 2018), which studies ways to strengthen a basic multilingual decoder; while closer to our work, Wang et al. (2019) consider a dual decoder relying on dual self-attention mechanism. Related techniques have also been used to simultaneously generate a transcript and a translation for a spoken input (Anastasopoulos and Chiang, 2018; Le et al., 2020) and to generate consistent caption and subtitle for an audio source (Ka"
2021.emnlp-main.671,P15-1166,0,0.168711,"are the decoder input and output matrices, while for dual decoder models, we share all four input and output decoder matrices (Press and Wolf, 2017; Inan et al., 2017). All models are trained with mixed precision and a batch size of 8192 tokens on 4 V100 GPUs. Pre-training last for 300k iterations, while all other models are trained until no improvement is found for 4 consecutive checkpoints on the development set. Performance is computed with SacreBLEU (Post, 2018). We call the dual decoder models dual. To study the effectiveness of dual decoding, we also train a simplified multi-task model (Dong et al., 2015), implementing the independent model of Equation (3) without decoder cross-attention. For indep, the only interaction between outputs is thus a shared loss computed on multi-parallel data. Baseline Transformer models trained separately on each language pair are denoted by base. Multi-target Machine Translation 3.1 Data We first evaluate our dual decoder model on the multi-target MT task for three directions: English to German/French (En→De/Fr), German to English/French (De→En/Fr) and English to Chinese/Japanese (En→Zh/Ja). Similarly to (Wang et al., 2019; He et al., 2021), we use the IWSLT17 d"
2021.emnlp-main.671,N13-1073,0,0.105807,"Missing"
2021.emnlp-main.671,D09-1117,0,0.477288,"and Tl2 than if they were performed tion systems better, they are also more versatile independently. As it turns out, a dual decoder comand have been extended in many ways to meet new puting joint translations can be used for several application demands. This is notably the case with other purposes, that we also consider: to simulmultilingual extensions (Firat et al., 2016; Ha et al., taneously decode in two directions, providing a 2016; Johnson et al., 2017), which aim to develop new implementation of the idea of Watanabe and systems capable of processing multiple translation Sumita (2002); Finch and Sumita (2009); to disendirections with one single model. tangle mixed language (code-switched) texts into Another common situation for MT applications their two languages (Xu and Yvon, 2021); finally, is the multi-source / multi-target scenario, where to generate coherent translation alternatives, an source documents in language Sl need to be pub- idea we use to compute polite and impolite variants lished in several target languages Tl1 , Tl2 , . . . . This of the same input (Sennrich et al., 2016a). Conis, for instance, what happens in multilingual insti- sidering multiple applications allows us to assess"
2021.emnlp-main.671,N16-1101,0,0.0240361,"enough for a growing num- (b) a better collaboration between the stronger and ber of services, both for the general public and the the weaker decoders; (c) more consistent translatranslation industry. Not only are neural transla- tions in Tl1 and Tl2 than if they were performed tion systems better, they are also more versatile independently. As it turns out, a dual decoder comand have been extended in many ways to meet new puting joint translations can be used for several application demands. This is notably the case with other purposes, that we also consider: to simulmultilingual extensions (Firat et al., 2016; Ha et al., taneously decode in two directions, providing a 2016; Johnson et al., 2017), which aim to develop new implementation of the idea of Watanabe and systems capable of processing multiple translation Sumita (2002); Finch and Sumita (2009); to disendirections with one single model. tangle mixed language (code-switched) texts into Another common situation for MT applications their two languages (Xu and Yvon, 2021); finally, is the multi-source / multi-target scenario, where to generate coherent translation alternatives, an source documents in language Sl need to be pub- idea we use to c"
2021.emnlp-main.671,2021.iwslt-1.26,0,0.0223636,"0), studies of the multi-target case are comparatively rarer (Neubig et al., 2015). Notable references are (Dong et al., 2015), which introduces a multi-task framework; (Wang et al., 2018), which studies ways to strengthen a basic multilingual decoder; while closer to our work, Wang et al. (2019) consider a dual decoder relying on dual self-attention mechanism. Related techniques have also been used to simultaneously generate a transcript and a translation for a spoken input (Anastasopoulos and Chiang, 2018; Le et al., 2020) and to generate consistent caption and subtitle for an audio source (Karakanta et al., 2021). Bi-directional Decoding is an old idea from the statistical MT era (Watanabe and Sumita, 2002; Table 9: Results of politeness MT models. Tags are Finch and Sumita, 2009). Instantiations of these used for the pre-train model to generate the detechniques for NMT are in (Zhang et al., 2018; sired variant. Decoders (Dec) of indep and dual compute two translations in one decoding step, while Su et al., 2019), where asynchronous search techthe results using sequential decoding for one decoder niques are considered; and in (Zhou et al., 2019; are obtained with the 2-step procedure of Section 2.4.2."
2021.emnlp-main.671,2020.coling-main.314,0,0.540388,"concrete solutions to mitigate exposure bias between two decoders; (v) quantitative evaluations of the increased consistency incurred by a tight interaction between decoders. An additional empirical finding that is of practical value is the benefits of exploiting multi-parallel corpora to finetune multilingual systems. 2 Architectures for Dual Decoding 2.1 Model and Notations In our setting, we consider the simultaneous translation of sentence f in source language Sl into two target sentences e1 and e2 in languages1 Tl1 and Tl2 . In this situation, various modeling choices can be entertained (Le et al., 2020): 1 2 P (e , e |f ) = P (e1 , e2 |f ) = T Y t=1 T Y P (e1t , e2t |f , e1<t , e2<t ) (1) P (e1t |f , e1<t , e2<t )× t=1 P (e2t |f , e1<t , e2<t ) P (e1 , e2 |f ) = T Y (2) P (e1t |f , e1<t )P (e2t |f , e2<t ), t=1 2.2 Our dual decoder model implements the encoderdecoder architecture of the Transformer model of (Vaswani et al., 2017). In this model, the input to each attention head consists of queries Q, keyvalue pairs K and V. Each head maps a query and a set of key-value pairs to an output, computed as a weighted sum of the values, where weights are based on a compatibility assessment between"
2021.emnlp-main.671,2020.coling-main.97,0,0.424786,"he forward and backward training target sentences are not always deterministically related, which forces each decoder to put less trust on tokens from the other direction. We also consider the pseudo-dup data, in which each source sentence is duplicated, occurring once with the reference data in each direction. Results in Table 5 show that this method again closes the gap between indep and dual, and yields systems that surpass the baseline by about 1 BLEU point in the pseudo setting, and by 1.5 BLEU point in the pseudo-dup setting. Bi-directional MT (Finch and Sumita, 2009; Zhou et al., 2019; Liu et al., 2020b) aims to integrate future information in the decoder by jointly translating in the forward (left to right, L2R) and in the backward (right to left, R2L) directions. Another expectation is that the two decoders, having different views of the source, will deliver complementary translations. Dual decoding readily applies in this By computing the BLEU score between the two setting, with one decoder for each direction, with output translations, we can also evaluate the inthe added benefit of generating more coherent out- crement of consistency incurred in dual decoding. puts than independent deco"
2021.emnlp-main.671,1983.tc-1.13,0,0.635811,"Missing"
2021.emnlp-main.671,P19-2049,0,0.0207911,"e viewed as a tight form of multi-task learning, and, as we have seen, can be effectively trained using actual or partly artificial data; it can also directly benefit from pre-trained models. Considering four applications of MT, we have observed that dual decoding was prone to exposure bias in the two decoders, and we have proposed practical remedies. Using these, we have achieved BLEU scores that match those of a simple multi-task learners, and display an increased level of consistency. In our future work, we plan to consider other strategies, such as scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019), to mitigate the exposure bias. Another area where we seek to improve is the relaxation of strict synchronicity in decoding. We finally wish to study more applications of this technique, notably to generate controlled variation: controlling gender variation (Zmigrod et al., 2019) or more complex form of formality levels, as in (Niu and Carpuat, 2020), are obvious candidates. Acknowledgements first author is partly funded by Systran and by a grant from Région Ile-de-France. References Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia Hirschberg, and Thamar Solorio. 2018. Named ent"
2021.emnlp-main.671,N15-1033,0,0.128004,", variant generation (§ 6). Schwartz, 2008; Zoph and Knight, 2016) to handle this generates a first translation into target language Tl1 , which, once revised, can be used in conjunction with the original source to generate the translation into language Tl2 . The expected benefit of this approach is to facilitate word disambiguation. An alternative, that we thoroughly explore here, 1 Introduction is to simultaneously generate translations in Tl1 Neural Machine Translation (NMT) is progress- and Tl2 , an approach termed multi-target translaing at a rapid pace. Since the introduction of tion by Neubig et al. (2015). While the same goal is the first encoder-decoder architecture (Sutskever achieved with a multilingual system translating inet al., 2014; Cho et al., 2014), then completed with dependently in Tl1 and Tl2 , several pay-offs are exan attention mechanism (Bahdanau et al., 2015; pected from a joint decoding: (a) improved disamVaswani et al., 2017), the performance of NMT biguation capacities (as for multi-source systems); systems is now good enough for a growing num- (b) a better collaboration between the stronger and ber of services, both for the general public and the the weaker decoders; (c) m"
2021.emnlp-main.671,2001.mtsummit-papers.46,0,0.520621,"applications their two languages (Xu and Yvon, 2021); finally, is the multi-source / multi-target scenario, where to generate coherent translation alternatives, an source documents in language Sl need to be pub- idea we use to compute polite and impolite variants lished in several target languages Tl1 , Tl2 , . . . . This of the same input (Sennrich et al., 2016a). Conis, for instance, what happens in multilingual insti- sidering multiple applications allows us to assess tutions, or with crowdsourced translations of TV the challenges and rewards of dual decoding unshows. The multi-source way (Och and Ney, 2001; der various angles and to better evaluate the actual 8533 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8533–8546 c November 7–11, 2021. 2021 Association for Computational Linguistics agreement between the two decoders’ outputs. Our main contributions are the following: (i) a comparative study of architectures for dual decoding (§ 2); (ii) four short experimental studies where we use these architectures to simultaneously generate several outputs from one input (§ 3–§ 6); (iii) practical remedies to the shortage of multi-parallel corpora that ar"
2021.emnlp-main.671,N19-4009,0,0.018735,"with the same pre-trained decoder. The decoder cross-attention matrices cannot benefit from pre-training and are initialized randomly. During fine-tuning, tags are no longer necessary as both target translations are required. and mecab11 respectively. For En→De/Fr and De→En/Fr, we use a shared source-target vocabulary built with 40K Byte Pair Encoding (BPE) units (Sennrich et al., 2016b) learned on WMT data with subword-nmt.12 For En→Zh/Ja, we build a 32K BPE model for En and a joint 32K BPE for Zh and Ja, both learned on the WMT data.13 3 We implement the dual decoder model using fairseq14 (Ott et al., 2019),15 with a hidden size of 512 and a feedforward size of 2048. We optimize with Adam, set up with a maximum learning rate of 0.0007 and an inverse square root decay schedule, as well as 4000 warmup steps. For finetuning models, we use Adam with a fixed learning rate of 8e−5. For standard Transformer models, we share the decoder input and output matrices, while for dual decoder models, we share all four input and output decoder matrices (Press and Wolf, 2017; Inan et al., 2017). All models are trained with mixed precision and a batch size of 8192 tokens on 4 V100 GPUs. Pre-training last for 300k"
2021.emnlp-main.671,P16-1162,0,0.671483,"implementation of the idea of Watanabe and systems capable of processing multiple translation Sumita (2002); Finch and Sumita (2009); to disendirections with one single model. tangle mixed language (code-switched) texts into Another common situation for MT applications their two languages (Xu and Yvon, 2021); finally, is the multi-source / multi-target scenario, where to generate coherent translation alternatives, an source documents in language Sl need to be pub- idea we use to compute polite and impolite variants lished in several target languages Tl1 , Tl2 , . . . . This of the same input (Sennrich et al., 2016a). Conis, for instance, what happens in multilingual insti- sidering multiple applications allows us to assess tutions, or with crowdsourced translations of TV the challenges and rewards of dual decoding unshows. The multi-source way (Och and Ney, 2001; der various angles and to better evaluate the actual 8533 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8533–8546 c November 7–11, 2021. 2021 Association for Computational Linguistics agreement between the two decoders’ outputs. Our main contributions are the following: (i) a comparative study of"
2021.emnlp-main.671,N19-1044,0,0.0213352,"dded within larger fragments in the primary language. When simultaneously translating into these two languages, we expect the following “copy” constraint to be satisfied: every word in the source text should appear in at least one of the two outputs. Our main interest in this experiment is to asModel Exclusive Both Punctuations Lost sess how much dual decoding actually enforces reference 81.56 8.10 10.34 0 this constraint. As tri-parallel corpora for this task base 79.14 8.85 11.29 0.72 are scarce (Menacer et al., 2019), we mostly folindep 78.86 9.13 11.35 0.67 dual 78.90 9.17 11.32 0.61 low (Song et al., 2019; Xu and Yvon, 2021) and automatically generate artificial CSW sentences from regular parallel data. Working with the En- Table 8: Analysis of the “copy” constraint. “Exclusive” refers to the percentage of test tokens appearing Fr pair, we use the WMT14 En-Fr data to gen- in only one hypothesis. “Both” and “Punctuations” are erate training data as well as a CSW version of for tokens and punctuations+digits appearing in both the newstest2014 test set. Approximately half hypotheses, and “Lost” is for tokens not found in eiof the test sentences are mostly English with in- ther. serts in French, a"
2021.emnlp-main.671,W18-6319,0,0.0136936,", as well as 4000 warmup steps. For finetuning models, we use Adam with a fixed learning rate of 8e−5. For standard Transformer models, we share the decoder input and output matrices, while for dual decoder models, we share all four input and output decoder matrices (Press and Wolf, 2017; Inan et al., 2017). All models are trained with mixed precision and a batch size of 8192 tokens on 4 V100 GPUs. Pre-training last for 300k iterations, while all other models are trained until no improvement is found for 4 consecutive checkpoints on the development set. Performance is computed with SacreBLEU (Post, 2018). We call the dual decoder models dual. To study the effectiveness of dual decoding, we also train a simplified multi-task model (Dong et al., 2015), implementing the independent model of Equation (3) without decoder cross-attention. For indep, the only interaction between outputs is thus a shared loss computed on multi-parallel data. Baseline Transformer models trained separately on each language pair are denoted by base. Multi-target Machine Translation 3.1 Data We first evaluate our dual decoder model on the multi-target MT task for three directions: English to German/French (En→De/Fr), Ger"
2021.emnlp-main.671,E17-2025,0,0.0176184,"32K BPE model for En and a joint 32K BPE for Zh and Ja, both learned on the WMT data.13 3 We implement the dual decoder model using fairseq14 (Ott et al., 2019),15 with a hidden size of 512 and a feedforward size of 2048. We optimize with Adam, set up with a maximum learning rate of 0.0007 and an inverse square root decay schedule, as well as 4000 warmup steps. For finetuning models, we use Adam with a fixed learning rate of 8e−5. For standard Transformer models, we share the decoder input and output matrices, while for dual decoder models, we share all four input and output decoder matrices (Press and Wolf, 2017; Inan et al., 2017). All models are trained with mixed precision and a batch size of 8192 tokens on 4 V100 GPUs. Pre-training last for 300k iterations, while all other models are trained until no improvement is found for 4 consecutive checkpoints on the development set. Performance is computed with SacreBLEU (Post, 2018). We call the dual decoder models dual. To study the effectiveness of dual decoding, we also train a simplified multi-task model (Dong et al., 2015), implementing the independent model of Equation (3) without decoder cross-attention. For indep, the only interaction between out"
2021.emnlp-main.671,W16-5806,0,0.0261709,"re-trained model. combines multi-target and bi-directional decoding 8540 within a single architecture, where, in each layer and block, all cross-attentions are combined with a single hidden state; four softmax layers are used for the output symbols in a proposal that creates an even stronger dependency between decoders than what we consider here. Code-switching is an important linguistic phenomenon in bilingual communities that is getting momentum within the natural language processing communities (Sitaram et al., 2019). Several tasks have been considered: token-level language identification (Samih et al., 2016), Language Modeling (Winata et al., 2019), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018) and Sentiment Analysis (Patwa et al., 2020). Machine Translation for CSW texts is considered in (Menacer et al., 2019). 8 Conclusion and Future Work In this paper, we have explored various possible implementations of dual decoding, as a way to generate pairs of consistent translation. Dual decoding can be viewed as a tight form of multi-task learning, and, as we have seen, can be effectively trained using actual or partly artificial data; it can also dire"
2021.emnlp-main.671,2008.amta-srw.6,0,0.030768,"dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations. François Yvon Univ. Paris-Saclay, & CNRS, LISN Orsay, France francois.yvon@limsi.fr f e1 e2 e1 e2 e1 e2 I could do that again if you want . 只要 你 愿意 我 可以 重复 一遍 。 もう 一 回 やり ましょ う か Je peux le refaire si vous le voulez . . voulez le vous si refaire le peux Je Ich kann das noch mal machen , wenn Sie wollen . Ich kann das noch mal machen , wenn du willst . Table 1: Instances of Dual Decoding: multi-target translation (§ 3), bi-directional decoding (§ 4), variant generation (§ 6). Schwartz, 2008; Zoph and Knight, 2016) to handle this generates a first translation into target language Tl1 , which, once revised, can be used in conjunction with the original source to generate the translation into language Tl2 . The expected benefit of this approach is to facilitate word disambiguation. An alternative, that we thoroughly explore here, 1 Introduction is to simultaneously generate translations in Tl1 Neural Machine Translation (NMT) is progress- and Tl2 , an approach termed multi-target translaing at a rapid pace. Since the introduction of tion by Neubig et al. (2015). While the same goal"
2021.emnlp-main.671,D19-1330,0,0.210303,"o train a simplified multi-task model (Dong et al., 2015), implementing the independent model of Equation (3) without decoder cross-attention. For indep, the only interaction between outputs is thus a shared loss computed on multi-parallel data. Baseline Transformer models trained separately on each language pair are denoted by base. Multi-target Machine Translation 3.1 Data We first evaluate our dual decoder model on the multi-target MT task for three directions: English to German/French (En→De/Fr), German to English/French (De→En/Fr) and English to Chinese/Japanese (En→Zh/Ja). Similarly to (Wang et al., 2019; He et al., 2021), we use the IWSLT17 dataset4 (Cettolo et al., 2012) as our main test bed.5 Pre-training experiments additionally use WMT20 De-En, De-Fr, En-Zh, En-Ja and WMT14 En-Fr bilingual datasets.6 We use the IWSLT tst2012 and tst2013 as development sets and test our model on tst2014.7 Table 2 summarizes the main statistics for trilingual training and test data. Train Dev tst2014 Train Dev tst2014 Original De 209522 2693 1305 Original Zh 235078 3064 1297 Original Fr 236653 3083 1306 Original Ja 226834 3024 1285 3-way 205397 2468 1168 3-way 213090 2837 1214 Table 2: Number of lines in t"
2021.emnlp-main.671,C02-1050,0,0.277891,"eferences are (Dong et al., 2015), which introduces a multi-task framework; (Wang et al., 2018), which studies ways to strengthen a basic multilingual decoder; while closer to our work, Wang et al. (2019) consider a dual decoder relying on dual self-attention mechanism. Related techniques have also been used to simultaneously generate a transcript and a translation for a spoken input (Anastasopoulos and Chiang, 2018; Le et al., 2020) and to generate consistent caption and subtitle for an audio source (Karakanta et al., 2021). Bi-directional Decoding is an old idea from the statistical MT era (Watanabe and Sumita, 2002; Table 9: Results of politeness MT models. Tags are Finch and Sumita, 2009). Instantiations of these used for the pre-train model to generate the detechniques for NMT are in (Zhang et al., 2018; sired variant. Decoders (Dec) of indep and dual compute two translations in one decoding step, while Su et al., 2019), where asynchronous search techthe results using sequential decoding for one decoder niques are considered; and in (Zhou et al., 2019; are obtained with the 2-step procedure of Section 2.4.2. Wang et al., 2019; Zhang et al., 2020) where, similar to our work, various ways to enforce a t"
2021.emnlp-main.671,K19-1026,0,0.020065,"and bi-directional decoding 8540 within a single architecture, where, in each layer and block, all cross-attentions are combined with a single hidden state; four softmax layers are used for the output symbols in a proposal that creates an even stronger dependency between decoders than what we consider here. Code-switching is an important linguistic phenomenon in bilingual communities that is getting momentum within the natural language processing communities (Sitaram et al., 2019). Several tasks have been considered: token-level language identification (Samih et al., 2016), Language Modeling (Winata et al., 2019), Named Entity Recognition (Aguilar et al., 2018), Part-of-Speech tagging (Ball and Garrette, 2018) and Sentiment Analysis (Patwa et al., 2020). Machine Translation for CSW texts is considered in (Menacer et al., 2019). 8 Conclusion and Future Work In this paper, we have explored various possible implementations of dual decoding, as a way to generate pairs of consistent translation. Dual decoding can be viewed as a tight form of multi-task learning, and, as we have seen, can be effectively trained using actual or partly artificial data; it can also directly benefit from pre-trained models. Con"
2021.emnlp-main.671,2021.calcs-1.11,1,0.885327,"ew puting joint translations can be used for several application demands. This is notably the case with other purposes, that we also consider: to simulmultilingual extensions (Firat et al., 2016; Ha et al., taneously decode in two directions, providing a 2016; Johnson et al., 2017), which aim to develop new implementation of the idea of Watanabe and systems capable of processing multiple translation Sumita (2002); Finch and Sumita (2009); to disendirections with one single model. tangle mixed language (code-switched) texts into Another common situation for MT applications their two languages (Xu and Yvon, 2021); finally, is the multi-source / multi-target scenario, where to generate coherent translation alternatives, an source documents in language Sl need to be pub- idea we use to compute polite and impolite variants lished in several target languages Tl1 , Tl2 , . . . . This of the same input (Sennrich et al., 2016a). Conis, for instance, what happens in multilingual insti- sidering multiple applications allows us to assess tutions, or with crowdsourced translations of TV the challenges and rewards of dual decoding unshows. The multi-source way (Och and Ney, 2001; der various angles and to better"
2021.emnlp-main.671,Q19-1006,0,0.35295,"Encoder-Decoder Attention Encoder-Decoder Attention Add & Norm Decoder Self-Attention Encoder Out Add & Norm Decoder Self-Attention Figure 1: A graphical view of the dual decoder. 2.4 Synchronous Beam Search 2.4.1 Full Synchronous Mode Our decoding algorithm uses a dual beam search. Assuming each decoder uses its own beam of size k, the cross-attention between decoders can be designed and implemented in multiple ways: for instance, one could have each hypothesis in decoder 1 attend to any hypotheses in decoder 2, which would however create an exponential blowup of the search space. Following Zhou et al. (2019), we only compute the attention between 1best candidates of each decoder, 2-best candidates in each decoder, etc. This heuristic ensures that the number of candidates in each decoder beam remains fixed. There is however an added complexity, due to the fact that the ranking of hypotheses in each decoder beam evolves over time: the best hypothesis in decoder 1 at time t may no longer be the best at time t + 1. Preserving the consistency of the decoder states therefore implies to recompute the entire prefix representation for each hypothesis and each decoder at each time step, thus creating a sig"
2021.emnlp-main.671,P19-1161,0,0.0135222,"ias in the two decoders, and we have proposed practical remedies. Using these, we have achieved BLEU scores that match those of a simple multi-task learners, and display an increased level of consistency. In our future work, we plan to consider other strategies, such as scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019), to mitigate the exposure bias. Another area where we seek to improve is the relaxation of strict synchronicity in decoding. We finally wish to study more applications of this technique, notably to generate controlled variation: controlling gender variation (Zmigrod et al., 2019) or more complex form of formality levels, as in (Niu and Carpuat, 2020), are obvious candidates. Acknowledgements first author is partly funded by Systran and by a grant from Région Ile-de-France. References Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia Hirschberg, and Thamar Solorio. 2018. Named entity recognition on code-switched data: Overview of the CALCS 2018 shared task. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages 138–147, Melbourne, Australia. Association for Computational Linguistics. Antonios Anastasopoulos and"
2021.emnlp-main.671,N16-1004,0,0.125442,"d to also thoroughly analyze the benefits of generating matched, rather than independent, translations. François Yvon Univ. Paris-Saclay, & CNRS, LISN Orsay, France francois.yvon@limsi.fr f e1 e2 e1 e2 e1 e2 I could do that again if you want . 只要 你 愿意 我 可以 重复 一遍 。 もう 一 回 やり ましょ う か Je peux le refaire si vous le voulez . . voulez le vous si refaire le peux Je Ich kann das noch mal machen , wenn Sie wollen . Ich kann das noch mal machen , wenn du willst . Table 1: Instances of Dual Decoding: multi-target translation (§ 3), bi-directional decoding (§ 4), variant generation (§ 6). Schwartz, 2008; Zoph and Knight, 2016) to handle this generates a first translation into target language Tl1 , which, once revised, can be used in conjunction with the original source to generate the translation into language Tl2 . The expected benefit of this approach is to facilitate word disambiguation. An alternative, that we thoroughly explore here, 1 Introduction is to simultaneously generate translations in Tl1 Neural Machine Translation (NMT) is progress- and Tl2 , an approach termed multi-target translaing at a rapid pace. Since the introduction of tion by Neubig et al. (2015). While the same goal is the first encoder-dec"
2021.jeptalnrecital-taln.2,W05-0909,0,0.0321372,"Missing"
2021.jeptalnrecital-taln.2,2020.winlp-1.25,0,0.0658596,"Missing"
2021.jeptalnrecital-taln.2,Q19-1004,0,0.0555066,"Missing"
2021.jeptalnrecital-taln.2,2020.acl-main.485,0,0.0610893,"Missing"
2021.jeptalnrecital-taln.2,W17-4705,1,0.899964,"Missing"
2021.jeptalnrecital-taln.2,E06-1032,0,0.32211,"Missing"
2021.jeptalnrecital-taln.2,W19-3824,0,0.0505196,"Missing"
2021.jeptalnrecital-taln.2,W19-3821,0,0.0323243,"Missing"
2021.jeptalnrecital-taln.2,2020.findings-emnlp.180,0,0.0273948,"Missing"
2021.jeptalnrecital-taln.2,D19-1275,0,0.0320781,"Missing"
2021.jeptalnrecital-taln.2,D17-1263,0,0.0385236,"Missing"
2021.jeptalnrecital-taln.2,D19-3019,0,0.0434061,"Missing"
2021.jeptalnrecital-taln.2,P18-1007,0,0.0552507,"Missing"
2021.jeptalnrecital-taln.2,P11-1023,0,0.049813,"Missing"
2021.jeptalnrecital-taln.2,W18-6301,0,0.0223701,"Missing"
2021.jeptalnrecital-taln.2,P02-1040,0,0.109321,"Missing"
2021.jeptalnrecital-taln.2,N18-2002,0,0.0419551,"Missing"
2021.jeptalnrecital-taln.2,2020.acl-main.690,0,0.0336518,"Missing"
2021.jeptalnrecital-taln.2,2020.gebnlp-1.4,0,0.0455014,"Missing"
2021.jeptalnrecital-taln.2,N16-1005,0,0.0433454,"Missing"
2021.jeptalnrecital-taln.2,P19-1164,0,0.0262775,"Missing"
2021.jeptalnrecital-taln.2,D18-1334,0,0.0398621,"Missing"
2021.jeptalnrecital-taln.2,J83-1005,0,0.70483,"Missing"
2021.jeptalnrecital-taln.2,N18-2003,0,0.0582771,"Missing"
2021.jeptalnrecital-taln.8,C08-1018,0,0.162564,"Missing"
2021.jeptalnrecital-taln.8,daelemans-etal-2004-automatic,0,0.300013,"Missing"
2021.jeptalnrecital-taln.8,P19-1331,0,0.0465244,"Missing"
2021.jeptalnrecital-taln.8,2020.iwslt-1.26,0,0.0415268,"Missing"
2021.jeptalnrecital-taln.8,2020.lrec-1.460,0,0.0391634,"Missing"
2021.jeptalnrecital-taln.8,D16-1140,0,0.0396304,"Missing"
2021.jeptalnrecital-taln.8,kobus-etal-2017-domain,0,0.0592646,"Missing"
2021.jeptalnrecital-taln.8,2020.amta-pemdt.6,0,0.0417771,"Missing"
2021.jeptalnrecital-taln.8,D18-2012,0,0.0363076,"Missing"
2021.jeptalnrecital-taln.8,2020.lrec-1.577,0,0.0598053,"Missing"
2021.jeptalnrecital-taln.8,W19-5209,0,0.0526931,"Missing"
2021.jeptalnrecital-taln.8,P02-1040,0,0.117154,"Missing"
2021.jeptalnrecital-taln.8,W18-6319,0,0.0409731,"Missing"
2021.jeptalnrecital-taln.8,D15-1044,0,0.119404,"Missing"
2021.jeptalnrecital-taln.8,N16-1005,0,0.0649173,"Missing"
2021.jeptalnrecital-taln.8,2006.amta-papers.25,0,0.384864,"Missing"
2021.jeptalnrecital-taln.8,N19-1401,0,0.0441964,"Missing"
2021.jeptalnrecital-taln.8,Q16-1029,0,0.0600188,"Missing"
2021.jeptalnrecital-taln.8,D17-1062,0,0.0485561,"Missing"
2021.mtsummit-research.21,W18-6318,0,0.020642,"ociating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and m"
2021.mtsummit-research.21,W17-4711,0,0.0229553,", 2018). Given pairs associating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-e"
2021.mtsummit-research.21,W18-1207,0,0.0244296,"ming a standard for many applications, several studies have started to investigate more closely the impact on these preprocessing decisions on the final performance. The implementation of SentencePiece10 reports a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias an"
2021.mtsummit-research.21,2020.findings-emnlp.414,0,0.0192747,"ons have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias and that LM-based tokenization produces subword units that qualitatively align with morphology much better than those produced by BPE, suggesting that the latter is better than the former for pretrained models. The work of Deguchi et al. (2020) is our main inspiration, and explore ways to optimize the subword segmentation, using, as we do, sampling techniques and length-based heuristics to chose the most appropriate target for each source, and obser"
2021.mtsummit-research.21,2020.emnlp-main.42,0,0.0123143,"d Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in s"
2021.mtsummit-research.21,D18-1461,0,0.0189276,"SentencePiece10 reports a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias and that LM-based tokenization produces subword units that qualitatively align with morphology much better than those produced by BPE, suggesting that the latter is better than t"
2021.mtsummit-research.21,2020.coling-main.378,0,0.3098,"roperty of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in standard asymmetric models such as IBM-1 and IBM-4 (Liu et al., 2015; Tomeh et al., 2014; Wang and Lepage, 2016); (b) they also enable to actively manipulate the lengths of the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 256 source and target sentences so as to make them more even, arguably a facilitating factor for alignment and translation models (Deguchi et al., 2020). In this work, we take a closer look at the interaction between alignment and subword tokenization and try to address the following research questions: how much of the reported improvements in alignment performance can be linked to subword splitting? which issue(s) of basic alignment models do they mitigate? is it possible to design more active segmentation strategies that would target the alignment problem for specific language pairs? Our conclusions rests on the analysis of a systematic study of word alignment for 6 language pairs from multiple language families. We notably show that subwor"
2021.mtsummit-research.21,N19-1423,0,0.0105144,"ng corpus at training time, meaning that there is no unknown word in the reference alignments. Basic statistics for these corpora are in Table 1.4 English-French and English-German training data (≥ 1.5M) are much larger than the rest (from 122K to under 400K) and we take them as representative of a ”large data” condition. Unsurprisingly, the vocabulary sizes of the German, Romanian and Czech corpora are substantially greater than the corresponding English, 1A method of generating alignment links based on the matrix of embedding similarities without parallel data. The options are to use mBert (Devlin et al., 2019) or the multilingual version of Fasttext are used to generate multilingual embeddings from monolingual data. In our experiments, we use the setting: mBert + Argmax. 2 http://www-i6.informatik.rwth-aachen.de/goldAlignment/ 3 https://code.google.com/archive/p/evbcorpus/ 4 We only use training sentences of length lower than 50. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 257 which contains a smaller number of inflected variants. The opposite pattern is observed for Japanese and Vietnamese, two synthetic languages"
2021.mtsummit-research.21,W19-6620,0,0.0192011,"a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias and that LM-based tokenization produces subword units that qualitatively align with morphology much better than those produced by BPE, suggesting that the latter is better than the former for pretrai"
2021.mtsummit-research.21,2021.eacl-main.181,0,0.031983,"alences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting"
2021.mtsummit-research.21,N13-1073,0,0.0592754,"Missing"
2021.mtsummit-research.21,D19-1141,0,0.0357127,"Missing"
2021.mtsummit-research.21,D19-1453,0,0.0179432,"vised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are diff"
2021.mtsummit-research.21,W17-4706,0,0.0224208,"). With BPE/unigram subtokenization becoming a standard for many applications, several studies have started to investigate more closely the impact on these preprocessing decisions on the final performance. The implementation of SentencePiece10 reports a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokeni"
2021.mtsummit-research.21,2020.findings-emnlp.147,1,0.885644,"Missing"
2021.mtsummit-research.21,2005.mtsummit-papers.11,0,0.15388,"al., 2020) ), outlining difficult issues for word alignment models such as the prediction of null links, of many-to-one links, as well as the alignment of rare words. Detailed analyses are in (Ngo Ho, 2021). Asymmetric alignment models associate each source word with exactly one target word; such alignments are denoted as English → Foreign, when English is the source language. As a preamble, we start with our data condition. 2.1 Datasets Our experiments consider multiple language pairs all having English on one side. Our training sets for French and German are made of sentences from Europarl (Koehn, 2005). For Romanian, we use both the NAACL 2003 corpus (Mihalcea and Pedersen, 2003) and the SETIMES corpus used in WMT’16 MT evaluation. For Czech, the parallel data from News Commentary V11 (Tiedemann, 2012) is considered, while we use the preprocessed parallel data for Vietnamese in IWSLT’15 (Luong and Manning, 2015) and the Japanese data from the KFTT (Neubig, 2011). Our evaluations use standard test sets whenever applicable: for French and Romanian, we use data from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003); the German test data is Europarl;2 for Czech we use the corpus d"
2021.mtsummit-research.21,J10-4005,0,0.0375815,"plications, notably for state-of-the-art open vocabulary machine translation systems. In this paper, we thoroughly study how this preprocessing step interacts with the word alignment task and propose several tokenization strategies to obtain well-segmented parallel corpora. Using these new techniques, we were able to improve baseline word-based alignment models for six language pairs. 1 Introduction Word alignment is a basic task in multilingual Natural Language Processing (NLP) and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation (SMT) systems (Koehn, 2010), to filter out noise from translation memories (Pham et al., 2018) or in quality estimation applications (Specia et al., 2018). Word alignment can also serve to explain MT decisions (Stahlberg et al., 2018). Given pairs associating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, wo"
2021.mtsummit-research.21,P18-1007,0,0.372535,"ed thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in standard asymmetric models such as IBM-1 and IBM-4 (Liu et al., 2015; Tomeh et al., 2014; Wang and Lepage, 2016); (b) they also enable to actively manipulate the lengths of the Proceedings of the 18th Biennial Machine Transla"
2021.mtsummit-research.21,D18-2012,0,0.114889,"th difference 5 1 1 2 10 600 500 400 300 200 100 0 # onetoone F1 Figure 2: Alignment types for asymmetrical alignments for English→German (left) and symmetrical alignments using Grow-diag-final (right). 15 Figure 3: F-score (red) and number of correct one-to-one alignments (blue) as a function of a length difference for the direction English-French, computed by Fastalign. The numbers in black are the corresponding number of sentences. subword units generated by Byte-Pair-Encoding (Sennrich et al., 2016) and the unigram method of (Kudo, 2018), both implemented with the SentencePiece package (Kudo and Richardson, 2018). All parameters of these models are set to their default values. We independently segment sentences in each language with varying vocabulary sizes V ∈ {2K, 4K, 8K, 16K, 32K, 48K}. For Japanese, we do not use the vocabulary size of 2K because it is smaller than the characterbased vocabulary size. For English-Vietnamese, experiments for English vocabulary size of 48K and Vietnamese vocabulary size larger than 32K were not performed. This is because they would imply larger vocabularies than their word-based counterparts. When using the sampling strategy of SentencePiece, we use α = 0.1. Our resu"
2021.mtsummit-research.21,W16-2207,0,0.018456,"ham et al., 2018) or in quality estimation applications (Specia et al., 2018). Word alignment can also serve to explain MT decisions (Stahlberg et al., 2018). Given pairs associating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they c"
2021.mtsummit-research.21,D15-1210,0,0.0235636,"l architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in standard asymmetric models such as IBM-1 and IBM-4 (Liu et al., 2015; Tomeh et al., 2014; Wang and Lepage, 2016); (b) they also enable to actively manipulate the lengths of the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 256 source and target sentences so as to make them more even, arguably a facilitating factor for alignment and translation models (Deguchi et al., 2020). In this work, we take a closer look at the interaction between alignment and subword tokenization and try to address the following research questions: how much of the reported improvements in alignment perform"
2021.mtsummit-research.21,2015.iwslt-evaluation.11,0,0.0105525,"ents are denoted as English → Foreign, when English is the source language. As a preamble, we start with our data condition. 2.1 Datasets Our experiments consider multiple language pairs all having English on one side. Our training sets for French and German are made of sentences from Europarl (Koehn, 2005). For Romanian, we use both the NAACL 2003 corpus (Mihalcea and Pedersen, 2003) and the SETIMES corpus used in WMT’16 MT evaluation. For Czech, the parallel data from News Commentary V11 (Tiedemann, 2012) is considered, while we use the preprocessed parallel data for Vietnamese in IWSLT’15 (Luong and Manning, 2015) and the Japanese data from the KFTT (Neubig, 2011). Our evaluations use standard test sets whenever applicable: for French and Romanian, we use data from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003); the German test data is Europarl;2 for Czech we use the corpus designed by Mareˇcek (2016); the Japanese test data is from the KFTT and the test corpus for Vietnamese is generated from the EVBCorpus.3 As is custom when evaluating unsupervised alignments, we append the test set to the training corpus at training time, meaning that there is no unknown word in the reference alignm"
2021.mtsummit-research.21,W03-0301,0,0.262604,"els such as the prediction of null links, of many-to-one links, as well as the alignment of rare words. Detailed analyses are in (Ngo Ho, 2021). Asymmetric alignment models associate each source word with exactly one target word; such alignments are denoted as English → Foreign, when English is the source language. As a preamble, we start with our data condition. 2.1 Datasets Our experiments consider multiple language pairs all having English on one side. Our training sets for French and German are made of sentences from Europarl (Koehn, 2005). For Romanian, we use both the NAACL 2003 corpus (Mihalcea and Pedersen, 2003) and the SETIMES corpus used in WMT’16 MT evaluation. For Czech, the parallel data from News Commentary V11 (Tiedemann, 2012) is considered, while we use the preprocessed parallel data for Vietnamese in IWSLT’15 (Luong and Manning, 2015) and the Japanese data from the KFTT (Neubig, 2011). Our evaluations use standard test sets whenever applicable: for French and Romanian, we use data from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003); the German test data is Europarl;2 for Czech we use the corpus designed by Mareˇcek (2016); the Japanese test data is from the KFTT and the tes"
2021.mtsummit-research.21,2020.emnlp-main.41,0,0.0111046,"y translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete mode"
2021.mtsummit-research.21,P03-1021,0,0.222983,"raining data word vocab. Eng. For. ∼106K ∼112K ∼96K ∼311K ∼74K ∼115K ∼62K ∼147K ∼156K ∼126K ∼42K ∼19K Corpus En-Fr En-Ge En-Ro En-Cz En-Ja En-Vi # sent. pairs ∼1.7M ∼1.5M ∼250K ∼182K ∼377K ∼122K char. vocab. Eng. For. 111 115 218 235 124 131 246 157 ∼2K ∼5K 133 171 # sent. pairs 447 509 246 2 501 1 235 3 447 Test data # words Eng. For. 7 020 7 761 10 413 9 945 5 455 5 315 59 724 52 881 30 822 34 403 70 049 94 753 # non-null links 17 438 10 533 5 991 67 423 33 377 81 748 Table 1: Basic statistics for the training data and test data 2.2 Evaluation protocol We use the alignment error rate (AER) (Och, 2003), F-score (F1), precision and recall as measures of performance. AER is based on a comparison of predicted alignment links (A) with a human reference including sure (S) and possible (P) links, and is defined as an average of the recall and precision taking into account the sets P and S. AER is defined as: AER = 1 − |A ∩ S |+ |A ∩ P | |A |+ |S| (1) where A is the set of predicted alignments. Note that the English-Romanian, English-Japanese and English-Vietnamese reference data only contain “sure” links, meaning that for these languages pairs, AER and F-measure are deterministically related. 2.3"
2021.mtsummit-research.21,J03-1002,0,0.159516,"ctionaries, to train statistical machine translation (SMT) systems (Koehn, 2010), to filter out noise from translation memories (Pham et al., 2018) or in quality estimation applications (Specia et al., 2018). Word alignment can also serve to explain MT decisions (Stahlberg et al., 2018). Given pairs associating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2016) and improvements of the state of the art for multiple language pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al."
2021.mtsummit-research.21,D18-1328,1,0.842079,"ine translation systems. In this paper, we thoroughly study how this preprocessing step interacts with the word alignment task and propose several tokenization strategies to obtain well-segmented parallel corpora. Using these new techniques, we were able to improve baseline word-based alignment models for six language pairs. 1 Introduction Word alignment is a basic task in multilingual Natural Language Processing (NLP) and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation (SMT) systems (Koehn, 2010), to filter out noise from translation memories (Pham et al., 2018) or in quality estimation applications (Specia et al., 2018). Word alignment can also serve to explain MT decisions (Stahlberg et al., 2018). Given pairs associating a sentence in a source language and a translation in a target language, word alignment aims to identify translational equivalences at the level of individual word tokens and has been initially approached with generative probabilistic models learning alignment in an unsupervised manner (Och and Ney, 2003; Tiedemann, 2011). With rapid advances in neural based NLP, word alignment has recently regained some traction (Legrand et al., 2"
2021.mtsummit-research.21,P17-2095,0,0.0238775,"The implementation of SentencePiece10 reports a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias and that LM-based tokenization produces subword units that qualitatively align with morphology much better than those produced by BPE, suggesting that the l"
2021.mtsummit-research.21,E17-2060,0,0.0190762,"nal performance. The implementation of SentencePiece10 reports a large number of MT experiments aimed to compare BPE and unigram in multiple conditions, concluding that both yield comparable BLEU scores across the board when used with a fixed tokenization in words. The shortcomings of BPE/unigram segmentations have been the subject of several studies, reporting comparisons with (a) linguistic segmentations (Huck et al., 2017; Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Weller-Di Marco and Fraser, 2020) and (b) alternative preprocessing schemes such as character-based models (eg. in Sennrich (2017); Sajjad et al. (2017); Cherry et al. (2018)). Ding et al. (2019) conduct a systematic exploration considering a large numbers of vocabulary sizes to better understand its impact on NMT performance, comparing several NMT architectures such as shallow/deep-transformer, tiny/shallow/deep-LSTM. Bostrom and Durrett (2020) evaluate the impact of tokenization on language model pre-training. They conclude that tokenization encodes a surprising amount of inductive bias and that LM-based tokenization produces subword units that qualitatively align with morphology much better than those produced by BPE,"
2021.mtsummit-research.21,P16-1162,0,0.73201,"pairs have been reported thanks to neuralized generative models (Alkhouli and Ney, 2017; Alkhouli et al., 2018; Ngo-Ho and Yvon, 2019), pre-trained multilingual embeddings (Jalili Sabet et al., 2020; Nagata et al., 2020; Dou and Neubig, 2021) or more powerful architectures based on the Transformer translation model of Vaswani et al. (2017), as reported for instance by Garg et al. (2019); Chen et al. (2020) and Chen et al. (2021). In addition to using neural architectures, these new models differ from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in standard asymmetric models such as IBM-1 and IBM-4 (Liu et al., 2015; Tomeh et al., 2014; Wang and Lepage, 2016); (b) they also enable to actively manipulate the lengths of the Proceedings of the 18th Biennial Ma"
2021.mtsummit-research.21,tiedemann-2012-parallel,0,0.0198843,"2021). Asymmetric alignment models associate each source word with exactly one target word; such alignments are denoted as English → Foreign, when English is the source language. As a preamble, we start with our data condition. 2.1 Datasets Our experiments consider multiple language pairs all having English on one side. Our training sets for French and German are made of sentences from Europarl (Koehn, 2005). For Romanian, we use both the NAACL 2003 corpus (Mihalcea and Pedersen, 2003) and the SETIMES corpus used in WMT’16 MT evaluation. For Czech, the parallel data from News Commentary V11 (Tiedemann, 2012) is considered, while we use the preprocessed parallel data for Vietnamese in IWSLT’15 (Luong and Manning, 2015) and the Japanese data from the KFTT (Neubig, 2011). Our evaluations use standard test sets whenever applicable: for French and Romanian, we use data from the 2003 word alignment challenge (Mihalcea and Pedersen, 2003); the German test data is Europarl;2 for Czech we use the corpus designed by Mareˇcek (2016); the Japanese test data is from the KFTT and the test corpus for Vietnamese is generated from the EVBCorpus.3 As is custom when evaluating unsupervised alignments, we append the"
2021.mtsummit-research.21,Y16-2012,0,0.0145546,"er from past approaches in that they compute alignments based on a decomposition into subword units (Sennrich et al., 2016; Kudo, 2018), which makes it possible to easily accommodate open-ended vocabularies and mitigate issues related to the alignment of unknown words, which has always been a challenge for discrete models. Another interesting property of subword units in the context of word alignment is that (a) they ease the generation of many-to-one / one-to-many links, which are difficult to handle in standard asymmetric models such as IBM-1 and IBM-4 (Liu et al., 2015; Tomeh et al., 2014; Wang and Lepage, 2016); (b) they also enable to actively manipulate the lengths of the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 256 source and target sentences so as to make them more even, arguably a facilitating factor for alignment and translation models (Deguchi et al., 2020). In this work, we take a closer look at the interaction between alignment and subword tokenization and try to address the following research questions: how much of the reported improvements in alignment performance can be linked to subword splitting? whi"
2021.mtsummit-research.21,2020.acl-main.389,0,0.0723206,"Missing"
2021.tacl-1.2,W17-4712,0,0.0471642,"Missing"
2021.tacl-1.2,2012.eamt-1.60,0,0.0159778,"ing, hoping that the loss in performance with respect to the baseline (using correct domain tags) will remain small. 4.1 Data and Metrics We experiment with translation from English into French and use texts initially originating from six domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 (MED);3 the European Central Bank corpus (BANK) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus (LAW) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME, and PHP from Opus collection (Tiedemann, 2009), collectively merged in a IT-domain; TED Talks (TALK) (Cettolo et al., 2012); and the Koran (REL). Complementary experiments also use v12 of the News Commentary corpus (NEWS). Most corpora are available from the Opus Web site.4 These corpora were deduplicated and tokenized with inhouse tools; statistics are in Table 1. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encoding (Sennrich et al., 2016b) with 30,000 merge operations on a corpus containing all sentences in both languages. We randomly select in each corpus a development and a test set of 1,000 lines and keep the rest for training.5 Validation sets are used to chose the best"
2021.tacl-1.2,2010.amta-papers.16,0,0.0653994,"Missing"
2021.tacl-1.2,2016.amta-researchers.10,0,0.0119147,"s (Zeng et al., 2018; Pham et al., 2019). It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1)"
2021.tacl-1.2,D19-1165,0,0.126219,"Missing"
2021.tacl-1.2,C18-1111,0,0.136754,"Missing"
2021.tacl-1.2,N16-1101,0,0.0795562,"Missing"
2021.tacl-1.2,W17-4713,0,0.538849,"rangroup.com Abstract in particular (e.g., Daum´e III and Marcu, 2006; Blitzer, 2007; Jiang and Zhai, 2007). Various techniques thus exist to handle both the situations where a (small) training sample drawn from Dt is available in training, or where only samples of source-side (or target-side) sentences are available (see Foster and Kuhn [2007]; Bertoldi and Federico [2009]; Axelrod et al. [2011]; for proposals from the statistical MT era, or Chu and Wang [2018] for a recent survey of DA for Neural MT). A seemingly related problem is multi-domain (MD) machine translation (Sajjad et al., 2017; Farajian et al., 2017b; Kobus et al., 2017; Zeng et al., 2018; Pham et al., 2019) where one single system is trained and tested with data from multiple domains. MD machine translation (MDMT) corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: For instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when"
2021.tacl-1.2,Q17-1024,0,0.0772896,"Missing"
2021.tacl-1.2,D12-1119,0,0.0748605,"Missing"
2021.tacl-1.2,D17-1156,0,0.0558605,"Missing"
2021.tacl-1.2,N16-1005,0,0.177442,"re expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1). Multi-domain learning, as defined in Dredze and Cram"
2021.tacl-1.2,2020.ngt-1.5,0,0.0344781,"Missing"
2021.tacl-1.2,D18-1041,0,0.0888128,"aum´e III and Marcu, 2006; Blitzer, 2007; Jiang and Zhai, 2007). Various techniques thus exist to handle both the situations where a (small) training sample drawn from Dt is available in training, or where only samples of source-side (or target-side) sentences are available (see Foster and Kuhn [2007]; Bertoldi and Federico [2009]; Axelrod et al. [2011]; for proposals from the statistical MT era, or Chu and Wang [2018] for a recent survey of DA for Neural MT). A seemingly related problem is multi-domain (MD) machine translation (Sajjad et al., 2017; Farajian et al., 2017b; Kobus et al., 2017; Zeng et al., 2018; Pham et al., 2019) where one single system is trained and tested with data from multiple domains. MD machine translation (MDMT) corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: For instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when the test distribution is unknown? Shoul"
2021.tacl-1.2,C16-1170,0,0.0155586,"8; Pham et al., 2019). It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information. 2.1 Formalizing Multi-Domain Translation We conventionally define a domain d as a distribution Dd (x) over some feature space X that is shared across domains (Pan and Yang, 2010): In machine translation, X is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content (Chen et al., 2016; Zhang et al., 2016), register (Sennrich et al., 2016a), style (Niu et al., 2018), an so forth. Translation in domain d is formalized by a translation function hd (y |x) pairing sentences in a source language with sentences in a target language y ∈ Y. hd is usually assumed to be deterministic (hence y = hd (x)), but can differ from one domain to the other. A typical learning scenario in MT is to have access to samples from nd domains, which means that the training distribution Ds is a mixture  = 1 . . . nd } Ds (x) = d λsd Dd (x), with {λsd , d the corresponding mixture weights ( d λsd = 1). Multi-domain learni"
C08-1056,P06-2005,0,0.765033,"ng (NLP) point of view, these messages contain an abnormally high rate of out-of-vocabulary forms, and the ambiguity of existing word forms is aggravated, two factors that contribute to degrade the performance of natural language processing tools. Recovering a normalized orthography seems thus to be a necessary preprocessing step for many real-world NLP applications, such as text-to-speech, translation, or text mining applications (filtering, routing, information retrieval, etc). These short messages have so far received relatively little attention from the NLP community2 : see, for English, (Aw et al., 2006; Choudhury et al., 2007), which both address the problem with statistical learning techniques, and, for French, (Guimier de Neef et al., 2007), which details a complete pipe-line of hand-crafted, symbolic, modules. In fact, the problem of normalizing SMS shares a lot of commonalities with other NLP applications, and can be addressed from several viewpoints. The first, maybe the most natural angle, is to make an analogy with the spelling correction problem. This problem has been extensively studied in the past and a variety of statistical approaches are readily available, most notably the “noi"
C08-1056,P00-1037,0,0.335755,"lem with statistical learning techniques, and, for French, (Guimier de Neef et al., 2007), which details a complete pipe-line of hand-crafted, symbolic, modules. In fact, the problem of normalizing SMS shares a lot of commonalities with other NLP applications, and can be addressed from several viewpoints. The first, maybe the most natural angle, is to make an analogy with the spelling correction problem. This problem has been extensively studied in the past and a variety of statistical approaches are readily available, most notably the “noisy channel” approach (see eg. (Church and Gale, 1991; Brill and Moore, 2000; Toutanova and Moore, 2002)). An alternative metaphor is the translation metaphor: under this view, the normalization task is accomplished by taking the SMS 2 A couple of on-line SMS-to-English translation systems are accessible on the Internet, see notably http://www. transl8it.com/ and http://www.lingo2word. com/; “Netspeak” dictionaries, again for English, also abound. The situation is more or less comparable for French, see eg http://www.traducteur-sms.com/. language as a foreign language, and using standard (statistical) translation techniques. Both views have their own merit, and their"
C08-1056,J90-2002,0,0.532128,", the case for common abbreviations (eg. btw for by the way) and for instances of “consonantic” spellings. The dictionnary used in the experiments reported above contains about 4,200 entries. This module is implemented as a finitestate transducer E which transduces letter sequences in Λ∗ into mixed grapheme and phoneme sequences (in (Λ ∪ Π)∗ ). Two normalization systems 3.1 The MT-like system Our first normalization system is entirely based on open-source, public domain packages for statistical machine translation. Giza++ (Och and Ney, 2003) is used to induce, based on statistical principles (Brown et al., 1990), an automatic word alignment of SMS tokens with their normalized counterparts; Moses (Koehn et al., 2007) is used to learn the various parameters of the phrase-based model, to optimize the weight combination and to perform the translation using a multi-stack search algorithm; the SRI language model toolkit (Stolcke, 2002) is finally used to estimate statistical language models. For this system, the training set has been split in a learning set3 (approximately 25000 messages) and a development set (about 11700 messages), which is used to tune parameters. As suggested in the previous sections,"
C08-1056,fairon-paumier-2006-translated,0,0.0864503,"Ol/ (for Paul). Upon recognition of any such sequence, two transitions loop back to the initial state: one carries the input symbol ’#’, which is used whenever a word separator is encountered; the other is an ε transition, which allows to re-segment the input stream. 4 4.1 Experiments Experimental protocol The experiments reported below use two corpora. The first one has been collected at the University of Aix-en-Provence (Hocq, 2006); it contains approximately 9700 messages. The second corpus has been gathered in Belgium by the Catholic University of Louvain, and totals about 30000 messages (Fairon and Paumier, 2006). Both corpora contain, for each message, a reference normalization which has been produced and validated by human annotators. Both corpora were merged, lowercased, stripped from punctuation signs and standardized (in particular with respect to the anonymization conventions). This database was split in a training set (about 36700 messages) and a distinct test set of about 3000 messages. The training set was used both to train and tune the MT-like system and to estimate a 3-gram language model required in both approaches, using standard back-off procedures. Some relevant statistics regarding th"
C08-1056,2008.jeptalnrecital-long.13,1,0.776365,"Mohri and Riley, 1998). In addition to these four main modules, the preprocessing module of the ASR-like system contains a number of small enhancements that improve the normalization of dates and hours. We furthermore had to modify the processing of outof-vocabulary words: in the architecture sketched above, any word that does not belong to the vocabulary has to be decomposed into smaller, known, words, causing systematic errors. Our final ASRlike system allows these forms to be either decomposed phonetically or copied verbatim in the output. A complete description of this system is given in (Kobus et al., 2008). &lt;eps&gt;:&lt;eps&gt; O:&lt;eps&gt; 1 2 p:paul l:&lt;eps&gt; 3 _#:&lt;eps&gt; 0/0 _#:&lt;eps&gt; l:louis 4 w:&lt;eps&gt; 5 i:&lt;eps&gt; 6 &lt;eps&gt;:&lt;eps&gt; Figure 1: Transducing phone sequences into word sequences with a dictionary This simplistic inverted dictionary recognizes two phonemic sequences: /lwi/ (for Louis) and /pOl/ (for Paul). Upon recognition of any such sequence, two transitions loop back to the initial state: one carries the input symbol ’#’, which is used whenever a word separator is encountered; the other is an ε transition, which allows to re-segment the input stream. 4 4.1 Experiments Experimental protocol The experiment"
C08-1056,J03-1002,0,0.00184899,"mic content of the corresponding lexical item(s). This is, for instance, the case for common abbreviations (eg. btw for by the way) and for instances of “consonantic” spellings. The dictionnary used in the experiments reported above contains about 4,200 entries. This module is implemented as a finitestate transducer E which transduces letter sequences in Λ∗ into mixed grapheme and phoneme sequences (in (Λ ∪ Π)∗ ). Two normalization systems 3.1 The MT-like system Our first normalization system is entirely based on open-source, public domain packages for statistical machine translation. Giza++ (Och and Ney, 2003) is used to induce, based on statistical principles (Brown et al., 1990), an automatic word alignment of SMS tokens with their normalized counterparts; Moses (Koehn et al., 2007) is used to learn the various parameters of the phrase-based model, to optimize the weight combination and to perform the translation using a multi-stack search algorithm; the SRI language model toolkit (Stolcke, 2002) is finally used to estimate statistical language models. For this system, the training set has been split in a learning set3 (approximately 25000 messages) and a development set (about 11700 messages), w"
C08-1056,2001.mtsummit-papers.68,0,0.0259089,"translation tools incorporate mechanisms to model the possible mismatch in word order between source and target, which are virtually nonexisting when it comes to translating SMS. This metaphor is, nonetheless, the one resorted to in (Aw et al., 2006), which uses a statistical phrase-based machine translation tool to convert English SMS texts into standardized English. This system incorporates some of the peculiarities of this translation task, which both simplifies the construction of the phrase-table and the decoding search algorithm. Using this system, (Aw et al., 2006) reports a 0.81 BLEU (Papineni et al., 2001) score on a set of 5,000 English SMS. Normalization as translation is certainly a natural, and simple to implement, idea. Using phrase-based systems, it becomes possible to model (context-dependant) one-to-many relationships that are out-of-reach of the spell checking approach. We feel that it still overlooks some aspects of the task, notably the fact that the lexical creativity attested in SMS messages can hardly be captured in a static phrase table, where correspondences between SMS phrases and normalized phrases are learned by rote, rather than modeled. 2.3 The &quot;speech recognition&quot; metaphor"
C08-1056,P02-1019,0,0.707037,"arning techniques, and, for French, (Guimier de Neef et al., 2007), which details a complete pipe-line of hand-crafted, symbolic, modules. In fact, the problem of normalizing SMS shares a lot of commonalities with other NLP applications, and can be addressed from several viewpoints. The first, maybe the most natural angle, is to make an analogy with the spelling correction problem. This problem has been extensively studied in the past and a variety of statistical approaches are readily available, most notably the “noisy channel” approach (see eg. (Church and Gale, 1991; Brill and Moore, 2000; Toutanova and Moore, 2002)). An alternative metaphor is the translation metaphor: under this view, the normalization task is accomplished by taking the SMS 2 A couple of on-line SMS-to-English translation systems are accessible on the Internet, see notably http://www. transl8it.com/ and http://www.lingo2word. com/; “Netspeak” dictionaries, again for English, also abound. The situation is more or less comparable for French, see eg http://www.traducteur-sms.com/. language as a foreign language, and using standard (statistical) translation techniques. Both views have their own merit, and their limitations, which we shall"
C08-1056,P02-1040,0,\N,Missing
C08-1056,P07-2045,0,\N,Missing
C08-1075,N06-1060,0,0.0210647,"Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Now at LIPN - Univ. Paris 13 & UMR CNRS 7030. 2 Which is essential in the Web People Search task. Olivier Capp´e Institut T´el´ecom ParisTech & LTCI CNRS cappe@enst.fr There are different ways to tackle the problem of NE matching: the first and certainly most reliable one consists in studying the specific features of the data, and then use any available tool to design a specialized method for the matching task. This approach will generally take advantage of language-specific (e.g. in (Freeman et al., 2006)) and domain-specific knowledge, of any external resources (e.g. database, names dictionaries, etc.), and of any information about the entities to process, e.g. their type (person name, organization, etc.), or internal structure (e.g. in (Prager et al., 2007)). In such an in-depth approach, supervised learning is helpful: it has been used for example in a database context3 in (Bilenko et al., 2003), but this approach requires labeled data which is usually costly. All those data specific appproaches would necessitate some sort of human expertise. The second approach is the robust one: we propos"
C08-2013,2007.mtsummit-papers.19,0,0.304317,"y as z is to t”, in a sense to be specified. See (Lepage, 1998) or (Stroppa and Yvon, 2005) for possible interpretations. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. If the principle of analogical learning is quite simple, it does involve complex steps that seriously limit its applicability. As a matter of fact, we are only aware of studies where analogical learning is applied to restricted tasks, either because they arbitrarily concentrate on words (Stroppa and Yvon, 2005; Langlais and Patry, 2007; Denoual, 2007) or because they focus on limited data (Lepage and Denoual, 2005; Denoual, 2007). In this study, we investigate different strategies for making step 1 of analogical learning tractable. We propose a data"
C08-2013,D07-1092,1,0.893605,"ance yvon@limsi.fr where [x : y = z : t] denotes an analogical proportion, that is a relation between these four items, meaning that “x is to y as z is to t”, in a sense to be specified. See (Lepage, 1998) or (Stroppa and Yvon, 2005) for possible interpretations. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. If the principle of analogical learning is quite simple, it does involve complex steps that seriously limit its applicability. As a matter of fact, we are only aware of studies where analogical learning is applied to restricted tasks, either because they arbitrarily concentrate on words (Stroppa and Yvon, 2005; Langlais and Patry, 2007; Denoual, 2007) or because they focus on limited data (Lepage and Denoual, 2005; Denoua"
C08-2013,P98-1120,0,0.698271,"e diagonal terms share some n-grams reminiscent of the number (This/These) and tense (drink /drank ) commutations involved. We thus propose a sampling strategy (hereafter EV ) which selects x-forms that share with t some sequences of characters. To this end, input forms are represented in a vector space whose dimensions are frequent character n-grams, retaining the k-most frequent n-grams, where n ∈ [min; max]. A form is thus encoded as a binary vector of Exhaustive tree-count search The strategy we propose here exploits a property on character counts that an analogical relation must fulfill (Lepage, 1998): [x : y = z : t] ⇒ |x|c + |t|c = |y|c + |z|c ∀c ∈ A where A is the alphabet on which the forms are built, and |x|c stands for the number of occurrences of character c in x. In the sequel, we denote C(hx, ti) = {hy, zi ∈ I 2 : |x|c + |t|c = |y|c + |z|c ∀c ∈ A} the set of pairs that satisfy the count property with respect to hx, ti . The strategy we propose consists in first selecting an x-form in the input space. This enforces a set of necessary constraints on the counts of characters that any two forms y and z must satisfy for [x : y = z : t] to be true. By considering all forms x in turn,2 w"
C08-2013,W05-0616,1,0.845567,"6128, Qu´ebec, H3C3J7, Canada felipe@iro.umontreal.ca Franc¸ois Yvon Univ. Paris Sud 11 & LIMSI-CNRS F-91401 Orsay, France yvon@limsi.fr where [x : y = z : t] denotes an analogical proportion, that is a relation between these four items, meaning that “x is to y as z is to t”, in a sense to be specified. See (Lepage, 1998) or (Stroppa and Yvon, 2005) for possible interpretations. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. If the principle of analogical learning is quite simple, it does involve complex steps that seriously limit its applicability. As a matter of fact, we are only aware of studies where analogical learning is applied to restricted tasks, either because they arbitrarily concentrate on words (Stroppa and Yvon"
C08-2013,C98-1116,0,\N,Missing
C10-1027,W09-0432,0,0.0225697,"ion system for 233 the pair A → B, for which the parallel data is sparse; assuming further that such parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other"
C10-1027,W08-0309,0,0.0145645,"ount all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the"
C10-1027,P96-1041,0,0.0252501,"am model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/projects/ srilm 235 5 models were trained on the target side of the bitext corpora. After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit6 is used to compute the word alignments. Then, the grow-diag-final-and heuristic is used to obtain the final alignments from which translation units are extr"
C10-1027,chen-etal-2008-improving,0,0.146052,"n the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effective implementation of multisource translation to date however consists in using mono-source system combination techniques (Schroeder et al., 2009). Translation through pivoting The use of auxiliary systems has also been proposed in another common situation, as a possible remedy t"
C10-1027,P07-1092,0,0.149323,"parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other works, we propose to make use of several MT systems (of any type) to improve translation perfor"
C10-1027,P03-2017,1,0.625206,"used as auxiliary information for the decoding of the direct system. Configurations 4 and 5 are instances of multisource translation, where a paraphrase or a translation of the source text is available. Lastly, configuration 6 illustrates the case where a human translator, with knowledge of the target language and at least of one of the available source languages, could influence the decoding by providing desired3 words (e.g. only for source words or phrases that would be judged difficult to translate). This human supervision through a feedback text in real time is similar to the proposal of (Dymetman et al., 2003). Given this framework, several questions arise, 3 The proposal as it is limits the hypotheses produced by the system to those that are attainable given its training data. It is conceivable, however, to find ways of introducing new knowledge in this framework. 234 the most important underlying this work being whether the performance of SMT systems can be improved by using other SMT systems. Another point of interest is whether improvements made to auxiliary systems can yield improvement to the direct system, without the latter undergoing any modification. 2.3 Furthermore, we are interested in"
C10-1027,2008.amta-srw.3,0,0.0297833,"iew here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on t"
C10-1027,N04-1022,0,0.043046,"has been implemented in many different ways which we briefly review here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by"
C10-1027,J06-4004,1,0.865906,"Missing"
C10-1027,max-etal-2010-contrastive,1,0.929341,"nerally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from external sources is used to boost lexical choices and guide the decoder into making more informed choices.1 1 We performed initial experiments where the complementary information was exploited during n-best list reranking (Max et al., 2010), but except for the multisource condition the list of hypotheses contained too little useful variation 232 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232–240, Beijing, August 2010 Complementary sources can be of different nature: they can involve other automatic systems (for the same or different language pairs) and/or human knowledge. Furthermore, complementary information is injected at the lexical level, thus making targeted fine-grained lexical predictions useful. Importantly, those predictions are exploited at the sentence level2 ,"
C10-1027,P04-1063,0,0.0322173,"slations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effecti"
C10-1027,2001.mtsummit-papers.46,0,0.187357,"the former and syntactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of"
C10-1027,N07-1061,0,0.303217,"t and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which informa"
C10-1027,vilar-etal-2006-error,0,0.0375447,"Missing"
C10-1027,P07-1108,0,0.039341,"ses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from ext"
C10-1027,N07-1029,0,0.0453162,"Missing"
C10-1027,E09-1082,0,0.137318,"., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the d"
C10-1027,2008.amta-srw.6,0,0.0501316,"tactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for"
C10-1027,2009.mtsummit-papers.14,0,0.0186901,"on hypotheses of a sentence in the target language, we can then boost the likeliness of the words and phrases occurring in these hypotheses by deriving an auxiliary language model for each test sentence. This allows us to integrate this auxiliary information during the search and thus provides a tighter integration with the direct system. This idea has successfully been used in speech recognition, using for instance close captions (Placeway and Lafferty, 1996) or an imperfect translation (Paulik et al., 2005) to provide auxiliary in-domain adaptation data for the recognizer’s language model. (Simard and Isabelle, 2009) proposed a similar approach in Machine Translation in which they use the target-side of an exact match in a translation memory to build language models on a per sentence basis used in their decoder. This strategy can be implemented in a straightforward manner, by simply training a language model using the n-best list as an adaptation corpus. Being automatically generated, hypotheses in the n-best list are not entirely reliable: in particular, they may contain very unlikely target sequences at the junction of two segments. It is however straightforward to filter these out using the available p"
C10-1027,N04-4026,0,0.0131259,"sh system for lexical boosting via triangulation through Spanish 3 Experiments and results 3.1 Translation engine In this study, we used our own machine translation engine, which implements the n-grambased approach to statistical machine translation (Mariño et al., 2006). The translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs. In addition to a bilingual n-gram model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/"
C10-1027,D08-1065,0,0.0285168,"An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores."
C10-2023,C04-1073,0,0.0519156,"ngle words are reordered within a relatively small window distance. It consist of the easiest case as typically, the use of phrases (in the sense of translation units of the phrase-based approach to SMT) is believed to adequately perform such reorderings. Mid-range reorderings involve reorderings between two or more phrases (translation units) which are closely positioned, typically within a window of about 6 words. Many alternatives have been proposed to tackle midrange reorderings through the introduction of linguistic information in MT systems. To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. They automatically build a set of linguistically grounded rewrite rules, aimed at reordering the source sentence so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder t"
C10-2023,E09-1043,0,0.0859444,"llow the above idea of making the ordering of the source sentence similar to the target sentence before decoding (Niehues and Kolss, 2009), long-range reorderings are typically better addressed by syntax-based and hierarchical (Chiang, 2007) models. In (Zollmann et al., 2008), an interesting comparison between phrase-based, hierarchical and syntax-augmented models is carried out, concluding that hierarchical and syntaxbased models slightly outperform phrase-based models under large data conditions and for sufficiently non-monotonic language pairs. Encouraged by the work reported in (Hoang and Koehn, 2009), we tackle the mid-range reordering problem in SMT by introducing a ngram language model of bilingual units built from POS information. The rationale behind such a model is double: on the one hand we aim at introducing morpho-syntactic information into the reordering model, as we believe it plays an important role for predicting systematic word ordering differences between language pairs; at the same time that it drastically reduces the sparseness problem of standard translation units built from surface forms. On the other hand, n-gram language modeling is a robust approach, that enables to a"
C10-2023,P02-1040,0,0.0786557,"rtion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. All language models used in this work are estimated using the SRI language modeling toolkit4 . According to our experience, KneserNey smoothing (Kneser and Ney, 1995) and interpolation of lower and higher n-grams options are used as they typically achieve the best performance. Optimization work is carried out by means of the widely used MERT toolkit5 which has been slightly modified to perform optimizations embedding our decoder. The BLEU (Papineni et al., 2002) score is used as objective function for MERT and to evaluate test performance. 4.3 Reordering in German-English and French-English Translation Two factors are found to greatly impact the overall translation performance: the morphological mismatch between languages, and their reordering needs. The vocabulary size is strongly influenced by the number of word forms for number, case, tense, mood, etc., while reordering needs refer to the difference in their syntactic structure. In this work, we are primarily interested on the reordering needs of each language pair. Figure 3 displays a quantitativ"
C10-2023,C08-1144,0,0.0514727,"Missing"
C10-2023,N04-4026,0,0.591269,"ing bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at extending the scope of their rewrite rules, encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for each translation unit it estimates the probability of being translated monotone, swapped or placed discontiguous with respect to its previous translation unit. Integrated within the Moses (Koehn, et al 2007) decoder, the model achieves state-ofthe-art results for many translation tasks. One of the main reasons that explains the success of the model is that it considers information of the source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, lon"
C10-2023,popovic-ney-2006-pos,0,0.0407673,"ngs between two or more phrases (translation units) which are closely positioned, typically within a window of about 6 words. Many alternatives have been proposed to tackle midrange reorderings through the introduction of linguistic information in MT systems. To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. They automatically build a set of linguistically grounded rewrite rules, aimed at reordering the source sentence so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder to decide for the best word order hypothesis. Word lattices are built by means of rewrite rules operating on POS tags; such rules are automatically extracted from the training bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at exten"
C10-2023,P05-1066,0,0.150159,"Missing"
C10-2023,W09-0435,0,0.0118598,"e source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, long-range reorderings imply reorderings in the structure of the sentence. Such reorderings are necessary to model the translation for pairs like Arabic-English, as English typically follows the SVO order, while Arabic sentences have different structures. Even if several attempts exist which follow the above idea of making the ordering of the source sentence similar to the target sentence before decoding (Niehues and Kolss, 2009), long-range reorderings are typically better addressed by syntax-based and hierarchical (Chiang, 2007) models. In (Zollmann et al., 2008), an interesting comparison between phrase-based, hierarchical and syntax-augmented models is carried out, concluding that hierarchical and syntaxbased models slightly outperform phrase-based models under large data conditions and for sufficiently non-monotonic language pairs. Encouraged by the work reported in (Hoang and Koehn, 2009), we tackle the mid-range reordering problem in SMT by introducing a ngram language model of bilingual units built from POS in"
C10-2023,P07-2045,0,0.0078003,"network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for each translation unit it estimates the probability of being translated monotone, swapped or placed discontiguous with respect to its previous translation unit. Integrated within the Moses (Koehn, et al 2007) decoder, the model achieves state-ofthe-art results for many translation tasks. One of the main reasons that explains the success of the model is that it considers information of the source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words. Finally, long-range reorderings imply reorderings in the structure of the sentence. Such reorderings are necessary to model the translation for pairs like Arabic-English, as English typically follows the SVO order, while Arabic sentence"
C10-2023,2007.iwslt-1.3,0,0.0187649,"so as to match the word order of the target side. Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of hand-crafted rules for GermanEnglish translation. (Crego and Mari˜no, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder to decide for the best word order hypothesis. Word lattices are built by means of rewrite rules operating on POS tags; such rules are automatically extracted from the training bi-text. (Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at extending the scope of their rewrite rules, encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder. These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses. However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time. Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004). Basically, for e"
C10-2023,C08-1098,0,0.0139348,"olution (a single POStagged version of each tuple). The training corpus composed of tagged units out of which our new model is estimated is accordingly modified to contain only those tagged units considered in decoding. Note that most of the ambiguity present in word tagging is resolved by the fact that translation units may contain multiple source and target side words. 4 French, German and English Part-of-speech tags are computed by means of the TreeTagger 1 toolkit. Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.). Lang. Sent. Train French 1.75 M English 1.75 M Tune French 2, 051 English 2, 051 Test French 2, 525 English 2, 525 Train German 1, 61 M English 1, 61 M Tune German 2, 051 English 2, 051 Test German 2, 525 English 2, 525 Evaluation Framework In this section, we perform evaluation experiments of our novel reordering model. First, we give details of the corpora and baseline system employed in our experiments and analyze the reordering needs of the translation tasks, FrenchEng"
C10-2023,P00-1056,0,0.259913,"Missing"
C10-2023,2005.iwslt-1.8,0,0.0403537,"− − − − 55.3 k 8, 957 1, 282 49.2 k 8, 359 1, 344 1 1 72.8 k 10, 832 1, 749 65.1 k 9, 568 1, 724 1 1 42.2 M 381 k 44.2 M 137 k − − − − 47, 8 k 10, 994 2, 153 49, 2 k 8, 359 1, 491 1 1 62, 8 k 12, 856 2, 704 65, 1 k 9, 568 1, 810 1 1 Table 1: Statistics for the training, tune and test data sets. 4.2 System Details After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit3 is used to compute the word alignments. Then, the grow-diag-final-and (Koehn et al., 2005) heuristic is used to obtain the alignments from which tuples are extracted. In addition to the tuple n-gram translation model, our SMT system implements six additional feature functions which are linearly com201 1 www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger www.ims.uni-stuttgart.de/projekte/corplex/RFTagger 3 http://www.fjoch.com/GIZA++.html 2 bined following a discriminative modeling framework (Och and Ney, 2002): a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models c"
C10-2023,P02-1038,0,0.0378209,"directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit3 is used to compute the word alignments. Then, the grow-diag-final-and (Koehn et al., 2005) heuristic is used to obtain the alignments from which tuples are extracted. In addition to the tuple n-gram translation model, our SMT system implements six additional feature functions which are linearly com201 1 www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger www.ims.uni-stuttgart.de/projekte/corplex/RFTagger 3 http://www.fjoch.com/GIZA++.html 2 bined following a discriminative modeling framework (Och and Ney, 2002): a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. All language models used in this work are estimated using the SRI language modeling toolkit4 . According to our experience, KneserNey smoothing (Kneser and Ney, 1995) and interpolation of lower and higher n-grams"
C10-2023,J06-4004,1,\N,Missing
C10-2023,J07-2003,0,\N,Missing
C16-1012,C16-1012,1,0.0512563,"Missing"
C16-1012,P10-1131,0,0.215117,"Missing"
C16-1012,D11-1005,0,0.151322,"Missing"
C16-1012,P04-1015,0,0.215986,"Missing"
C16-1012,W07-0414,0,0.210204,"Missing"
C16-1012,N13-1014,0,0.0997753,"Missing"
C16-1012,C12-1059,0,0.0860819,"Missing"
C16-1012,H05-1021,0,0.0965053,"Missing"
C16-1012,N16-1121,1,0.885879,"Missing"
C16-1012,E09-1061,0,0.179527,"Missing"
C16-1012,P13-2017,0,0.0982958,"Missing"
C16-1012,P12-1066,0,0.234096,"Missing"
C16-1012,L16-1262,0,0.0602675,"Missing"
C16-1012,J08-4003,0,0.179659,"Missing"
C16-1012,petrov-etal-2012-universal,0,0.16977,"Missing"
C16-1012,P15-2040,0,0.240982,"Missing"
C16-1012,P11-2120,0,0.20281,"Missing"
C16-1012,W14-1614,0,0.058532,"Missing"
C16-1012,N01-1026,0,0.563907,"Missing"
C16-1012,I08-3008,0,0.362763,"Missing"
C16-1012,P11-2033,0,0.0717267,"Missing"
C16-1142,2012.eamt-1.33,0,0.0940354,"Missing"
C16-1142,J93-2003,0,0.0943225,"then repeat the same argument with the arc labeled fk . It is also routine to check that all possible grammatical compressions can be obtained in this way. Transitions (qk , fl , w, ql , ), with l > k + 1, are weighted according to a score w aggregating: • SLM - a 2-gram language model (LM) score : SLM (fl |fk ) = log P (fl |fk ). The generalization to higher-order n-grams is straightforward. In our implementation, we have used a POSbased LM, as we believe it will provide a better generalization than a word-based LM; • SIBM (fl |e0 ) - the posterior log-probability of fl in the IBM model 1 of Brown et al. (1993): 0 J X  1 SIBM (fl |e ) = log t(fl |e0j ) . 0 (J + 1) 0 j=1 These scores are summed along a path and yield the total IBM score: IBM1 (f0 |e0 ). 1504 (1) • Sali (fl ) - this score approximates the contribution of fl to the posterior log-probability of the IBM1 (e0 |f) at the sentence level: 0 Sali (fl ) = J X 1{fl = argmax t(e0i |f )}t(e0i |fl ), f i=1 (2) where 1{T } is the indicator function for predicate T . This approximation is required due to the impossibility to decompose the inverse IBM model 1 score over the arcs of Af . The use of IBM model 1 scores is meant to ensure the preservati"
C16-1142,D14-1082,0,0.0123705,"ided by the following questions: (1) Does the proposed bilingual compression methodology ensure the resulting corpus parallelism (compressed target text succeeds in preserving the meaning of the input compressed source text)? (2) Which of the proposed bilingual compression methods is more efficient? 3.1 Metrics To answer the above questions, we compared the results of our bilingual methods DPbi and ILPbi to the results of our monolingual methods DPmono and ILPmono (baselines) using the compression rate (CompR) estimation, F-score metric for the relations in grammatical dependency parse trees (Chen and Manning, 2014), as well as the standard MT metric BLEU (Papineni et al., 2002; Clarke and Lapata, 2008; Napoles et al., 2011b). F-score measures how much meaning is preserved in the compression using the grammatical-functional information, BLEU measures the fluency of the produced compressions. We also computed the confidence score (CS) of the parallelism between the produced compressions and the compressed source. We used a Logistic Regression model trained on the parallel sentences extracted from a corpus of manual alignments. The model exploits such features as the length difference ratio, IBM model 1 sc"
C16-1142,C08-1018,0,0.222553,"r NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motivations that have been put forward for monolingual sentence compression can be also used to motivate bilingual sentence compression, understood here as the generation of parallel compressions of parallel sentences. Bilingual Sentence Compression can be used, for instance, to produce simpler versions of a parallel text for learning purposes, or to generate summaries and subtitles in different languages, or even to build simplified parallel corpora for training a Machine Translation (MT) system. Parallel sentence compression can be approached in ma"
C16-1142,D13-1155,0,0.114784,"ure based on dynamic programming techniques. More recent DP solutions to the sentence compression problem use neural network architectures (Filippova et al., 2015). The ILP approach to compression was introduced by Clarke and Lapata (2008). The main motivation was the necessity to take global features into account (e.g., the constraint to have at least one verb in the compressed sentences). This approach has been widely reused in research related to text compression with various modifications to syntactic and informativeness scores used by Clarke and Lapata (2008) (see also (Wei et al., 2015; Filippova and Altun, 2013)). In our bilingual framework we compare the performance of DP and ILP approaches. As far as we know this is the first attempt to create compressed parallel bitext in asymmetrical setting. A closely related work is that of Aziz et al. (2012), who also exploits bilingual information. The authors propose a PBSMT solution for joint translation and compression of subtitles, which dynamically decides where it is necessary to impose a space/time constraint on the translated text. 5 Conclusions In this paper we consider sentence compression in a bilingual setting. We adopt an asymmetrical view to the"
C16-1142,D15-1042,0,0.285365,"h for f0 = f10 , f20 , . . . , fI0 0 that translates e0 . f0 should both preserve the meaning of e0 and respect the grammaticality requirements of the target language. Most approaches to monolingual compression further assume that a (dependency) parse tree of f is available, taking the form of a set of (dependent, head) pairs. We make the same assumption here, denoting τ = {(fi , h(fi )), 1 ≤ i ≤ I} this dependency tree. Recent proposals for solving this task use dynamic programming (DP) techniques (McDonald, 2006; Filippova, 2010) or integer linear programming (ILP) (Clarke and Lapata, 2008; Filippova et al., 2015), in both cases actively taking syntactic information into account. Inspired by those approaches, we propose below two methods for bilingual compression, enriched with MT-related bilingual information. We also include a description of our baseline compression system, based on two independent monolingual compressions. 2.1 Compressing with Finite-State Machines and Dynamic Programming (DPbi) Our first approach to compression (DPbi) uses finite-state techniques. Recall that a weighted finite-state automaton (WFSA) over a set of weights K is represented by a 7-tuple A = (Σ, Q, B, F, E, λ, ρ), wher"
C16-1142,C10-1037,0,0.245043,"1 , e02 , . . . , e0J 0 and its translation f = f1 , f2 , . . . , fI , we search for f0 = f10 , f20 , . . . , fI0 0 that translates e0 . f0 should both preserve the meaning of e0 and respect the grammaticality requirements of the target language. Most approaches to monolingual compression further assume that a (dependency) parse tree of f is available, taking the form of a set of (dependent, head) pairs. We make the same assumption here, denoting τ = {(fi , h(fi )), 1 ≤ i ≤ I} this dependency tree. Recent proposals for solving this task use dynamic programming (DP) techniques (McDonald, 2006; Filippova, 2010) or integer linear programming (ILP) (Clarke and Lapata, 2008; Filippova et al., 2015), in both cases actively taking syntactic information into account. Inspired by those approaches, we propose below two methods for bilingual compression, enriched with MT-related bilingual information. We also include a description of our baseline compression system, based on two independent monolingual compressions. 2.1 Compressing with Finite-State Machines and Dynamic Programming (DPbi) Our first approach to compression (DPbi) uses finite-state techniques. Recall that a weighted finite-state automaton (WFS"
C16-1142,W08-0509,0,0.0710515,"Missing"
C16-1142,N09-2045,0,0.0179497,"is contrasted on a parallel corpus of News articles. 1 Introduction Text simplification is a well studied application of Natural Language Processing (NLP) techniques. Its main goal is to reduce the complexity of a text without degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motiv"
C16-1142,W13-2902,0,0.0254975,"ocessing (NLP) tasks. We compare two ways to take bilingual information into account when compressing parallel sentences. Their efficiency is contrasted on a parallel corpus of News articles. 1 Introduction Text simplification is a well studied application of Natural Language Processing (NLP) techniques. Its main goal is to reduce the complexity of a text without degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lex"
C16-1142,W15-3016,1,0.891652,"Missing"
C16-1142,E06-1038,0,0.244918,"version e0 = e01 , e02 , . . . , e0J 0 and its translation f = f1 , f2 , . . . , fI , we search for f0 = f10 , f20 , . . . , fI0 0 that translates e0 . f0 should both preserve the meaning of e0 and respect the grammaticality requirements of the target language. Most approaches to monolingual compression further assume that a (dependency) parse tree of f is available, taking the form of a set of (dependent, head) pairs. We make the same assumption here, denoting τ = {(fi , h(fi )), 1 ≤ i ≤ I} this dependency tree. Recent proposals for solving this task use dynamic programming (DP) techniques (McDonald, 2006; Filippova, 2010) or integer linear programming (ILP) (Clarke and Lapata, 2008; Filippova et al., 2015), in both cases actively taking syntactic information into account. Inspired by those approaches, we propose below two methods for bilingual compression, enriched with MT-related bilingual information. We also include a description of our baseline compression system, based on two independent monolingual compressions. 2.1 Compressing with Finite-State Machines and Dynamic Programming (DPbi) Our first approach to compression (DPbi) uses finite-state techniques. Recall that a weighted finite-st"
C16-1142,J97-2003,0,0.224498,"mpression system, based on two independent monolingual compressions. 2.1 Compressing with Finite-State Machines and Dynamic Programming (DPbi) Our first approach to compression (DPbi) uses finite-state techniques. Recall that a weighted finite-state automaton (WFSA) over a set of weights K is represented by a 7-tuple A = (Σ, Q, B, F, E, λ, ρ), where Σ is a finite alphabet, Q is a finite set of states, B ⊆ Q contains the initial states and F ⊆ Q the final states; E ⊆ Q × Σ × K × Q is a set of weighted transitions, λ : B → K and ρ : F → K are respectively the initial and final weight functions (Mohri, 1997). Given f, the search space for compression is built as follows: assuming f conventionally starts (respectively ends) with <s> at index f0 (resp. </s> at index fI+1 ), we first build the standard automaton Af for f, the states of which correspond to the prefixes of f. We then add “skip” transitions (qk , fl , w, ql , ), ∀l > k+1. In this step, we make sure to preserve the syntactic dependency relationships so as to ensure that the subgraph induced by words in the compression is a subtree of the complete dependency tree. To this end, skip transitions (qk , fl , w, ql , ) are created subject to"
C16-1142,W11-1610,0,0.0560515,"Missing"
C16-1142,W11-1611,0,0.0545503,"Missing"
C16-1142,N16-3013,0,0.0289958,"an opposition party.’ Table 4: Examples of ILPbi and DPbi compressions 4 Related Work The deletion-based compression problem has been studied using a series of modeling paradigms. We mention first the work of Knight and Marcu (2002), who use the noisy channel model. This approach aims to maximize P (f0 |f) ∝ P (f0 )P (f|f0 ), where P (f0 ) is the source model, and P (f|f0 ) models the syntactic parse tree probability of the long sentence being an expansion of the compressed one. The noisy channel model is also used by approaches that consider compression as a monolingual translation problem (Napoles et al., 2016). McDonald (2006) formulates the problem as a binary sequence labeling problem with a rich syntactic feature set, and proposes a solving procedure based on dynamic programming techniques. More recent DP solutions to the sentence compression problem use neural network architectures (Filippova et al., 2015). The ILP approach to compression was introduced by Clarke and Lapata (2008). The main motivation was the necessity to take global features into account (e.g., the constraint to have at least one verb in the compressed sentences). This approach has been widely reused in research related to tex"
C16-1142,W13-4813,0,0.0250842,"degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motivations that have been put forward for monolingual sentence compression can be also used to motivate bilingual sentence compression, understood here as the generation of parallel compressions of parallel sentences. Bilingual Se"
C16-1142,L16-1491,0,0.0240937,"hniques. Its main goal is to reduce the complexity of a text without degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motivations that have been put forward for monolingual sentence compression can be also used to motivate bilingual sentence compression, understood here as the gen"
C16-1142,P02-1040,0,0.0996641,"compression methodology ensure the resulting corpus parallelism (compressed target text succeeds in preserving the meaning of the input compressed source text)? (2) Which of the proposed bilingual compression methods is more efficient? 3.1 Metrics To answer the above questions, we compared the results of our bilingual methods DPbi and ILPbi to the results of our monolingual methods DPmono and ILPmono (baselines) using the compression rate (CompR) estimation, F-score metric for the relations in grammatical dependency parse trees (Chen and Manning, 2014), as well as the standard MT metric BLEU (Papineni et al., 2002; Clarke and Lapata, 2008; Napoles et al., 2011b). F-score measures how much meaning is preserved in the compression using the grammatical-functional information, BLEU measures the fluency of the produced compressions. We also computed the confidence score (CS) of the parallelism between the produced compressions and the compressed source. We used a Logistic Regression model trained on the parallel sentences extracted from a corpus of manual alignments. The model exploits such features as the length difference ratio, IBM model 1 scores, the cosine similarity of the distributional representatio"
C16-1142,W11-2802,0,0.0249153,"complexity of a text without degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motivations that have been put forward for monolingual sentence compression can be also used to motivate bilingual sentence compression, understood here as the generation of parallel compression"
C16-1142,P08-1040,0,0.0346425,"1 Introduction Text simplification is a well studied application of Natural Language Processing (NLP) techniques. Its main goal is to reduce the complexity of a text without degrading the informational content. This task proves useful for a wide range of applications, be they human-oriented (e.g. text adaptation for language learning purposes, for people with reading disabilities etc. (Siddharthan, 2014; Klaper et al., 2013)) or machine-oriented, serving as a basis to improve the efficiency of other NLP downstream components (e.g., parsing (Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) etc.). Simplification can be performed at different linguistic levels: lexical (Paetzold and Specia, 2016), syntactic (Siddharthan, 2011), or both (Paetzold and Specia, 2013). Sentence compression is a way to perform simplification at the level of sentences, by reducing the sentence length without sacrificing important information. Many works only consider purely syntactic simplifications, though lexical changes are also possible, especially in language learning scenarios (Cohn and Lapata, 2008; Napoles et al., 2011a). By and large, the motivations that have been put forward for monolingual s"
C16-1142,P15-2009,0,0.0165636,"s a solving procedure based on dynamic programming techniques. More recent DP solutions to the sentence compression problem use neural network architectures (Filippova et al., 2015). The ILP approach to compression was introduced by Clarke and Lapata (2008). The main motivation was the necessity to take global features into account (e.g., the constraint to have at least one verb in the compressed sentences). This approach has been widely reused in research related to text compression with various modifications to syntactic and informativeness scores used by Clarke and Lapata (2008) (see also (Wei et al., 2015; Filippova and Altun, 2013)). In our bilingual framework we compare the performance of DP and ILP approaches. As far as we know this is the first attempt to create compressed parallel bitext in asymmetrical setting. A closely related work is that of Aziz et al. (2012), who also exploits bilingual information. The authors propose a PBSMT solution for joint translation and compression of subtitles, which dynamically decides where it is necessary to impose a space/time constraint on the translated text. 5 Conclusions In this paper we consider sentence compression in a bilingual setting. We adopt"
C18-1270,L16-1241,1,0.851785,"sing the whole source treebank but only 10 target sentences. In the following, this strategy is referred to as KL-B EAM. Composite scores Similarly to what has been done for monolingual parsers, we can express the performance of KL-B EAM in terms of simple-only UAS and complex-only UAS (using the partition computed monolingually with B EAM for instance). Those values alone are, however, hard to interpret, and notably to relate to the actual amount of knowledge transferred via KL-B EAM. We therefore propose to position those scores along the learning curves of the monolingual parser, following Aufrant et al. (2016): if the cross-lingual parser achieves the same score as a parser trained on n sentences, we consider that the amount of transferred knowledge is the amount of knowledge contained in n sentences. Figure 3 consequently pictures the learning curves of each system on simple and complex classes (using the PoS/direction criterion), as well as the split UAS for KL-B EAM on the same categories. 100 100 Beam UAS 90 100 80 UDPipe MSTParser 90 90 80 80 73.7 73.4 70 70 66.1 66.1 60 72.7 70 60 57.9 55.8 66.1 60 50 50 50 40 40 40 14 32 60 10 20 50 #snt 100 200 500 24 10 20 59 simple all complex 67 124 50 1"
C18-1270,E17-2051,1,0.843285,"an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies: by departing from individual dependencies, we a"
C18-1270,C12-1059,0,0.0228729,"dding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies:"
C18-1270,N16-1121,1,0.8591,"ented here do not replace any prior knowledge on transfer results for a given known language. However, when tackling a new language, they can provide a rough estimate to choose between the monolingual and cross-lingual approaches. 16 While this amount can appear already substantial, it also denotes the fact that Tiedemann and Agi´c (2016)’s evaluation is based on a set of languages with marked relatedness, and thus good transfer properties. When averaging our measures only over that set of languages (but still not the same treebanks), the data-size equivalent drops to 92 sentences. Similarly, Lacroix et al. (2016)’s projection approach achieves 79.62 UAS, which is higher than with 500 sentences when considering all UD 2.0 treebanks, but corresponds to 295 sentences when retaining the same set of languages. Overall, the magnitude remains around a few hundred sentences for projection methods. 3198 We build a cross-lingual parser for Romanian, using KL-B EAM as before but with a much smaller source set: French, Italian, Spanish and Bulgarian. The choice of the first three is motivated by their language family (Romance, like Romanian), and Bulgarian by geographic proximity (and hence various influences acr"
C18-1270,lynn-etal-2012-irish,0,0.0764864,"Missing"
C18-1270,P13-1028,0,0.0636125,"Missing"
C18-1270,E17-1022,0,0.0557687,"Missing"
C18-1270,D07-1013,0,0.0396146,"highly deterministic, is a simple class for all but 7 treebanks (mostly Old Church Slavonic, Arabic, LatinPROIEL, but also Basque, Estonian, Korean and Polish to a lesser extent). 4 Application 1: fine-grained comparison of parsers The metrics we have proposed can now be used for large-scale computation of fine-grained evaluations, and therefore detailed comparison of parsers. This newly defined notion of complexity opens indeed new evaluation perspectives, as a complement to the more explicit properties (length, PoS tags, projectivity, etc.) used in prior work on comparative error analysis (McDonald and Nivre, 2007). Complexity variations Coming back to Table 1, comparing the rankings between all 3 systems also emphasizes on their respective shortcomings. UDP IPE notably seems to have troubles with determiners: x not only does it achieve a lower score on DET dependencies, but it is also much slower learning those, compared to the other systems; UDP IPE consequently appears to under-exploit the determinism of that x class.13 It is conversely particularly efficient on CCONJs. As for MSTPARSER, it handles VERB significantly faster than B EAM and UDP IPE, presumably because it does not rely on mostly local f"
C18-1270,H05-1066,0,0.090463,"lly a sequence of transitions affecting the parser’s inner state) and graphbased parsers (which compute attachment scores for all pairs of tokens and then optimize the sentence score globally). In this work, we consider three dependency parsers, based on diverse parsing algorithms and classifiers, to assess the generality of our findings: UDP IPE (transition-based parser, with a feedforward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to i"
C18-1270,W04-0308,0,0.04351,"forward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges fac"
C18-1270,P15-2040,0,0.0578209,"link with our work above is twofold: cross-lingual parsers also focus on low ranges of training sizes, and at the same time many of them yield UAS around the range covered by our 5 to 500-sentence long treebanks. We therefore propose to exploit our upper results in this new frame, with the goal of quantifying and characterizing the amount of knowledge that has been effectively transferred: what kind of information is learned by cross-lingual parsers – only simple classes or complex knowledge about non-trivial classes? Multi-source weighted delexicalized transfer Our analysis first focuses on Rosa and Zabokrtsky (2015)’s state-of-the-art method for cross-lingual parsing: it consists in delexicalized transfer, where the hypotheses stemming from multiple sources are weighted and combined based on the KLcpos3 metric (the Kullback-Leibler divergence of PoS trigram distributions between the source and the target). It is meant to favour the languages that are syntactically close to the target, while still benefiting from the diverse information conveyed by a large set of sources. We reimplement the method of Rosa and Zabokrtsky (2015) on top of the B EAM system: we include as sources the delexicalized B EAM model"
C18-1270,C12-1147,0,0.0174207,"his section on classes defined by the child PoS and its attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete dat"
C18-1270,K17-3001,0,0.0983599,"then optimize the sentence score globally). In this work, we consider three dependency parsers, based on diverse parsing algorithms and classifiers, to assess the generality of our findings: UDP IPE (transition-based parser, with a feedforward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘e"
C18-1270,W15-2137,0,0.0131759,"es: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also excluded. 5 Resulting trainsets contain 5, 10, 20, 50, 100, 200 and 500 sentences. 6 While less representative of real-world processing capacities (Tiedemann, 2015), we believe this choice to be crucial in such studies focusing on syntactic properties, whose measurement would otherwise be biased by properties of the taggers and language-dependent vocabulary issues. 2 3192 y attachment decisions on ADJ tokens seem more complex, first of all because the PoS of the head is uncertain (sometimes a VERB, a NOUN, another ADJ, etc.). What remains to ascertain is whether this simple/complex distinction can relate to measurable properties. Our first experiment aims at studying the rate at which the different dependency classes are learned. In this experiment, we a"
C18-1270,C16-1043,0,0.0242944,"Missing"
C18-1270,W17-0419,1,0.854629,"defined by the child PoS and its attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also ex"
C18-1270,N18-2064,1,0.835989,"ts attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also excluded. 5 Resulting trains"
C18-1270,N01-1026,0,0.0287084,"Missing"
C18-1270,I08-3008,0,0.0575011,"Missing"
C18-1270,P11-2033,0,0.0414234,"e-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies: by departing from individual dependencies, we aim at discovering higherlevel properties relate"
D10-1076,W09-0417,1,0.829036,"this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector in"
D10-1076,J92-4003,0,0.180781,"irical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 1997), of generalized back-off strategies (Bilmes et al., 1997) or the explicit integration of morphological information in the random-forest model (Xu and Jelinek, 2004; Oparin et al., 2008). One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This 778 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778–788, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational L"
D10-1076,P96-1041,0,0.032208,"L Y P (wl |w1l−1 ) l=1 Modeling the joint distribution of several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of wor"
D10-1076,P07-2045,0,0.0086703,"atallah Mesyats Langlois 1 1 vector init. 4160th 3651st 3487th 3378th 3558th best list is accordingly reordered to produce the final translations. The different language models discussed in this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. #"
D10-1076,J10-4005,0,0.0044806,"n the small vocabulary tasks occurred more than several hundreds times in the training corpus, which was more than sufficient to guide the model towards satisfactory projection matrices. This finally suggests that there still exists room for improvement if we can find more efficient initialization strategies than starting from one or several random points. 4.4 Statistical machine translation experiments As a last experiment, we compare the various models on a large scale machine translation task. Statistical language models are key component of current statistical machine translation systems (Koehn, 2010), where they both help disambiguate lexical choices in the target language and influence the choice of the right word ordering. The integration of a neural network language model in such a system is far from easy, given the computational cost of computing word probabilities, a task that is performed repeatedly during the search of the best translation. We then had to resort to a two pass decoding approach: the first pass uses a conventional back-off language model to produce a n-best list (the n most likely translations and their associated scores); in the second pass, the probability of the n"
D10-1076,H93-1021,0,0.0464304,"These two models differ only by the activation function of their hidden layer (linear for the LBL model and tangent hyperbolic for the standard model) and by their definition of the prediction space: for the LBL model, the context space and the prediction space are the same (R = Who , and thus H = m), while in the standard model, the prediction space is defined independently from the context space. This restriction drastically reduces the number of free parameters of the LBL model. It is finally noteworthy to outline the similarity of this model with standard maximum entropy language models (Lau et al., 1993; Rosenfeld, 1996). Let x denote the binary vector formed by stacking the (n-1) 1-of-V encodings of the history words; then the conditional probability distributions estimated in the model are proportional to exp F (x), where F is an affine transform of x. The main difference with MaxEnt language models are thus the restricted form of the feature functions, which only test one history word, and the particular representation of F , which is defined as: T F (x) = RWih R0 v + Rbih + bho where, as before, R0 is formed by concatenating (n − 1) copies of the projection matrix R. 2.3 Training and inf"
D10-1076,P03-1021,0,0.0834137,"the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one o"
D10-1076,P02-1040,0,0.104017,"to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one obtained with the standard model, the training time is reduced here"
D10-1076,P06-2093,0,0.69272,"Missing"
D10-1076,P06-1124,0,0.0109844,"f several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 199"
D10-1076,W04-3242,0,\N,Missing
D10-1091,2007.mtsummit-papers.3,0,0.0519115,"Missing"
D10-1091,W09-0437,0,0.394639,"l., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors. The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap2 the 3 the ttl option of Moses, defaulting to 20. dl option of Moses, whose default value is 7. 933 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction errors correspond to ca"
D10-1091,W05-0909,0,0.0448439,"ses and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluation metric is necessary in our approach. To circumvent this difficulty, we propose to evaluate the similarity between a translation hypothesis and a reference by the number of their common words. This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ). This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty). We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle"
D10-1091,P07-1020,0,0.0173698,"y accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various constraints that are typically used in conventional PBTS. In 17 The best BLEU-4 oracle they achieve on E"
D10-1091,D08-1064,0,0.028483,"igh 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible translation links between potential phrases of the source and of the target, find the subset of links so that the unigram precision and recall are the highest possible. The corresponding oracle hypothesis can then be easily generated by selecting the target phrases that are aligned with one s"
D10-1091,P08-2007,0,0.041734,"Missing"
D10-1091,W07-0414,0,0.320042,"rious causes of errors is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report meas"
D10-1091,P01-1030,0,0.0693371,"f (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models. Following the latter reference, we introduce the following variables: fi,j (resp. ek,l ) is a binary indicator variable that is true when the phrase contains all spans from betweenword position i to j (resp. k to l) of the source (resp. target) sentence. We also introduce a binary variable, denoted ai,j,k,l , to describe a possible link between source phrase fi,j and target phrase ek,l . These variables are built from the entries of the phrase table according to selection strategies introduced in Secti"
D10-1091,N03-1010,0,0.0268074,"is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT"
D10-1091,W08-0302,0,0.0314872,"roximation of the BLEU-4 oracle score. We have shown that this approximation could be computed fairly accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various cons"
D10-1091,N03-1017,0,0.0262102,"el corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simpl"
D10-1091,P07-2045,0,0.0154746,"oring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et a"
D10-1091,koen-2004-pharaoh,0,0.0452448,"and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are com"
D10-1091,H05-1021,0,0.0253438,"r uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase chosen for translation cannot be more than d9 words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: ∀aijkl , ai0 j 0 k0 l0 s.t. k > k 0 , aijkl · ai0 j 0 k0 l0 · |j − i0 + 1 |≤ d, The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assuming this constraint is parameterized by d): ∀l &lt; m − 1, ai,j,k,l ·ai0 ,j 0 ,l+1,l0 · |i0 − j − 1 |&lt; d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: X X ∀i, k, ai0 ,j 0 ,k0 ,l0 − ai0 ,j 0 ,k0 ,l0 ≤ d i0 ≤i k0 ≤k Similarly, It is possible to simulate decoding under the so-called IBM(d) reordering constraints10 by considering the following constraints: X ∀µ ≤ m, max ai,j,k,l · j − ai,j,k,l · (j − i) ≤ d i,k,l j≤µ 9 This i,j,k,l corresponds to the dl parameter of Moses In these constraints, the first factor corresponds to the rightmost translated word of the source and the second one to the number of translated source words. The constraints simply enfor"
D10-1091,D08-1088,0,0.441812,"scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of po"
D10-1091,N09-2003,0,0.199476,"ally generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BL"
D10-1091,P06-1096,0,0.271375,"Missing"
D10-1091,E09-1061,0,0.0614161,"compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 1 1.1 Phrase-Based Machine Translation Principle A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009). The ruleset, represented in the phrase table, is a set of phrase1 pairs {(f, e)}, each pair expressing that the source phrase f can be rewritten (translated) into a target phrase e. Translation hypotheses are generated by iteratively rewriting portions of the source sentence as prescribed by the ruleset, until each source word has been consumed by exactly one rule. The order of target words in an hypothesis is uniquely determined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following"
D10-1091,J03-1002,0,0.0140971,"ined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004)."
D10-1091,P03-1021,0,0.032078,"utive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source"
D10-1091,P02-1040,0,0.0801182,"ifficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed on several standard benchmarks. The main contributions of this paper are twofold. We first introduce an ILP program able to efficiently find the best hypothesis a PBTS can achieve. This program can be easily extended to test various improvements to 4 We omit here optimization err"
D10-1091,2007.tmi-papers.28,0,0.0659647,"Missing"
D10-1091,P06-1091,0,0.0995125,"ems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice"
D10-1091,W08-0305,0,0.082699,"Missing"
D10-1091,W05-0834,0,0.024682,"not directly comparable with ours17 , it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations"
D10-1091,lavie-etal-2004-significance,0,\N,Missing
D10-1091,W09-0401,0,\N,Missing
D14-1187,P11-1061,0,0.027859,"Missing"
D14-1187,D13-1205,0,0.0388242,"ssue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and should be looked at with great care (Section 4.2). All tools and resources used in this study are available a"
D14-1187,P09-1042,0,0.0101862,"ly annotated data. Several attempts have recently been made to mitigate the lack of annotated corpora using parallel data pairing a (source) text in a resource-rich language with its counterpart in a less-resourced language. By transferring labels from the source to the target, it becomes possible to obtain noisy, yet useful, annotations that can be used to train a model for the target language in a weakly supervised manner. This research trend was initiated by Yarowsky et al. (2001), who consider the transfer of POS and other syntactic information, and further developed in (Hwa et al., 2005; Ganchev et al., 2009) for syntactic dependencies, in (Padó and Lapata, 2009; Kozhevnikov and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration"
D14-1187,D12-1127,0,0.0275695,"ame a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and"
D14-1187,petrov-etal-2012-universal,0,0.16465,"Missing"
D14-1187,W11-0328,0,0.0240082,"other using, for instance, a linear model: yi∗ = arg max hw|φ(x, i, y, hi )i y∈Y (1) where h·|·i is the standard dot product operation, yi∗ the predicted label for position i, w the weight ∗ vector, hi = y1∗ , ..., yi−1 the history of past decisions and φ a joint feature map. Inference can therefore be seen as a greedy search in the space of the # {Y}n possible labelings of the input sequence. Trading off the global optimality of inference for additional flexibility in the design of features and long range dependencies between labels has proved useful for many sequence labeling tasks in NLP (Tsuruoka et al., 2011). The training procedure, sketched in Algorithm 1, consists in performing inference on each input sentence and correcting the weight vector each time a wrong decision is made. Importantly (Ross and Bagnell, 2010), the history used during training has to be made of the previous predicted labels so that the training samples reflect the fact that the history will be imperfectly known at test time. This reduction of sequence labeling to multiclass classification allows us to learn a sequence model in an ambiguous setting by building on the theoretical results of Bordes et al. (2010) and Cour et al"
D14-1187,Q13-1001,0,0.149557,"ng that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and should be looked at with"
D14-1187,C14-1121,0,0.0202466,"Missing"
D14-1187,Q14-1005,0,0.0238298,"and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysi"
D14-1187,H01-1035,0,0.124832,"plication domains and/or less-resourced languages, alternative ML techniques need to be designed to accommodate unannotated or partially annotated data. Several attempts have recently been made to mitigate the lack of annotated corpora using parallel data pairing a (source) text in a resource-rich language with its counterpart in a less-resourced language. By transferring labels from the source to the target, it becomes possible to obtain noisy, yet useful, annotations that can be used to train a model for the target language in a weakly supervised manner. This research trend was initiated by Yarowsky et al. (2001), who consider the transfer of POS and other syntactic information, and further developed in (Hwa et al., 2005; Ganchev et al., 2009) for syntactic dependencies, in (Padó and Lapata, 2009; Kozhevnikov and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yaro"
D14-1187,D12-1125,0,0.139036,"Missing"
D14-1187,P12-1073,0,\N,Missing
D14-1187,P13-1117,0,\N,Missing
D15-1121,P14-2023,0,0.0715303,"train the CTM provide an explanation for this difference: in training, the N -best lists contain hypotheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data. As a result, adding the CTM significantly 7 www.statmt.org/wmt14/medical-task/ ncode.limsi.fr/ 9 The threshold δ is set to 250 for 300-best and to 500 for 600-best lists, while α is set empirically. 8 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some investigation. Gao et al. (2014) and Auli"
D15-1121,D13-1106,0,0.0314769,"hieve impressive results, and with which efficient tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N -best rescoring is used in this work to facilitate the discriminative training, other CTM’s integration into SMT systems exist, such as lattice reranking (Auli et al., 2013) or direct decoding with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). 6 Conclusions In this paper, we have proposed a new discriminative training procedure for continuous-space translation models, which correlates better with translation quality than conventional training methods. This procedure has been validated using an n-gram-based CTM, but the general idea could be applied to other continuous models which compute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and P"
D15-1121,J04-2004,0,0.0322429,"his proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingua"
D15-1121,N12-1047,0,0.0218534,"ers, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are s"
D15-1121,P14-1129,0,0.0598631,"Missing"
D15-1121,2014.iwslt-papers.6,1,0.601875,"h) < α∆i,k SBLEU (h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(θ) is defined with respect to this critical set and can be written as:4 X α∆i,k SBLEU (h) − ∆i,k Gλ,θ (s, hi ) (i,k)∈Cδα (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients λ. To initialize θ, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize λ by optimizing it on the development set. This strategy forces the training of θ to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the"
D15-1121,2015.jeptalnrecital-long.23,1,0.625921,"(h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(θ) is defined with respect to this critical set and can be written as:4 X α∆i,k SBLEU (h) − ∆i,k Gλ,θ (s, hi ) (i,k)∈Cδα (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients λ. To initialize θ, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize λ by optimizing it on the development set. This strategy forces the training of θ to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the adaptation scenari"
D15-1121,N13-1025,0,0.0162701,"training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted SBLEU (hi ). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2 http://www.statmt.org/moses/ Cδα (s) = {(i, k) : 1 ≤ k, i ≤ N, rk − ri ≥ δ, ∆i,k Gλ,θ (s, h) < α∆i,k SBLEU (h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the diffe"
D15-1121,N13-1048,0,0.0883008,"do for P mini-batch do . λ is fixed Compute the sub-gradient of L(θ) for each sentence s in the mini-batch Update θ end for Update λ on development set . θ is fixed end for cumulates, over all contexts c and word w, the CTM log-score log bθ (w, c). Gλ,θ depends both on the NN parameters θ and on the log-linear coefficients λ. We propose to train these two sets of parameters, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Sim"
D15-1121,P14-1066,0,0.0193361,"n) measured on the N -best lists used to train the CTM provide an explanation for this difference: in training, the N -best lists contain hypotheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data. As a result, adding the CTM significantly 7 www.statmt.org/wmt14/medical-task/ ncode.limsi.fr/ 9 The threshold δ is set to 250 for 300-best and to 500 for 600-best lists, while α is set empirically. 8 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some"
D15-1121,P12-1031,0,0.168758,"for each iteration do for P mini-batch do . λ is fixed Compute the sub-gradient of L(θ) for each sentence s in the mini-batch Update θ end for Update λ on development set . θ is fixed end for cumulates, over all contexts c and word w, the CTM log-score log bθ (w, c). Gλ,θ depends both on the NN parameters θ and on the log-linear coefficients λ. We propose to train these two sets of parameters, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins"
D15-1121,D11-1125,0,0.205097,"addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et"
D15-1121,P12-1092,0,0.0232118,"w that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize"
D15-1121,N12-1005,1,0.90363,"two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann a"
D15-1121,J06-4004,0,0.0883567,"Missing"
D15-1121,2012.iwslt-papers.3,0,0.0569966,"c)), where aθ (w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as f"
D15-1121,P02-1040,0,0.0998119,"2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score bθ (w, c) for each word w given its context c; this score is simply computed as bθ (w, c) = exp(aθ (w, c)), where aθ (w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the re"
D15-1121,D07-1045,0,0.0228854,"Missing"
D15-1121,P12-1002,0,0.0968245,"and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced i"
D15-1121,P13-1045,0,0.0266231,"this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function r"
D15-1121,D13-1140,0,0.0299684,"he activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as follows: Gλ,θ (s, h) ="
D15-1121,D07-1080,0,0.255024,"paper, we study an alternative training regime aimed at addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Transla"
D15-1121,P13-1017,0,0.0241565,"ce gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alt"
D15-1121,2002.tmi-tutorials.2,0,0.0658344,"ay, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pai"
D15-1121,D14-1179,0,\N,Missing
D17-1044,P10-1052,1,0.814937,"Missing"
D17-1044,P03-1006,0,0.0149295,"grams and w ∈ W, a generic feature function is then fw,g (w, x, t) = I(yt−s . . . yt = w ∧ g(x, t)). In (order-p) VoCRFs, the computational cost of training and inference is proportional to the size of a finite-state automaton A[W] encoding the patterns in W,2 which can be much less than |Y|p . Our procedure for building A[W] is sketched in Algorithm 1, where TrieInsert inserts a string in a trie, Pref(W) computes the set of prefixes of the strings in W,3 LgSuff(v, U) returns the longest suffix of v in U, and FailureTrans is a special ε-transition used only when no labelled transition exists (Allauzen et al., 2003).4 Each state (or pattern prefix) v in A[W] is associated with a set of feature functions {fu,g , ∀u ∈ Suff(v), g}.5 The forward step of the gradient computation maintains one value α(v, t) per state and time step, which is recursively accumulated over all paths ending in v at time t. The next question is to identify W. The simplest method keeps all the ngrams viewed in training, additionally filtering rare patterns (Cuong et al., 2014). However, frequency based feature selection does not take interactions into account and is not the best solution. Ideally, one would like to train a complete o"
D17-1044,D11-1139,0,0.0248897,"this difficulty are based on a greedy approach which starts with firstorder dependencies between labels and iteratively increases the scope of dependency patterns under the constraint that a high-order dependency is selected only if it extends an existing lower order feature (Müller et al., 2013). As a result, feature selection may only choose only few higherorder features, motivating the need for an effective variable-order CRF (voCRF) training procedure (Ye et al., 2009).1 The latest implementation of this idea (Vieira et al., 2016) relies on (structured) sparsity promoting regularization (Martins et al., 2011) and on finite-state techniques, handling high-order features at a small extra cost (see § 2). In this approach, the sparse set of label dependency patterns is represented in a finite-state automaton, which arises as the result of the feature selection process. In this paper, we somehow reverse the perspective and consider VoCRF training mostly as an automaton inference problem. This leads us to consider alternative techniques for learning the finitestate machine representing the dependency structure of sparse VoCRFs (see § 3). Two lines of enquiries are explored: (a) to take into account the"
D17-1044,J96-1002,0,0.0501966,"Missing"
D17-1044,D15-1272,0,0.0325289,"Missing"
D17-1044,D13-1032,0,0.109046,"çois Yvon LIMSI, CNRS, Univ. Paris-Sud, Université Paris Saclay Campus Universitaire, F-91 403 Orsay, France {lavergne,yvon}@limsi.fr Abstract ging and text chunking) are considered, For such tasks, processing first-order models is demanding, and full size higher-order models are out of the question. Attempts to overcome this difficulty are based on a greedy approach which starts with firstorder dependencies between labels and iteratively increases the scope of dependency patterns under the constraint that a high-order dependency is selected only if it extends an existing lower order feature (Müller et al., 2013). As a result, feature selection may only choose only few higherorder features, motivating the need for an effective variable-order CRF (voCRF) training procedure (Ye et al., 2009).1 The latest implementation of this idea (Vieira et al., 2016) relies on (structured) sparsity promoting regularization (Martins et al., 2011) and on finite-state techniques, handling high-order features at a small extra cost (see § 2). In this approach, the sparse set of label dependency patterns is represented in a finite-state automaton, which arises as the result of the feature selection process. In this paper,"
D17-1044,N15-1094,0,0.0129322,"y complemented with a regularization term so as to avoid overfitting and stabilize the optimization. Common regularizers use the `1 - or the `2 norm of the parameter vector, the former having the benefit to promote sparsity, thereby performing automatic feature selection (Tibshirani, 1996). 2.2 2 More precisely, Vieira et al. (2016) consider W, the closure of W under suffix and last character substitution, which factors as W = H × Y. The complexity of training depends on the size of the finite-state automaton representing W. 3 A trie has one state for each prefix. 4 This was also suggested by Cotterell and Eisner (2015) as a way to build a more compact pattern automaton. 5 Upon reaching a state v, we need to access the features that fire for that pattern, and also for all its suffixes. Each state thus stores a set of pattern; each pattern is associated with a set of tests on the observation (cf. 2.1). 6 Recall that the size of parameter set is exponential wrt. the model order. Variable order CRFs (VoCRFs) When the label set is large, many pairs of labels never occur in the training data and the sparsity of label ngrams quickly increases with the order p of the model. In the variable order CRF model, it is as"
D17-1044,J13-1005,0,0.0288316,"nguage model, this approach discards n-grams if their removal causes a sufficiently small drop in cross-entropy. We used the implementation of Stolcke (2002). cf. the discussion in (Vieira et al., 2016, § 4). 435 (MELMs) (Rosenfeld, 1996). MELMs decompose the probabililty of a sequence y1 . . . yT using the chain rule, where each term pλ (yt |y<t ) is a locally normalized exponential model including all possible ngram features up to order p: 4.2 Experiments are run on two MRLs: for Czech, we use the CoNLL 2009 data set (Hajiˇc et al., 2009) and for German, the Tiger Treebank with the split of Fraser et al. (2013)). Both datasets include rich morphological attributes (cf. Table 1). All the patterns in W are combined with lexical features testing the current word xt , its prefixes and suffixes of length 1 to 4, its capitalization and the presence of digit or punctuation symbols. Additional contextual features also test words in a local window around position t. These tests greatly increase the feature count and are not provided for all label patterns: for unigram patterns, we test the presence of all unigrams and bigrams of words in a window of 5 words; for bigrams patterns we only test for all unigrams"
D17-1044,A00-2013,0,0.156407,"Missing"
D17-1044,C16-1160,0,0.0173878,"le and report experimental results where we outperform strong baselines on a tagging task. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) are a method of choice for many sequence labelling tasks such as Part of Speech (PoS) tagging, Text Chunking, or Named Entity Recognition. Linearchain CRFs are easy to train by solving a convex optimization problem, can accomodate rich feature patterns, and enjoy polynomial exact inference procedures. They also deliver state-of-the-art performance for many tasks, sometimes surpassing seq2seq neural models (Schnober et al., 2016). A major issue with CRFs is the complexity of training and inference procedures, which are quadratic in the number of possible output labels for first order models and grow exponentially when higher order dependencies are considered. This is problematic for tasks such as precise PoS tagging for Morphologically Rich Languages (MRLs), where the number of morphosyntactic labels is in the thousands (Hajiˇc, 2000; Müller et al., 2013). Large label sets also naturally arise when joint labelling tasks (eg. simultaneous PoS tag1 This is reminiscent of variable order HMMs, introduced eg. in (Schütze a"
D17-1044,P94-1025,0,0.529056,"l., 2016). A major issue with CRFs is the complexity of training and inference procedures, which are quadratic in the number of possible output labels for first order models and grow exponentially when higher order dependencies are considered. This is problematic for tasks such as precise PoS tagging for Morphologically Rich Languages (MRLs), where the number of morphosyntactic labels is in the thousands (Hajiˇc, 2000; Müller et al., 2013). Large label sets also naturally arise when joint labelling tasks (eg. simultaneous PoS tag1 This is reminiscent of variable order HMMs, introduced eg. in (Schütze and Singer, 1994; Ron et al., 1996). 433 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 433–439 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics surpass strong baselines for two MRLs (see § 4). 2 Algorithm 1: Building A[W] W : list of patterns, A[W] initially empty U = Pref(W) foreach w ∈ W do TrieInsert(w, A[W]) Variable order CRFs In this section, we recall the basics of CRFs and VoCRFs and introduce some notations. 2.1 // Add missing transitions foreach u = vy ∈ U do new FailureTrans(u, LgSuff(v, U)) Basics First-orde"
D17-1044,P06-2098,0,0.0786904,"Missing"
D17-1044,H05-1060,0,0.0755964,"Missing"
D17-1044,P09-1055,0,0.0780347,"Missing"
D18-1328,S16-1081,0,0.0271883,"elated Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features."
D18-1328,D18-1549,0,0.0254976,"performance on a parallel corpus. Therefore, the quality of MT engines is heavily dependent on the amount but also the quality of available parallel sentences.1 Parallel texts are unfortunately, scarce resources: There are relatively few language pairs for which parallel corpora of large sizes exist, and even for those pairs, available corpora only concern few restricted domains. To alleviate the lack of parallel data, several approaches have been developed over the years. They range from methods using non-parallel, or comparable data (Zhao and 1 Recent work on neural MT (Lample et al., 2018; Artetxe et al., 2018) completely dispenses with parallel data, using unsupervised methods to obtain performance improvements over word-by-word statistical MT. These systems however lag far behind supervised systems, as considered in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and glos"
D18-1328,W17-3209,0,0.0693794,"egree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity Figure 1: Illustration of the model. It computes the similarity of any source-target sentence pair (s, t), where s = (s1"
D18-1328,2016.amta-researchers.8,0,0.233467,"nglais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding a"
D18-1328,W17-4714,0,0.0456606,"Missing"
D18-1328,W04-3208,0,0.0366978,"Missing"
D18-1328,2012.amta-papers.7,0,0.04818,"with a different, arguably simpler, topology. We model sentence similarity by means of optimising a loss function based on word alignments. Furthermore, the network predicts word similarity scores that we further use to correct divergent sentences. 3 Neural Divergence Classifier The architecture of our network is inspired by the work on word alignment of Legrand et al. (2016), using however contextual, rather than fixed, word embeddings (see Figure 1). Related Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sent"
D18-1328,C18-1122,0,0.0465598,"Missing"
D18-1328,P17-3003,0,0.0250778,"Missing"
D18-1328,N16-1108,0,0.0341746,"they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity"
D18-1328,W16-2207,0,0.213438,"Missing"
D18-1328,C14-1055,0,0.109632,"are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics scope of this paper. In this work, we present an unsupervised method for building cross-lingual sentence embeddings based on modelling word similarity, relying on a neural architecture (see § 3) that is able to identify several types of common cross-lingual divergences. The resulting embeddings are then used to measure semantic equivalence between sentenc"
D18-1328,L16-1147,0,0.182552,"ed in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and gloss of French (gl). Divergences are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November"
D18-1328,J05-4003,0,0.208936,"Missing"
D18-1328,P18-2037,0,0.0386913,"Missing"
D18-1328,P16-1009,0,0.0359762,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,P16-1162,0,0.0649685,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,N18-1136,0,0.168216,"Missing"
D18-1328,L16-1099,1,0.844686,"ystrangroup.com ‡ LIMSI, CNRS, Universit´e Paris-Saclay 91405 Orsay, France firstname.lastname@limsi.fr Abstract Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Gr´egoire and Langlais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores,"
de-mareuil-etal-2000-french,C94-1097,0,\N,Missing
duclaye-etal-2002-using,P01-1008,0,\N,Missing
duclaye-etal-2002-using,P99-1062,0,\N,Missing
duclaye-etal-2002-using,W99-0613,0,\N,Missing
E09-1056,J93-2003,0,0.00944004,"If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487–495, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 487 2.2 In the remainder of this paper, we first present in Section 2 the principle of analogical learning. Practical issues in analogical learning are discussed in Section 3 al"
E09-1056,W05-0616,1,0.948584,"ector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recogn"
E09-1056,W02-1001,0,0.0246147,"Missing"
E09-1056,P08-1059,0,0.0441301,"Missing"
E09-1056,2007.mtsummit-papers.19,0,0.512696,"such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms of the medical domain; a domain well-known for its tendency to create new words, many of which being complex lexical constructions."
E09-1056,W07-0705,0,0.0544343,"Missing"
E09-1056,W97-0119,0,0.0759318,"Missing"
E09-1056,2007.mtsummit-papers.36,0,0.0122855,"nguage pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements. 1 Introduction If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487–495, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 487 2.2 In the remainder of this pap"
E09-1056,koen-2004-pharaoh,0,0.0664297,"Missing"
E09-1056,D07-1092,1,0.880221,"gree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms o"
E09-1056,C08-2013,1,0.8318,"f I(u), denoted N (t). Those solutions that belong to the input space are the z-forms retained; EI (u) = { hx, y, zi : EI (u) = { hx, y, zi : x ∈ I, hy, zi ∈ C(hx, ti), [x : y = z : t] } where C(hx, ti) denotes the set of pairs hy, zi which satisfy the count property. This strategy will only work if (i) the number of quadruplets to check is much smaller than the number of triplets we can form in the input space (which happens to be the case in practice), and if (ii) we can efficiently identify the pairs hy, zi that satisfy a set of constraints on character counts. To this end, we proposed in (Langlais and Yvon, 2008) to organize the input space into a data structure which supports efficient runtime retrieval. x ∈ N (t) , y ∈ N (x), z ∈ [y : x = t : ? ] ∩ I } This strategy (hereafter named LP) directly follows from a symmetrical property of an analogy ([x : y = z : t] ⇔ [y : x = t : z]), and reduces the search procedure to the resolution of a number of analogical equations which is quadratic with the number of pairs hx, yi sampled. We found this strategy to be of little use for input spaces larger than a few tens of thousands forms. To solve this problem, we exploit a property on symbol counts that an anal"
E09-1056,2005.iwslt-1.4,0,0.0842875,"another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an"
E09-1056,P98-1120,0,0.343527,"set. Our solver, depicted in Algorithm 2, is thus controlled by a sampling size s, the impact of which is illustrated in Table 1. By increasing s, the solver generates more (mostly spurious) solutions, but also increases the relative frequency with which the expected output is generated. In practice, provided a large enough sampling size,2 the expected form very often appears among the most frequent ones. selecting good candidates involves some practical issues. Since searching for input triplets might involve the need for solving (input) equations, we discuss the solver first. 3.1 The solver Lepage (1998) proposed an algorithm for solving an analogical equation [x : y = z : ? ]. An alignment between x and y and between x and z is first computed (by edit-distance) as illustrated in Figure 1. Then, the three strings are synchronized using x as a backbone of the synchronization. The algorithm can be seen as a deterministic finite-state machine where a state is defined by the two edit-operations being visited in the two tables. This is schematized by the two cursors in the figure. Two actions are allowed: copy one symbol from y or z into the solution and move one or both cursors. x: r e a d er y:"
E09-1056,P06-1096,0,0.0437426,"Missing"
E09-1056,P07-1084,0,0.0158288,"ing: Application to Translating multi-Terms of the Medical Domain Philippe Langlais DIRO Univ. of Montreal, Canada felipe@iro.umontreal.ca Franc¸ois Yvon and Pierre Zweigenbaum LIMSI-CNRS Univ. Paris-Sud XI, France {yvon,pz}@limsi.fr Abstract with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several"
E09-1056,C98-1116,0,\N,Missing
E09-1056,2007.iwslt-1.7,0,\N,Missing
E12-1013,W09-0437,0,0.0134914,"ms has the form of a very large directed acyclic graph. In several softwares, an approximation of this search space can be outputted, either as a n-best list containing the n top hypotheses found by the decoder, or as a phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning. Lattices usually contain much more hypotheses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-l"
E12-1013,W05-0909,0,0.106821,"BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-"
E12-1013,P10-2006,0,0.0846147,"Missing"
E12-1013,D11-1003,0,0.112918,"is the subset of edges generating w, and ξ∈Ω(w) ξ is the number of occurrences of w in the solution and cw (r) is the number of occurrences of w in the reference r. Using the γ variables, we define a “clipped” approximation of 1- BLEU :   #{Ξ} X X X Θ1 · γw − Θ2 ·  ξi − γw  w i=1 #{Ξ} (Θ1 + Θ2 ) · ξ∈P,γw X γw − Θ2 · w X ξi i=1 (7) s.t. γw ≥ 0, γw ≤ cw (r), γw ≤ X 5.3 ξ∈Ξ− (qF ) X ξ∈Ξ+ (q) X ξ = 1, ξ− Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints. The oracle decoding problem with clipping constraints amounts to solving: #{Ξ} ξ − arg min ξ∈Ω(w) X Shortest Path Oracle (SP) As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6). As no clipping constraints ap"
E12-1013,D08-1024,0,0.239204,"e hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations"
E12-1013,W07-0414,0,0.0612882,"am counts (7) uni/bi-gram counts (8) search exact appr. appr. exact exact appr. exact clipping no no no no no yes yes brevity no no yes yes yes yes yes Table 1: Recapitulative overview of oracle decoders. 3 Existing Algorithms In this section, we describe our reimplementation of two approximate search algorithms that have been proposed in the literature to solve the oracle decoding problem for BLEU. In addition to their approximate nature, none of them accounts for the fact that the count of each matching word has to be clipped. 3.2 Partial BLEU Oracle (PB) Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path π is the sentence level log BLEU(π) score of the corresponding complete or partial hypothesis: log BLEU(π) = 3.1 Language Model Oracle (LM) The simplest approach we consider is introduced in (Li and Khudanpur, 2009), where oracle decoding is reduced to the problem of finding the most likely hypothesis under a n-gram language model trained with the sole reference translation. Let us suppose we have a n-gram language model that gives a probability P (en |e1 . . . en−1 ) of"
E12-1013,D08-1088,0,0.341595,"achability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, su"
E12-1013,N09-2003,0,0.102506,"understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total mod"
E12-1013,P06-1096,0,0.240983,"Missing"
E12-1013,P02-1040,0,0.0835406,"ctors with the parameters λ tuning. In oracle decoding, the decoder’s job is quite different, as we assume that at least a reference rf is provided to evaluate the quality of each individual hypothesis. The decoder therefore aims at finding the path π ∗ that generates the hypothesis that best matches rf . For this task, only the output labels ei will matter, the other informations can be left aside.4 Oracle decoding assumes the definition of a measure of the similarity between a reference and a hypothesis. In this paper we will consider sentence-level approximations of the popular BLEU score (Papineni et al., 2002). BLEU is formally defined for two parallel corpora, E = {ej }Jj=1 and R = {rj }Jj=1 , each containing J sentences as: Y 1/n n n-BLEU(E, R) = BP · pm , (1) m=1 where BP = min(1, e1−c1 (R)/c1 (E) ) is the brevity penalty and pm = cm (E, R)/cm (E) are clipped or modified m-gram precisions: cm (E) is the total number of word m-grams in E; cm (E, R) accumulates over sentences the number of mgrams in ej that also belong to rj . These counts are clipped, meaning that a m-gram that appears k times in E and l times in R, with k &gt; l, is only counted l times. As it is well known, BLEU performs a compr"
E12-1013,D10-1001,0,0.0350101,"e. 125 where PΩ (w) is the subset of edges generating w, and ξ∈Ω(w) ξ is the number of occurrences of w in the solution and cw (r) is the number of occurrences of w in the reference r. Using the γ variables, we define a “clipped” approximation of 1- BLEU :   #{Ξ} X X X Θ1 · γw − Θ2 ·  ξi − γw  w i=1 #{Ξ} (Θ1 + Θ2 ) · ξ∈P,γw X γw − Θ2 · w X ξi i=1 (7) s.t. γw ≥ 0, γw ≤ cw (r), γw ≤ X 5.3 ξ∈Ξ− (qF ) X ξ∈Ξ+ (q) X ξ = 1, ξ− Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints. The oracle decoding problem with clipping constraints amounts to solving: #{Ξ} ξ − arg min ξ∈Ω(w) X Shortest Path Oracle (SP) As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6). As"
E12-1013,2006.amta-papers.25,0,0.0356563,"roximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et"
E12-1013,D08-1065,0,0.364928,"0–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010). In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation. Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based"
E12-1013,W08-0305,0,0.0294122,"Missing"
E12-1013,D10-1091,1,0.927891,"theses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracl"
E17-2051,P16-1017,0,0.0420794,"Missing"
E17-2051,P04-1015,0,0.17142,"Missing"
E17-2051,P16-1231,0,0.0693266,"Missing"
E17-2051,D16-1001,0,0.0225945,"Missing"
E17-2051,C12-1059,0,0.0974604,"Missing"
E17-2051,Q13-1033,0,0.038875,"Missing"
E17-2051,N12-1015,0,0.0446608,"Missing"
E17-2051,Q16-1023,0,0.0272666,"Missing"
E17-2051,N16-1121,1,0.873308,"Missing"
E17-2051,J08-4003,0,0.060757,"Missing"
E17-2051,D08-1059,0,0.0935067,"Missing"
E17-2051,W02-1001,0,\N,Missing
E17-2051,J13-1002,0,\N,Missing
E17-2051,P11-2033,0,\N,Missing
F12-2009,C88-1016,0,0.755749,"Missing"
F12-2009,J93-2003,0,0.0485365,"Missing"
F12-2009,A94-1006,0,0.396761,"Missing"
F12-2009,P07-1003,0,0.0485499,"Missing"
F12-2009,H05-1022,0,0.0483634,"Missing"
F12-2009,J93-1003,0,0.367015,"Missing"
F12-2009,D07-1006,0,0.0354864,"Missing"
F12-2009,C94-2178,0,0.315955,"Missing"
F12-2009,P98-1069,0,0.213132,"Missing"
F12-2009,H91-1026,0,0.660246,"Missing"
F12-2009,P08-1112,0,0.0401714,"Missing"
F12-2009,W08-0509,0,0.0397229,"Missing"
F12-2009,D07-1103,0,0.0539592,"Missing"
F12-2009,2005.mtsummit-papers.11,0,0.0929043,"Missing"
F12-2009,P07-2045,0,0.0105362,"Missing"
F12-2009,N03-1017,0,0.0377534,"Missing"
F12-2009,N06-1014,0,0.0864734,"Missing"
F12-2009,Y11-1016,1,0.752487,"Missing"
F12-2009,W02-1018,0,0.0583631,"Missing"
F12-2009,J00-2004,0,0.057226,"Missing"
F12-2009,W04-3243,0,0.0800367,"Missing"
F12-2009,W05-0801,0,0.0483887,"Missing"
F12-2009,P03-1021,0,0.0178865,"Missing"
F12-2009,J03-1002,0,0.0268946,"Missing"
F12-2009,P02-1040,0,0.0929894,"Missing"
F12-2009,J96-1001,0,0.158519,"Missing"
F12-2009,2006.amta-papers.25,0,0.038671,"Missing"
F12-2009,2005.mtsummit-papers.33,0,0.103911,"Missing"
F12-2009,C96-2141,0,0.473813,"Missing"
F12-2009,J97-3002,0,0.135811,"Missing"
F12-2044,W10-2417,0,0.0454004,"Missing"
F12-2044,D08-1030,0,0.0748056,"Missing"
F12-2044,P07-1033,0,0.130704,"Missing"
F12-2044,P11-2071,0,0.0309203,"Missing"
F12-2044,gahbiche-braham-etal-2012-joint,1,0.890958,"Missing"
F12-2044,W11-1207,1,0.89968,"Missing"
F12-2044,P08-1045,0,0.079455,"Missing"
F12-2044,P07-1034,0,0.0679876,"Missing"
F12-2044,P10-1052,1,0.806755,"Missing"
F12-2044,W98-1002,0,0.154777,"Missing"
F12-2044,W04-2405,0,0.0996275,"Missing"
F12-2044,zaghouani-etal-2010-adapting,0,0.0443458,"Missing"
F12-2044,W05-0709,0,0.0834481,"Missing"
F13-1033,P08-1024,0,0.0509722,"Missing"
F13-1033,P96-1041,0,0.0629406,"Missing"
F13-1033,N12-1047,0,0.0840101,"Missing"
F13-1033,P05-1033,0,0.20525,"Missing"
F13-1033,N09-1025,0,0.0523268,"Missing"
F13-1033,2009.eamt-1.10,1,0.89492,"Missing"
F13-1033,D08-1113,0,0.0420633,"Missing"
F13-1033,N10-1128,0,0.0524758,"Missing"
F13-1033,N10-1112,0,0.0607107,"Missing"
F13-1033,D11-1125,0,0.0471838,"Missing"
F13-1033,J10-4005,0,0.0549819,"Missing"
F13-1033,P07-2045,0,0.00926885,"Missing"
F13-1033,N03-1017,0,0.0205266,"Missing"
F13-1033,H05-1021,0,0.0736623,"Missing"
F13-1033,W11-2168,1,0.869656,"Missing"
F13-1033,P06-1096,0,0.0733762,"Missing"
F13-1033,J06-4004,0,0.421516,"Missing"
F13-1033,P03-1021,0,0.111567,"Missing"
F13-1033,P02-1040,0,0.0880358,"Missing"
F13-1033,2010.iwslt-evaluation.1,0,0.0423573,"Missing"
F13-1033,H05-1095,0,0.0566484,"Missing"
F13-1033,P12-1002,0,0.0264726,"Missing"
F13-1033,E12-1013,1,0.863456,"Missing"
F13-1033,takezawa-etal-2002-toward,0,0.0489745,"Missing"
F13-1033,N04-4026,0,0.12837,"Missing"
F13-1033,J03-1005,0,0.122695,"Missing"
F13-1033,P06-1091,0,0.0473605,"Missing"
F13-1033,D07-1080,0,0.0524939,"Missing"
F13-1033,2002.tmi-tutorials.2,0,0.416816,"Missing"
F13-2028,abekawa-etal-2010-community,0,0.0475848,"Missing"
F13-2028,W12-3102,0,0.076723,"Missing"
F13-2028,2012.eamt-1.60,0,0.0681271,"Missing"
F13-2028,2004.tmi-1.8,0,0.0596356,"Missing"
F13-2028,S10-1003,0,0.100773,"Missing"
F13-2028,potet-etal-2012-collection,0,0.042695,"Missing"
F13-2028,2006.amta-papers.25,0,0.1378,"Missing"
F13-2028,W12-3120,1,0.885309,"Missing"
F14-1016,C04-1080,0,0.0983607,"Missing"
F14-1016,H92-1026,0,0.486931,"Missing"
F14-1016,J92-4003,0,0.582882,"Missing"
F14-1016,W06-2920,0,0.058481,"Missing"
F14-1016,D10-1056,0,0.0296351,"Missing"
F14-1016,J03-4003,0,0.197779,"Missing"
F14-1016,P11-1061,0,0.0364566,"Missing"
F14-1016,N13-1014,0,0.0240925,"Missing"
F14-1016,D07-1033,0,0.0471334,"Missing"
F14-1016,P07-2045,0,0.00361318,"Missing"
F14-1016,P08-1068,0,0.105394,"Missing"
F14-1016,D12-1127,0,0.0348292,"Missing"
F14-1016,J94-2001,0,0.460186,"Missing"
F14-1016,N13-1039,0,0.0244234,"Missing"
F14-1016,petrov-etal-2012-universal,0,0.0265866,"Missing"
F14-1016,W09-1119,0,0.040984,"Missing"
F14-1016,J03-3002,0,0.0974294,"Missing"
F14-1016,N12-1052,0,0.0210751,"Missing"
F14-1016,W11-0328,0,0.265713,"Missing"
F14-1016,Q13-1001,0,0.0261175,"Missing"
F14-1016,Q14-1005,0,0.0213256,"Missing"
F14-1016,H01-1035,0,0.174923,"Missing"
F14-1023,D07-1090,0,0.0356393,"Missing"
F14-1023,N12-1047,0,0.0684646,"Missing"
F14-1023,P13-1031,0,0.0612609,"Missing"
F14-1023,P12-1092,0,0.082064,"Missing"
F14-1023,D13-1176,0,0.0585586,"Missing"
F14-1023,N12-1005,1,0.879962,"Missing"
F14-1023,W12-2701,1,0.89996,"Missing"
F14-1023,C90-3038,0,0.665071,"Missing"
F14-1023,P13-1045,0,0.0313874,"Missing"
F14-1023,P10-1040,0,0.0139435,"Missing"
F14-1023,P13-1017,0,0.0293458,"Missing"
F14-1025,P12-2023,0,0.0298293,"Missing"
F14-1025,D10-1044,0,0.0565326,"Missing"
F14-1025,W07-0717,0,0.0770098,"Missing"
F14-1025,gahbiche-braham-etal-2012-joint,1,0.875754,"Missing"
F14-1025,W11-1207,1,0.893881,"Missing"
F14-1025,W08-0509,0,0.0214724,"Missing"
F14-1025,W12-3154,0,0.0232309,"Missing"
F14-1025,P07-2045,0,0.00637307,"Missing"
F14-1025,W07-0733,0,0.0779944,"Missing"
F14-1025,W02-1405,0,0.0517198,"Missing"
F14-1025,W08-0320,0,0.0478919,"Missing"
F14-1025,2010.eamt-1.29,0,0.0382186,"Missing"
F14-1025,P03-1021,0,0.0189432,"Missing"
F14-2002,2013.iwslt-papers.7,1,0.830631,"Missing"
F14-2002,2010.amta-papers.21,0,0.0722326,"Missing"
F14-2002,2012.eamt-1.62,1,0.90627,"Missing"
F14-2002,2010.eamt-1.37,0,0.0750873,"Missing"
F14-2002,C08-1064,0,0.0624856,"Missing"
F14-2002,J03-1002,0,0.0176318,"Missing"
F14-2002,P02-1040,0,0.106788,"Missing"
F14-2002,P13-1135,0,0.0569748,"Missing"
F14-2002,2006.amta-papers.25,0,0.117792,"Missing"
F14-2002,J97-3002,0,0.115511,"Missing"
F14-2002,W06-3111,0,0.0455425,"Missing"
gahbiche-braham-etal-2012-joint,P05-1071,0,\N,Missing
gahbiche-braham-etal-2012-joint,P02-1040,0,\N,Missing
gahbiche-braham-etal-2012-joint,W05-0909,0,\N,Missing
gahbiche-braham-etal-2012-joint,P07-2045,0,\N,Missing
gahbiche-braham-etal-2012-joint,W05-0701,0,\N,Missing
gahbiche-braham-etal-2012-joint,P07-1104,0,\N,Missing
gahbiche-braham-etal-2012-joint,N06-2013,0,\N,Missing
gahbiche-braham-etal-2012-joint,W06-3103,0,\N,Missing
gahbiche-braham-etal-2012-joint,2010.iwslt-papers.15,0,\N,Missing
gahbiche-braham-etal-2012-joint,W11-1207,1,\N,Missing
gahbiche-braham-etal-2012-joint,W08-0509,0,\N,Missing
gahbiche-braham-etal-2012-joint,P03-1021,0,\N,Missing
gahbiche-braham-etal-2012-joint,P10-1052,1,\N,Missing
K17-3017,C16-1012,1,0.833828,", R2 and R3 ; each token belongs to exactly one region. On the input sentence, the white areas represent tokens whose head is unknown, while the black areas represent tokens whose head has already been predicted. UDPipe We apply the official UDPipe 1.1 baseline models (Straka, 2017). For the surprise languages, we train our own model.4 2.1.3 Cross-lingual For each treebank under 1,000 training sentences, we apply cross-treebank techniques to build additional parsers. First, for each target treebank, we transform every source treebank by delexicalizing it and applying the WALS rewrite rules of Aufrant et al. (2016). We then compute, for each such treebank, its similarity to the target treebank, using the KLcpos3 divergence metric (Rosa and Zabokrtsky, 2015). We select the source among treebanks over 2,000 sentences, by retaining the languages requiring the smallest number of rewrite rules (i.e. the smallest number of divergent WALS features), and then choosing the (transformed) treebank minimizing the KLcpos3 divergence. When the selected source is of the same language, we use domain adaptation techniques, otherwise we turn to cross-lingual methods. However, domain adaptation was not used in the final s"
K17-3017,E17-2051,1,0.808497,"e corresponding treebanks (FrenchParTUT, Galician-TreeGal and Czech-CLTT), and is not detailed here. We consider five cross-lingual parsers: PanParser This is an in-house implementation (Aufrant and Wisniewski, 2016) of a transition-based parser, using the ArcEager system and an averaged perceptron. Hyperparameters to tune are the number of epochs, the use of the universal morphological features, the use of word embeddings concatenated to the feature vectors, and the size of the beam (either 8 or 1, i.e. greedy). In any case the parser trains with dynamic oracles, with the restart strategy of Aufrant et al. (2017). Relation labels are predicted in a second step, which enables to use features of the whole parse tree for this prediction. Delex This is the same as the PanParser models, except that all lexicalized features are removed, including word embeddings. UDPipe+PanParser As relation labels are sometimes better predicted by PanParser than UDPipe, we also consider combining their outputs at prediction time: we first annotate the input with UDPipe, discard the predicted labels and replace them with labels predicted by PanParser on UDPipe trees. Project-en Based on parallel data with English, we use th"
K17-3017,N13-1073,0,0.010447,"ges, retaining the 10 first sentences for the trainset and the 4 last sentences for the devset. We always use gold tokenization and segmentation during training, but to improve robustness to noisy tags, all models are trained on treebanks with predicted tags, provided by the task organizers.2 We use the word embeddings provided by the organizers, computed on monolingual data preprocessed by UDPipe.3 Parallel data from the OPUS platform (Tiedemann, 2012) is preprocessed as follows: for each pair, all corpora are concatenated, tokenized and annotated by UDPipe, and word aligned with fast align (Dyer et al., 2013). ity with a selective combination of several base parsers. For instance, a cross-lingual delexicalized parser intuitively provides insights on the main syntactic structures, presumably shared because of linguistic similarities (typically assessed using linguistic knowledge), while a monolingual parser can learn target-specific structures in target data. On one hand, if monolingual data is too small, it does not contain enough information on the main syntactic structures, and the cross-lingual parser will be more accurate; it should be preferred for this kind of dependencies. On the other hand"
K17-3017,P15-2040,0,0.11629,"the black areas represent tokens whose head has already been predicted. UDPipe We apply the official UDPipe 1.1 baseline models (Straka, 2017). For the surprise languages, we train our own model.4 2.1.3 Cross-lingual For each treebank under 1,000 training sentences, we apply cross-treebank techniques to build additional parsers. First, for each target treebank, we transform every source treebank by delexicalizing it and applying the WALS rewrite rules of Aufrant et al. (2016). We then compute, for each such treebank, its similarity to the target treebank, using the KLcpos3 divergence metric (Rosa and Zabokrtsky, 2015). We select the source among treebanks over 2,000 sentences, by retaining the languages requiring the smallest number of rewrite rules (i.e. the smallest number of divergent WALS features), and then choosing the (transformed) treebank minimizing the KLcpos3 divergence. When the selected source is of the same language, we use domain adaptation techniques, otherwise we turn to cross-lingual methods. However, domain adaptation was not used in the final submission as it did not bring significant improvements on the corresponding treebanks (FrenchParTUT, Galician-TreeGal and Czech-CLTT), and is not"
K17-3017,N16-1121,1,0.878232,"Missing"
K17-3017,L16-1680,0,0.0735301,"Missing"
K17-3017,P11-2093,0,0.0117225,"rawled monolingual data (after UDPipe tokenization, to segment punctuation); in case of OOV the pair remains unchanged. We set the PMI lowerand upperbounds to log 5 and log 400. Model selection For each treebank, we compare all base and cascade parsers, and retain the parser yielding the best LAS on the provided development set (using gold tokenization). However, in some languages this dataset was particularly small and consequently biased, which often led to selecting the wrong model, as will be seen in §4. Chinese and Japanese We rely on UDPipe for sentence segmentation, and then use KyTea (Neubig et al., 2011) to tokenize each sentence. KyTea models are trained on UD Chinese and Japanese training treebanks. 6 Depending on the data sizes, the cascades train in a few hours to two days on CPU, using 5 threads. For all three languages, the newly tokenized input is then morphologically annotated by UDPipe. 167 3 Overall results UDPipe [off.] As part of the CoNLL 2017 UD Shared Task, we evaluated our system on the TIRA platform (Potthast et al., 2014). Evaluation runs on the virtual machine took 10.5 hours on a single thread, using up to 6GB RAM. Table 1 presents our overall results as published by the o"
K17-3017,tiedemann-2012-parallel,0,0.058503,"the UD data (Nivre et al., 2017a), following the splits provided with the official baseline (Straka, 2017).1 We perform a similar split for the surprise languages, retaining the 10 first sentences for the trainset and the 4 last sentences for the devset. We always use gold tokenization and segmentation during training, but to improve robustness to noisy tags, all models are trained on treebanks with predicted tags, provided by the task organizers.2 We use the word embeddings provided by the organizers, computed on monolingual data preprocessed by UDPipe.3 Parallel data from the OPUS platform (Tiedemann, 2012) is preprocessed as follows: for each pair, all corpora are concatenated, tokenized and annotated by UDPipe, and word aligned with fast align (Dyer et al., 2013). ity with a selective combination of several base parsers. For instance, a cross-lingual delexicalized parser intuitively provides insights on the main syntactic structures, presumably shared because of linguistic similarities (typically assessed using linguistic knowledge), while a monolingual parser can learn target-specific structures in target data. On one hand, if monolingual data is too small, it does not contain enough informat"
K17-3017,L16-1262,0,0.0959654,"Missing"
L16-1099,P06-1009,0,0.0362251,"000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015). Word alignment is employed in bilingual lexica extraction (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), etc. Thanks to a sustained research effort, many alignment methods have been proposed. Two recent reviews of bitext alignment are in (Wu, 2010; Tiedemann, 2011). Manually annotated reference alignment data sets are valuable resources for the development of alignment techniques. On the one hand, they can be used as the supervision examples for the methods (M´ujdricza-Maydt et al., 2013; Blunsom and Cohn, 2006); on the other hand, they provide ways to directly evaluate automatic alignment quality, and warrant the investigation of error patterns. However, constructing manually annotated alignment data sets can be challenging. For some tasks, this can be due to a lack of a clear annotation scheme. For others, annotation schemes can vary a lot, depending on the targeted applications, language pairs, etc. In this paper, we describe our contribution to manual sentence-level alignment annotations in Section 2., followed by word-level alignment annotations in Section 3.. For sentence alignment, the researc"
L16-1099,P91-1022,0,0.749299,"e, if both [Ei ; Fj ] and [Ei+1 ; Fj+1 ] are good alignment links, then it is incorrect to form a larger link [Ei , Ei+1 ; Fj , Fj+1 ]. • Alignment links are monotone. Thus, if [Ei ; Fj ] is a link, then no source sentences following Ei (e.g. Ei+1 ) can link to target sentences preceding Fj (e.g. Fj−1 ). A main advantage of these assumptions is that they warrant the use of dynamic programming to perform efficient search. To our knowledge, all automatic sentence alignment systems make such assumptions. Classical sentence alignment systems were initially designed to align institutional bitexts (Brown et al., 1991; Gale and Church, 1991), such as the Canadian Hansards and the Europarl corpus (Koehn, 2005). The ARCADE 1 If one side of a link is empty, it is called a null link. However a 0-to-0 link makes no sense and is not allowed. evaluation campaigns (V´eronis and Langlais, 2000; Chiao et al., 2006) have demonstrated that the quality of automatic alignments is variable, depending on the bitext genres and languages. For certain types of bitexts which are relatively regular, such as institutional bitexts, the task is easy and all systems tend to deliver good results (the basic system of Brown et al. (1"
L16-1099,J93-2003,0,0.144621,"l prove useful to evaluate alignment software and quality estimation tools for automatic alignment. Keywords: Parallel corpora, Sentence Alignments, Word Alignments, Confidence Estimation 1. Introduction Bitext alignment consists of finding corresponding units in bitexts, where a bitext is defined as the association of two texts assumed to be mutual translations. Such a mapping can be established at various levels of granularity: between paragraphs, between sentences, between phrases, or between words. Primarily because of the development of Statistical Machine Translation (SMT) technologies (Brown et al., 1993), sentence-level and word-level alignments have been studied for a long time. In state-of-the-art phrasebased SMT, sentence alignment aims at providing parallel sentence pairs for word alignment which is an important component of the complete pipeline (Koehn et al., 2003). Their uses extend to many other natural language processing (NLP) applications. For instance, sentence alignment has been applied in translator training (Simard et al., 1993), translation checking (Macklovitch, 1994), language learning (Nerbonne, 2000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015)."
L16-1099,chiao-etal-2006-evaluation,0,0.0882799,"Missing"
L16-1099,P02-1033,0,0.0483188,"SMT, sentence alignment aims at providing parallel sentence pairs for word alignment which is an important component of the complete pipeline (Koehn et al., 2003). Their uses extend to many other natural language processing (NLP) applications. For instance, sentence alignment has been applied in translator training (Simard et al., 1993), translation checking (Macklovitch, 1994), language learning (Nerbonne, 2000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015). Word alignment is employed in bilingual lexica extraction (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), etc. Thanks to a sustained research effort, many alignment methods have been proposed. Two recent reviews of bitext alignment are in (Wu, 2010; Tiedemann, 2011). Manually annotated reference alignment data sets are valuable resources for the development of alignment techniques. On the one hand, they can be used as the supervision examples for the methods (M´ujdricza-Maydt et al., 2013; Blunsom and Cohn, 2006); on the other hand, they provide ways to directly evaluate automatic alignment quality, and warrant the investigation of error patterns. However, constructing manually annotated alignme"
L16-1099,P11-1042,0,0.0455922,"Missing"
L16-1099,J07-3002,0,0.0362909,"gold alignments, which distinguishes between Sure links and Possible links. AER amounts to a F1 measure where recall and precision are computed differently for these two types of links. This metric and the corresponding annotation scheme have been 4 630 Downloadable from http://opus.lingfil.uu.se/. Figure 1: Sentence alignment confidence annotation. For each alignment link, the color of the special symbol “$$$” encodes its label: green for “sure”, violet for “partial”, etc. Note the untranslated part of the pair 3003 (labelled “partial”) appears in gray. criticized in many subsequent studies (Fraser and Marcu, 2007), notably due to the lack of clear semantics of P-links, which tend to be used in too many situations (non-literal translation, many-to-many alignments, etc.). Regarding extrinsic metrics, a widely used approach is to consider SMT output quality measured by automatic scores such as BLEU. As repeatedly noted (Lopez and Resnik, 2006; Fraser and Marcu, 2007; Lambert et al., 2010), AER poorly correlates with translation quality, especially for large corpora, which makes the direct comparison of alignment systems more difficult. Building reference alignments The construction of gold word alignments"
L16-1099,P91-1023,0,0.736596,"Missing"
L16-1099,W08-0509,0,0.0227761,"“Saturday”); • partial: the pair of words do not constitute a good link by themselves, but they should be included in a larger link (group of words), e.g. “(make) use (of) – (se) servir (de)”; • wrong: the corresponding pair of words should not be aligned. This annotation scheme has been tested using highconfidence 1-to-1 links produced automatically. This set of alignments was prepared as follows. For each language pair, we first combined the sentence-aligned “Candide” and the Europarl data for this language pair (Koehn, 2005) into a parallel corpus, which was word-aligned by running MGIZA (Gao and Vogel, 2008) in both directions. We then formed a small candidate corpus, by taking all sentence pairs of “Candide” and a few hundreds of the Europarl.6 Finally, for each sentence pair in the candidate corpus, we have selected at most five 1-to-1 links in the intersection of the directional alignments, thereby ensuring that the potential alignment points were sensible choices. Each link was then manually annotated with one of the four labels described above. Figure 2 illustrates the annotation process for one parallel sentence from “Candide” (French-English). Using this methodology, we were able to collec"
L16-1099,P08-4006,0,0.136885,"Missing"
L16-1099,graca-etal-2008-building,0,0.0551692,"Missing"
L16-1099,W11-4615,0,0.0182192,"84 sentence pairs of the Hansard corpus (English-French), further introducing the Sure/Possible distinction. Mihalcea and Pedersen (2003) collected a set of English-Romanian word alignments for 265 sentence pairs, again using the Blinker guidelines and the S/P scheme. Lambert et al. (2005) created guidelines to align 500 sentence pairs of the English-Spanish version of Europarl, with the explicit purpose to create high recall alignments. Some more recent works are (Kruijff-Korbayova et al., 2006) (EnglishCzech), (Grac¸a et al., 2008) (multiple language pairs), (Macken, 2010) (English-Dutch), (Holmqvist and Ahrenberg, 2011) (English-Swedish), etc, most of them sticking to the S/P scheme. We propose new methodologies to collect evaluation data for word alignment. Our proposal relies on two distinct protocols: the first focuses on 1-to-1 alignments and proposes on a much clarified version of the S/P distinction (see § 3.2.); the second specifically targets many-to-many alignments, and is based on a divisive annotation strategy which proceeds iteratively (see § 3.3.). For both tasks, the annotations are carried out with adapted versions of Yawat. 3.2. A new annotation scheme for 1-to-1 alignments The S/P annotation"
L16-1099,P09-1105,0,0.0264802,"s, we have proposed new annotation schemes for both sentence and word level alignments. We contribute also a method for collecting reference many-to-many alignments, which, we believe, is an innovative attempt for direct evaluation of this kind of alignments. The resources and corresponding annotation guidelines are publicly available.8 We plan to use these annotations to evaluate results delivered by standard sentence alignment and word alignment tools. In particular, we are interested in using these data to evaluate confidence estimation measures, e.g. based on posterior link probabilities (Huang, 2009). Another lesson learned in this annotation exercices is that sentence-level and word-level alignments are quite sensitive to the pre-processing, e.g. sentence segmentation, tokenization in words, etc. It might be beneficial to investigate new ways to overcome these man-made noises so as to produce gold annotations that would be less dependent on these early steps. 5. Acknowledgements This work was partly supported by French National Research Agency under project Transread (ANR-12CORD-0015). We thank L. Berenice, C. Cl´ement and M. Sgourelli for performing the annotations. We have c made good"
L16-1099,N03-1017,0,0.00984486,"where a bitext is defined as the association of two texts assumed to be mutual translations. Such a mapping can be established at various levels of granularity: between paragraphs, between sentences, between phrases, or between words. Primarily because of the development of Statistical Machine Translation (SMT) technologies (Brown et al., 1993), sentence-level and word-level alignments have been studied for a long time. In state-of-the-art phrasebased SMT, sentence alignment aims at providing parallel sentence pairs for word alignment which is an important component of the complete pipeline (Koehn et al., 2003). Their uses extend to many other natural language processing (NLP) applications. For instance, sentence alignment has been applied in translator training (Simard et al., 1993), translation checking (Macklovitch, 1994), language learning (Nerbonne, 2000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015). Word alignment is employed in bilingual lexica extraction (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), etc. Thanks to a sustained research effort, many alignment methods have been proposed. Two recent reviews of bitext alignment are in (Wu, 2"
L16-1099,2005.mtsummit-papers.11,0,0.114936,"label. ing only in the specific context, e.g. “tomorrow – samedi” (French for “Saturday”); • partial: the pair of words do not constitute a good link by themselves, but they should be included in a larger link (group of words), e.g. “(make) use (of) – (se) servir (de)”; • wrong: the corresponding pair of words should not be aligned. This annotation scheme has been tested using highconfidence 1-to-1 links produced automatically. This set of alignments was prepared as follows. For each language pair, we first combined the sentence-aligned “Candide” and the Europarl data for this language pair (Koehn, 2005) into a parallel corpus, which was word-aligned by running MGIZA (Gao and Vogel, 2008) in both directions. We then formed a small candidate corpus, by taking all sentence pairs of “Candide” and a few hundreds of the Europarl.6 Finally, for each sentence pair in the candidate corpus, we have selected at most five 1-to-1 links in the intersection of the directional alignments, thereby ensuring that the potential alignment points were sensible choices. Each link was then manually annotated with one of the four labels described above. Figure 2 illustrates the annotation process for one parallel se"
L16-1099,kruijff-korbayova-etal-2006-annotation,0,0.0154972,"rse pairs of the Bible (English-French) with a binary annotation scheme. Och and Ney (2000) used the Blinker guidelines to align 484 sentence pairs of the Hansard corpus (English-French), further introducing the Sure/Possible distinction. Mihalcea and Pedersen (2003) collected a set of English-Romanian word alignments for 265 sentence pairs, again using the Blinker guidelines and the S/P scheme. Lambert et al. (2005) created guidelines to align 500 sentence pairs of the English-Spanish version of Europarl, with the explicit purpose to create high recall alignments. Some more recent works are (Kruijff-Korbayova et al., 2006) (EnglishCzech), (Grac¸a et al., 2008) (multiple language pairs), (Macken, 2010) (English-Dutch), (Holmqvist and Ahrenberg, 2011) (English-Swedish), etc, most of them sticking to the S/P scheme. We propose new methodologies to collect evaluation data for word alignment. Our proposal relies on two distinct protocols: the first focuses on 1-to-1 alignments and proposes on a much clarified version of the S/P distinction (see § 3.2.); the second specifically targets many-to-many alignments, and is based on a divisive annotation strategy which proceeds iteratively (see § 3.3.). For both tasks, the"
L16-1099,2010.eamt-1.7,0,0.0231566,"color of the special symbol “$$$” encodes its label: green for “sure”, violet for “partial”, etc. Note the untranslated part of the pair 3003 (labelled “partial”) appears in gray. criticized in many subsequent studies (Fraser and Marcu, 2007), notably due to the lack of clear semantics of P-links, which tend to be used in too many situations (non-literal translation, many-to-many alignments, etc.). Regarding extrinsic metrics, a widely used approach is to consider SMT output quality measured by automatic scores such as BLEU. As repeatedly noted (Lopez and Resnik, 2006; Fraser and Marcu, 2007; Lambert et al., 2010), AER poorly correlates with translation quality, especially for large corpora, which makes the direct comparison of alignment systems more difficult. Building reference alignments The construction of gold word alignments is a complicated task: their specification must address deep linguistic issues (which are often specific to language pairs), but also take into account the intended use of these alignments, notwithtanding more concrete issues such as interface design and disagreement resolution procedures. Melamed (1998) was the first to propose a complete annotation guideline for the Blinker"
L16-1099,2013.mtsummit-papers.10,0,0.0272042,"Missing"
L16-1099,N06-1014,0,0.219009,"Missing"
L16-1099,2006.amta-papers.11,0,0.0284615,"idence annotation. For each alignment link, the color of the special symbol “$$$” encodes its label: green for “sure”, violet for “partial”, etc. Note the untranslated part of the pair 3003 (labelled “partial”) appears in gray. criticized in many subsequent studies (Fraser and Marcu, 2007), notably due to the lack of clear semantics of P-links, which tend to be used in too many situations (non-literal translation, many-to-many alignments, etc.). Regarding extrinsic metrics, a widely used approach is to consider SMT output quality measured by automatic scores such as BLEU. As repeatedly noted (Lopez and Resnik, 2006; Fraser and Marcu, 2007; Lambert et al., 2010), AER poorly correlates with translation quality, especially for large corpora, which makes the direct comparison of alignment systems more difficult. Building reference alignments The construction of gold word alignments is a complicated task: their specification must address deep linguistic issues (which are often specific to language pairs), but also take into account the intended use of these alignments, notwithtanding more concrete issues such as interface design and disagreement resolution procedures. Melamed (1998) was the first to propose"
L16-1099,macken-2010-annotation,0,0.0280087,"he Blinker guidelines to align 484 sentence pairs of the Hansard corpus (English-French), further introducing the Sure/Possible distinction. Mihalcea and Pedersen (2003) collected a set of English-Romanian word alignments for 265 sentence pairs, again using the Blinker guidelines and the S/P scheme. Lambert et al. (2005) created guidelines to align 500 sentence pairs of the English-Spanish version of Europarl, with the explicit purpose to create high recall alignments. Some more recent works are (Kruijff-Korbayova et al., 2006) (EnglishCzech), (Grac¸a et al., 2008) (multiple language pairs), (Macken, 2010) (English-Dutch), (Holmqvist and Ahrenberg, 2011) (English-Swedish), etc, most of them sticking to the S/P scheme. We propose new methodologies to collect evaluation data for word alignment. Our proposal relies on two distinct protocols: the first focuses on 1-to-1 alignments and proposes on a much clarified version of the S/P distinction (see § 3.2.); the second specifically targets many-to-many alignments, and is based on a divisive annotation strategy which proceeds iteratively (see § 3.3.). For both tasks, the annotations are carried out with adapted versions of Yawat. 3.2. A new annotatio"
L16-1099,1994.amta-1.21,0,0.0851041,"r between words. Primarily because of the development of Statistical Machine Translation (SMT) technologies (Brown et al., 1993), sentence-level and word-level alignments have been studied for a long time. In state-of-the-art phrasebased SMT, sentence alignment aims at providing parallel sentence pairs for word alignment which is an important component of the complete pipeline (Koehn et al., 2003). Their uses extend to many other natural language processing (NLP) applications. For instance, sentence alignment has been applied in translator training (Simard et al., 1993), translation checking (Macklovitch, 1994), language learning (Nerbonne, 2000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015). Word alignment is employed in bilingual lexica extraction (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), etc. Thanks to a sustained research effort, many alignment methods have been proposed. Two recent reviews of bitext alignment are in (Wu, 2010; Tiedemann, 2011). Manually annotated reference alignment data sets are valuable resources for the development of alignment techniques. On the one hand, they can be used as the supervision examples for the methods"
L16-1099,W03-0301,0,0.0751156,"eep linguistic issues (which are often specific to language pairs), but also take into account the intended use of these alignments, notwithtanding more concrete issues such as interface design and disagreement resolution procedures. Melamed (1998) was the first to propose a complete annotation guideline for the Blinker project, which was used to align 250 verse pairs of the Bible (English-French) with a binary annotation scheme. Och and Ney (2000) used the Blinker guidelines to align 484 sentence pairs of the Hansard corpus (English-French), further introducing the Sure/Possible distinction. Mihalcea and Pedersen (2003) collected a set of English-Romanian word alignments for 265 sentence pairs, again using the Blinker guidelines and the S/P scheme. Lambert et al. (2005) created guidelines to align 500 sentence pairs of the English-Spanish version of Europarl, with the explicit purpose to create high recall alignments. Some more recent works are (Kruijff-Korbayova et al., 2006) (EnglishCzech), (Grac¸a et al., 2008) (multiple language pairs), (Macken, 2010) (English-Dutch), (Holmqvist and Ahrenberg, 2011) (English-Swedish), etc, most of them sticking to the S/P scheme. We propose new methodologies to collect e"
L16-1099,C00-2163,0,0.567234,"ficult, if possible at all, for annotators to agree on the correctness of certain alignment links. On the other hand, the notion of alignment quality can only be understood in reference to some targeted application. Applications such as bilingual lexical extraction prefer high precision word alignments, while others such as SMT might prefer high recall alignments (Och and Ney, 2004). Therefore, evaluation of word alignments typically include both intrinsic and extrinsic metrics. The most commonly used intrinsic evaluation metric for word alignment is the Alignment Error Rate (AER) proposed by Och and Ney (2000). It relies on a particular annotation scheme for gold alignments, which distinguishes between Sure links and Possible links. AER amounts to a F1 measure where recall and precision are computed differently for these two types of links. This metric and the corresponding annotation scheme have been 4 630 Downloadable from http://opus.lingfil.uu.se/. Figure 1: Sentence alignment confidence annotation. For each alignment link, the color of the special symbol “$$$” encodes its label: green for “sure”, violet for “partial”, etc. Note the untranslated part of the pair 3003 (labelled “partial”) appear"
L16-1099,J04-4002,0,0.0650314,"Missing"
L16-1099,J96-1001,0,0.0630217,"for a long time. In state-of-the-art phrasebased SMT, sentence alignment aims at providing parallel sentence pairs for word alignment which is an important component of the complete pipeline (Koehn et al., 2003). Their uses extend to many other natural language processing (NLP) applications. For instance, sentence alignment has been applied in translator training (Simard et al., 1993), translation checking (Macklovitch, 1994), language learning (Nerbonne, 2000; Kraif and Tutin, 2011), and bilingual reading (Pillias and Cubaud, 2015). Word alignment is employed in bilingual lexica extraction (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), etc. Thanks to a sustained research effort, many alignment methods have been proposed. Two recent reviews of bitext alignment are in (Wu, 2010; Tiedemann, 2011). Manually annotated reference alignment data sets are valuable resources for the development of alignment techniques. On the one hand, they can be used as the supervision examples for the methods (M´ujdricza-Maydt et al., 2013; Blunsom and Cohn, 2006); on the other hand, they provide ways to directly evaluate automatic alignment quality, and warrant the investigation of error pattern"
L16-1099,Q13-1001,0,0.0505105,"Missing"
L16-1099,D15-1209,0,0.0402708,"Missing"
L16-1099,D14-1187,1,0.893267,"Missing"
L16-1099,J93-1004,0,\N,Missing
L16-1241,chrupala-etal-2008-learning,0,0.0892488,"Missing"
L16-1241,P13-2111,0,0.0133489,"E(ys , yt , (s(i) , s(j) ), (t(i) , t(j) ))   +1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ yt     + 1 if (s(i) , s(j) ) ∈ ys and t(i) == t(j)  4   + 1 if s == s and (t , t ) ∈ y t (i) (j) (i) (j) 4 =  −1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ / yt      −1 if (s(i) , s(j) ) ∈ / ys and (t(i) , t(j) ) ∈ yt    0 otherwise Experimental Evaluation In all experiments, we train transition-based dependency parsers with the arceager transition system, an averaged perceptron, beam search of size 8 and early update, using our own implementation based on the recommendations of (Goldberg et al., 2013). We use universal PoS and the feature templates of (Zhang and Nivre, 2011), without labels and with decision history of size 8. These features, designed for English, have not been tailored to the specificities of Romanian. We remove dependency annotations from RSAC-train to use it both as tagger trainset and parser relexicalization data. Source models are trained on UDT and Europarl is PoS annotated with supervised taggers and truncated to 80,000 sentences to limit the bias towards projection. The supervised parser is trained on annotated RSAC-train, thus enabling the comparison with the mode"
L16-1241,2005.mtsummit-papers.11,0,0.0555234,"sess the interest of cross-lingual methods (§5.) for under-resourced languages. All tools and resources used in this work are available at https://perso.limsi.fr/aufrant/. 1520 2. Resources for Romanian Over the years, several corpora have been collected for Romanian. We are particularly interested in parallel corpora that will allow us to transfer annotations and corpora annotated with PoS and dependencies that can be used to evaluate cross-lingual taggers and parsers. In this section, we will quickly describe existing corpora. Most work on cross-lingual transfer rely on the Europarl corpus (Koehn, 2005) as a source for parallel sentences. It notably includes Romanian sentences, with their translation in 20 European languages such as English or Spanish, that we will use in our experiments. (Perez, 2012) released the treebank Romanian Syntactic Annotated Corpus (RSAC) of 67,686 tokens (punctuations excluded) and 3,587 sentences from various sources (JRC-Acquis, Wikipedia, 1984, textbook exercises and translations from FrameNet). Two other corpora exist: MULTEXT-East, a multilingual corpus extracted from the novel 1984, sentence-aligned and with morphosyntactic annotations; a corpus of 36,150 t"
L16-1241,N16-1121,1,0.897387,"Missing"
L16-1241,D11-1006,0,0.447562,"hat is why, we apply state-of-the-art methods for cross-lingual transfer on Romanian tagging and parsing, from English and several Romance languages. We compare the performance with monolingual systems trained with sets of different sizes and establish that training on a few sentences in target language yields better results than transferring from large datasets in other languages. Keywords: Cross-Lingual transfer, Part-of-Speech tagging, Dependency parsing, Romanian 1. Introduction • we implement two state-of-the-art transfer methods for PoS-tagging (T¨ackstr¨om et al., 2013) and dependency (McDonald et al., 2011) parsing for Romanian; While most Romance languages are well studied in the Natural Language Processing field and have large sets of annotated data, Romanian still stays behind. As a result, most of the tools generally used in the preprocessing steps of NLP pipelines such as lemmatizer, PoS-tagger or dependency parser are not available for Romanian and, when they exist, their performance often fall far short of the performance achieved, for instance, on French or English (Straka et al., 2015). Romanian is therefore a prime candidate for applying transfer methods (Pan and Yang, 2010). Many work"
L16-1241,P13-2017,0,0.0430893,"Missing"
L16-1241,petrov-etal-2012-universal,0,0.0937942,"Missing"
L16-1241,Q13-1001,0,0.0522485,"Missing"
L16-1241,D14-1187,1,0.898835,"Missing"
L16-1241,I08-3008,0,0.08678,"15 extra rules, that improve the scores by 3 points) and since they represent an important part of the prediction errors, alleviating them may turn weakly supervised taggers into truly competitive solutions for low-resourced languages. 4. Cross-lingual dependency parser We conduct a similar study on dependency parsing, considering the framework proposed by McDonald et al. (2011) to train parsers in Romanian from various combinations of source languages. Transfer Method We briefly present here McDonald et al. (2011)’s algorithm. The transfer process starts with a delexicalized model transfer (Zeman and Resnik, 2008; McDonald et al., 2013): assuming all languages are annotated using a common PoS tagset, a model considering only PoS features is trained on a source language and used to parse directly Romanian. Using a common representation enables combination of multiple sources with raw treebank concatenation. This crude approach has proven effective for many languages even if it is hindered by the lack of lexical information. To overcome this limit, McDonald et al. (2011) propose to relexicalize the model, in a second step: a small set of unannotated target data is first annotated by the delexicalized mo"
L16-1241,P11-2033,0,0.0296188,"nd (t(i) , t(j) ) ∈ yt     + 1 if (s(i) , s(j) ) ∈ ys and t(i) == t(j)  4   + 1 if s == s and (t , t ) ∈ y t (i) (j) (i) (j) 4 =  −1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ / yt      −1 if (s(i) , s(j) ) ∈ / ys and (t(i) , t(j) ) ∈ yt    0 otherwise Experimental Evaluation In all experiments, we train transition-based dependency parsers with the arceager transition system, an averaged perceptron, beam search of size 8 and early update, using our own implementation based on the recommendations of (Goldberg et al., 2013). We use universal PoS and the feature templates of (Zhang and Nivre, 2011), without labels and with decision history of size 8. These features, designed for English, have not been tailored to the specificities of Romanian. We remove dependency annotations from RSAC-train to use it both as tagger trainset and parser relexicalization data. Source models are trained on UDT and Europarl is PoS annotated with supervised taggers and truncated to 80,000 sentences to limit the bias towards projection. The supervised parser is trained on annotated RSAC-train, thus enabling the comparison with the model relexicalized on the same data. All methods are evaluated on RSAC-test wi"
max-etal-2010-contrastive,E09-1010,0,\N,Missing
max-etal-2010-contrastive,W09-0441,0,\N,Missing
max-etal-2010-contrastive,P02-1040,0,\N,Missing
max-etal-2010-contrastive,W09-0401,0,\N,Missing
max-etal-2010-contrastive,P07-2045,0,\N,Missing
max-etal-2010-contrastive,J04-2004,0,\N,Missing
max-etal-2010-contrastive,W07-0734,0,\N,Missing
max-etal-2010-contrastive,J06-4004,1,\N,Missing
max-etal-2010-contrastive,E09-1082,0,\N,Missing
max-etal-2010-contrastive,2005.mtsummit-papers.11,0,\N,Missing
max-etal-2010-contrastive,vilar-etal-2006-error,0,\N,Missing
max-etal-2010-contrastive,2008.amta-srw.6,0,\N,Missing
max-etal-2010-contrastive,carpuat-wu-2008-evaluation,0,\N,Missing
N12-1005,N03-2002,0,0.0449754,"earned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchic"
N12-1005,J92-4003,0,0.190024,"n the size of the output vocabulary (i.e. the number of words that have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed of the most frequent words (Schwenk, 2007). However, the usual size of the short-list is under 20k, which does not seem sufficient to faithfully represent the translation models of section 2. 3.2 Principles of SOUL To circumvent this problem, Structured Output Layer (SOUL) LMs are introduced in (Le et al., 2011a). Following Mnih and Hinton (2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). Structuring the output layer and using word class information makes the estimation of distributions over the entire vocabulary computationally feasible. wi-1 To meet this goal, the output vocabulary is structured as a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the ith word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for word wi in the clustering tree, with D being the depth of the tree, cd (wi ) a class or sub-class assigned to wi , and cD (wi ) being the leaf associated with wi (the word itself). The"
N12-1005,J93-2003,0,0.0566446,"e, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those"
N12-1005,J04-2004,0,0.412345,"ural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 40 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely path in this lattice using a n-gram translation model 2 of bilingual units. 2.1 The standard n-gram translation model n-gram translation models (TMs) rely on a specific decomposition of the joint probability P (s, t"
N12-1005,P96-1041,0,0.836328,"e random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization power of the model and reduces its ability to adapt to other domains. Another consequence is that phrase-based models usually consider a very restricted context1 . This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1 typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 39 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, c Montr´eal, Canada, June 3-8, 2012."
N12-1005,W06-1607,0,0.0733779,"an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented her"
N12-1005,D07-1091,0,0.0607795,"Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchical architecture (SOUL). By considering several decompositions o"
N12-1005,P07-2045,0,0.0140012,"eral issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1 typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 39 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvem"
N12-1005,2011.iwslt-evaluation.7,1,0.825382,"ights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to some of the best official submissions. 4.3 Small task evaluation Table 2 summarizes the results obtained with the baseline and different SOUL models, TMs and a target LM. The first comparison concerns the standard n-gram TM, defined by equation (2), when estimated conventionally or as a SOUL model. Adding the latter model yields a slight BLEU improvement of 0.5 point over the baseline. When the SOUL TM"
N12-1005,J06-4004,0,0.680784,"Missing"
N12-1005,W11-2124,0,0.23067,"system. The rest of this paper is organized as follows. We first recollect, in Section 2, the n-gram based approach, and discuss various implementations of this framework. We then describe, in Section 3, the neural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 40 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely"
N12-1005,J04-4002,0,0.136322,"derlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization p"
N12-1005,P03-1021,0,0.161984,"are included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to so"
N12-1005,D07-1045,0,0.820912,"Missing"
N12-1005,P06-1124,0,0.0126685,"e sequences of tokens (typically words) w1L in V + as follows: P (w1L ) = L Y i−1 P (wi |wi−n+1 ) (6) i=1 Modeling the joint distribution of several discrete random variables (such as words in a sentence) is difficult, especially in NLP applications where V typically contains dozens of thousands words. In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams. Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order dis42 tributions, thus providing an estimate for the probability of these unseen events. One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This turns n-grams distributions into smooth functions of the word representations. These representations and the associated estimates are jointly computed in a multi-layer neural network architecture. Figure 2 provides a partial representation"
N12-1005,N04-4026,0,0.250662,"In a nutshell, the TM is implemented as a stochastic finite-state transducer trained using a ngram model of (source, target) pairs as described in section 2.1. Training this model requires to reorder source sentences so as to match the target word order. This is performed by a non-deterministic finitestate reordering model, which uses part-of-speech information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities. In addition to the TM, fourteen feature functions are included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task,"
N12-1005,2010.iwslt-evaluation.4,0,0.0733703,"ork To the best of our knowledge, the first work on machine translation in continuous spaces is (Schwenk et al., 2007), where the authors introduced the model referred here to as the the standard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model of (Bengio et al., 2003), the basic unit is the tuple (or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors had thus to restrict the set of the predicted units to a 8k short-list . Moreover, in (Zamora-Martinez et al., 2010), the authors propose a tighter integration of a continuous space model with a n-gram approach but only for the target LM. A different approach, described in (Sarikaya et al., 2008), divides the problem in two parts: first the continuous representation is obtained by an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the"
N12-1005,2002.tmi-tutorials.2,0,0.312467,"ine translation (SMT) is based on the following inference rule, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological"
N12-1005,W11-2135,1,\N,Missing
N16-1121,D12-1133,0,0.0715131,"Missing"
N16-1121,D11-1005,0,0.459085,"Missing"
N16-1121,P04-1015,0,0.319073,"Missing"
N16-1121,C12-1059,0,0.22509,"Missing"
N16-1121,2005.mtsummit-papers.11,0,0.103953,"Missing"
N16-1121,W16-1203,1,0.662422,"Missing"
N16-1121,P14-1126,0,0.136031,"Missing"
N16-1121,D11-1006,0,0.225391,"Missing"
N16-1121,P13-2017,0,0.160124,"Missing"
N16-1121,P12-1066,0,0.478366,"Missing"
N16-1121,J03-1002,0,0.0201885,"Missing"
N16-1121,D15-1039,0,0.502587,"Missing"
N16-1121,P11-2120,0,0.329687,"Missing"
N16-1121,W09-1104,0,0.0849806,"Missing"
N16-1121,N13-1126,0,0.510409,"Missing"
N16-1121,C14-1175,0,0.257607,"Missing"
N16-1121,I08-3008,0,0.586422,"Missing"
N16-1121,P11-2033,0,0.275108,"Missing"
N16-1121,W03-3017,0,\N,Missing
N16-3006,S15-2050,1,0.849792,"s could help (Moore, 2005). Additional functionalities in reading are also envisioned, such as an enhanced and non-distracting access to dictionary information for difficult words. Currently, Web Readers and mobile reading devices offer such functionality through a pop-up window presenting the complete dictionary entry. No assistance is however offered to access the right sense in context, which would be especially helpful for polysemous words or when language proficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropr"
N16-3006,aziz-etal-2012-pet,0,0.0568317,"Missing"
N16-3006,P91-1022,0,0.324818,"Missing"
N16-3006,J93-2003,0,0.160503,"Missing"
N16-3006,P05-3025,0,0.0441529,"nd to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , DuoLir2 and Parallel Text Reader on iOS, the selec"
N16-3006,P08-2007,0,0.0230647,"omputing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): bo"
N16-3006,P91-1023,0,0.735615,"Missing"
N16-3006,W08-0509,0,0.0892199,"Missing"
N16-3006,P08-4006,0,0.0225559,"es difficult challenges: it first requires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full transl"
N16-3006,gilmanov-etal-2014-swift,0,0.0263098,"Missing"
N16-3006,C14-2026,0,0.0220746,"many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , DuoLir2 and Parallel Text Reader on iOS, the selection is performed at the sentence level, using alignments. Whatever level is used, this kind of switch-on-demand inter"
N16-3006,P09-1105,0,0.0233122,"in both directions. Alignments in the intersection were checked and corrected following the recommandations of Och and Ney (2003). Even for such simple texts, alignment errors were numerous, with an AER close to 0.17 (‘The Promise’), and to 0.19 (‘The Verger’). This confirms the intuition that computing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignment"
N16-3006,P07-2045,0,0.00445779,"so calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks"
N16-3006,2012.eamt-1.62,1,0.851957,"002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks to select segmentation points. The resulting alignments are deliberately pretty coarse and primarily meant to be used in a contrastive condition for the human tests. 4 4.1 A Bilingual Reader Design The current version of the T RANS R EAD bilingual reader displays paginated versions of the bitext in parallel views. In Figure 1, the source text is displayed on the right side of the screen and its translation on the left. The user has selected a word in the source ver"
N16-3006,W02-1018,0,0.0654582,"s the intuition that computing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lar"
N16-3006,W15-3048,1,0.846898,"oficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropriate spans and dictionary entries for MWEs found in texts. An experimental evaluation of the interface general design is currently being conducted. We study, notably, the effect of the depth of the alignment structure on human readers behavior. As short term future work, we shall also investigate other interaction techniques for focus management, such as distortion and 3D views for page turning (Cubaud, 2008). The graphic composition engine developed for the curre"
N16-3006,moore-2002-fast,0,0.20677,"Missing"
N16-3006,H05-1011,0,0.0500251,"e software in order to investigate a large design space of interaction for tablets. We have selected the Kivy framework for Python, which en8 As described in http://defoe.sourceforge. net/folio/knuth-plass.html 9 Provided by http://tug.org/tex-hyphen/ Figure 1: The T RANS R EAD bilingual reader application running on tablet ables cross-platform development for Android or iOS, and GPU-based graphics with OpenGL ES. 5 Perspectives As reflected in this paper, a top priority is to pursue our efforts towards high-precision alignments, an application where supervised learning techniques could help (Moore, 2005). Additional functionalities in reading are also envisioned, such as an enhanced and non-distracting access to dictionary information for difficult words. Currently, Web Readers and mobile reading devices offer such functionality through a pop-up window presenting the complete dictionary entry. No assistance is however offered to access the right sense in context, which would be especially helpful for polysemous words or when language proficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015)"
N16-3006,J03-1002,0,0.00851268,"Missing"
N16-3006,P14-1143,0,0.025515,"nse disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropriate spans and dictionary entries for MWEs found in texts. An experimental evaluation of the interface general design is currently being conducted. We study, notably, the effect of the depth of the alignment structure on human readers behavior. As short term future work, we shall also investigate other interaction techniques for focus management, such as distortion and 3D views for page turning (Cubaud, 2008). The graphic composition engine developed for the current application already allows such effects. A research agenda should also include long term e"
N16-3006,smith-jahr-2000-cairo,0,0.0372623,"age. Such endeavour poses difficult challenges: it first requires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit"
N16-3006,P15-4021,0,0.0136487,"ires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , Du"
N16-3006,J97-3002,0,0.348397,"ning sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks to select segmentation points. The resulting alignments are deliberately pretty coarse and primarily meant to be used in a contrastive condition for the human tests. 4 4.1 A Bilingual Reader Design The current version of the T RANS R EAD bilingual reader displays paginated versions of the bitext in parallel views. In Figure 1, the"
N16-3006,W12-2505,1,0.896833,"Missing"
N18-2064,E17-2001,0,0.0784208,"representing dependency structures (Hajiˇc et al., 2001; De Marneffe et al., 2014). The divergence between annotation guidelines can result from the theoretical linguistic principles governing the choices of head status and dependency inventories, the tree-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al"
N18-2064,J08-4003,0,0.0572887,"that will be the most similar to the ones seen when predicting a new dependency tree: case det Input: W the input sentence, T the set of gold trees c ← I NITIAL(W ) while ¬T ERMINAL(c) do C ORRECT ← {t|∃T ∈ T , O RACLE(t, c, T ) = 0} tp ← arg maxt∈L EGAL(c) w · φ(c, t) to ← arg maxt∈C ORRECT(c) w · φ(c, t) if tp ∈ / C ORRECT then U PDATE (w, φ(c, to ), φ(c, tp )) tnext ← to else tnext ← tp 12 root case case Algorithm 1: Training on one sentence with multiple references (see text for notations). Training a Dependency Parser with Multiple References Dynamic Oracle In a transition-based parser (Nivre, 2008), a parse is computed by performing a sequence of transitions building the parse tree in an incremental fashion. A partially built dependency tree is represented by a configuration c; when in c, applying a transition t results in the parser moving to a new configuration denoted c◦t. At each step of the parsing process, every possible transition is scored by a classifier (e.g. a linear model), given a feature representation of c and 3 In this work we only consider greedy parsers. Extending the proposed approach to beam parsers would prevent discarding a reference because one of the its transiti"
N18-2064,W16-1202,0,0.0346928,"Missing"
N18-2064,C12-1147,0,0.551312,"-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al., 2016), to build corpora in which each sentence is annotated by a set of possible trees. The UD project aims at developing crosslinguistically consistent treebank annotations for many languages by harmonizing annotation schemes between languages and conve"
N18-2064,W15-2134,0,0.0233015,"ation conventions have been proposed over the years for representing dependency structures (Hajiˇc et al., 2001; De Marneffe et al., 2014). The divergence between annotation guidelines can result from the theoretical linguistic principles governing the choices of head status and dependency inventories, the tree-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically"
N18-2064,W17-0419,1,0.874843,"Missing"
N18-2064,P11-2033,0,0.130738,"ability to detect whether a transition will cause an erroneous dependency. It can naturally be extended to the case of multiple references: a transition is considered correct as long as it can predict at least one of the gold trees; when moving to a new configuration, trees that can no longer be generated are removed from the set of references, in order to make sure the parser will not mix the dependencies of two gold trees (l.11). Parser We use our own implementation of an arc-eager unlabeled dependency parser with a dynamic oracle and an averaged perceptron, using the features described in (Zhang and Nivre, 2011) which have been designed for English and have not been adapted to the specificities of the other languages.6 Training stops when the UAS estimated on the validation set has converged. Upon full completion of parsing, there will remain only one surviving reference that has been selected according to the model current predictions. This reference corresponds to the dependency structure that is the most similar to the hypothesis the parser would have predicted at test time and can therefore be described as the reference the parser prefers: intuitively, Algorithm 1 will thus identify the reference"
N18-2064,P11-2000,0,0.196669,"Missing"
N18-2064,I05-4002,0,0.0603797,"isions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al., 2016), to build corpora in which each sentence is annotated by a set of possible trees. The UD project aims at developing crosslinguistically consistent treebank annotations for many languages by harmonizing annotation schemes between languages and converting existing treebanks to this new scheme. S"
N18-2064,W15-2127,0,0.0135392,"LT 2018, pages 401–406 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2013) have investigated whether the choices made to increase the sharing of structures between languages hurt parsing performance and have identified a variety of choice points in which more than one design could be advocated. Most of these points are related to the issue of headness: contrary to most works in theoretical linguistic, UD assumes that function words should be categorically subordinated to content words to maximize the similarity of dependency trees across languages (Osborne and Maxwell, 2015). The alternative representations we consider are summarized in Table 1. They mostly consist in demoting the lexical head and making it dependent on a functional head. We designed a set of handcrafted rules2 to convert dependencies between these two schemes. Each application of a rule creates a new tree in the set of references that is being built. As shown in Figure 1, the resulting set of references encodes all possible combinations of the considered transformations. root 1 2 3 4 5 6 7 8 9 10 11 ... pour la peine ... root det ... pour la peine ... root det ... pour la peine ... case det ..."
N18-2064,P13-1051,0,0.0730734,"Missing"
N18-2066,P16-1231,0,0.0305474,"Missing"
N18-2066,Q14-1010,0,0.28313,"Missing"
N18-2066,P15-2042,0,0.20838,"Missing"
N18-2066,E17-2051,1,0.882008,"Missing"
N18-2066,C14-1023,0,0.0323952,"Missing"
N18-2066,D14-1082,0,0.108154,"Missing"
N18-2066,H05-1066,0,0.316186,"Missing"
N18-2066,P04-1015,0,0.105485,"Missing"
N18-2066,W03-3017,0,0.29759,"Missing"
N18-2066,W04-0308,0,0.0609338,"Missing"
N18-2066,P15-1033,0,0.0315316,"Missing"
N18-2066,P09-1040,0,0.0753061,"Missing"
N18-2066,C96-1058,0,0.421517,"Missing"
N18-2066,P05-1013,0,0.352684,"Missing"
N18-2066,K17-3009,0,0.0242893,"Missing"
N19-1019,K17-3001,0,0.01863,"ng is known to be a difficult task as annotation guidelines are not always interpreted in a consistent manner (Marcus et al., 1993). For instance, Manning (2011) shows that many errors in the WSJ corpus are just mistakes rather than uncertainties or difficulties in the task ; Table 2 reports some of these annotation divergences that can be found in UD project. The situation is naturally worse in cross-corpora settings, in which treebanks are annotated by different laboratories or groups. The contribution of this paper is threefold : — we show that, as already pointed out by de Marneffe et al. (2017), the variation principle of Boyd et al. (2008) can be used to flag potential annotation discrepancies in the UD project. Building on this principle, we introduce, to evaluate the annotation consistency of a corpus, several methods and metrics that can be used, during the annotation to improve the quality of the corpus. — we generalize the conclusions of Manning (2011), highlighting how error rates in PoS tagging are stemming from the poor quality of annotations and inconsistencies in the resources ; we also systematically quantify the impact of annotation variation on PoS tagging performance"
N19-1019,H92-1026,0,0.31469,"Missing"
N19-1019,W13-2308,0,0.0229098,"scores are averaged over 10 training sessions. 6 for English, 5 for Czech and 4 for Swedish, Chinese, Japanese, Russian and Italian. Overall, it is possible to train and test 290 taggers (i.e. there are 290 possible combinations of a train and a test set of the same language), 191 of these conditions (i.e. pairs of a train set and a test set) correspond to a cross-corpus setting and can be considered for domain adaptation experiments. Many of these corpora 3 result from an automatic transformation (with, for some of them, manual corrections) from existing dependency or constituent treebanks (Bosco et al., 2013; Lipenkova and Souˇcek, 2014). Because most treebanks have been annotated and/or converted independently by different groups, 4 the risk of inconsistencies and errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out"
N19-1019,petrov-etal-2012-universal,0,0.123549,"Missing"
N19-1019,D14-1104,0,0.346871,"; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train and test sets considered, the performance can vary by a factor of more than 25. Many studies (Foster, 2010; Plank et al., 2014) attribute this drop in accuracy to covariate shift (Shimodaira, 2000), characterizing the differences between domains by a change in the marginal distribution p(x) of the input (e.g. increase of out-of-vocabulary words, missing capitalization, different usage of punctuation, etc), while assuming that the conditional label distribution remains unaffected. This work adopts a different point of view : we believe that the variation in tagging performance is due to a dataset shift (Candela et al., 2009), i.e. a change in the joint distribution of the features and labels. We assume that this change"
N19-1019,K17-3009,0,0.0712344,"Missing"
N19-1019,K17-3016,0,0.030071,"ated and/or converted independently by different groups, 4 the risk of inconsistencies and errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out that treebanks for the same language are not consistently annotated (Vilares and Gómez-Rodríguez, 2017; Aufrant et al., 2017). In a closely related context, Wisniewski et al. (2014) have also shown that, in spite of common annotation guidelines, one of the main bottleneck in cross-lingual transfer between UD corpora is the difference in the annotation conventions across treebanks and languages. works do (e.g. to evaluate the quality of a domain adaptation method or the measure the difficulty of the domain adaptation task) can be flawed and that this metrics has to be corrected to take into account the annotation divergences that exists between corpora. The rest of this paper is organized as fo"
N19-1019,L18-1711,1,0.840526,"g. Extracting maximal repeats allows us to find all sequence of words common to at least two sentences without extracting all their substrings. This problem can be solved efficiently using Generalized Suffix Tree (GST) (Gusfield, 1997) : if the corpus contains n words, extracting all the maximal repeats takes O (n) to build the GST and O (n) to list all the repeats. PoS annotations for these repeats can then be easily extracted and the ones that are identical can be filtered out to gather all suspicious repeats in a set of corpora. A detailed description of our implementation can be found in (Wisniewski, 2018). Annotation variation principle Filtering heuristics Suspicious repeats can of course correspond to words or structures that are The annotation variation principle (Boyd et al., 2008) states that if two identical sequences appear 220 ambiguity  The early voting suggests that this time the Latin Americans will come out toPART vote in greater numbers , but it is unclear whether the increase will have an impact .  Keep his cage open and go on your computer , or read a book , etc and maybe he will come out toADP you . inconsistency  Trudeau will extend that invitation to the 45th presidentNOUN"
N19-1019,D14-1187,1,0.832878,"nd errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out that treebanks for the same language are not consistently annotated (Vilares and Gómez-Rodríguez, 2017; Aufrant et al., 2017). In a closely related context, Wisniewski et al. (2014) have also shown that, in spite of common annotation guidelines, one of the main bottleneck in cross-lingual transfer between UD corpora is the difference in the annotation conventions across treebanks and languages. works do (e.g. to evaluate the quality of a domain adaptation method or the measure the difficulty of the domain adaptation task) can be flawed and that this metrics has to be corrected to take into account the annotation divergences that exists between corpora. The rest of this paper is organized as follows. We first present the corpora and the tools used in our experiments (§ 2)"
N19-1019,P11-2033,0,0.0867945,"Missing"
N19-1019,P11-2000,0,0.102847,"ation is naturally worse in cross-corpora settings, in which treebanks are annotated by different laboratories or groups. The contribution of this paper is threefold : — we show that, as already pointed out by de Marneffe et al. (2017), the variation principle of Boyd et al. (2008) can be used to flag potential annotation discrepancies in the UD project. Building on this principle, we introduce, to evaluate the annotation consistency of a corpus, several methods and metrics that can be used, during the annotation to improve the quality of the corpus. — we generalize the conclusions of Manning (2011), highlighting how error rates in PoS tagging are stemming from the poor quality of annotations and inconsistencies in the resources ; we also systematically quantify the impact of annotation variation on PoS tagging performance for a large number of languages and corpora. — we show that the evaluation of PoS taggers in cross-corpora settings (typically in domain adaptation experiments) is hindered by systematic annotation discrepancies between the corpora and quantify the impact of this divergence on PoS tagger evaluation. Our observations stress the fact that comparing in- and out-domain sco"
N19-1019,E03-1068,0,0.418061,"ers in cross-corpora settings (typically in domain adaptation experiments) is hindered by systematic annotation discrepancies between the corpora and quantify the impact of this divergence on PoS tagger evaluation. Our observations stress the fact that comparing in- and out-domain scores as many The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project. This work points out that these variations may result from divergences between the annotation of train and test sets. We show how the annotation variation principle, introduced by Dickinson and Meurers (2003) to automatically detect errors in gold standard, can be used to identify inconsistencies between annotations ; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train"
N19-1019,N10-1060,0,0.0388102,"en annotations ; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train and test sets considered, the performance can vary by a factor of more than 25. Many studies (Foster, 2010; Plank et al., 2014) attribute this drop in accuracy to covariate shift (Shimodaira, 2000), characterizing the differences between domains by a change in the marginal distribution p(x) of the input (e.g. increase of out-of-vocabulary words, missing capitalization, different usage of punctuation, etc), while assuming that the conditional label distribution remains unaffected. This work adopts a different point of view : we believe that the variation in tagging performance is due to a dataset shift (Candela et al., 2009), i.e. a change in the joint distribution of the features and labels. We as"
N19-1019,E14-4028,0,0.0535209,"Missing"
N19-1019,J93-2004,0,0.0643183,"-treebank settings than in situations where the train and the test sets belong to the same treebank. This observation suggests that there may be systematic differences in the annotations of different treebanks which could make the domain adaptation setting artificially more difficult. Table 4: Percentage of suspicious repeats between the EWT and PUD corpora that contain an annotation inconsistency according to a human annotator either when the disjoint heuristic is used or when only suspicious repeats with at least n words are considered. 4.2 the same experiments with the Wall Street Journal (Marcus et al., 1993), 6 the iconic corpus of PoS tagging for which a thorough manual analysis of the annotation quality is described in (Manning, 2011). The observations reported in Table 5 show that the number of repeats varies greatly from one corpus to another, which is not surprising considering the wide array of genres covered by the treebanks that includes sentences written by journalists or learner of English (the genres with the largest number of repeats) or sentences generated by users on social media (that contain far less repeated parts). These observations also show that the percentage of repeats that"
N19-1019,W17-6514,0,0.0553318,"Missing"
P10-1052,N09-1051,0,0.00938076,"component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function l(θ) + ρ1 kθk1 + ρ2 kθk22 2 (6) The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θk+ − θk− , where θk+ and θk− are"
P10-1052,P08-1109,0,0.0726813,"Missing"
P10-1052,P07-1104,0,0.00910618,"the observation that the `1 norm is differentiable when restricted to a set of points in which each coordinate never changes its sign (an “orthant”), and that its second derivative is then zero, meaning that the `1 penalty does not change the Hessian of the objective on each orthant. An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components. In (Gao et al., 2007), the authors show that OWL-QN is faster than the algorithm proposed by Kazama and Tsujii (2003) and can perform model selection even in very high-dimensional problems, with no loss of performance compared to the use of `2 penalty terms. 3.3 The coordinate descent approach of Dud´ık et al. (2004) and Friedman et al. (2008) uses the fact that optimizing a mono-dimensional quadratic function augmented with a `1 penalty can be performed analytically. For arbitrary functions, this idea can be adapted by considering quadratic approximations of the objective around the current value θ¯ lk,θ¯(θk ) ="
P10-1052,P09-2071,0,0.0139751,"observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by `1 -penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement `1 penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007),"
P10-1052,W03-1018,0,0.0581744,"er of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θk+ − θk− , where θk+ and θk− are positive. The `1 penalty thus becomes ρ1 (θ+ − θ− ). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow `1 Regularization in CRFs Regularization The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2"
P10-1052,P07-1096,0,0.041186,"Missing"
P10-1052,J93-2004,0,0.0398862,"t allowed to divide the total training time by almost 2. It has finally often been found useful to fine tune the non-zero parameters by running a final handful of L-BFGS iterations using only a small `2 penalty; at this stage, all the other features are removed from the model. This had a small impact BCD and SGD’s performance and allowed them to catch up with OWL-QN’s performance. Table 2: Sparse vs standard forward-backward (training times and percentages of sparsity of M ) 5.1.2 Part-of-Speech Tagging Our second benchmark is a part-of-speech (POS) tagging task using the PennTreeBank corpus (Marcus et al., 1993), which provides us with a quite different condition. For this task, the number of labels is smaller (|Y |= 45) than for Nettalk, and the set of observations is much larger (|X |= 43207). This benchmark, which has been used in many studies, allows for direct comparisons with other published work. We thus use a standard experimental set-up, where sections 0-18 of the Wall Street Journal are used for training, sections 19-21 for development, and sections 22-24 for testing. Features are also standard and follow the design of (Suzuki and Isozaki, 2008) and test the current words (as written and lo"
P10-1052,P08-1076,0,0.0142428,"(POS) tagging task using the PennTreeBank corpus (Marcus et al., 1993), which provides us with a quite different condition. For this task, the number of labels is smaller (|Y |= 45) than for Nettalk, and the set of observations is much larger (|X |= 43207). This benchmark, which has been used in many studies, allows for direct comparisons with other published work. We thus use a standard experimental set-up, where sections 0-18 of the Wall Street Journal are used for training, sections 19-21 for development, and sections 22-24 for testing. Features are also standard and follow the design of (Suzuki and Isozaki, 2008) and test the current words (as written and lowercased), prefixes and suffixes up to length 4, and typographical characteristics (case, etc.) of the words. Our baseline feature set also contains tests on individual and pairs of words in a window of 5 words. 5.2 Speed, Sparsity, Convergence Using Large Feature Sets 5.3.2 Sparsity and the Forward-Backward As explained in section 4.1, the forward-backward algorithm can be written so as to use the sparsity of the matrix My,y0 ,x . To evaluate the resulting speed-up, we ran a series of experiments using Nettalk (see Table 2). In this table, the 3-g"
P10-1052,N03-1033,0,0.0807386,"Missing"
P10-1052,P09-1054,0,0.389248,"implify the forward-backward recursions. In this paper, we show that the sparsity that is induced by `1 -penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement `1 penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complemenAbstract Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds"
P10-1052,W04-3223,0,0.0712806,"function l(θ) + ρ1 kθk1 + ρ2 kθk22 2 (6) The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θk+ − θk− , where θk+ and θk− are positive. The `1 penalty thus becomes ρ1 (θ+ − θ− ). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow `1 Regularization in CRFs Regularization The standard approach for parameter"
P97-1055,C94-1037,0,0.19843,"es model (PCP for short) promotes an alternative view of analogical processes, which relies upon a linguistically motivated similarity measure between words. 428 The basic idea of our model is to take advantage of the internal structure of ""natural"" lexicons. In fact, a lexicon is a very complex object, whose elements are intimately tied together by a number of fine-grained relationships (typically induced by morphological processes), and whose content is severely restricted, on a language-dependant basis, by a complex of graphotactic, phonotactic and morphotactic constraints. Following e.g. (Pirrelli and Federici, 1994), we assume that these constraints surface simultaneously in the orthographical and in the phonological domain in the recurring pattern of paradigmatically alterning pairs of lexical items. Extending the idea originally proposed in (Federici, Pirrelli, and Yvon, 1995), we show that it is possible to extract these alternation patterns, to associate alternations in one domain with the related alternation in the other domain, and to construct, using this pairing, a fairly reliable pronunciation procedure. exchanging the prefixes re and f. These alternations are represented on figure 1. reactor 2."
P97-1055,E93-1007,0,0.374532,"Missing"
P97-1055,C96-2121,0,0.0980743,"y related to (c, d) E / : x / : iff there exits two partial functions f and g from E* to E*, where f exchanges prefixes and g exchanges suffixes, and: g(a) = b Paradigmatic Relationships and Alternations The paradigmatic cascades model crucially relies upon the existence of numerous paradigmatic relationships in lexical databases. A paradigmatic relationship involves four lexical entries a, b, c, d, and expresses that these forms are involved in an analogical (in the Saussurian (de Saussure, 1916) sense) proportion: a is to b as e is to d (further along abbreviated as a : b = c : d, see also (Lepage and Shin-Ichi, 1996) for another utilization of this kind of proportions). Morphologically related pairs provide us with numerous examples of orthographical proportions, as in: reactor : reaction = factor : faction (1) Considering these proportions in terms of orthographical alternations, that is in terms of partial fnnctions in the graphemic domain, we can see that each proportion involves two alternations. The first one transforms r e a c t o r into r e a c t i o n (and f a c t o r into f a c t i o n ) , and consists in exchanging the suffixes or and ion. The second one transforms reactor into f a c t o r (and"
P97-1055,C96-1046,0,\N,Missing
pecheux-etal-2014-rule,popovic-ney-2006-pos,0,\N,Missing
pecheux-etal-2014-rule,D12-1127,0,\N,Missing
pecheux-etal-2014-rule,N04-4026,0,\N,Missing
pecheux-etal-2014-rule,C04-1030,0,\N,Missing
pecheux-etal-2014-rule,C04-1073,0,\N,Missing
pecheux-etal-2014-rule,D09-1105,0,\N,Missing
pecheux-etal-2014-rule,W12-3102,0,\N,Missing
pecheux-etal-2014-rule,H05-1021,0,\N,Missing
pecheux-etal-2014-rule,W09-0435,0,\N,Missing
pecheux-etal-2014-rule,J92-4003,0,\N,Missing
pecheux-etal-2014-rule,J04-2004,0,\N,Missing
pecheux-etal-2014-rule,petrov-etal-2012-universal,0,\N,Missing
pecheux-etal-2014-rule,P11-2031,0,\N,Missing
pecheux-etal-2014-rule,J06-4004,0,\N,Missing
pecheux-etal-2014-rule,P02-1038,0,\N,Missing
pecheux-etal-2014-rule,J97-3002,0,\N,Missing
pecheux-etal-2014-rule,Q13-1001,0,\N,Missing
pecheux-etal-2014-rule,P05-1066,0,\N,Missing
pecheux-etal-2014-rule,D08-1078,0,\N,Missing
pecheux-etal-2014-rule,ramanathan-visweswariah-2012-study,0,\N,Missing
pecheux-etal-2014-rule,P13-2069,0,\N,Missing
pecheux-etal-2014-rule,W13-2201,0,\N,Missing
pecheux-etal-2014-rule,D08-1065,0,\N,Missing
pecheux-etal-2014-rule,W07-0414,0,\N,Missing
pecheux-etal-2014-rule,P03-1019,0,\N,Missing
pecheux-etal-2014-rule,2010.iwslt-evaluation.1,0,\N,Missing
W05-0616,P98-1120,0,0.852946,"Σ? , w(i) denotes the ith symbol in w. In this context, definition (2) can be re-stated as: Definition 3 (Analogical proportion in (Σ? ,.)) (x, y, z, t) ∈ Σ? form an analogical proportion, denoted by x : y :: z : t if and only if there exists some integer d and some factorizations x1 . . . xd = x, y1 . . . yd = y, z1 . . . zd = z, t1 . . . td = t such that ∀i, (yi , zi ) ∈ {(xi , ti ), (ti , xi )}. An example of analogy between words is: viewing : reviewer :: searching : researcher with x1 = , x2 = view, x3 = ing and t1 = re, t2 = search, t3 = er. This definition generalizes the proposal of (Lepage, 1998). It does not ensure the existence of a solution to an analogical equation, nor its uniqueness when it exists. (Lepage, 1998) gives a set of necessary conditions for a solution to exist. These conditions also apply here. In particular, if t is a solution of x : y :: z :?, then t contains, in the same relative order, all the symbols in y and z that are not in x. As a consequence, all solutions of an equation have the same length. 3.2.2 A Finite-state Solver Definition (3) yields an efficient procedure for solving analogical equations, based on finite-state transducers. The main steps of the pro"
W05-0616,J97-3003,0,0.0962346,"in Section 2.2. The first experiment consists in computing one or several vector(s) of morphosyntactic features to be associated with a form. Each vector comprises the lemma, the part-of-speech, and, based on the part-of-speech, additional features such as number, gender, case, tense, mood, etc. An (English) input/output pair for this tasks thus looks like: input=replying; output={reply; V-pp--}, where the placeholder ’-’ denotes irrelevant features. Lexical analysis is useful for many applications: a POS tagger, for instance, needs to “guess” the possible part(s)-of-speech of unknown words (Mikheev, 1997). For this task, we use the definition of analogical proportions for “flat” feature vectors (see section 3.1) and for word strings (section 3.2). The training data is a list of fully informed lexical entries; the test data is a list of isolated word forms not represented in the lexicon. Bins are constructed based on inflectional families. The second experiment consists in computing a morphological parse of unknown lemmas: for each input lemma, the output of the system is one or several parse trees representing a possible hierarchical decomposition of the input into (morphologically categorized"
W05-0616,P99-1037,0,\N,Missing
W05-0616,C98-1116,0,\N,Missing
W08-0310,allauzen-bonneau-maynard-2008-training,1,0.882861,"Missing"
W08-0310,D07-1091,0,0.12789,"ond pass, the use of the Neural Network LMs, if used with an appropriate (tuned) weight, yields a small, yet consistent improvement of B LEU for all pairs. Performance on the news task are harder to analyze, due to the lack of development data. Throwing in large set of in-domain data was obviously helpful, even though we are currently unable to adequately measure this effect. 4 Experiments with factored models Even though these models were not used in our submissions, we feel it useful to comment here our (negative) experiments with factored models. 4.1 Overview In this work, factored models (Koehn and Hoang, 2007) are experimented with three factors : the surface form, the lemma and the part of speech (POS). The translation process is composed of different mapping steps, which either translate input factors into output factors, or generate additional output factors from existing output factors. In this work, four mapping steps are used with two decoding paths. The first path corresponds to the standard and direct mapping of surface forms. The second decoding path consists in two translation steps for respectively POS tag and the lemmas, followed by a generation step which produces the surface form give"
W08-0310,W07-0733,0,0.0277945,"stical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data, as depicted on Figure 1. This setup was found to be more favorable than training on Europarl data only (for obvious mismatching domain reasons) and than training on news-commentary data only, most probably because of a lack of coverage. Another, alternative way of benefitting from the coverage of the Europarl corpus and the relevance of the news-commentary corpus is to use two phrase-tables in parallel, an interesting feature of Moses. (Koehn and Schroeder, 2007) found that this was the best way to “adapt” a translation system to the news-commentary task. These results are corroborated in (Déchelotte, 2007)1 , which adapts a “European Parliament” system using a “European and Spanish Parliaments” development set. However, we were not able to reproduce those findings for this evaluation. This might be caused by the increase of the number of feature functions, from 14 to 26, due to the duplication of the phrase table and the lexicalized reordering model. 2.2 2.2.1 Language Models Europarl language models The training of Europarl language models (LMs) was"
W08-0310,N03-1017,0,0.00383999,"data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were"
W08-0310,P07-2045,0,0.0336357,"d Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl pa"
W08-0310,P02-1038,0,0.0651661,"s. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with 107 Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data,"
W08-0310,J04-4002,0,0.016144,"l data and on News data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the eur"
W08-0310,P03-1021,0,0.0607674,"condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with 107 Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data, as depicted"
W08-2106,W06-1644,0,0.0281851,"ken is thus: P (wi |θd , β) = nT X P (ti = t|θd )P (wi |ti , β) (1) t=1 = nT X θdt βtw (2) t=1 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model Conditioned on β an"
W08-2106,J97-1003,0,\N,Missing
W09-0417,2007.tmi-papers.28,0,0.0611781,"Missing"
W09-0417,2007.mtsummit-papers.11,0,0.0245406,"ights, since they dispense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate trans"
W09-0417,J04-2004,0,0.0544024,"mate such large LMs, a vocabulary was first defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended by adding the most frequent words observed in the additional training data. This procedure yielded a vocabulary of one million words in both languages. 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models a"
W09-0417,P96-1041,0,0.135168,"es so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. Language model training The training data were divided into several sets based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tu"
W09-0417,W08-0310,1,0.911154,"Missing"
W09-0417,W08-0302,0,0.0772343,"f the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to sho"
W09-0417,P07-2045,0,0.0111904,"Missing"
W09-0417,2008.eamt-1.17,1,0.839871,"ense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source"
W09-0417,J06-4004,1,0.915103,"Missing"
W09-0417,P03-1021,0,0.00849652,"ts based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the Out of vocabulary word and perplexity To evaluate our vocabulary and LMs, we used the official devtest and test sets. The out-of-vocabulary (OOV) rate was drastically reduced by increasing 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering m"
W09-0417,E03-1076,0,\N,Missing
W09-0417,C08-1098,0,\N,Missing
W09-0417,E09-3008,0,\N,Missing
W09-0417,J04-2003,0,\N,Missing
W09-0417,H05-1085,0,\N,Missing
W10-1704,W09-0417,1,0.716697,", 2003) distributed with the Moses decoder, using the development corpus (news-test2008). The N -code systems were also tuned by the same implementation of MERT, which was slightly modified to match the requirements of our decoder. The BLEU score is used as objective function for MERT and to evaluate test performance. The interpolation experiment for FrenchEnglish was tuned on news-test2008a (first 1025 lines). Optimization was carried out over newstest2008b (last 1026 lines). Language Models The English and French language models (LMs) are the same as for the last year’s French-English task (Allauzen et al., 2009) and are heavily tuned to the newspaper/newswire genre, using the first part of the WMT09 official development data (dev2009a). We used all the authorized news corpora, including the French and English Gigaword corpora, for translating both into French (1.4 billion tokens) and English (3.7 billion tokens). To estimate such LMs, a vocabulary was defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended with the most frequent words observed in the training data, yielding a vocabulary of one million words in both languages"
W10-1704,J04-2004,0,0.140666,"lgorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and no, 2006). French-English systems 4.1 Baseline N -coder systems For this language pair, we used our in-house N -code system, which implements the n-grambased approach to SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orien"
W10-1704,corston-oliver-gamon-2004-normalizing,0,0.0994184,"ies both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Tra"
W10-1704,W08-0310,1,0.8106,"Missing"
W10-1704,H05-1085,0,0.0320767,"g time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages"
W10-1704,E09-3008,0,0.0133135,"to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computation"
W10-1704,E03-1076,0,0.457998,", German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 201"
W10-1704,N04-4026,0,0.276697,"the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether the current phrase is translated monotone, swapped or discontiguous with respect to the 4.2 A bilingual POS-based reordering model For this year evaluation, we also experimented with an additional reordering model, which is estimated as a standard n-gram language model, over generalized translation units. In the experiments reported below, we generalized tuples using POS tags, instead of raw word forms. Figure 1 displays the same sequence of tuples when built"
W10-1704,J06-4004,1,0.805664,"Missing"
W10-1704,J04-2003,0,0.0577646,"s a number of difficulties both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Wor"
W10-1704,P03-1021,0,0.100273,"o SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether"
W10-1704,C08-1098,0,0.0783579,"vocabulary and to improve the robustness of the alignment probabilities, we considered various normalization strategies for the different word classes. In a nutshell, normalizing amounts to collapsing several German forms of a given lemma into a unique representative, using manually written normalization patterns. A pattern typically specifies which forms of a given morphological paradigm should be considered equivalent when translating into English. These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags. Table 1 displays the analysis of an example sentence. 2 In most cases, normalization patterns replace a word form by its lemma; in order to partially preserve some inflection marks, we introduced two generic suffixes, +s and +en which respectively denote plural and genitive wherever needed. Typical normalization rules take the following form: • For articles, adjectives, and pronouns (Indefinite , possessive, demonstrative, relative and reflexive), if a token has; – Genitive case: replace with lemma+en (Ex. des, der, des, der → d+en) – Pl"
W10-1704,P07-2045,0,\N,Missing
W11-1207,E09-1003,0,0.143455,"s the length ratio, the word-to-word (IBM 1) alignment scores with supplementary scores aimed at rewarding sentences containing identical words, etc. More recently, (Smith et al., 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. For lack of find enough parallel sentences, (Munteanu and Marcu, 2006; Kumano and Tokunaga, 2007) consider the more difficult issue of mining parallel phrases. In (Abdul-Rauf and Schwenk, 2009), the authors, rather than computing a similarity score between a source and a target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (WER or TER). This approach is generalized to much larger collections in (Uszkoreit et al., 2010), which draw advantage of working in one language to adopt efficient parallelism detection techniques (Broder, 2000). 2.2 Comparable corpora for adaptation Another very productive use of comparable cor"
W11-1207,2010.iwslt-papers.3,0,0.431026,"ate, with a tiny probability, every phrase in a source document with the most frequent target phrases found in a comparable corpus specifically built for this document. The study in (Schwenk, 2008) considers selftraining, which allows to adapt an existing system to new domains using monolingual (source) data. The idea is to automatically translate the source side of an in-domain corpus using a reference translation system. Then, according to some confidence score, the best translations are selected to form an adaptation corpus, which can serve to retrain the translation model. The authors of (Cettolo et al., 2010) follow similar goals with different means: here, the baseline translation model is used to obtain a phrase alignment between source and target sentences in a comparable corpus. These phrase alignments are further refined, before new phrases not in the original phrase-table, can be collected. The approaches developed below borrow from both traditions: given (i) the supposed high degree of parallelism in our data and (ii) the size of the available comparable data, we are in a position to apply any of the above described technique. This is all the easier to do as all stories are timestamped, whi"
W11-1207,C04-1151,0,0.0227552,"of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with comparable or monolingual data. For instance, the work reported in (Schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve a baseline system trained mostly with out-of-domain data. As discussed e.g. in (Fung and Cheung, 2004), comparable corpora are of various nature: there exists a continuum between truly parallel and completely unrelated texts. Algorithms for exploiting comparable corpora should thus be tailored to the peculiarities of the data on which they are applied. In this paper, we report on experiments aimed at using a noisy parallel corpus made out of news stories in French and Arabic in two different ways: first, to extract new, in-domain, parallel sentences; second, to adapt our translation and language models. This approach is made possible due to the specificities of our corpus. In fact, our work is"
W11-1207,P98-1069,0,0.0437278,"registers, and language pairs. In fact, there are a few language pairs for which parallel corpora can be accessed, except for very narrow domains such as political debates or international regulatory texts. Another very valuable resource for SMT studies, especially for under-resource languages, are comparable corpora, made of pairs of monolingual corpora that contain texts of similar genres, from similar periods, and/or about similar topics. The potential of comparable corpora has long been established as a useful source from which to extract bilingual word dictionaries (see eg. (Rapp, 1995; Fung and Yee, 1998)) or to learn multilingual terms (see e.g. (Lang´e, 1995; Smadja et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with com"
W11-1207,W08-0509,0,0.0357004,"were preprocessed by first transliterating the Arabic text with the BAMA (Buckwalter, 2002) transliteration tool. Then, the Arabic data are segmented into sentences. A CRF-based sentence segmenter for Arabic was built with the Wapiti3 (Lavergne et al., 2010) package. A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). The preprocessed Arabic and French data were aligned using MGiza++4 (Gao and Vogel, 2008). The Moses toolkit (Koehn et al., 2007) is then used to make the alignments symmetric using the growdiag-final-and heuristic and to extract phrases with maximum length of 7 words. A distortion model lexically conditioned on both the Arabic phrases and French phrases is then trained. Feature weights were set by running MERT (Och, 2003) on the development set. 5.3 Extraction of the in-domain parallel corpus We follow the method described in Section 3: Arabic documents are first translated into French using the baseline SMT system. For the document selection step each translated (ar:fr) document"
W11-1207,N06-2013,0,0.0347476,"age, and therefore data preprocessing is necessary to deal with data scarcity. All Arabic data were preprocessed by first transliterating the Arabic text with the BAMA (Buckwalter, 2002) transliteration tool. Then, the Arabic data are segmented into sentences. A CRF-based sentence segmenter for Arabic was built with the Wapiti3 (Lavergne et al., 2010) package. A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). The preprocessed Arabic and French data were aligned using MGiza++4 (Gao and Vogel, 2008). The Moses toolkit (Koehn et al., 2007) is then used to make the alignments symmetric using the growdiag-final-and heuristic and to extract phrases with maximum length of 7 words. A distortion model lexically conditioned on both the Arabic phrases and French phrases is then trained. Feature weights were set by running MERT (Och, 2003) on the development set. 5.3 Extraction of the in-domain parallel corpus We follow the method described in Section 3: Arabic documents are first translated into French usin"
W11-1207,P07-2045,0,0.00498319,"ng the Arabic text with the BAMA (Buckwalter, 2002) transliteration tool. Then, the Arabic data are segmented into sentences. A CRF-based sentence segmenter for Arabic was built with the Wapiti3 (Lavergne et al., 2010) package. A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). The preprocessed Arabic and French data were aligned using MGiza++4 (Gao and Vogel, 2008). The Moses toolkit (Koehn et al., 2007) is then used to make the alignments symmetric using the growdiag-final-and heuristic and to extract phrases with maximum length of 7 words. A distortion model lexically conditioned on both the Arabic phrases and French phrases is then trained. Feature weights were set by running MERT (Och, 2003) on the development set. 5.3 Extraction of the in-domain parallel corpus We follow the method described in Section 3: Arabic documents are first translated into French using the baseline SMT system. For the document selection step each translated (ar:fr) document is compared only to the French document"
W11-1207,P10-1052,1,0.638563,"Missing"
W11-1207,J05-4003,0,0.581134,"res, from similar periods, and/or about similar topics. The potential of comparable corpora has long been established as a useful source from which to extract bilingual word dictionaries (see eg. (Rapp, 1995; Fung and Yee, 1998)) or to learn multilingual terms (see e.g. (Lang´e, 1995; Smadja et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with comparable or monolingual data. For instance, the work reported in (Schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve a baseline system trained mostly with out-of-domain data. As discussed e.g. in (Fung and Cheung, 2004), comparable corpora are of various"
W11-1207,P06-1011,0,0.045473,"gistic regression model aimed at detecting parallel sentences. This formalism allows to enrich baseline features such as the length ratio, the word-to-word (IBM 1) alignment scores with supplementary scores aimed at rewarding sentences containing identical words, etc. More recently, (Smith et al., 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. For lack of find enough parallel sentences, (Munteanu and Marcu, 2006; Kumano and Tokunaga, 2007) consider the more difficult issue of mining parallel phrases. In (Abdul-Rauf and Schwenk, 2009), the authors, rather than computing a similarity score between a source and a target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (WER or TER). This approach is generalized to much larger collections in (Uszkoreit et al., 2010), which draw advantage of working in one language to adopt efficient parallel"
W11-1207,P03-1021,0,0.00456825,"much worse in quality than the others. 47 Considering these three corpora, different adaptation methods of the translation models are explored. The first approach is to concatenate the baseline and in-domain training data (either extracted or translated) to train a new translation model. Given the difference in size between the two corpus, this approach may introduce a bias in the translation model in favor of out-of-domain. The second approach is to train separate translation models with baseline on the one hand, and with in-domain on the other data and to weight their combination with MERT (Och, 2003). This alleviates the former problem but increases the number of features that need to be trained, running the risk to make MERT less stable. A last approach is also considered, which consists in using only the in-domain data to train the translation model. In that case, the question is the small size of the in-domain data. The comparative experiments on the three approaches, using the three corpora are described in next section. 5 Experiments and results 5.1 Context and data The experiments have been carried out in the context of the Cap Digital SAMAR1 project which aims at developping a plat"
W11-1207,P95-1050,0,0.0697741,"ins, genres, registers, and language pairs. In fact, there are a few language pairs for which parallel corpora can be accessed, except for very narrow domains such as political debates or international regulatory texts. Another very valuable resource for SMT studies, especially for under-resource languages, are comparable corpora, made of pairs of monolingual corpora that contain texts of similar genres, from similar periods, and/or about similar topics. The potential of comparable corpora has long been established as a useful source from which to extract bilingual word dictionaries (see eg. (Rapp, 1995; Fung and Yee, 1998)) or to learn multilingual terms (see e.g. (Lang´e, 1995; Smadja et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improv"
W11-1207,J03-3002,0,0.342589,"ain texts of similar genres, from similar periods, and/or about similar topics. The potential of comparable corpora has long been established as a useful source from which to extract bilingual word dictionaries (see eg. (Rapp, 1995; Fung and Yee, 1998)) or to learn multilingual terms (see e.g. (Lang´e, 1995; Smadja et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with comparable or monolingual data. For instance, the work reported in (Schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve a baseline system trained mostly with out-of-domain data. As discussed e.g. in (Fung and Cheung, 2004), compara"
W11-1207,2008.iwslt-papers.6,0,0.131994,"et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with comparable or monolingual data. For instance, the work reported in (Schwenk, 2008) draws inspiration from recent advances in unsupervised training of acoustic models for speech recognition and proposes to use self-training on in-domain data to adapt and improve a baseline system trained mostly with out-of-domain data. As discussed e.g. in (Fung and Cheung, 2004), comparable corpora are of various nature: there exists a continuum between truly parallel and completely unrelated texts. Algorithms for exploiting comparable corpora should thus be tailored to the peculiarities of the data on which they are applied. In this paper, we report on experiments aimed at using a noisy pa"
W11-1207,J96-1001,0,0.152003,"hich parallel corpora can be accessed, except for very narrow domains such as political debates or international regulatory texts. Another very valuable resource for SMT studies, especially for under-resource languages, are comparable corpora, made of pairs of monolingual corpora that contain texts of similar genres, from similar periods, and/or about similar topics. The potential of comparable corpora has long been established as a useful source from which to extract bilingual word dictionaries (see eg. (Rapp, 1995; Fung and Yee, 1998)) or to learn multilingual terms (see e.g. (Lang´e, 1995; Smadja et al., 1996)). More recently, the relative corpus has caused the usefulness of comparable corpora be reevaluated as a potential source of parallel fragments, be they paragraphs, sentences, phrases, terms, chunks, or isolated words. This tendency is illustrated by the work of e.g. (Resnik and Smith, 2003; Munteanu and Marcu, 2005), which combines Information Retrieval techniques (to identify parallel documents) and sentence similarity detection to detect parallel sentences. There are many other ways to improve SMT models with comparable or monolingual data. For instance, the work reported in (Schwenk, 2008"
W11-1207,N10-1063,0,0.0255855,"sed on surface and/or formal similarity of the web addresses or of the page internal structure. This line of work is developed notably in (Munteanu and Marcu, 2005): candidate parallel texts are found using Cross-Lingual Information Retrieval (CLIR) techniques; sentence similarity is indirectly computed using a logistic regression model aimed at detecting parallel sentences. This formalism allows to enrich baseline features such as the length ratio, the word-to-word (IBM 1) alignment scores with supplementary scores aimed at rewarding sentences containing identical words, etc. More recently, (Smith et al., 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. For lack of find enough parallel sentences, (Munteanu and Marcu, 2006; Kumano and Tokunaga, 2007) consider the more difficult issue of mining parallel phrases. In (Abdul-Rauf and Schwenk, 2009), the authors, rather than computing a similarity score between a source and a target sentence, propose to use an existing translation engine to process the"
W11-1207,D08-1090,0,0.0184786,"ctions in (Uszkoreit et al., 2010), which draw advantage of working in one language to adopt efficient parallelism detection techniques (Broder, 2000). 2.2 Comparable corpora for adaptation Another very productive use of comparable corpora is to adapt or specialize existing resources (dictionaries, translation models, language models) to specific domains and/or genres. We will only focus here on adapting the translation model; a review of the literature on language model adaptation is in (Bellagarda, 2001) and the references cited therein. Figure 1: Extraction of parallel corpora The work in (Snover et al., 2008) is a first step towards augmenting the translation model with new translation rules: these rules associate, with a tiny probability, every phrase in a source document with the most frequent target phrases found in a comparable corpus specifically built for this document. The study in (Schwenk, 2008) considers selftraining, which allows to adapt an existing system to new domains using monolingual (source) data. The idea is to automatically translate the source side of an in-domain corpus using a reference translation system. Then, according to some confidence score, the best translations are s"
W11-1207,N09-2024,0,0.0662987,"tences, while our phrase-table adaptation strategies are described in Section 4. In Section 5, we describe our experiments and contrast the results obtained with several adaptation strategies. Finally, Section 6 concludes the paper. 2 Related work From a bird’s eye view, attempts to use comparable corpora in SMT fall into two main categories: first, approaches aimed at extracting parallel fragments; second, approaches aimed at adapting existing resources to a new domain. 2.1 Extracting parallel fragments Most attempts at automatically extracting parallel fragments use a two step process (see (Tillmann and Xu, 2009) for a counter-example): a set of candidate parallel texts is first identified; within this short list of possibly paired texts, parallel sentences are then identified based on some similarity score. The work reported in (Zhao and Vogel, 2002) concentrates on finding parallel sentences in a set of comparable stories pairs in Chinese/English. Sentence similarity derives from a probabilistic alignment model for documents, which enables to recognize parallel sentences based on their length ratio, as well as on the IBM 1 model score of their word45 to-word alignment. To account for various levels"
W11-1207,C10-1124,0,0.0290102,"al Random Fields (CRFs) model. For lack of find enough parallel sentences, (Munteanu and Marcu, 2006; Kumano and Tokunaga, 2007) consider the more difficult issue of mining parallel phrases. In (Abdul-Rauf and Schwenk, 2009), the authors, rather than computing a similarity score between a source and a target sentence, propose to use an existing translation engine to process the source side of the corpus, thus enabling sentence comparison to be performed in the target language, using the edit distance or variants thereof (WER or TER). This approach is generalized to much larger collections in (Uszkoreit et al., 2010), which draw advantage of working in one language to adopt efficient parallelism detection techniques (Broder, 2000). 2.2 Comparable corpora for adaptation Another very productive use of comparable corpora is to adapt or specialize existing resources (dictionaries, translation models, language models) to specific domains and/or genres. We will only focus here on adapting the translation model; a review of the literature on language model adaptation is in (Bellagarda, 2001) and the references cited therein. Figure 1: Extraction of parallel corpora The work in (Snover et al., 2008) is a first st"
W11-1207,C98-1066,0,\N,Missing
W11-2135,W10-1704,1,0.806759,"the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require to postprocess the output to undo the pre-processing. As in our last year’s experiments (Allauzen et al., 2010), this pre-processing step could be achieved with a two-step decoding. However, by stacking two decoding steps, we may stack errors as well. Thus, for this direction, we used the German tokenizer provided by the organizers. 3.2 contains large portions that are not useful for translating news text. The first filter aime"
W11-2135,J92-4003,0,0.317321,"Missing"
W11-2135,J04-2004,0,0.208795,"is estimated and tuned as described in Section 4.1. Moreover, we also introduce in Section 4.2 the use of the SOUL language model (LM) (Le et al., 2011) in SMT. Based on neural networks, the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption (up to 10-gram in this work). Finally, experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates (TER) measured on the provided newstest2010 dataset. 2 System Overview Our in-house n-code SMT system implements the bilingual n-gram approach to Statistical Machine Translation (Casacuberta and Vidal, 2004). Given a 1 This kind of characters was used for Teletype up to the seventies or early eighties. 309 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309–315, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics source sentence sJ1 , a translation hypothesis tˆ1I is defined as the sentence which maximizes a linear combination of feature functions: ( ) M tˆ1I = arg max t1I ∑ λm hm (sJ1 ,t1I ) (1) m=1 a word-aligned corpus (using MGIZA++2 with default settings) in such a way that a unique segmentation of the bilingual corpus is achi"
W11-2135,W08-0310,1,0.792506,"Missing"
W11-2135,D10-1044,0,0.0594607,"Missing"
W11-2135,D08-1076,0,0.0565273,"iew of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11"
W11-2135,P03-1021,0,0.271147,"s (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT), see details in Section 5.4), using the provided newstest2009 data as development set. 2.1 Training Our translation model is estimated over a training corpus composed of tuple sequences using classical smoothing techniques. Tuples are extracted from 310 The resulting sequence of tuples (1) is further refined to avoid NULL words in the source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations"
W11-2135,P02-1040,0,0.0853055,"n models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require t"
W11-2135,W10-1748,0,0.0323177,"for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold. First, we have shown that n-gram based systems can achieve state-of-the-art performance on large scale tasks in terms of automatic metrics such as BLEU. Then, as already sho"
W11-2135,C08-1098,0,0.0510396,"action and reordering rules. 3 Data Pre-processing and Selection We used all the available parallel data allowed in the constrained task to compute the word alignments, except for the French-English tasks where the United Nation corpus was not used to train our translation models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the G"
W11-2135,2011.eamt-1.33,1,0.771526,"system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold."
W11-2135,N04-4026,0,0.357669,"estimated by using the n-gram assumption: K p(sJ1 ,t1I ) = ∏ p((s,t)k |(s,t)k−1 . . . (s,t)k−n+1 ) k=1 Figure 1: Tuple extraction from a sentence pair. where s refers to a source symbol (t for target) and (s,t)k to the kth tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 4 for details); four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimu"
W11-2135,D07-1080,0,0.0221974,"ever, showed any consistent and significant improvement for the majority of setups tried (with the exception of the BBN approach, that had almost always improved over n-best MERT, but for the sole French to English translation direction). Additional experiments with 9 complementary translation models as additional features were performed with lattice-MERT, but neither showed any substantial improvement. In the view of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a gen"
W11-2135,D07-1055,0,0.0524218,"Missing"
W11-2168,P07-1020,0,0.013269,"a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “l"
W11-2168,D08-1023,0,0.22415,"rmed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training and inference over a graph"
W11-2168,P08-1024,0,0.660098,"The model thus defines the probability of a segmented target e e = eeI1 given the segmented and reordered source sentence ˜f = fe1I . To complete the model, one just needs to define a distribution over source segmentations P (˜f |f ). Given the deterministic relationship between e and e e expressed by the “unsegmentation” function φ which maps e e with e = φ(e e), we then have: X P (e|f ) = P (e e, ˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f , f )P (˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f )P (˜f |f ) ˜ f ,e e|φ(e e)=e 2 Assuming first order dependencies. This is a significant difference with (Blunsom et al., 2008), as we do not need to introduce latent variables during training. 3 544 In practice, we will only consider a restricted number of possible segmentation/reorderings of the source, denoted L(f ), and compute the best translation e∗ as φ(e e∗ ), where: e e∗ = arg max P (e e|f ) e e e, |˜f , f )P (˜f |f ) ≈ arg max P (e (1) ˜ f ∈L(f ),e e Even with these simplifying assumptions, this approach raises several challenging computational problems. First, training a CRF is quadratic in the number of labels, of which we will have plenty (typically hundreds of thousands). A second issue is decoding: as w"
W11-2168,J90-2002,0,0.716657,". Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to mak"
W11-2168,J04-2004,0,0.319641,"ity of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems. 1 To overcome the NP-hard problems that derive from the need to consider all possible permutations of the source sentence, we make here a radical simplification and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and"
W11-2168,D08-1024,0,0.0537665,"Missing"
W11-2168,N09-1025,0,0.0413879,"use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/output mapping faithfully"
W11-2168,P05-1033,0,0.130734,"extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented"
W11-2168,P05-1066,0,0.0417081,"f tuples such as: (demanda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). sid"
W11-2168,W02-1001,0,0.0556563,"word aligned sentences (f , e), but lack the explicit derivation h from f to e that is required to train the model in a fully supervised way. The approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists"
W11-2168,P07-1033,0,0.0599041,"Missing"
W11-2168,P08-2007,0,0.0160335,"is idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallu"
W11-2168,W06-3105,0,0.0286103,"sed approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically inform"
W11-2168,N06-4004,0,0.0285908,"systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to data sparsity issues and ("
W11-2168,N10-1128,0,0.0385918,"o marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, especially when using a target side language model, which seems to preclude the application to large-scale translation tasks11 . The recent work of (Dyer and Resnik, 2010) proceeds from a similar vein: translation is however modeled as a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the"
W11-2168,P08-1112,0,0.059883,"Missing"
W11-2168,W08-0509,0,0.0149225,"ing (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM), word and phrase penalties, etc). 2.2 Translating with CRFs A discriminative version of the n-gram approach consists in modeling P (e|f ) instead of P (e, f ), which can be efficiently performed with CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). Assuming matched sequences of observations (x = L xL 1 ) and labels (y = y1 ), CRFs express the conditional probability of labels as: P (y1L |xL 1) = 1 L exp(θT G(xL 1 , y1 )), Z(xL 1 ; θ) where θ is a parameter vector and G denotes a vector of feature functions testing various properties of x and y. In the linear-"
W11-2168,W08-0302,0,0.0125396,"segmentations, and the use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/outp"
W11-2168,P07-1019,0,0.034987,"e. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training"
W11-2168,D09-1107,0,0.0313224,"Missing"
W11-2168,N03-1017,0,0.093172,"is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual"
W11-2168,N04-1022,0,0.0760989,": it suffices to perform the search in π2 (S ◦R)◦ −log(D)◦T ◦F ◦L, where L represents a n-gram language model. When combining several models, notably a source segmentation model and/or a target language model for rescoring, we have made sure to rescale the (log)probabilities so as to balance the language model scores with the CRF scores, and to use a fixed word bonus to make hypotheses of different length more comparable. All these parameters are tuned as part of the decoder development process. It is finally noteworthy that, in our architecture, alternative decoding strategies, such as MBR (Kumar and Byrne, 2004) are also readily implemented. 4 Experiments 4.1 Corpora and metrics For these experiments, we have used a medium size training corpus, extracted from the datasets made available for WMT 20116 evaluation campaign, and have focused on one translation direction, from French to English7 . Translation model training uses the entire NewsCommentary subpart of the WMT’2011 training 6 7 statmt.org/wmt11 Results in the other direction suggest similar conclusions. le : the/θle,the ∗ : the/0 0 ∗ : the/θthe ∗ : the/0 0 1 0 ∗ : cat/θthe,cat 1 chat : cat/θchat,cat DET : the/θDET,the Figure 2: Feature matche"
W11-2168,P10-1052,1,0.823642,"th a very small number of different labels. A first simplification is thus to consider that the set of possible “labels” ee for a source sequence fe is limited to those that are seen in training: all the other associations (fe, ee) are deemed impossible, which amounts to setting the corresponding parameter value to −∞. A second speed-up is to enforce sparsity in the model, through the use of a `1 regularization term (Tibshirani, 1996): on the one hand, this greatly reduces the memory usage; furthermore, sparse models are also prone to various optimization of the forward-backward computations (Lavergne et al., 2010). As discussed in (Ng, 2004; Turian et al., 2007), this feature selection strategy is well suited to the task at hand, where the number of possible features is extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability i"
W11-2168,P06-1096,0,0.128,"Missing"
W11-2168,N06-1014,0,0.677324,"ing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to dat"
W11-2168,W02-1018,0,0.0546135,"on and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools t"
W11-2168,J06-4004,1,0.936184,"Missing"
W11-2168,D09-1022,0,0.0228507,"s of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “log-linear” model. This line of research has been continued notably in (K¨aa¨ ri¨ainen, 2009), which introduces an exponential model of bag of phrases (allowing some overlap), that enables to capture localized dependencies between target words, while preserving (to some extend) the efficiency of training and inference. Supervision is here indirectly provided by word alignment and correlated phrase extraction processes implemented in conventional phrase-based systems (Koehn"
W11-2168,P02-1040,0,0.0865962,"ng. Various statistics regarding these corpora are reproduced on Table 1. All the training corpora were aligned using MGIZA++ with standard parameters8 , and processed in the standard tuple extraction pipeline. The development and test corpora were also processed analogously. For the sake of comparison, we also trained a standard n-gram-based and a Moses system (Koehn et al., 2007) with default parameters and a 3-gram target LM trained using only the target side of our parallel corpus. The development set (test 2009) was used to tune these two systems. All performance are measured using BLEU (Papineni et al., 2002). 8 As part of a much larger batch of texts. 4.2 Features The baseline system is composed only of translation features [trs] and target bigram features [t2g]. The former correspond to functions of the form e, i) = I(fei = s ∧ eei = t), where s gus,t (˜f , e and t respectively denote source and target phrases and I() is the indicator function. These are also generalized to part-of-speech and also to any possible source phrase, giving rise to features such as e, i) = I(e ei = t). Target bigram features gu∗,t = (˜f , e e, i) = correspond to functions of the form gbt,t0 (˜f , e I(e ei−1 = t ∧ eei"
W11-2168,N04-4026,0,0.119029,"Missing"
W11-2168,D07-1080,0,0.0463638,"he approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, es"
W11-2168,P10-1049,0,0.099124,"Missing"
W11-2168,C04-1073,0,0.0361538,"anda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM),"
W11-2168,P07-2045,0,\N,Missing
W11-2168,2009.eamt-smart.4,0,\N,Missing
W11-2168,N04-1021,0,\N,Missing
W11-4415,P03-1006,0,0.0336437,"ting the pronunciation dictionary with pronunciations as inputs and words as outputs is constructed. Its weights are the conditional probabilities of a pronunciation given a word. When no pronunciation probabilities are available, a uniform distribution over the probabilities of pronunciations of each word is applied. This FST is composed with each phoneme-pronunciation FSTs A ◦ P resulting from the previous composition. A final composition is made with the FSA G that models the backoff language model, with word probabilities as weights. G is constructed as described in (Riccardi et al, 1996; Allauzen et al, 2003). This results in FSTs with phonemes as input and words as output, which are projected to the output and determinized. Then, the arc weights of each FST are normalized per state, i.e. scaled such that the probability of arcs leading out of a state (plus the probability of state finality) sums to 1 for each state. A general weight-pushing algorithm in the log semiring (Mohri et al, 1997) is applied for the normalization and the weights in the new stochastic FSA are converted to the desired posterior probabilities given the pronunciations. What is calculated is the conditional probability p(w |a"
W11-4415,P07-2045,0,0.00278875,"Thus, finally there are no great differences between the dictionary with counts and the dictionary without counts. Moreover, it could be that counts only for a few words create an inconsistency that explains the light deterioration of the pronunciation entropy. The increase in entropy is much greater when more pronunciations are added in the dictionary as can be seen in Tables 3 and 4. The n-best pronunciations are added in the “longest” and the “most frequent” baselines. The M1, M2 and M5 in these tables correspond to the 1-, 2- and 5-best pronunciations generated automatically using Moses (Koehn et al, 2007) as a g2p converter, being trained on the baseline dictionary (with 1.2 pronunciations per 113 Training condition Multiple pronunciations Baseline most frequent M1 5.185 M2 M5 6.914 10.077 3.669 word). Moses has been successfully used as a g2p converter for several languages, and for English it gives state-of-the-art results (Karanasou & Lamel, 2011). The results in Tables 3 and 4 are calculated with the 4-gram LM. These results suggest that there can be a large influence of the pronunciation dictionary in the confusability of an ASR system, not sufficiently compensated by the language model."
W11-4415,J97-2003,0,0.29242,"SR system, using the newly introduced pronunciation entropy. The remainder of the paper is organized as follows. Section 2 describes the necessary Finite State Tranducers (FSTs) background. Section 3 presents the FST decoding and details the new confusability measure. Sections 4 and 5 present the recognition experiments and the pronunciation entropy results. The paper concludes with a discussion of the results and of some future work in Section 6. 2 2.1 Background Generalities In the last decade, FSTs have been shown to be useful for a number of applications in speech and language processing (Mohri et al, 1997). FST operations such as composition, determinization, and minimization make manipulating FSTs both effective and efficient. Weighted transducers (resp. automata) are finitestate transducers (resp. automata) in which each transition carries some weight in addition to the input and output (resp. input) labels. The interpretation of the weights depends on the algebraic structure of the semiring in which they are defined. A semiring is a system (K, ⊕, ⊗, ¯0, ¯1) containing the weights K and the operators ⊕ and ⊗, such that: (K, ⊕, ¯0) is a commutative monoid with ¯0 as the identity element for ⊕;"
W12-2505,C10-2010,0,0.0128657,"hurch, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In most applications, only high-confidence oneto-one sentence alignments are considered useful and kept for subsequent processing stages. Indeed, when the objective is to build subsentential align1 See, for instance, the Uplug toolbox which integrates several sentence alignment tools in a unified framework: http://sourceforge.net/projects/uplug/ 36 Workshop on Computational Linguistics for Literature, pages 36–44, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics ments (at the level of words, terms or phrases), other types of mappings between sentences are dee"
W12-2505,P91-1022,0,0.889898,"alignments have to be computed. Sentence alignment is generally thought to be fairly easy and many efficient sentence alignment programs are freely available1 . Such programs rely on two main assumptions: (i) the relative order of sentences is the same on the two sides of the bitext, and (ii) sentence parallelism can be identified using simple surface cues. Hypothesis (i) warrants efficient sentence alignment algorithms based on dynamic programming techniques. Regarding (ii), various surface similarity measures have been proposed: on the one hand, length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In mos"
W12-2505,J93-2003,0,0.134342,"gnments Public domain tools Baseline alignments are computed using two open-source sentence alignment packages, the sentence alignment tool of Moore (2002)6 , and Hunalign (Varga et al., 2005). These two tools were chosen as representative of the current state-of-theart in sentence alignment. Moore’s approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of lengthbased approaches (Brown et al., 1991; Gale and Church, 1991). This alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores; these scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high confidence, one-to-one, sentence alignments to be used as training material for data-intensive MT. Sentences that cannot be reliably aligned are discarded from the resulting alignment. 3 Getting access to more recent books (or their translation) is problematic, due to copyright issues: literary works fall in the public domain 70 years after the death of their author. 4 http://www.gutenberg.o"
W12-2505,P91-1023,0,0.656701,"applications, sentence alignments have to be computed. Sentence alignment is generally thought to be fairly easy and many efficient sentence alignment programs are freely available1 . Such programs rely on two main assumptions: (i) the relative order of sentences is the same on the two sides of the bitext, and (ii) sentence parallelism can be identified using simple surface cues. Hypothesis (i) warrants efficient sentence alignment algorithms based on dynamic programming techniques. Regarding (ii), various surface similarity measures have been proposed: on the one hand, length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and"
W12-2505,1994.amta-1.21,0,0.326632,"literary texts, translation often departs from a straight sentence-by-sentence alignment and using such a constraint can discard a significant proportion of the bitext. For MT, this is just a regrettable waste of potentially useful training material (Uszkoreit et al., 2010), all the more so as parallel literary texts constitute a very large reservoir of parallel texts online. For other applications implying to mine, visualize or read the actual translations in their context (second language learning (Nerbonne, 2000; Kraif and Tutin, 2011), translators training, automatic translation checking (Macklovitch, 1994), etc.), the entire bitext has to be aligned. Furthermore, areas where the translation is only partial or approximative need to be identified precisely. The work reported in this study aims to explore the quality of existing sentence alignment techniques for literary work and to explore the usability of a recently proposed multiple-pass approach, especially designed for recovering many-to-one pairings. In a nutshell, this approach uses sure one-to-one mappings detected in a first pass to train a discriminative sentence alignment system, which is then used to align the regions which remain prob"
W12-2505,moore-2002-fast,0,0.686607,"length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In most applications, only high-confidence oneto-one sentence alignments are considered useful and kept for subsequent processing stages. Indeed, when the objective is to build subsentential align1 See, for instance, the Uplug toolbox which integrates several sentence alignment tools in a unified framework: http://sourceforge.net/projects/uplug/ 36 Workshop on Computational Linguistics for Literature, pages 36–44, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics ments (at the level of words, terms or phrases), oth"
W12-2505,J05-4003,0,0.450031,"training sentence pairs, the optimal values of the parameters are set by optimizing numerically the conditional likelihood; optimization is performed here using L-BFGS (Liu and Nocedal, 1989); a Gaussian prior over the parameters is used to ensure numerical stability of the optimization. In this study, we used the following set of feature functions: Figure 2: Filling alignment gaps 3.1 1 + exp[− 1 PK Detecting parallelism Assuming the availability of a set of example parallel sentences, the first step of our approach consists in training a function for scoring candidate alignments. Following (Munteanu and Marcu, 2005), we train a Maximum Entropy classifier9 (Rathnaparkhi, 1998); in principle, many other binary classifiers would be possible here. Our motivation for using a maxent approach was to obtain, for each possible pair of sentences (f ,e), a link posterior probability P (link|f , e). We take the sentence alignments of the first step as positive examples. Negative examples are artificially generated as follows: for all pairs of positive instances (e, f ) and (e0 , f 0 ) such that e0 immediately follows e, we select the pair (e, f 0 ) as a negative example. This strategy produced a balanced corpus cont"
W12-2505,C10-1124,0,0.060009,"of words, terms or phrases), other types of mappings between sentences are deemed to be either insufficiently reliable or inappropriate. As it were, the one-to-one constraint is viewed as a proxy to literalness/compositionality of the translation and warrants the search of finer-grained alignments. However, for certain types of bitexts2 , such as literary texts, translation often departs from a straight sentence-by-sentence alignment and using such a constraint can discard a significant proportion of the bitext. For MT, this is just a regrettable waste of potentially useful training material (Uszkoreit et al., 2010), all the more so as parallel literary texts constitute a very large reservoir of parallel texts online. For other applications implying to mine, visualize or read the actual translations in their context (second language learning (Nerbonne, 2000; Kraif and Tutin, 2011), translators training, automatic translation checking (Macklovitch, 1994), etc.), the entire bitext has to be aligned. Furthermore, areas where the translation is only partial or approximative need to be identified precisely. The work reported in this study aims to explore the quality of existing sentence alignment techniques f"
W12-2505,J93-1004,0,\N,Missing
W12-2701,D07-1090,0,0.0636077,"am Model? On the Future of Language Modeling for HLT, pages 1–10, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics neural network language models (NNLMs) that has often been overlooked is the ability of the latter to fare with extended contexts (Schwenk and Koehn, 2008; Emami et al., 2008); in comparison, standard n-gram LMs rarely use values of n above n = 4 or 5, mainly because of data sparsity issues and the lack of generalization of the standard estimates, notwithstanding the complexity of the computations incurred by the smoothing procedures (see however (Brants et al., 2007) for an attempt to build very large models with a simple smoothing scheme). The recent attempts of Mikolov et al. (2011b) to resuscitate recurrent neural network architectures goes one step further in that direction, as a recurrent network simulates an unbounded history size, whereby the memory of all the previous words accumulates in the form of activation patterns on the hidden layer. Significant improvements in ASR using these models were reported in (Mikolov et al., 2011b; Mikolov et al., 2011a). It must however be emphasized that the use of a recurrent structure implies an increased compl"
W12-2701,J92-4003,0,0.308759,"iculty with the neural network approach is the complexity of inference and training, which largely depends on the size of the output vocabu2 The test sets used in MT experiments are made of various News extracts. Their content is thus not homogeneous and using words from previous sentences doesn’t seem to be relevant. 4 lary ,i.e. of the number of words that have to be predicted. To overcome this problem, Le et al. (2011a) have proposed the structured Output Layer (SOUL) architecture. Following (Mnih and Hinton, 2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). Structuring the output layer and using word class information makes the estimation of distribution over large output vocabulary computationally feasible. In the SOUL LM, the output vocabulary is structured in a clustering tree, where every word is associated to a unique path from the root node to a leaf node. Denoting wi the ith word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for word wi in this tree, with D the tree depth, cd (wi ) the class or sub-class assigned to wi , and cD (wi ) the leaf associated with wi , comprising just the word itself. The probabilit"
W12-2701,P96-1041,0,0.146216,"mply note that our parallel training data includes a large Web corpus, referred to as the GigaWord parallel corpus. After various preprocessing and filtering steps, the total amount of training data is approximately 12 million sentence pairs for the bilingual part, and about 2.5 billion of words for the monolingual part. To built the target language models, the monolingual corpus was first split into several sub-parts 3 http://www.statmt.org/wmt11 based on date and genre information. For each of these sub-corpora, a standard 4-gram LM was then estimated with interpolated Kneser-Ney smoothing (Chen and Goodman, 1996). All models were created without any pruning nor cutoff. The baseline back-off n-gram LM was finally built as a linear combination of several these models, where the interpolation coefficients are chosen so as to minimize the perplexity of a development set. All NNLMs are trained following the prescriptions of Le et al. (2011b), and they all share the same inner structure: the dimension of the projection word space is 500; the size of two hidden layers are respectively 1000 and 500; the short-list contains 2000 words; and the non-linearity is introduced with the sigmoid function. For the recu"
W12-2701,P05-1063,0,0.0247968,"mances as reported in (Mikolov et al., 2011a). To the best of our knowledge, it is the first recurrent NNLM trained on a such large dataset (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it invol"
W12-2701,H93-1021,0,0.119997,"set (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it involves a sum over all the possible sentences; second, it seems that data sparsity issues for this model are also adversely affecting the"
W12-2701,P03-1021,0,0.0102337,"uted on newstest2009-2011. On x axis, the number k represents the k th previous word. 1.0 0.8 0.6 0.4 INT > 15 AD J NO M AB R NA M AB K 10 &lt;s O R PR VE N KO V 5 AD 0 PU N DE T SY M PR P NU M 0.0 SE NT 0.2 Figure 3: Average selection rate of max function of the first previous word in terms of word POS-tag information, computed on newstest2009-2011. The green line represents the distribution of occurrences of each tag. of each hypothesis is computed and the k-best list is accordingly reordered. The NNLM weights are optimized as the other feature weights using Minimum Error Rate Training (MERT) (Och, 2003). For all our experiments, we used the value k = 300. To clarify the impact of the language model order in translation performance, we considered three different ways to use NNLMs. In the first setting, the NNLM is used alone and all the scores provided by the MT system are ignored. In the second setting (replace), the NNLM score replaces the score of the standard back-off LM. Finally, the score of the NNLM can be added in the linear combination (add). In the last two settings, the weights used for 7 Table 2: Results for the English to French task obtained with the baseline system and with var"
W12-2701,P11-1063,0,0.0137897,"(Mikolov et al., 2011a). To the best of our knowledge, it is the first recurrent NNLM trained on a such large dataset (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it involves a sum over all the p"
W12-2701,I08-2089,0,0.022956,"e translation tasks (Allauzen et al., 2011). Following these initial successes, the neural approach has recently been extended in several promising ways (Mikolov et al., 2011a; Kuo et al., 2010; Liu et al., 2011). Another difference between conventional and 1 NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 1–10, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics neural network language models (NNLMs) that has often been overlooked is the ability of the latter to fare with extended contexts (Schwenk and Koehn, 2008; Emami et al., 2008); in comparison, standard n-gram LMs rarely use values of n above n = 4 or 5, mainly because of data sparsity issues and the lack of generalization of the standard estimates, notwithstanding the complexity of the computations incurred by the smoothing procedures (see however (Brants et al., 2007) for an attempt to build very large models with a simple smoothing scheme). The recent attempts of Mikolov et al. (2011b) to resuscitate recurrent neural network architectures goes one step further in that direction, as a recurrent network simulates an unbounded history size, where"
W12-2701,P06-1124,0,0.0358957,"Missing"
W12-2701,W11-2135,1,\N,Missing
W12-3120,P07-1038,0,0.0144971,"data. Standard results from machine learning show that such structures can be described either by a linear model using a large number of features or by a non-linear model using a (potentially) smaller set of features. As only a small number of training examples is available, we decided to focus on non-linear models in this work. 3 Inferring quality scores Predicting the quality scores can naturally be cast as a standard regression task, as the reference scores used in the evaluation are numerical (real) values. Regression is the approach adopted in most works on confidence estimation for MT (Albrecht and Hwa, 2007; Specia et al., 2010b). A simpler way to tackle the problem would be to recast it as binary classification task aiming at distinguishing “good” translations from “bad” ones (Blatz et al., 2004; Quirk, 2004). It is also possible, as shown by (Soricut and Echihabi, 2010), to use ranking approaches. However, because the shared task is evaluated by comparing the actual value of the predictions with the human scores, using these last two frameworks is not possible. In our experiments, following the observations reported in the previous section, we use two wellknown non-linear regression methods: p"
W12-3120,P11-1022,0,0.0139301,"dom forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now, very few datasets were publicly available and co"
W12-3120,C04-1046,0,0.495273,"tes our approach; ii) we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine tr"
W12-3120,P98-1032,0,0.0232108,"ld better be produced from scratch. The test contains 422 sentence pairs, the quality of which has to be predicted. The training set also contains additional material, namely two references (the reference originally given by WMT and a human post-edited one), which will allow us to better interpret our results. No references were provided for the test set. 2.2 Features Several works have studied the problem of confidence estimation (Blatz et al., 2004; Specia et al., 2010b) or related problems such as predicting readability (Kanungo and Orr, 2009) or developing automated essay scoring systems (Burstein et al., 1998). They all use the same basic features: IBM 1 score measures the quality of the “association” of the source and the target sentence using bag-of-word translation models; Language model score accounts for the “fluency”, “grammaticality” and “plausibility” of a target sentence; Simple surface features like the sentence length, the number of out-of-vocabulary words or words that are not aligned. These features are used to account for the difficulty of the translation task. Figure 1: Distribution of the human scores on the train set. (HS∗ stands for Human Scores) Figure 2 plots the distribution of"
W12-3120,quirk-2004-training,0,0.06031,". As only a small number of training examples is available, we decided to focus on non-linear models in this work. 3 Inferring quality scores Predicting the quality scores can naturally be cast as a standard regression task, as the reference scores used in the evaluation are numerical (real) values. Regression is the approach adopted in most works on confidence estimation for MT (Albrecht and Hwa, 2007; Specia et al., 2010b). A simpler way to tackle the problem would be to recast it as binary classification task aiming at distinguishing “good” translations from “bad” ones (Blatz et al., 2004; Quirk, 2004). It is also possible, as shown by (Soricut and Echihabi, 2010), to use ranking approaches. However, because the shared task is evaluated by comparing the actual value of the predictions with the human scores, using these last two frameworks is not possible. In our experiments, following the observations reported in the previous section, we use two wellknown non-linear regression methods: polynomial regression and random forests. We also consider linear regression as a baseline. We will now quickly describe these three methods. Linear regression (Hastie et al., 2003) is a simple model in which"
W12-3120,P10-1063,0,0.388873,"on-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now, very few datasets were publi"
W12-3120,specia-etal-2010-dataset,0,0.106075,") we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now,"
W12-3120,C98-1032,0,\N,Missing
W12-3141,W10-1704,1,0.815478,"eriments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built in “true-case”. Compared to last year, the pre-processing of utf-8 characters was significantly improved. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel"
W12-3141,E09-1010,1,0.834206,"lpful. As the IBM1 model is asymmetric, two models are estimated, one in both directions. Contrary to the reported results, these additional features do not yield significant improvements over the baseline system. We assume that the difficulty is to add information to an already extensively optimized system. Moreover, the IBM1 models are estimated on the same training corpora as the translation system, a fact that may explain the redundancy of these additional features. In a separate series of experiments, we also add WSD features calculated according to a variation of the method proposed in (Apidianaki, 2009). For each word of a subset of the input (source language) vocabulary, a simple WSD classifier produces a probability distribution over a set of translations8 . During reranking, each translation hypothesis is scanned and the word translations that match one of the proposed variant are rewarded using an additional score. While this method had given some Conclusion In this paper, we described our submissions to WMT’12 in the French-English and GermanEnglish shared translation tasks, in both directions. As for our last year’s participation, our main systems are built with n-code, the open source"
W12-3141,J93-2003,0,0.0934316,"ize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will a"
W12-3141,P05-1032,0,0.0155644,"e hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the s"
W12-3141,J04-2004,0,0.03102,"code, an open source in-house Statistical Machine Translation (SMT) system based on bilingual n-grams1 . The main novelty of this year’s participation is the use, in a large scale system, of the continuous space translation models described in (Hai-Son et al., 2012). These models estimate the n-gram probabilities of bilingual translation units using neural networks. We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated “on-the-fly” 1 http://ncode.limsi.fr/ 2 System overview n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. 2.1 Features Given a source sentence s of I words, the best translation hypothesis ˆt is defined as the sequence of J words that maximizes a linear combination of fea33"
W12-3141,2010.iwslt-papers.6,0,0.024222,"Missing"
W12-3141,W08-0310,1,0.883725,"Missing"
W12-3141,N12-1005,1,0.885285,"Missing"
W12-3141,P07-2045,0,0.00674765,"lt (31.7 BLEU point) that is slightly worst than the n-code baseline (32.0) and slightly better than the equivalent Moses baseline (31.5), but does it much faster. Model estimation for the test file is reduced to 2 hours and 50 minutes, with an additional overhead for loading and writing files of one and a half hours, compared to roughly 210 hours for our baseline systems under comparable hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that th"
W12-3141,C08-1064,0,0.0749035,"rimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the standard traini"
W12-3141,J06-4004,0,0.217743,"Missing"
W12-3141,P03-1021,0,0.0736745,"x lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, sour"
W12-3141,P02-1040,0,0.0874389,"ation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, source sentences are represented in the form of word lattices containing the most promising reordering hypotheses, s"
W12-3141,C08-1098,0,0.0308747,"ing (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel data was selected. Word alignment models were trained using all the data, whereas the translation models were estimated on a subpart of the parallel data: the UN corpus was discarded for this step and about half of the French-English Giga corpus was filtered based on a perplexity criterion as in (Allauzen et al., 2011)). For French-English, we mainly upgraded the training material from last year by extracting th"
W12-3141,N04-4026,0,0.0279162,"max t,a M X ) λm hm (a, s, t) (1) L Y P (ui |ui−1 , ..., ui−n+1 ) (2) i=1 m=1 where λm is the weight associated with feature function hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitute one of the main difference between the n-gram approach and standard phrase-based systems. This will be further detailled in section 2.2 and 3. In addition to the translation model, fourteen feature functions are combined: a target-language model (Section 5.3); four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT))"
W12-3141,2002.tmi-tutorials.2,0,0.0391067,"French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will actually allow us to take into account larger cont"
W12-3141,D08-1039,0,\N,Missing
W12-3141,W11-2135,1,\N,Missing
W12-3141,N04-1021,0,\N,Missing
W12-4201,apidianaki-2008-translation,1,0.852565,"ained (because the two variants of the adjective are reduced to the same lemma). All lexicon entries satisfying the above criteria are retained and used for disambiguation. In these initial experiments, we disambiguate English words having less than 20 French translations in the lexicon. Each French translation of an English word that appears more than once in the training corpus4 is characterized by a weighted English feature vector built from the training data. Vector building The feature vectors corresponding to the translations are built by exploiting information from the source contexts (Apidianaki, 2008; Grefenstette, 1994). For each translation of an EN word w, we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. the content words that occur in the same sentence as w whenever it is translated by this translation). The extracted source language words constitute the features of the vector built for the translation. For each translation Ti of w, let N be the number of features retained from the corresponding source context. Each feature Fj (1 ≤ j ≤ N) receives a total weight tw(Fj , Ti ) defined as the product of the feature’s glo"
W12-4201,E09-1010,1,0.961121,"at and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source word; the quantity and type of data used for training the classifier (e.g., use of raw data or"
W12-4201,P05-1048,0,0.0813922,"ing some avenues for future work. 2 Related work Word sense disambiguation systems generally work at the word level: given an input word and its context, they predict its (most likely) meaning. At the same time, state-of-the-art translation systems all consider groups of words (phrases, tuples, etc.) rather than single words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambig"
W12-4201,D07-1007,0,0.397955,"improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT. 1 Introduction Word Sense Disambiguation (WSD) is the task of identifying the sense of words in texts by reference to some pre-existing sense inventory. The selection of the appropriate inventory and WSD method strongly depends on the goal WSD intends to serve: recent methods are increasingly oriented towards the disambiguation needs of specific end applications, and explicitly aim at improving the overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009"
W12-4201,P07-1005,0,0.209625,"overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source w"
W12-4201,C10-1027,1,0.871747,"If this is the case, the corresponding probabilities are additively accumulated for the current hypothesis. At the end, two features are appended to each hypothesis in the n-best list: the total score accumulated for the hypothesis and 5 the same score normalized by the number of words in the hypothesis. Two MERT initialization schemes were considered: (1) all model weights are initialized to zero, and (2) all the weights of “standard” features are initialized to the values found by MERT and the new WSD features to zero. 4.2 Local Language Models We propose to adapt the approach introduced in Crego et al. (2010) as an alternative way to integrate the WSD predictions within the decoder: for each sentence to be translated, an additional language model (LM) is estimated and taken into account during decoding. As this additional “local” model depends on the source sentence, it can be used as an external source of knowledge to reinforce translation hypotheses complying with criteria predicted from the whole source sentence. For instance, the unigram probabilities of the additional LM can be derived from the (word) predictions of a WSD system, bigram probabilities from the prediction of phrases and so on a"
W12-4201,2009.eamt-1.32,0,0.0163645,"er, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers t"
W12-4201,2010.amta-papers.23,0,0.0316251,"Missing"
W12-4201,P07-2045,0,0.00398791,". 5 Evaluation 5.3 5.1 Table 2 reports the results of our experiments. It appears that, for the considered task, sense disambiguation improves translation performance: n-best rescoring results in a 0.37 BLEU improvement and using an additional language model brings about an improvement of up to a 0.88 BLEU. In both cases, MERT assigns a large weight to the additional feaExperimental Setting In all our experiments, we considered the TEDtalk English to French data set provided by the IWSLT’11 evaluation campaign, a collection of public speeches on a variety of topics. We used the Moses decoder (Koehn et al., 2007). The TED-talk corpus is a small data set made of a monolingual corpus (111, 431 sentences) used 6 Results 7 http://statmt.org/wmt08/scripts.tgz method baseline rescoring additional LM — WSD (zero init) WSD (reinit) oracle 3-gram oracle 2-gram oracle 1-gram IBM 1 WSD BLEU 29.63 30.00 29.58 43.56 39.36 42.92 30.18 30.51 METEOR 53.78 54.26 53.96 64.64 62.92 69.39 54.36 54.38 Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions. baseline 67.57 45.97 51.79 52.17 PoS Nouns Verbs Adjectives Adverbs WSD 69.06 47.76 53.94 56.25 Table 3: Contrastive lexical e"
W12-4201,D09-1022,0,0.0449313,"n system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of t"
W12-4201,max-etal-2010-contrastive,1,0.859696,"to the WSD method introduced in Section 3, these oracle experiments rely on sense predictions for all source words and not only content words. Surprisingly enough, predicting phrases instead of words results only in a small improvement. Additional experiments are required to explain why 2-gram oracle achieved such a low performance. 7 5.4 Contrastive lexical evaluation All the measures used for evaluating the impact of WSD information on translation show improvements, as discussed in the previous section. We complement these results with another measure of translation performance, proposed by Max et al. (2010), which allows for a more fine-grained contrastive evaluation of the translations produced by different systems. The method permits to compare the results produced by the systems on different word classes and to take into account the source words that were actually translated. We focus this evaluation on the classes of content words (nouns, adjectives, verbs and adverbs) on which WSD had an important coverage. Our aim is, first, to explore how these words are handled by a WSDinformed SMT system (the system using the local language models) compared to the baseline system that does not exploit a"
W12-4201,W09-2412,0,0.034286,"Missing"
W12-4201,J03-1002,0,0.00530526,"tions of a word and to assign a probability to each translation for new instances of the word in context. Each translation is represented by a source language feature vector that the classifier uses for disambiguation. All experiments carried out in this study are for the English (EN) - French (FR) language pair. 3.1 Source Language Feature Vectors Preprocessing The information needed by the classifier is gathered from the EN-FR training data provided for the IWSLT’11 evaluation task.1 The dataset consists of 107,268 parallel sentences, wordaligned in both translation directions using GIZA++ (Och and Ney, 2003). We disambiguate EN words found in the parallel corpus that satisfy the set of criteria described below. Two bilingual lexicons are built from the alignment results and filtered to eliminate spurious alignments. First, translation correspondences with a probability lower than a threshold are discarded;2 then translations are filtered by part-of-speech (PoS), keeping for each word only translations pertaining to the same grammatical category;3 finally, only intersecting alignments (i.e., correspondences found in the lexicons of both directions) are retained. Given that the lexicons contain wor"
W12-4201,I11-1074,0,0.589525,"redictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of the words in the input sentence"
W12-4201,P06-3010,0,0.142762,"slation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to"
W12-4201,H05-1097,0,0.178488,"ngle words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical transla"
W12-4201,W09-2413,0,\N,Missing
W12-4201,S10-1002,0,\N,Missing
W12-4201,N04-1021,0,\N,Missing
W13-2204,W10-1704,1,0.883815,"Missing"
W13-2204,D07-1091,0,0.0403744,"Spanishto-English direction but yields a 0.2 BLEU point decrease in the opposite direction. For the following experiments, all the available corpora are therefore used: News-Commentary, Europarl, filtered CommonCrawl and UN. For each of these corpora, a bilingual n-gram model is estimated and used by n-code as one individual model score. An additionnal TM is trained on the concatenation all these corpora, resulting in a total of 5 TMs. Moreover, n-code is able to handle additional “factored” bilingual models where the source side words are replaced by the corresponding lemma or even POS tag (Koehn and Hoang, 2007). Table 2 reports the scores obtained with different settings. In Table 2, big denotes the use of a wider context for n-gram TMs (n = 4, 5, 4 instead of 3, 4, 3 respectively for word-based, POS-based and lemma-based TMs). Using POS factored Spanish language model To train the language models, we assumed that the test set would consist in a selection of recent news texts and all the available monolingual data for Spanish were used, including the Spanish Gigaword, Third Edition. A vocabulary is first defined by including all tokens observed in the NewsCommentary and Europarl corpora. This vocabu"
W13-2204,J04-2004,0,0.0169884,"h and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this year’s participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated. 1 2 n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. Introduction This paper describes LIMSI’s submissions to the shared translation task of the Eighth Workshop on Statistical Machine Translation. LIMSI participated in th"
W13-2204,N12-1005,1,0.88366,"model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to u"
W13-2204,W12-2701,1,0.912005,"model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to u"
W13-2204,W12-2700,0,0.287257,"short range reorderings, they are inadequate to capture long-range reorderings, especially for language pairs that differ significantly in their syntax. A promising workaround is the source preordering method that can be considered similar, to some extent, to the reordering strategy implemented in n-code; the main difference is that the latter uses one deterministic (long-range) reordering on top of conventional distortion-based models, while the former only considers one single model delivering permutation lattices. The preordering approach is illustrated by the recent work of Neubig et al. (2012), where the authors use a discriminatively trained ITG parser to infer a single permutation of the source sentence. In this section, we investigate the use of this pre-ordering model in conjunction with the bilingual n-gram approach for translating English into German (see (Collins et al., 2005) for similar experiments with the reverse translation direction). Experiments are carried out with the same settings as described in (Neubig et al., 2012): given the source side of the parallel data (en), the parser is estimated to modify the original word order and to generate a new source side (en-mod"
W13-2204,P05-1066,0,0.110387,"Missing"
W13-2204,J12-4004,0,0.0182804,"language, so for sentences originally in this language, the baseline system was used. This system is used as our primary submission to the evaluation, with additional SOUL rescoring step. Therefore, to translate from English to German, the submitted system includes three BOLMs: one trained on all the monolingual data, one on artificial texts and a third one that uses the freely available deWack corpus3 (1.7 billion words). target LM base +genText +SOUL +genText+SOUL BLEU dev nt09 test nt10 15.3 16.5 15.5 16.8 16.4 17.6 16.5 17.8 8 Different tunings for different original languages As shown by Lembersky et al. (2012), the original language of a text can have a significant impact on translation performance. In this section, this effect is assessed on the French to English translation task. Training one SMT system per original language is impractical, since the required information is not available for most of parallel corpora. However, metadata provided by the WMT evaluation allows us to split the development and test sets according to the original language of the text. To ensure a sufficient amount of texts for each condition, we used the concatenation of newstest corpora for the years 2008, 2009, 2011, a"
W13-2204,W08-0310,1,0.86537,"Missing"
W13-2204,J06-4004,0,0.081732,"Missing"
W13-2204,moore-2002-fast,0,0.0869027,"Missing"
W13-2204,P03-1021,0,0.0399909,"ed reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more comp"
W13-2204,padro-stanilovsky-2012-freeling,0,0.0628704,"Missing"
W13-2204,P02-1040,0,0.087289,"(Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default pol"
W13-2204,C08-1098,0,0.0472101,"Missing"
W13-2204,P06-2093,0,0.0775468,"Missing"
W13-2204,N04-4026,0,0.0181647,"on hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitutes one of the main difference between the n-gram approach and standard phrasebased systems. http://ncode.limsi.fr/ 62 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62–69, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 3 In addition to the translation model (TM), fourteen feature functions are combined: a targetlanguage model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al."
W13-2204,D11-1034,0,\N,Missing
W13-2204,D12-1077,0,\N,Missing
W13-2204,W11-2135,1,\N,Missing
W13-2204,2010.iwslt-papers.6,0,\N,Missing
W13-2250,W12-3102,0,0.14645,"Missing"
W13-2250,1993.eamt-1.1,0,0.511454,"Missing"
W13-2250,P03-1021,0,0.0272251,"search space E (generally represented as a lattice); δu (E) has the value 1 if u occurs in the translation hypothesis E and 0 otherwise and P (E, A|F ) is the probability that the source sentence F is translated by the hypothesis E using a derivation A. Following (Gispert et al., 2013), this probability is estimated by applying a soft-max function to the score of the decoder: exp (α × H(E, A, F )) 0 0 (A0 ,E 0 )∈E exp (H(E , A , F )) 0.00 P (A, E|F ) = P where the decoder score H(E, A, F ) is typically a linear combination of a handful of features, the weights of which are estimated by MERT (Och, 2003). n-gram posteriors therefore aggregate two pieces of information: first, the number of paths in the lattice (i.e. the number of translation hypotheses of the search path) the n-gram appears in; second, the decoder scores of these paths that can be roughly interpreted as a quality of the path. Computing P (u|E) requires to enumerate all ngram contained in E and to count the number of paths in which this n-gram appears at least once. An efficient method to perform this computation in a single traversal of the lattice is described in (Gispert et al., 2013). This algorithm has been reimplemented1"
W13-2250,P10-1063,0,0.0272722,"tion (SMT) systems in the professional translation industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear"
W13-2250,2011.eamt-1.12,0,0.180488,"n industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear classifier (such as the scores estim"
W13-2250,2013.tc-1.10,0,0.038752,"Description LIMSI has participated to the tasks 1-1 (prediction of the hTER) and 1-3 (prediction of the postedition time). Similar features and learning algorithms have been considered for the two tasks. We will first quickly describe them before discussing the specific development made for task 1-3. 1 Our implementation can be downloaded from http:// perso.limsi.fr/Individu/wisniews/. 399 3.1 Features 3.2 In addition to the features described in the previous section, 176 ‘standard’ features for quality estimation have been considered. The full list of features we have considered is given in (Wisniewski et al., 2013) and the features set can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last ye"
W13-2250,W12-3120,1,0.636027,"can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last year campaign datasets (Zhuang et al., 2012). As we are considering a larger features set this year and the number of examples is comparatively quite small, we also considered elastic regression, a linear model trained with L1 and L2 priors as regularizers, hoping that training a sparse model would reduce the risk of overfitting. In this study, we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). As detailed in Section 4.1, cross-validation has been used to choose the hyper-parameters of all regressors, namely the number of estimators, the maximal depth of a tree and the minimum number of examples in a leaf"
W13-2250,P10-1064,0,\N,Missing
W14-3307,N12-1047,0,0.166607,"or WMT 2014 ∗ Quoc Khanh Do, † Teresa Herrmann, ∗† Jan Niehues, Alexandre Allauzen, ∗ Franc¸ois Yvon and † Alex Waibel ∗ LIMSI-CNRS, Orsay, France † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ surname@limsi.fr † firstname.surname@kit.edu ∗ Abstract ples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approac"
W14-3307,W06-1607,0,0.0257787,"gopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper descri"
W14-3307,W13-0805,1,0.843382,"ection 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model"
W14-3307,P03-1054,0,0.00441034,"uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999)."
W14-3307,E03-1076,0,0.0233691,"is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on"
W14-3307,2005.iwslt-1.8,0,0.0327583,". 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source"
W14-3307,P07-2045,0,0.00834948,"erent language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield"
W14-3307,N12-1005,1,0.95525,"nts in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. The system participates in the German-to-English translation task. It consists of two main components. First, a k-best list is generated using a phrasebased machine translation system. This system will be described in Section 2. Afterwards, the kbest list is reranked using SOUL (Structured OUtput Layer) models. Thereby, a neural network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases fo"
W14-3307,P96-1041,0,0.212941,"target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Cr"
W14-3307,J06-4004,0,0.187497,"Missing"
W14-3307,W05-0836,1,0.776382,"second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006"
W14-3307,2011.iwslt-evaluation.9,1,0.848234,"llel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation"
W14-3307,P10-2041,0,0.0330092,". In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation"
W14-3307,W09-0435,1,0.847502,"splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. Th"
W14-3307,W11-2124,1,0.856532,"ition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target phrase t. Using the n-gram assumption, the joint probability of a"
W14-3307,J03-1002,0,0.0106255,"e corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration"
W14-3307,E99-1010,0,0.058547,"ning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target ph"
W14-3307,P03-1021,0,0.00840255,"and the target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ciated target hypothesis. The goal is to recover the information that is illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT ) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA ) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. model estimates the joint probability of a sentence pair using two sliding windows of length n, one for each language; however, the moves of these windows remain synchronized by the tuple"
W14-3307,W08-1006,0,0.0195332,"the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MK"
W14-3307,2007.tmi-papers.21,0,0.039184,"as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014"
W14-3307,W12-3141,1,\N,Missing
W14-3330,W08-0310,1,0.895121,"Missing"
W14-3330,W11-2123,0,0.0112757,"22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with distinctive features. As outof-domain data, we reuse the data proc"
W14-3330,D11-1125,0,0.0362648,"Missing"
W14-3330,P05-1032,0,0.0608395,"Missing"
W14-3330,J04-2004,0,0.042241,"al articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompos"
W14-3330,W04-3237,0,0.0452726,"for E MEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). Language Models A medical-domain 4-gram language model is built by concatenating the target side of the paral1 Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combi"
W14-3330,P07-2045,0,0.00663677,"Missing"
W14-3330,P96-1041,0,0.209049,"pos-lm term dictionary short titles -3 26 22 6 4 -7 -5 -15 -1 21 2 -17 -22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with di"
W14-3330,P13-1126,0,0.179519,"franc¸ais4 {firstname.lastname}@limsi.fr 2 Abstract 2.1 This paper describes LIMSI’s submission to the first medical translation task at WMT’14. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduc"
W14-3330,P10-1052,1,0.873159,"Missing"
W14-3330,N12-1047,0,0.0389371,"Missing"
W14-3330,2011.iwslt-evaluation.7,1,0.81625,"osal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, continuous models can also be easily and efficiently adapted as in (Lavergne et al., 2011). Starting from a previously trained S OUL model, only a few more training epochs are (2) where the f req(·) is the number of occurrences of the given phrase in the whole corpus, and the numerator p(¯ e|f¯) × f req(f¯) represents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model"
W14-3330,P11-2031,0,0.0126431,"ces from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum random sampling size of 1 000 examples and a maximum phrase length of 15. However, all sub-corpora but G IGA3 are used to compute the vectors for VSM features. Decoding is done with M OSES4 (Koehn et al., 2007). Experiments Tunin"
W14-3330,N12-1005,1,0.885486,"score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, conti"
W14-3330,C08-1064,0,0.0173213,"sents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on on-the-fly model estimation for SMT (CallisonBurch et al., 2005; Lopez, 2008), we first build a suffix array for the source corpus. Only a limited number of translation examples, selected by deterministic random sampling, are then used by traversing the suffix array appropriately. A coherent translation probability (Lopez, 2008) (which also takes into account examples where translation extraction failed) is then estimated. As we cannot compute exactly an inverse translation probability (because sampling is performed independently for each source phrase), we resort to the following approximation:   p(¯ e|f¯) × f req(f¯) ¯ p(f |¯ e) = min 1.0, f req(¯ e) J X K X Vector"
W14-3330,J06-4004,0,0.0612575,"Missing"
W14-3330,P03-1021,0,0.0609898,"Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combination As N CODE and OTF differ in many aspects and make different errors, we use system combination techniques to take advantage of their complementarity. This is done by reranking the concatenation of the 1 000-best lists of both systems. For each hypothesis within this list, we use two global features, corresponding either to the score computed by the corresponding system or 0 otherwise. We then learn reranking weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the S OUL rescoring, and add instead the five S OUL scores as features in the system combination reranking (SysComb-7). For this first edition of a Medical Translation Task, only a very small development set was made available (D EVEL in Table 1). This made both system design and tuning challenging. In fact, with such a small development set, conventional tuning methods are known to be very unstable and prone to overfitting, and it would be"
W14-3330,P02-1040,0,0.0986367,"it would be suboptimal to select a configuration based on results on the development set only.2 To circumvent this, we artificially created our own internal test set by randomly selecting 3 000 sentences out from the 30 000 sentences from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum ra"
W14-3330,P06-2093,0,0.0274218,"ment data, respectively, and countdev (f¯j , e¯k ) is the joint count of phrase pairs (f¯j , e¯k ) found in the development set. The similarity score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allo"
W14-3330,N04-4026,0,0.0199797,"(§2.3), and POS-tagging adaptation to the medical domain (§3.3). We also performed a small-scale error analysis of the outputs of some of our systems (§5). K X λk fk (f , e, a) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match 246 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246–253, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (see Table 1). Each component wc (f¯, e¯) is a standard"
W14-3330,vilar-etal-2006-error,0,0.0370322,"Missing"
W14-3344,W08-0330,0,0.0318141,"ms of precision, recall and f1 score computed on the BAD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that qua"
W14-3344,padro-stanilovsky-2012-freeling,0,0.0227896,"Missing"
W14-3344,petrov-etal-2012-universal,0,0.0134501,"lities table. The second kind of association features relies on pseudo-references, that is to say, translations of the source sentence produced by an independent MT system. Many works have considered As the classes are unbalanced, prediction performance will be evaluated in terms of precision, recall and f1 score computed on the BAD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in"
W14-3344,W13-2250,1,0.900286,"et al., 2011) (also used by our MT system) and a 4-gram language model based on Part-of-Speech sequences. The latter model was estimated on the Spanish side of the bilingual data provided in the translation shared task in 2013. These data were POS-tagged with FreeLing (Padr´o and Stanilovsky, 2012). All these language models have been used to define two different features : 4 4.1 Learning Methods Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many"
W14-3344,P10-1063,0,0.023156,"AD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system out"
W14-3344,W12-3121,0,0.0174012,"the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system output (Gispert et al., 2013)."
W14-3344,W12-3120,1,0.934531,"used by our MT system) and a 4-gram language model based on Part-of-Speech sequences. The latter model was estimated on the Spanish side of the bilingual data provided in the translation shared task in 2013. These data were POS-tagged with FreeLing (Padr´o and Stanilovsky, 2012). All these language models have been used to define two different features : 4 4.1 Learning Methods Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar • the probabil"
W15-3012,W13-0805,1,0.885747,"SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word p"
W15-3012,P03-1054,0,0.0197594,"he actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et"
W15-3012,N03-1017,0,0.0324773,". 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source"
W15-3012,2005.iwslt-1.8,0,0.122737,"g, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to"
W15-3012,P07-2045,0,0.0106185,"rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in ord"
W15-3012,J04-2004,0,0.0881722,"s Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of"
W15-3012,N12-1005,1,0.956768,"oint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translated candidates. Second, an n-gram translation model from LIMSI, named SOUL (Structured OUtput Layer), helps to rescore the k-best list by utilizing features extracted from translated tuples. In this year participation, we also use a version of the neural network translation models (Le et al., 2012) trained using NCE algorithm (Gutmann and Hyv¨arinen, 2010) as counterpart to SOUL models. A ListNet2.1 Data and Preprocessing The parallel data mainly used are the corpora extracted from Europarl Parliament (EPPS), News Commentary (NC) and the common part of webcrawled data (Common Crawl). The monolingual data are the monolingual part of those corpora. A preprocessing step is applied to the raw data before the actual training. It includes removing excessively long and length-mismatched sentences pairs. Special symbols and nummeric data are normalized, and smartcasing is applied. Sentence pair"
W15-3012,E99-1010,0,0.0623657,"onolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words. All of the language models are trained using the SRILM toolkit (Stolcke, 2002); The word-based 121 org : .... à recevoir le prix nobel de la paix s : .... s8: à s9: recevoir s10: le s11: nobel de la paix s12: prix .... t : .... t8: to t9: receive t10: the t11: nobel peace t12: prize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just abov"
W15-3012,P06-1096,0,0.312547,"Missing"
W15-3012,2007.tmi-papers.21,0,0.269383,"which contain textual elements in different 120 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and Germa"
W15-3012,J06-4004,0,0.194243,"Missing"
W15-3012,D07-1045,0,0.386972,"Missing"
W15-3012,D09-1022,0,0.247402,"Missing"
W15-3012,W05-0836,1,0.875755,"(Vogel, 2003) which finds the best combinations of features in a log-linear framework. The features consist of translation scores, distortion-based and lexicalized reordering scores as well as conventional and non-word language models. In addition, several reordering rules, including short-range, long-range and tree-based reorderings, are applied before decoding step as they are encoded as word lattices. The decoder then generates a list of the best candidates from the lattices. To optimize the factors of individual features on a development dataset, we use minimum error rate training (MERT) (Venugopal et al., 2005). We are going to describe those components in detail as follows. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points. 1 KIT Phrase-based Translation System Introduction In this paper, we present the English→German joint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translat"
W15-3012,W09-0435,1,0.878365,"eedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ to"
W15-3012,W08-0303,1,0.817804,"C. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy class"
W15-3012,W13-2264,1,0.876138,"ose phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to train those classifiers. 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et"
W15-3012,W11-2124,1,0.841127,"sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a Besides word-based n-gram language models trained on all preprocessed monolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words."
W15-3012,W15-3030,1,0.645912,"(Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs. Therefore, NCE models deliver a positive score, by applying the exponential function to the output layer activities, 122 4 Rescoring 5 After generating translation probabilities using the neural network translation models, we need to combine them with the baseline scores of the phrase-based system in order to select better translations from the k-best lists. As it is done in the baseline decoder, we used a log-linear combination of all features. We trained the model using the ListNet algorithm (Niehues et al., 2015; Cao et al., 2007). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. Therefore, a sentence-based translation quality metric is necessary. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then the model was trained by minimizing the cross entropy between both distributions on the development data. Using this loss function, we can compute the gradient with respect to the weight ωk as follows: System Baseline + ListNet rescoring + NCE + SOUL + NCE + SOUL"
W15-3012,J03-1002,0,0.0188001,"are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice o"
W15-3012,W11-2123,0,\N,Missing
W15-3012,P14-1129,0,\N,Missing
W15-3016,N12-1005,1,0.878657,"plexity differences for both in-domain and out-of-domain LMs. Sentences pairs are ranked according to the MML score and the top N parallel sentences are used to learn the translation table used during decoding. For LM adaptation, we used a log-linear combination of our large LM with a smaller one trained only on the monolingual in-domain corpus.6 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. As in our previous participations (Le et al., 2012b; Allauzen et al., 2013; P´echeux et al., 2014), we take advantage of the proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large output vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4"
W15-3016,D11-1033,0,0.0857942,"Missing"
W15-3016,N12-1047,0,0.227279,"Missing"
W15-3016,P06-2093,0,0.251311,"Missing"
W15-3016,W08-0310,1,0.88318,"Missing"
W15-3016,J07-1003,0,0.0605213,"Missing"
W15-3016,N13-1073,0,0.0478227,"En corpus, no improvement could be observed (Koehn and Haddow, 2012). 2 145 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 145–151, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. pora, we finally removed all sentence pairs that did not match the default criteria of the M OSES script clean-corpus-n.pl or that contained more than 70 tokens. Statistics regarding the parallel corpora used to train SMT systems are reported in Table 1 for the three language pairs under study. Word-level alignments are computed using fast align (Dyer et al., 2013) with options ”-d -o -v”. 2.2 challenge for this task is domain adaptation, for which only monolingual data are distributed. 3.1 Since this is the first time this translation task is considered, only a small development set of newsdiscusssions is available. In order to properly tune and test our systems, we performed a 3-fold crossvalidation, splitting the 1,500 in-domain sentences in two parts. Each random split respects document boundary, and yields roughly 1,000 sentences for tuning and 500 sentences for testing. The source of the documents, the newspapers Le Monde and The Guardian are also"
W15-3016,P13-2121,0,0.0327932,"Missing"
W15-3016,2008.amta-srw.3,0,0.109957,"Missing"
W15-3016,W12-3139,0,0.01708,"on and word alignments Tokenization for French and English text relies on in-house text processing tools (D´echelotte et al., 2008). All bilingual corpora provided by the organizers were used, except for the FrenchEnglish tasks where the UN corpus was not considered.3 We also used a heavily filtered version of the Common Crawl corpus, where we discard all sentences pairs that do not look like proper French/English parallel sentences. For all cor1 http://ncode.limsi.fr http://www.statmt.org/moses/ 3 In fact, when used in combination with the Giga Fr-En corpus, no improvement could be observed (Koehn and Haddow, 2012). 2 145 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 145–151, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. pora, we finally removed all sentence pairs that did not match the default criteria of the M OSES script clean-corpus-n.pl or that contained more than 70 tokens. Statistics regarding the parallel corpora used to train SMT systems are reported in Table 1 for the three language pairs under study. Word-level alignments are computed using fast align (Dyer et al., 2013) with options ”-d -o -v”. 2.2 challenge for this t"
W15-3016,P07-2045,0,0.00467831,"irst into simplified Russian, followed by a conversion into inflected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side. 1 2 Systems Overview Our experiments use N CODE1 , an open source implementation of the n-gram approach, as well as M OSES, which implements a vanilla phrase-based approach.2 For more details about these toolkits, the reader can refer to (Koehn et al., 2007) for M OSES and to (Crego et al., 2011) for N CODE. Introduction This paper documents LIMSI’s participation to the machine translation shared task for three language pairs: French-English and Russian-English in both directions, as well as Finnish-into-English. Each of these tasks poses its own challenges. For French-English, the task differs slightly from previous years as it considers user-generated news discusssions. While the domain remains the same, the texts that need to be translated are of a less formal type. To cope with the style shift, new monolingual corpora have been made available"
W15-3016,P10-1052,1,0.875811,"Missing"
W15-3027,W14-3348,0,0.034514,"edits transforming a sentence into another. While this sequence is not necessarily of minimal length, it is faster to compute, easier to use and, above all, more interpretable than the one computed using the standard minimum edit distance algorithm. In particular, difflib is able to automatically find edits between ‘phrases’ rather than between single words. 3 matic translation as a source sentence and its postedition as the target sentence. The word alignment between the automatic translation and the post-edited sentence, used as input in our APE-MT pipeline, has been computed using Meteor (Denkowski and Lavie, 2014). The APE-MT system has then been trained following the usual steps.3 In our experiments, we used our in-house MT system NC ODE (Crego et al., 2011) that implements a n-gram based translation model. As main features we used a 3-gram bilingual language model on words, a 4-gram bilingual language model on PoS factors and a 4-gram target language model trained only on the post-editions sentences, along with the conventional features (4 lexical features, 6 lexicalized reordering, distortion model, word and phrase penalty). We did allow reorderings during decoding. The training data is used to extr"
W15-3027,N13-1073,0,0.0287951,"translation and the length of the corresponding post-edition was higher than 1.2 or lower than 0.8. As shown in Table 1, these examples correspond mainly to errors in sentence boundaries or to ‘over-translation’ (e.g. when the post-editor added the translated title in the third example of Table 1), that could have a negative impact on the training of an APE system. At the end, the training set we used in all our experiments is made of 10,404 sentences. The source sentences and the automatic translation of the training and development set have been aligned at the word level using FASTA L IGN (Dyer et al., 2013) and the grow-diag-final symmetrization heuristic. To improve alignment quality, the sources and the translations have been first concatenated to the English-Spanish Europarl dataset and the resulting corpus has been aligned as a whole. Spanish MT outputs and post-editions have also been PoS-tagged using F REE L ING,1 a state-of-the-art rule-based PoS tagger for Spanish. We used a CRF-based model trained on the Penn Treebank for the English source sentences. All PoS tags have been mapped to the universal PoS Introduction This paper describes LIMSI submission to the WMT’15 Shared Task on Automa"
W15-3027,petrov-etal-2012-universal,0,0.0411748,"Missing"
W15-3027,N07-1064,0,0.742334,"E L ING,1 a state-of-the-art rule-based PoS tagger for Spanish. We used a CRF-based model trained on the Penn Treebank for the English source sentences. All PoS tags have been mapped to the universal PoS Introduction This paper describes LIMSI submission to the WMT’15 Shared Task on Automatic Post-Editing (APE). This task aims at automatically correcting errors produced by an unknown Machine Translation (MT) system by learning from human posteditions. For the first edition of this Shared Task we have submitted two APE systems. The first one, described in Section 3, is based on the approach of Simard et al. (2007) and considers the APE task as the automatic translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in improving translation quality. To understand the reasons of this failure, we present, in Section 4 a detailed analysis of the training data that highlights some of the difficulties of training an APE system. The second submitted system implements a series of sieves, applying, each, a simple postediting rule. The definition of these rules is based on our analysis of the most frequent error corrections. Experiments with this approach (S"
W15-3027,2013.mtsummit-papers.15,1,0.841472,"Missing"
W16-1203,D12-1133,0,0.0531772,"ich allows us to make all the experiments required to compare the various design decisions. The results of Table 1 also show that using the grow-diag heuristic to symmetrize the alignments rather than the intersection heuristic hurts performance for all languages. source de es fr it sv Experiments The parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and Nivre (2011). The greedy version of the parser is used in all but one experiments of the Section 3 while a beam-search (with a beamsize of 8 for learning & parsing) is used to achieve 2 2.5 target 2.3 the best performances of the proposed m"
W16-1203,D11-1005,0,0.239751,"Missing"
W16-1203,C12-1059,0,0.0592431,", such as the symmetrization heuristic or the filtering threshold, that can have a large impact on the quality of the transferred parser. We will evaluate, in Section 3 trough 3.1 the impact of these design decisions. 2.2 Partial Transition-based Learning We consider a transition-based dependency parser based on the arc-eager algorithm (Nivre, 2003): this parser builds a dependency tree incrementally by performing a sequence of actions. At each step of the parsing process, a classifier scores each possible action and the highest scoring one is applied. Training relies on the dynamic oracle of Goldberg and Nivre (2012): for each sentence, a parse tree is built incrementally; at each step, if the predicted action creates an erroneous dependency (or, equivalently, prevents the creation of a gold dependency), a weight vector is updated, according to the perceptron rule. The set of all ‘correct’ actions is built considering the (potentially wrong) predicted tree and the gold action is defined as the correct action with the highest model score. It is crucial to note that the training algorithm is an 1 We also remove from target sentences containing nonprojective dependencies, as sentences containing non-projecti"
W16-1203,P08-1068,0,0.129895,"Missing"
W16-1203,N16-1121,1,0.627771,"popularity and been improved by many works (see the overview in Section 2). In spite of the simplicity of the annotation transfer principle, all these methods have several (hidden) parameters, such as the symmetrization heuristic or filtering thresholds, that make any direct comparison of their performance very hard. That is why, in this work, we aim at analyzing the impact of external factors used as pre- and post-processing steps and their significance in the whole transfer process. To this end, we propose to use the simple transfer strategy exploiting partially annotated data introduced in Lacroix et al. (2016) to systematically compare various design decisions. The transfer strategy used in our experiments is explained in Section 2. We then propose to explore and analyze different external factors: projected data filtering (Section 3.1), enhancement of the parsing strategy (Section 3.2) and multi-source transfer (Section 3.3). Finally, we compare the efficiency of dependency transfer and supervised parsing (Section 3.4) and analyze the performance achieved for the different kind of labels (Section 3.5). Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 20–29, c San"
W16-1203,C14-1075,0,0.456056,"Missing"
W16-1203,P14-1126,0,0.137642,"Missing"
W16-1203,D11-1006,0,0.327188,"Missing"
W16-1203,P13-2017,0,0.184343,"Missing"
W16-1203,D10-1120,0,0.027799,"ed to under-resourced languages through the use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires various heuristic transformation rules to cope with the non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments. It has since enjoyed a great popularity and been improved by many works (see the overview"
W16-1203,W03-3017,0,0.184308,"ted corpus for the target language that contains partial but accurate annotations.1 We will describe, in the following Section, how a parser can be trained on such data. In spite of its simplicity, this way to transfer dependency has several (hidden) parameters, such as the symmetrization heuristic or the filtering threshold, that can have a large impact on the quality of the transferred parser. We will evaluate, in Section 3 trough 3.1 the impact of these design decisions. 2.2 Partial Transition-based Learning We consider a transition-based dependency parser based on the arc-eager algorithm (Nivre, 2003): this parser builds a dependency tree incrementally by performing a sequence of actions. At each step of the parsing process, a classifier scores each possible action and the highest scoring one is applied. Training relies on the dynamic oracle of Goldberg and Nivre (2012): for each sentence, a parse tree is built incrementally; at each step, if the predicted action creates an erroneous dependency (or, equivalently, prevents the creation of a gold dependency), a weight vector is updated, according to the perceptron rule. The set of all ‘correct’ actions is built considering the (potentially w"
W16-1203,J03-1002,0,0.00721589,"re clarified respectively in sections 3.1 and 3.2. This method achieves results that are competitive with recent state-of-the-art methods such as (Ma and Xia, 2014; Rasooli and Collins, 2015), at a much cheaper computational cost,5 which allows us to make all the experiments required to compare the various design decisions. The results of Table 1 also show that using the grow-diag heuristic to symmetrize the alignments rather than the intersection heuristic hurts performance for all languages. source de es fr it sv Experiments The parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and N"
W16-1203,D15-1039,0,0.108875,"Missing"
W16-1203,P11-2120,0,0.0781757,"he use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires various heuristic transformation rules to cope with the non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments. It has since enjoyed a great popularity and been improved by many works (see the overview in Section 2). In spite of the sim"
W16-1203,W09-1104,0,0.394698,"Missing"
W16-1203,N13-1126,0,0.127947,"Missing"
W16-1203,C14-1175,0,0.398116,"Missing"
W16-1203,D14-1187,1,0.845368,"Missing"
W16-1203,I08-3008,0,0.155315,"Processing (NLP) tools. Their use is however hindered by the scarcity of annotated data, which are only available for a restricted number of tasks, genres, domain, and languages. The supervision information that exists for well-resourced languages can however be transferred to under-resourced languages through the use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires"
W16-1203,P11-2033,0,0.0597534,"Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and Nivre (2011). The greedy version of the parser is used in all but one experiments of the Section 3 while a beam-search (with a beamsize of 8 for learning & parsing) is used to achieve 2 2.5 target 2.3 the best performances of the proposed method (Section 2.5).3 intersection (en) (multi) 73.7 76.8 76.8 79.3 77.9 80.9 77.8 80.1 82.1 83.3 grow-diag (en) (multi) 71.2 75.6 75.4 79.0 76.7 81.0 76.2 80.1 80.1 83.1 sup. 84.4 85.5 85.8 86.9 87.8 Table 1: Results of our transfer method. ‘sup’ present the fully supervised scores. 3 Analysis 3.1 The importance of filtering To assess the usefulness of filtering on tra"
W16-1203,C12-2136,0,0.0150353,"attached token per sentence is specified. Greedy target parsing. Evaluation on gold PoS-tagged data. de es fr it sv clusters It is well known that different techniques can boost parsing performance. For instance, clusters (Koo et al., 2008) may be used to reduce lexical sparseness, which is particularly appropriate in the case of dependency parser transfer since parallel data are generally not from the same domain as the corpus used to train and evaluate the parser. Another approach for boosting parsing performance is the use of a beam-search strategy that reduces the number of search errors (Zhang and Nivre, 2012). In this section, we aim at assessing, first, how parsing performance of the source language impacts the quality of the transferred parser, and second, how using more ‘advanced’ parsing techniques may boost parsing in the target language. Using a similar transfer process as in the previous section, we conduct experiments in which the source and target parsers will be progressively enriched: we consider, in a first experiment, a greedy parser to predict the dependencies of the source and target sentences; the source greedy parsers are then replaced by a beam-search parser and features describi"
W16-1205,P10-1131,0,0.0153098,"ions to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Align"
W16-1205,N10-1083,0,0.0655573,"Missing"
W16-1205,J93-2003,0,0.0932332,"Missing"
W16-1205,W10-2906,0,0.0133926,"taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by"
W16-1205,N09-1009,0,0.0274379,"been mentioned: indeed, taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which t"
W16-1205,D11-1005,0,0.0292803,"Missing"
W16-1205,P11-1061,0,0.0261148,"uch projections are less appropriate for fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameter"
W16-1205,N13-1073,0,0.0240163,"s 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, parameters are learned in an unsupervised way using the EM algorithm. Many refinements to these algorithms have been proposed, often to improve computational performance (Dyer et al., 2013). Another line of work tries to improve IBM and HMM models’ low generalization power by using feature-based models (Moore, 2005; Berg-Kirkpatrick et al., 2010). However, the IBM models remain today the most widely used approach both because of their efficiency and because they do not require any annotated data. They will thus serve as our main baseline. 3.2 Real-world situations for alignment transfer Scenarios for improving word alignment with crosslingual transfer fall into two categories, depending on whether the source and target languages play a symmetric role. We first consider the stand"
W16-1205,R11-1017,0,0.0245574,"o transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations acros"
W16-1205,P09-1042,0,0.030751,"ropean 36 languages; such projections are less appropriate for fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target langua"
W16-1205,W04-3229,0,0.0839814,"Missing"
W16-1205,W11-4615,0,0.0577353,"Missing"
W16-1205,P05-1058,0,0.0443402,"2006)’s works on English-Japanese, using Chinese as a bridge language, but their cross-language word similarity does not exploit Chinese-Japanese linguistic similarity. Finally, in the D IALECT scenario, T is a dialect of T˜, and even though parallel T˜-T data is not necessarily available, the transfer process can rely on the large number of common word forms. This would, for instance, be the case with the alignment of English with MS Arabic and dialects. Thanks to the large linguistic overlap, and contrarily to the previous scenarios, here again methods from the domain adaptation literature (Hua et al., 2005) may also successfully apply. Before closing this section, we would finally like to stress the fact that the motivations for transferring alignments can be many: one might want to get alignments for a small parallel bitext, to then transfer other annotations, or one might want to bootstrap an alignment model with transferred parameters, or even to train a small SMT, etc. Each such motivation may call for different strategies. 4 Methods for transferring alignment In this section, we exemplify with simple systems how general transfer methods can be instantiated for alignment transfer. From now o"
W16-1205,2005.mtsummit-papers.11,0,0.125988,"Missing"
W16-1205,P14-2037,0,0.0498453,"Missing"
W16-1205,S13-1044,0,0.0212217,"esourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in l"
W16-1205,D07-1005,0,0.0286754,"test data in L; concatenate with en:L data; train PARAMETER SPACE L GLOSSES -L PARAM -L train an en:L model; apply on test data train an en:L model; apply on test data word-for-word translated in L train an en:L model; translate the parameters; apply on test data Table 1: Summary of proposed methods, for bridge language L. in any scenario: while annotation projection for a monolingual task needs parallel data, for a bilingual model it would require multiparallel data. The converse is not true however, and the M ULTIPARALLEL scenario can be successfully exploited without annotation projection (Kumar et al., 2007). Second, the delexicalized approach causes a chicken-and-egg situation in real-life scenarios. Indeed, when the target language is under-resourced, one cannot assume the availability of a PoS tagger that is needed to compute delexicalized representations. Conversely, methods like (Wisniewski et al., 2014)’s cross-lingual PoS tagger projection and (T¨ackstr¨om et al., 2012)’s clusters are not applicable without a word aligned corpus. Finding common, even coarse-grained, representations then becomes a huge obstacle in many scenarios where alignment transfer is needed, which makes this approach"
W16-1205,N16-1121,1,0.907044,"ng them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for t"
W16-1205,W16-1203,1,0.894027,"ng them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for t"
W16-1205,2009.mtsummit-posters.12,0,0.0389405,"Missing"
W16-1205,2010.eamt-1.7,0,0.0387332,"Missing"
W16-1205,N15-1129,0,0.063574,"nting this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in literature (see however (Wang et al., 2006; Levinboim and Chiang, 2015)). Based on a high-level typology of cross-lingual transfer methodologies (§ 2), our contribution is to formalize realistic scenarios (defined in § 3) as well as some basic methodologies for projecting knowledge about bilingual alignments crosslinguistically (§ 4). Experiments in § 5 show that, at least for some of these scenarios, simple-minded methods can be surprisingly effective and open a discussion on further prospects and perspectives for future work. 2 Techniques for cross-lingual transfer In this section, we briefly review existing crosslingual transfer techniques for various NLP appl"
W16-1205,C14-1075,0,0.0374239,"Missing"
W16-1205,2006.amta-papers.11,0,0.0743748,"Missing"
W16-1205,D11-1006,0,0.05283,"Missing"
W16-1205,P13-2017,0,0.0605937,"Missing"
W16-1205,H05-1011,0,0.0373811,"el, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, parameters are learned in an unsupervised way using the EM algorithm. Many refinements to these algorithms have been proposed, often to improve computational performance (Dyer et al., 2013). Another line of work tries to improve IBM and HMM models’ low generalization power by using feature-based models (Moore, 2005; Berg-Kirkpatrick et al., 2010). However, the IBM models remain today the most widely used approach both because of their efficiency and because they do not require any annotated data. They will thus serve as our main baseline. 3.2 Real-world situations for alignment transfer Scenarios for improving word alignment with crosslingual transfer fall into two categories, depending on whether the source and target languages play a symmetric role. We first consider the standard symmetric B RIDGE scenario: it involves two languages S and T , for 37 which large bitexts with a ‘bridge’ language B exist"
W16-1205,C00-2163,0,0.231053,"Missing"
W16-1205,J03-1002,0,0.00967054,"s demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Aligning words The most popular models for statistical word alignment are the IBM models 1 to 6 (Brown et al., 1993; Och and Ney, 2003) and the HMM model of Vogel et al. (1996). These probabilistic generative models decompose the probability of an aligned sentence pair as the conjunction of a word translation model, a distortion model (models 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word cl"
W16-1205,N12-1052,0,0.0574107,"Missing"
W16-1205,C14-1175,0,0.0201019,"round to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameters in the former can then readily be used for the latter. A first instance of model transfer has already been mentioned: indeed, taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to proce"
W16-1205,C96-2141,0,0.400003,"ual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Aligning words The most popular models for statistical word alignment are the IBM models 1 to 6 (Brown et al., 1993; Och and Ney, 2003) and the HMM model of Vogel et al. (1996). These probabilistic generative models decompose the probability of an aligned sentence pair as the conjunction of a word translation model, a distortion model (models 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, para"
W16-1205,Q14-1005,0,0.0191771,"r fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameters in the former can then readily be used f"
W16-1205,P06-2112,0,0.122955,"ing, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in literature (see however (Wang et al., 2006; Levinboim and Chiang, 2015)). Based on a high-level typology of cross-lingual transfer methodologies (§ 2), our contribution is to formalize realistic scenarios (defined in § 3) as well as some basic methodologies for projecting knowledge about bilingual alignments crosslinguistically (§ 4). Experiments in § 5 show that, at least for some of these scenarios, simple-minded methods can be surprisingly effective and open a discussion on further prospects and perspectives for future work. 2 Techniques for cross-lingual transfer In this section, we briefly review existing crosslingual transfer te"
W16-1205,D14-1187,1,0.898649,"Missing"
W16-1205,H01-1035,0,0.0456015,"an effective way to remedy, at least partially, to this unsatisfactory situation. Among them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. I"
W16-1205,I08-3008,0,0.481971,"of the presentation, the resource-rich is viewed as the source language, and the resource-poor is accordingly the target language. 2.1 Transfer in data space This family of techniques seeks to automatically supply the annotations that are lacking on the target side, so that a model can be learned on these artificially generated data. Direct Transfer Two main lines of reasoning have been considered: the first assumes that the source and target languages are sufficiently similar, to the point that source annotations can be readily used to train a model in the target language (Hana et al., 2004; Zeman and Resnik, 2008). When such assumption does not hold, a necessary preliminary step will be to map the source and target data in a shared representation space: delexicalization, i.e. the replacement of words with (universal) PoS (McDonald et al., 2013) readily yields such mappings (Wisniewski et al., 2014), but it is also conceivable to consider automatically inferred multilingual representations (Jagarlamudi et al., 2011; Koˇcisk´y et al., 2014; Gouws et al., 2015). This simple approach has one downside: learning can only use features based on this inter-lingual representation – in particular this makes it im"
W16-1205,J07-3002,0,\N,Missing
W16-1205,D11-1086,0,\N,Missing
W16-1205,Q13-1001,0,\N,Missing
W16-2304,D12-1133,0,0.0799569,"Missing"
W16-2304,J04-2004,0,0.202109,"006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate conti"
W16-2304,N12-1047,0,0.110665,"to be too large to be handled by the CRF, so the following experiments were performed on the 300-best output. • words are projected into a 500-dimensional vector space; • the feed-forward architecture includes two hidden layers of size 1000 and 500; • the non-linearity is a sigmoid function; All models are trained for 20 epochs, then the selection relies on the perplexity measured on a validation set. For CTMs, the validation sets are sampled from the parallel training data. 3 Development and test sets Experiments For all our experiments, the MT systems are tuned using the kb-mira algorithm (Cherry and Foster, 2012) implemented in M OSES, including the reranking step. POS tagging is performed using the TreeTagger (Schmid, 1994) for English and Russian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were ob"
W16-2304,W08-0310,1,0.725891,"Missing"
W16-2304,L16-1241,1,0.787062,"ns in Romanian and Russian corpora is a lot higher than in English corpora. In such a situation, surface heuristics are less reliable. In order to address this limitation, we tried to extend the output of the decoder with morphological variations of nouns, pronouns and adjectives. Therefore, for each word in the output baring one of these PoS-tags, we introduced all forms in the paradigm as possible alternatives. The paradigm generation was performed for Russian using pymorphy, a dictionary implemented as a Python module.4 For Romanian, we used the crawled (thus sparse) lexicon introduced in (Aufrant et al., 2016). Once the outputs were extended, we used a CRF model to rescore this new search space. The CRF can use the features of the MT decoder, but can also include morphological or syntactic features in order to estimate output scores, even for words that were not observed in the training data. In the Russian experiment, oracle scores show that a maximum gain of 6.3 BLEU points can be obtained if the extension is performed on the full search space and 2.3 BLEU points on 300-best output of the N CODE decoder. The full search space, while being more promising, proved to be too large to be handled by th"
W16-2304,P14-1129,0,0.110474,"Missing"
W16-2304,W11-2123,0,0.0412066,"s then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. ∗ e = arg max e,a K X λk fk (f, e, a) k=1 Various English, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke,"
W16-2304,P07-2045,0,0.00443495,"translation into Russian and Romanian, we have attempted to extend the output of the decoder with morphological variations and to use a CRF model to rescore this new search space; as for the translation into German, we have been experimenting with source-side pre-ordering based on a dependency structure allowing permutations in order to reproduce the target word order. 1 2 System Overview Our experiments mainly use N CODE,1 an open source implementation of the n-gram approach, as well as M OSES2 for some contrastive experiments. For more details about these toolkits, the reader can refer to (Koehn et al., 2007) for M OSES and to (Crego et al., 2011) for N CODE. 2.1 Data pre-processing and word alignments All the English and Russian data have been cleaned by normalizing character encoding. Tokenization for English text relies on in-house text processing tools (D´echelotte et al., 2008). For the Russian corpora, we used the TreeTagger tokenizer. For Romanian, we developed and used tokro, a rule-based tokenizer. After normalization of diacritics, it repeatedly applies 3 rules: (a) word splitting on slashes, except for url addresses, (b) isolation of punctuation characters from a predefined set (includi"
W16-2304,F13-1033,1,0.854254,"ssian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were obtained by searching for oracle translations in the augmented output. Models were trained using inhouse implementation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims"
W16-2304,P06-2093,0,0.0731995,"Missing"
W16-2304,D07-1045,0,0.394133,"Missing"
W16-2304,N12-1005,1,0.933551,"sely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner, available at http:// research.microsoft.com/apps/catalog/ 240 3.1 As in our previous participations (Le et al., 2012b; Allauzen et al., 2013; P´echeux et al., 2014; Marie et al., 2015), we take advantage of the proposal of (Le et al., 2012a). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large output vocabulary, thereby making the training of large neural network language models feasible both for target language models (Le et al., 2011) and translati"
W16-2304,D13-1049,0,0.0232522,"ntation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims at taking advantage of the dependency structure of the source sentence to predict a permutation of the source words that is as close as possible to a correct syntactic word order in the target language: starting from the root of the dependency tree a classifier is used to recursively predict the order of a node and all its children. More precisely, for a family5 of size n, a multiclass classifier is used to select the best ordering of this family among its n! permutations. A different classifier is trained for each possible family size. Experimental results The experimenta"
W16-2304,N04-4026,0,0.0644058,"e used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match the target word order by unfolding the word alignments (Crego and Mari˜no, 2006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation model"
W16-2304,W15-3016,1,0.856173,"Missing"
W16-2304,tufis-etal-2008-racais,0,0.271893,"Missing"
W16-2304,J06-4004,0,0.296295,"Missing"
W16-2304,2002.tmi-tutorials.2,0,0.147945,"ram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner,"
W16-2304,P10-2041,0,0.0322506,"ish, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional fe"
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W16-2337,W14-3310,0,0.0473738,"Missing"
W16-2337,W15-3004,0,0.046834,"Missing"
W16-2337,W11-2123,0,0.0354078,"Missing"
W16-2337,2007.mtsummit-papers.33,0,0.146139,"Missing"
W16-2337,P96-1041,0,0.133579,"Missing"
W16-2337,N12-1047,0,0.0418397,"Missing"
W16-2337,2008.amta-srw.3,0,0.0600494,"Missing"
W16-2337,W08-0310,1,0.794017,"Missing"
W16-2337,P03-1054,0,0.0149962,"Missing"
W16-2337,W15-1001,0,0.0608976,"Missing"
W16-2337,P07-2045,0,0.0110378,"Missing"
W16-2337,2009.mtsummit-posters.7,0,0.0562614,"Missing"
W16-2337,N13-1073,0,0.0999667,"Missing"
W16-2337,E14-2008,0,0.0421507,"Missing"
W16-2337,N12-1005,1,0.905568,"Missing"
W16-2337,N03-1033,0,0.121475,"Missing"
W16-2337,2015.mtsummit-wpslt.8,0,0.0624102,"Missing"
W16-2337,I13-1128,0,0.0593276,"Missing"
W16-2337,W15-3016,1,0.873473,"Missing"
W16-2337,E06-1005,0,0.104963,"Missing"
W16-2337,P02-1040,0,0.0954205,"Missing"
W16-2337,W08-0329,0,0.056727,"Missing"
W16-2337,W12-3124,0,0.0441953,"Missing"
W17-4703,P16-2058,0,0.0535605,"Missing"
W17-4703,N06-2001,0,0.0221876,"vocabularies partially mitigates these issues, yet may cause serious instability (when computing embeddings of rare or unseen words) and complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning"
W17-4703,W08-0310,1,0.904814,"Missing"
W17-4703,W17-3203,0,0.0172074,"e helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem"
W17-4703,W07-0735,0,0.207772,"ting from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during norma"
W17-4703,W16-2302,0,0.135293,"Missing"
W17-4703,P15-1166,0,0.028189,"chitecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely"
W17-4703,W12-1514,0,0.0397072,"Missing"
W17-4703,W12-3130,0,0.0137968,"attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Rand"
W17-4703,2012.eamt-1.6,0,0.0469218,"Missing"
W17-4703,W10-1705,0,0.0619956,"into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a spec"
W17-4703,N16-1101,0,0.0286375,"features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenat"
W17-4703,W17-4705,1,0.915106,"acks many of the morphological contrasts that exist in the MRL. Normalization is needed to reduce the morphological variability on the MRL side so as to limit the number of types in the target, and to mitigate sparsity issues. This strategy is used for instance by Burlot et al. (2016) who remove the case mark from Czech nouns, which is not predictable from their English counterpart(s). Normalization is usually performed using handcrafted rules and requires expert knowledge for each language pair. In this paper, normalized words are obtained with an automatic data-driven method1 introduced in (Burlot and Yvon, 2017b). In a nutshell, this method performs a clustering of the MRL vocabulary by grouping together words that tend to share the same translation(s) in English. This translational similarity is based on the conditional entropy of lexical translation models estimated, for each MRL word form, using automatic word alignments. The clustering procedure merges two words whenever the resulting cluster does not increase the conditional entropy, which ensures a minimal loss of information during the whole process. Figure 1: Factored NMT system. The encoder and the attention mechanism of the Factored NMT ar"
W17-4703,E12-1068,0,0.0214011,"ate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Random Field (CRF) model for each morphological attribute to predict. Finally, word forms are produced via look-up in a morphological dictionary. This approach is extended by Weller et al. (2013), who takes verbal subcategorization frames into account, thus"
W17-4703,2016.amta-researchers.10,0,0.021389,"anslation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from"
W17-4703,P15-1001,0,0.0627746,"Missing"
W17-4703,W14-4012,0,0.101439,"Missing"
W17-4703,P16-1009,0,0.515486,"procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units, which crucially are all seen in training; (b) all possible target words can also be generated; (c) the size of the output layer can be set to remain within tractable limits; (d) most frequent words are kept as BP"
W17-4703,P07-2045,0,0.00736517,"r architecture, since the former (the cluster ID) constraints the set of possible values of the latter (the fine-grained PoS), which is notably used in our constrained decoding procedure (§ 5.4). 4.2 5 Experiments We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian. 5.1 Data and Preprocessing Our experimental setting follows the guidelines of the WMT’172 news translation task. The preprocessing of English data relies on in-house tools (D´echelotte et al., 2008). All the Czech data were tokenized and truecased the Moses toolkit (Koehn et al., 2007). PoS-tagging was performed with Morphodita (Strakov´a et al., 2014). The pre-processing of Latvian was provided by Tilde.3 Latvian PoS-tags were obtained with the LU MII Tagger (Paikens et al., 2013). For English-to-Czech, the parallel data used consisted in nearly 20M sentences from a subset of WMT data relevant to the news domain: Newscommentary, Europarl and specific categories of the Czeng corpus (news, paraweb, EU, fiction). Newstest-2015 was used for validation and the systems are tested on Newstest-2016 and 2017. The normalization of the Czech data was trained on the parallel data used"
W17-4703,P16-1162,0,0.820115,"procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units, which crucially are all seen in training; (b) all possible target words can also be generated; (c) the size of the output layer can be set to remain within tractable limits; (d) most frequent words are kept as BP"
W17-4703,D14-1025,0,0.150007,"Missing"
W17-4703,P14-5003,0,0.124455,"Missing"
W17-4703,P08-1059,0,0.100124,"). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morpho"
W17-4703,P16-1100,0,0.0305342,"es LIUM, University of Le Mans Franc¸ois Yvon LIMSI, CNRS, Universit´e Paris Saclay Abstract language (MRL), exhibiting larger token/type ratio than is observed for English. One strategy is to improve NMT’s internal procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units"
W17-4703,W16-2342,0,0.0356469,"tor (Strakov´a et al., 2014). Since we had no such tool for Latvian, all monolingual data available at WMT’17 were automatically tagged using the LU MII Tagger (Paikens et al., 2013) and we gathered the result in a look-up table. As one could expect, we obtained a large table (nearly 2.5M forms) in which we observed a lot of noise. 5.4 Automatic Evaluation Results are reported using the following automatic metrics: BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) which tunes a large number of features to maximize the human ranking correlation at sentence level and CharacTER (Wang et al., 2016), a character-level version of TER which has shown a high correlation with human rankings (Bojar et al., 2016). Each score on fully inflected word systems is averaged from two independent runs (for both single and ensembled models). 6.1 Experiments with Bitext The results using the bitext provided at the WMT’17 the evaluation campaign are presented in Table 2 for English-to-Czech 8 and in Table 3 for English-to-Latvian. We can observe that using the constrained decoding consistently improves the results, except when using split clusters. In this last case, the system is forced to predict a PoS"
W17-4703,P07-1017,0,0.0250328,"ktranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followe"
W17-4703,P13-1058,0,0.0126976,"e performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Random Field (CRF) model for each morphological attribute to predict. Finally, word forms are produced via look-up in a morphological dictionary. This approach is extended by Weller et al. (2013), who takes verbal subcategorization frames into account, thus enabling the CRFs to make better predictions. Note that Burlot et al. (2016) and El Kholy and Habash (2012b,a) propose related approaches respectively for translating into Czech and Arabic. Factored word representations have also been considered in neural language models (Niehues 3 Model Architectures The baseline NMT system used in this paper is an implementation of a standard NMT model with attention mechanism (Bahdanau et al., 2015). It consists of a sequence to sequence encoderdecoder of two recurrent neural networks (RNN), one"
W17-4703,2012.iwslt-papers.11,0,0.0257771,"hese issues, yet may cause serious instability (when computing embeddings of rare or unseen words) and complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also b"
W17-4703,W16-2208,0,0.0566494,"Missing"
W17-4703,W13-5624,0,0.411502,"2 5 Experiments We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian. 5.1 Data and Preprocessing Our experimental setting follows the guidelines of the WMT’172 news translation task. The preprocessing of English data relies on in-house tools (D´echelotte et al., 2008). All the Czech data were tokenized and truecased the Moses toolkit (Koehn et al., 2007). PoS-tagging was performed with Morphodita (Strakov´a et al., 2014). The pre-processing of Latvian was provided by Tilde.3 Latvian PoS-tags were obtained with the LU MII Tagger (Paikens et al., 2013). For English-to-Czech, the parallel data used consisted in nearly 20M sentences from a subset of WMT data relevant to the news domain: Newscommentary, Europarl and specific categories of the Czeng corpus (news, paraweb, EU, fiction). Newstest-2015 was used for validation and the systems are tested on Newstest-2016 and 2017. The normalization of the Czech data was trained on the parallel data used to train the MT systems, except Czeng fiction and paraweb subcorpora, which amounts to over 10M sentences. A part of these systems was also trained on synthetic parallel data (Sennrich et al., 2016a)"
W17-4703,P02-1040,0,0.0994593,"ugh each word and keeping at each step the k-best reinflection hypotheses according to the unigram model mentioned above. For Czech reinflection, we used the Morphodita generator (Strakov´a et al., 2014). Since we had no such tool for Latvian, all monolingual data available at WMT’17 were automatically tagged using the LU MII Tagger (Paikens et al., 2013) and we gathered the result in a look-up table. As one could expect, we obtained a large table (nearly 2.5M forms) in which we observed a lot of noise. 5.4 Automatic Evaluation Results are reported using the following automatic metrics: BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) which tunes a large number of features to maximize the human ranking correlation at sentence level and CharacTER (Wang et al., 2016), a character-level version of TER which has shown a high correlation with human rankings (Bojar et al., 2016). Each score on fully inflected word systems is averaged from two independent runs (for both single and ensembled models). 6.1 Experiments with Bitext The results using the bitext provided at the WMT’17 the evaluation campaign are presented in Table 2 for English-to-Czech 8 and in Table 3 for English-to-Latvian. We ca"
W17-4703,W16-2209,0,0.0441737,"nd complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training b"
W17-4705,W13-2208,0,0.0602545,"Missing"
W17-4705,W05-0909,0,0.0178122,"lation (MT) engines based on the neural encoder-decoder architecture with attention (Cho et al., 2014; Bahdanau et al., 2014) constitute the new state-of-the-art in statistical MT, at least for open-domain tasks (Sennrich et al., 2016a). The previous phrase-based (PBMT) architectures were complex (Koehn, 2010) and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect. Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) less appropriate to evaluate and compare modern NMT systems. To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation procedures. Error analysis protocols, as proposed eg. by 43 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 43–55 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tion tasks for the pairs English-Czech and EnglishLatvian.1 We finally relate our protocol to conventional metrics (§ 4), and conclude in § 5 by"
W17-4705,D16-1025,0,0.0378757,"To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score (Birch and Osborne, 2010) to measure the correctness of reordering decisions, MEANT (Lo and Wu, 2011) to measure the transfer of entailment relationships, or CharacTER (Wang et al., 2016) Error typologies Error analysis protocols, as proposed by Vilar et al. (2006); Popovi´c and Ney (2011); Stymne (2011) for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by Bentivogli et al. (2016); Toral Ruiz and S´anchez-Cartagena (2017); Costajuss`a (2017); Klubiˇcka et al. (2017). These works differ in the language pairs and in the error typology considered. Bentivogli et al. (2016) only recognizes three main error types which are automatically recognized based on aligning the hypotheses and references – for instance a morphological error is detected when the word form is wrong, whereas the lemma is correct; this definition is also adopted in (Toral Ruiz and S´anchezCartagena, 2017), and decomposed at the level of morphological features in (Peter et al., 2016); (Klubiˇcka et al., 20"
W17-4705,W10-1749,0,0.0197551,"l purpose” automatic metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) or METEOR (Banerjee and Lavie, 2005) remain the preferred way to measure progress in Machine Translation. Evaluation campaigns aimed at comparing systems have long abandoned these measures and resort to human judgments, such as ranking (Callison-Burch et al., 2007) or direct assessment (Bojar et al., 2016). To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score (Birch and Osborne, 2010) to measure the correctness of reordering decisions, MEANT (Lo and Wu, 2011) to measure the transfer of entailment relationships, or CharacTER (Wang et al., 2016) Error typologies Error analysis protocols, as proposed by Vilar et al. (2006); Popovi´c and Ney (2011); Stymne (2011) for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by Bentivogli et al. (2016); Toral Ruiz and S´anchez-Cartagena (2017); Costajuss`a (2017); Klubiˇcka et al. (2017). These works differ in the language pairs and in the error typology considered. Bentivogli et al. (2016) only r"
W17-4705,W07-0718,0,0.110522,"Missing"
W17-4705,J16-2001,0,0.00636087,"e aimed at specifically assessing the morphological competence of MT engines translating from English into a Morphologically Rich Language (MRL). Morphology poses two main types of problems in MT: (a) morphological variation in the source increases the occurrence of Out-ofVocabulary (OOV) source tokens, the translation of which is difficult to coin; (b) morphological variation in the target forces the MT to generate forms that have not been seen in training. Morphological complexity is alo often associated to more flexible word orderings, which is mostly a problem when translating from a MRL (Bisazza and Federico, 2016). Reducing these issues is a legitimate and important goal for many language pairs. Our method for measuring the morphological competence of MT systems (detailed in § 2) is mainly based on the analysis of minimal pairs, each representing a contrast that is expressed syntactically in English and morphologically in the MRL. By comparing the automatic translations of these pairs, it is then possible to approximately assess whether a given MT system has succeeded in generating the correct word form, carrying the proper morphological marks. In § 3, we illustrate the potential of our evaluation prot"
W17-4705,W14-4012,0,0.0109865,"Missing"
W17-4705,W17-1207,0,0.0347103,"Missing"
W17-4705,W11-2123,0,0.0140118,"Missing"
W17-4705,W17-4720,0,0.0340327,"Missing"
W17-4705,D17-1263,0,0.0465861,"hese studies is that NMT generates better agreements than alternatives such as PBMT or Hierarchical MT. culties for the pair English-German. Test suites enable to directly evaluate and compare specific abilities of MT Engines, including morphological competences: again, both studies found that NMT is markedly better than PBMT when it comes to phenomena such as word agreement. The downside is the requirement to have expert linguists prepare the data as well as evaluate the success of the MT system, which is a rather expensive price to pay to get a diagnostic evaluation. Test suites The work of Isabelle et al. (2017); Burchardt et al. (2017) resuscitates an old tradition of using carefully designed test suites King and Falkedal (1990); Lehmann et al. (1996) to explore the ability of NMT to handle specific classes of difficulties. Test suites typically include a small set of handcrafted sentences for each targeted type of difficulty. For instance, Isabelle et al. (2017) focuses on translating from English into French and is based on a set of 108 short sentences illustrating situations of morphosyntactic, lexico-syntactic and syntactical divergences between these two languages. Assessing a system’s ability"
W17-4705,C90-2037,0,0.849717,"e pair English-German. Test suites enable to directly evaluate and compare specific abilities of MT Engines, including morphological competences: again, both studies found that NMT is markedly better than PBMT when it comes to phenomena such as word agreement. The downside is the requirement to have expert linguists prepare the data as well as evaluate the success of the MT system, which is a rather expensive price to pay to get a diagnostic evaluation. Test suites The work of Isabelle et al. (2017); Burchardt et al. (2017) resuscitates an old tradition of using carefully designed test suites King and Falkedal (1990); Lehmann et al. (1996) to explore the ability of NMT to handle specific classes of difficulties. Test suites typically include a small set of handcrafted sentences for each targeted type of difficulty. For instance, Isabelle et al. (2017) focuses on translating from English into French and is based on a set of 108 short sentences illustrating situations of morphosyntactic, lexico-syntactic and syntactical divergences between these two languages. Assessing a system’s ability to handle these difficulties requires a human judge to decide whether the automated translation has successfully “crosse"
W17-4705,J10-4005,0,0.0462609,"a. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT’17 for English into Czech and Latvian. 1 Introduction It is nowadays unanimously recognized that Machine Translation (MT) engines based on the neural encoder-decoder architecture with attention (Cho et al., 2014; Bahdanau et al., 2014) constitute the new state-of-the-art in statistical MT, at least for open-domain tasks (Sennrich et al., 2016a). The previous phrase-based (PBMT) architectures were complex (Koehn, 2010) and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect. Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) less appropriate to evaluate and compare modern NMT systems. To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation procedures. Error analysis protocols, as proposed eg. by 43 Proceedings of the Co"
W17-4705,C96-2120,0,0.194696,"t suites enable to directly evaluate and compare specific abilities of MT Engines, including morphological competences: again, both studies found that NMT is markedly better than PBMT when it comes to phenomena such as word agreement. The downside is the requirement to have expert linguists prepare the data as well as evaluate the success of the MT system, which is a rather expensive price to pay to get a diagnostic evaluation. Test suites The work of Isabelle et al. (2017); Burchardt et al. (2017) resuscitates an old tradition of using carefully designed test suites King and Falkedal (1990); Lehmann et al. (1996) to explore the ability of NMT to handle specific classes of difficulties. Test suites typically include a small set of handcrafted sentences for each targeted type of difficulty. For instance, Isabelle et al. (2017) focuses on translating from English into French and is based on a set of 108 short sentences illustrating situations of morphosyntactic, lexico-syntactic and syntactical divergences between these two languages. Assessing a system’s ability to handle these difficulties requires a human judge to decide whether the automated translation has successfully “crossed” the bridge between l"
W17-4705,W17-4737,0,0.0501605,"Missing"
W17-4705,J11-4002,0,0.0202586,"Missing"
W17-4705,Q16-1037,0,0.00939429,"(2017) focuses on translating from English into French and is based on a set of 108 short sentences illustrating situations of morphosyntactic, lexico-syntactic and syntactical divergences between these two languages. Assessing a system’s ability to handle these difficulties requires a human judge to decide whether the automated translation has successfully “crossed” the bridge between languages.13 A similar methodology is used in the work of Burchardt et al. (2017), who use a test suite of approximately 800 segments covering a wide array of translation diffiAutomatic test suites The work by Linzen et al. (2016) specifically looks at the prediction of the correct agreement features in increasingly complex contexts generated by augmenting the distance between the head and its dependent and the number of intervening distractors. A language model is deemed correct if it scores the correct agreement higher than any wrong one. One intriguing finding of this study is the very good performance of RNNs, provided that they receive the right kind of feedback in training. A similar approach is adapted for MT by Sennrich (2017), who looks at a wider range of phenomena. Contrastive pairs as automatically produced"
W17-4705,P11-1023,0,0.0188214,"., 2006) or METEOR (Banerjee and Lavie, 2005) remain the preferred way to measure progress in Machine Translation. Evaluation campaigns aimed at comparing systems have long abandoned these measures and resort to human judgments, such as ranking (Callison-Burch et al., 2007) or direct assessment (Bojar et al., 2016). To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score (Birch and Osborne, 2010) to measure the correctness of reordering decisions, MEANT (Lo and Wu, 2011) to measure the transfer of entailment relationships, or CharacTER (Wang et al., 2016) Error typologies Error analysis protocols, as proposed by Vilar et al. (2006); Popovi´c and Ney (2011); Stymne (2011) for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by Bentivogli et al. (2016); Toral Ruiz and S´anchez-Cartagena (2017); Costajuss`a (2017); Klubiˇcka et al. (2017). These works differ in the language pairs and in the error typology considered. Bentivogli et al. (2016) only recognizes three main error types which are automatically recognized based on"
W17-4705,E17-2060,0,0.10283,"ents covering a wide array of translation diffiAutomatic test suites The work by Linzen et al. (2016) specifically looks at the prediction of the correct agreement features in increasingly complex contexts generated by augmenting the distance between the head and its dependent and the number of intervening distractors. A language model is deemed correct if it scores the correct agreement higher than any wrong one. One intriguing finding of this study is the very good performance of RNNs, provided that they receive the right kind of feedback in training. A similar approach is adapted for MT by Sennrich (2017), who looks at a wider range of phenomena. Contrastive pairs as automatically produced as follows: given a correct (source, target) pair p = (f , e), introduce one error in e yielding an alternative couple p0 = (f , e0 ). A system is deemed to perform correctly wrt. this contrastive pair if it scores p higher than p0 . This approach is fully automatic, looks at a wide range of contexts and phenomena and 12 http://www.qt21.eu/mqm-definition Note that this is a local evaluation – a system can produce a bad overall translation, yet pass the test. 13 51 NMT systems with BPE outperform in many ways"
W17-4705,W13-5624,0,0.145,"rˇ idiˇcem souhlas´ım s mal´ırˇ em I see him I see a crazy researcher I agree with the president I agree with the director I agree with the minister I agree with the driver I agree with the painter Result A-set negation found noun and adjective both have accusative form all nouns bear the same intrumental case (Entropy = 0.0) Table 3: Examples of sentences that pass the tests. • Word based NMT: NMT words is a system trained on WMT’17 parallel data with a target vocabulary of 80k tokens. It was not submitted at WMT’17 and is used for contrast. for Latvian and therefore used the LU MII Tagger (Paikens et al., 2013) to parse all Latvian monolingual data available at WMT’17. We then extracted a dictionary consisting of words and associated PoS from the automatic parses. We finally performed a coarse cleaning of this dictionary by removing the PoS that were predicted less than 100 times for a specific word. To run the morphological analysis of Latvian, we parsed the translated sentences with the tagger, then augmented the tagger predictions with our dictionary, producing the desired ambiguous analysis of the Latvian outputs. For the C-set, the translated sentence analyses are disambiguated: each word is ma"
W17-4705,E17-3017,0,0.031976,"Missing"
W17-4705,P02-1040,0,0.130988,"usly recognized that Machine Translation (MT) engines based on the neural encoder-decoder architecture with attention (Cho et al., 2014; Bahdanau et al., 2014) constitute the new state-of-the-art in statistical MT, at least for open-domain tasks (Sennrich et al., 2016a). The previous phrase-based (PBMT) architectures were complex (Koehn, 2010) and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect. Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) less appropriate to evaluate and compare modern NMT systems. To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation procedures. Error analysis protocols, as proposed eg. by 43 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 43–55 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tion tasks for the pairs English-Czech and EnglishLatvian.1 We finally relate our protocol to conventional m"
W17-4705,P16-1162,0,0.0163595,"the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT’17 for English into Czech and Latvian. 1 Introduction It is nowadays unanimously recognized that Machine Translation (MT) engines based on the neural encoder-decoder architecture with attention (Cho et al., 2014; Bahdanau et al., 2014) constitute the new state-of-the-art in statistical MT, at least for open-domain tasks (Sennrich et al., 2016a). The previous phrase-based (PBMT) architectures were complex (Koehn, 2010) and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect. Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) less appropriate to evaluate and compare modern NMT systems. To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation proc"
W17-4705,2006.amta-papers.25,0,0.146703,"Missing"
W17-4705,D14-1025,0,0.0244527,"Missing"
W17-4705,P14-5003,0,0.0981977,"Missing"
W17-4705,P11-4010,0,0.00480443,"t to human judgments, such as ranking (Callison-Burch et al., 2007) or direct assessment (Bojar et al., 2016). To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score (Birch and Osborne, 2010) to measure the correctness of reordering decisions, MEANT (Lo and Wu, 2011) to measure the transfer of entailment relationships, or CharacTER (Wang et al., 2016) Error typologies Error analysis protocols, as proposed by Vilar et al. (2006); Popovi´c and Ney (2011); Stymne (2011) for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by Bentivogli et al. (2016); Toral Ruiz and S´anchez-Cartagena (2017); Costajuss`a (2017); Klubiˇcka et al. (2017). These works differ in the language pairs and in the error typology considered. Bentivogli et al. (2016) only recognizes three main error types which are automatically recognized based on aligning the hypotheses and references – for instance a morphological error is detected when the word form is wrong, whereas the lemma is correct; this definition is also adopted in (Toral Ruiz and S´anc"
W17-4705,W16-2325,0,0.0197106,"a morphology accuracy for these systems. Using output-to-reference alignments produced by METEOR on lemmas, we Experiments We have run the presented morphological evaluation6 on several systems among which some were submitted at WMT’17. The description of the latter can be found in the proceedings of the Second Conference on Machine Translation (2017a). We briefly summarize the types of systems included in the English-to-Czech study: • Phrase-based systems: The Moses baseline was trained on WMT’17 data and was not submitted at WMT’17. UFAL Chimera7 was submitted at WMT’16 and is described in (Tamchyna et al., 2016). 8 6 We were not able to provide such scores for the other systems, since we did not have access to their translations of WMT’17 official test sets. 9 Outputs were taken from matrix.statmt.org. The scores are computed on tokenized and truecased outputs. The test suite and the scripts used for evaluation can be downloaded at github.com/franckbrl/morpheval. 7 Chimera (Bojar et al., 2013) consists in a phrase-based factored system (Moses), a deep-syntactic transfer-based system (TectoMT) and a rule-based post-processing system. 47 System LIMSI NMT UFAL NMT UEDIN NMT LIMSI FNMT LIUM FNTM UFAL NMT"
W17-4705,E17-1100,0,0.0248003,"Missing"
W17-4705,vilar-etal-2006-error,0,0.0617843,"s have long abandoned these measures and resort to human judgments, such as ranking (Callison-Burch et al., 2007) or direct assessment (Bojar et al., 2016). To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score (Birch and Osborne, 2010) to measure the correctness of reordering decisions, MEANT (Lo and Wu, 2011) to measure the transfer of entailment relationships, or CharacTER (Wang et al., 2016) Error typologies Error analysis protocols, as proposed by Vilar et al. (2006); Popovi´c and Ney (2011); Stymne (2011) for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by Bentivogli et al. (2016); Toral Ruiz and S´anchez-Cartagena (2017); Costajuss`a (2017); Klubiˇcka et al. (2017). These works differ in the language pairs and in the error typology considered. Bentivogli et al. (2016) only recognizes three main error types which are automatically recognized based on aligning the hypotheses and references – for instance a morphological error is detected when the word form is wrong, whereas the lemma is correct; this definition"
W17-4705,W16-2342,0,0.0140156,"All these models also use BPE segmentation. These systems are representative of different models across statistical MT history. Phrase-based systems are a former state of the art that wordbased NMT struggled to improve. The new state of the art is an NMT setup with an open vocabulary provided by byte pair encoding (BPE) segmentation (Sennrich et al., 2016b). Finally, we have a set of systems that are optimized in order to improve target morphology. The automatic scores of the systems submitted at WMT’178 are in Table 4 where we report BLEU, BEER (Stanojevi´c and Sima’an, 2014) and CharacTER (Wang et al., 2016).9 We also computed a morphology accuracy for these systems. Using output-to-reference alignments produced by METEOR on lemmas, we Experiments We have run the presented morphological evaluation6 on several systems among which some were submitted at WMT’17. The description of the latter can be found in the proceedings of the Second Conference on Machine Translation (2017a). We briefly summarize the types of systems included in the English-to-Czech study: • Phrase-based systems: The Moses baseline was trained on WMT’17 data and was not submitted at WMT’17. UFAL Chimera7 was submitted at WMT’16 a"
W17-4705,W16-2301,0,\N,Missing
W17-4705,W17-4739,0,\N,Missing
W17-4721,P07-2045,0,0.00542119,"Missing"
W17-4721,W16-2302,0,0.0393102,"Missing"
W17-4721,W13-5624,0,0.0674526,"Missing"
W17-4721,W17-4703,1,0.801097,"Missing"
W17-4721,E17-3017,0,0.0686027,"Missing"
W17-4721,W17-4705,1,0.865179,"Missing"
W17-4721,P16-1009,0,0.0612462,"Missing"
W17-4721,W08-0310,1,0.831832,"Missing"
W17-4721,P16-1162,0,0.0630628,"Missing"
W17-4721,D14-1025,0,0.0589701,"Missing"
W17-4721,P14-5003,0,0.0639461,"Missing"
W17-4721,W16-2342,0,0.056585,"Missing"
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W18-5804,U17-1006,0,0.0305163,"world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As discussed below, both"
W18-5804,C16-1086,0,0.0160834,"nguage, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of any other section,7 thereby al"
W18-5804,L18-1531,1,0.878737,"Missing"
W18-5804,W17-0123,0,0.0281211,"troduction A large number of the world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As d"
W18-5804,W14-2201,0,0.0714917,"Missing"
W18-5804,P06-1085,0,0.108277,"Missing"
W18-5804,Q14-1008,0,0.0362652,"Missing"
W18-5804,D13-1034,0,0.0370391,"Missing"
W18-5804,J99-1004,0,0.0168827,"of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent variations and extensions. Adaptor grammars (AGs) (Johnson et al., 2007) define a powerful mechanism to manipulate PCFG distributions to better match the oc"
W18-5804,J98-4004,0,0.0832912,"e useful for word discovery. A CFG is a 4-tuple G = (N, W, R, S) where N and W are respectively the non-terminal and terminal symbols, R a finite set of rules of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent v"
W18-5804,W08-0704,0,0.775201,"Missing"
W18-5804,P08-1046,0,0.0606297,"Missing"
W18-5804,P14-1027,0,0.0181962,"es of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentation is a way to test linguistic hypotheses during a language documentation process. Conversely, we have also shown that specializing a generic grammar with language s"
W18-5804,N09-1036,0,0.0291861,"phology of a well-studied Bantu language, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of an"
W18-5804,Q13-1021,0,0.298022,"licit hierarchical model of word internal structure ; an observation that was one of our primary motivations for using AGs in our language documentation work. In this series of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentat"
W18-6315,N13-1073,0,0.0228396,"n (Park et al., 2017; Crego and Senellart, 2016), artificial parallel data obtained through forward-translation (FT) can also prove advantageous and we also consider a FT system (fwdtrans-nmt): in this case the target side of the corpus is artificial and is generated using the baseline NMT applied to a natural source. 1. backtrans-bad: this is a very poor SMT system trained using only 50k parallel sentences from the out-of-domain data, and no additional monolingual data. For this system as for the next one, we use Moses (Koehn et al., 2007) out-of-the-box, computing alignments with Fastalign (Dyer et al., 2013), with a minimal pre-processing (basic tokenization). This setting provides us with a pessimistic estimate of what we could get in lowresource conditions. 3.1.2 BT quality does matter Our results (see Table 2) replicate the findings of (Sennrich et al., 2016a): large gains can be obtained from BT (nearly +2 BLEU in French and German); better artificial data yields better translation systems. Interestingly, our best Moses system is almost as good as the NMT and an order of magnitude faster to train. Improvements obtained with the bad system are much smaller; contrary to the better MTs, this sys"
W18-6315,W10-1749,0,0.0161361,"that both boost the baseline. Properties of back-translated data Comparing the natural and artificial sources of our parallel data wrt. several linguistic and distributional properties, we observe that (see Fig. 2 - 3): (i) artificial sources are on average shorter than natural ones: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent. (ii) automatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall τ of source-target alignments (Birch and Osborne, 2010): for FrenchEnglish the respective numbers are 0.048 (natural) and 0.018 (artificial); for GermanEnglish 0.068 and 0.053. Using more mono4 Stupid Back-Translation We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. 3 Parses were automatically computed with CoreNLP (Manning et al., 2014). 147 (a) (b) (c) (d) Figure 2: Properties of pseudo-English data obtained with backtrans-nmt from French. The synthetic source contains shorter sentences (a) and slightly simpler syntax (b). The vocabulary growth wrt. a"
W18-6315,eisele-chen-2010-multiun,0,0.0138978,"ptation scenario. For both language pairs, we use the Europarl tests from 2007 and 20082 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014. 2.2 NMT setups and performance Our baseline NMT system implements the attentional encoder-decoder approach (Cho et al., 2014; Bahdanau et al., 2015) as implemented in Nematus (Sennrich et al., 2017) on 4 million out-of-domain parallel sentences. For French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN (Eisele and Chen, 2010) and EUBookshop (Skadin¸sˇ et al., 2014) corpora. For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and MultiUN (see table 1). Bilingual BPE units (Sennrich et al., 2016b) are learned with 50k merge operations, yielding vocabularies of about respectively 32k and 36k for English→French and 32k and 44k for English→German. Both systems use 512-dimensional word embeddings and a single hidden layer with 1024 cells. They are optimized using Adam (Kingma and Ba, 1 2 3 Using artificial parallel data in NMT A simple way to use monolingual data in MT is to turn it into s"
W18-6315,W11-2138,0,0.0486714,"samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and MultiUN (see table 1). Bilingual BPE units (Sennrich et al., 2016b) are learned with 50k merge operations, yielding vocabularies of about respectively 32k and 36k for English→French and 32k and 44k for English→German. Both systems use 512-dimensional word embeddings and a single hidden layer with 1024 cells. They are optimized using Adam (Kingma and Ba, 1 2 3 Using artificial parallel data in NMT A simple way to use monolingual data in MT is to turn it into synthetic parallel data and let the training procedure run as usual (Bojar and Tamchyna, 2011). In this section, we explore various ways to implement this strategy. We first reproduce results of Sennrich et al. (2016a) with BT of various qualities, that we then analyze thoroughly. 3.1 3.1.1 The quality of Back-Translation Setups BT requires the availability of an MT system in the reverse translation direction. We consider here Version 7, see www.statmt.org/europarl/. www.statmt.org/wmt08. 145 Baseline backtrans-bad backtrans-good backtrans-nmt fwdtrans-nmt backfwdtrans-nmt natural BLEU 31.25 31.55 32.99 33.30 31.93 33.09 35.10 test-07 BEER 62.14 62.39 63.43 63.33 62.55 63.19 64.71 Base"
W18-6315,N16-1101,0,0.0223157,"rt (2016); Park et al. (2017) also consider forward translation and Chinea-Rios et al. (2017) expand these results to domain adaptation scenarios. Our results are complementary to these earlier studies. As shown above, many alternatives to BT exist. The most obvious is to use target LMs (Domhan and Hieber, 2017; Gulcehre et al., 2017), as we have also done here; but attempts to improve the encoder using multi-task learning also exist (Zhang and Zong, 2016). This investigation is also related to recent attempts to consider supplementary data with a valid target side, such as multi-lingual NMT (Firat et al., 2016), where source texts in several languages are fed in the same encoder-decoder architecture, with partial sharing of the layers. This is another realistic scenario where additional resources can be used to selectively improve parts of the model. Round trip training is another important source of inspiration, as it can be viewed as a way to use BT to perform semi-unsupervised (Cheng et al., 2016) or unsupervised (He et al., 2016) training of NMT. The most convincing attempt to date along these lines has been proposed by Lample et al. 9 Conclusion In this paper, we have analyzed various ways to i"
W18-6315,P16-1185,0,0.0192191,"der using multi-task learning also exist (Zhang and Zong, 2016). This investigation is also related to recent attempts to consider supplementary data with a valid target side, such as multi-lingual NMT (Firat et al., 2016), where source texts in several languages are fed in the same encoder-decoder architecture, with partial sharing of the layers. This is another realistic scenario where additional resources can be used to selectively improve parts of the model. Round trip training is another important source of inspiration, as it can be viewed as a way to use BT to perform semi-unsupervised (Cheng et al., 2016) or unsupervised (He et al., 2016) training of NMT. The most convincing attempt to date along these lines has been proposed by Lample et al. 9 Conclusion In this paper, we have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using GANs to make the pseudo-source sentences closer"
W18-6315,W17-4714,0,0.283817,"of what we could get in lowresource conditions. 3.1.2 BT quality does matter Our results (see Table 2) replicate the findings of (Sennrich et al., 2016a): large gains can be obtained from BT (nearly +2 BLEU in French and German); better artificial data yields better translation systems. Interestingly, our best Moses system is almost as good as the NMT and an order of magnitude faster to train. Improvements obtained with the bad system are much smaller; contrary to the better MTs, this system is even detrimental for the out-of-domain test. Gains with forward translation are significant, as in (Chinea-Rios et al., 2017), albeit about half as good as with BT, and result in small improvements for the in-domain and for the out-of-domain tests. Experiments combining forward and backward translation (backfwdtrans-nmt), each 2. backtrans-good: these are much larger SMT systems, which use the same parallel data as the baseline NMTs (see § 2.2) and all the English monolingual data available for the WMT 2017 shared tasks, totalling approximately 174M sentences. These systems are strong, yet relatively cheap to build. 3. backtrans-nmt: these are the best NMT systems we could train, using settings that replicate the fo"
W18-6315,W14-4012,0,0.128003,"Missing"
W18-6315,W17-4715,0,0.039431,"ource vocabulary with a copy of the target vocabulary. In this setup, Ha et al. (2016) ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume. We use the following cheap ways to generate pseudo-source texts: 1. copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, Currey et al. (2017) decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. 3. copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences. 2. copy-marked: another way to integrate 148 (a) (b) (c) (d) Figure 3: Properties of pseudo-English data obtained with backtrans-nmt (back-translated from German). Tendencies similar to Engli"
W18-6315,D17-1158,0,0.27621,"iments reported in the literature only use a subset of the available monolingual resources. This suggests that standard recipes for BT might be sub-optimal. This paper aims to better understand the strengths and weaknesses of BT and to design more principled techniques to improve its effects. More specifically, we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance? Which properties of back-translated sentences actually matter for MT quality? Does BT act as some kind of regularizer (Domhan and Hieber, 2017)? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT is often used for domain adaptation: can the effect of having more indomain data be sorted out from the mere increase of training material (Sennrich et al., 2016a)? For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of Poncelas et al. (2018). To answer these questions, we have reimplemented several strategies to use monolingual data in NMT and have run experiments on two language pairs in a very controlled se"
W18-6315,P16-1009,0,0.561302,"e specifically, we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance? Which properties of back-translated sentences actually matter for MT quality? Does BT act as some kind of regularizer (Domhan and Hieber, 2017)? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT is often used for domain adaptation: can the effect of having more indomain data be sorted out from the mere increase of training material (Sennrich et al., 2016a)? For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of Poncelas et al. (2018). To answer these questions, we have reimplemented several strategies to use monolingual data in NMT and have run experiments on two language pairs in a very controlled setting (see § 2). Our main results (see § 4 and § 5) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT. Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (P"
W18-6315,2005.mtsummit-papers.11,0,0.155939,"tups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests. Table 1: Size of parallel corpora 2 Experimental Setup 2.1 In-domain and out-of-domain data We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus1 (Koehn, 2005) for two translation directions: English→German and English→French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 20082 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014. 2.2 NMT s"
W18-6315,J10-4005,0,0.0242813,"generation of Neural Machine Translation (NMT) systems is known to be extremely data hungry (Koehn and Knowles, 2017). Yet, most existing NMT training pipelines fail to fully take advantage of the very large volume of monolingual source and/or parallel data that is often available. Making a better use of data is particularly critical in domain adaptation scenarios, where parallel adaptation data is usually assumed to be small in comparison to out-of-domain parallel data, or to in-domain monolingual texts. This situation sharply contrasts with the previous generation of statistical MT engines (Koehn, 2010), which could seamlessly integrate very large amounts of nonparallel documents, usually with a large positive effect on translation quality. Such observations have been made repeatedly and have led to many innovative techniques to in144 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 144–155 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64015 en-fr en-de Out-of-domain Sents Token 4.0M 86.8M/97.8M 4.1M 84.5M/77.8M Sents 1.9M 1.8M In-domain Token 46.0M/50.6M 45"
W18-6315,P07-2045,0,0.00642649,"p between the worst and best systems (for both languages). As noted eg. in (Park et al., 2017; Crego and Senellart, 2016), artificial parallel data obtained through forward-translation (FT) can also prove advantageous and we also consider a FT system (fwdtrans-nmt): in this case the target side of the corpus is artificial and is generated using the baseline NMT applied to a natural source. 1. backtrans-bad: this is a very poor SMT system trained using only 50k parallel sentences from the out-of-domain data, and no additional monolingual data. For this system as for the next one, we use Moses (Koehn et al., 2007) out-of-the-box, computing alignments with Fastalign (Dyer et al., 2013), with a minimal pre-processing (basic tokenization). This setting provides us with a pessimistic estimate of what we could get in lowresource conditions. 3.1.2 BT quality does matter Our results (see Table 2) replicate the findings of (Sennrich et al., 2016a): large gains can be obtained from BT (nearly +2 BLEU in French and German); better artificial data yields better translation systems. Interestingly, our best Moses system is almost as good as the NMT and an order of magnitude faster to train. Improvements obtained wi"
W18-6315,P16-1162,0,0.741019,"e specifically, we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance? Which properties of back-translated sentences actually matter for MT quality? Does BT act as some kind of regularizer (Domhan and Hieber, 2017)? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT is often used for domain adaptation: can the effect of having more indomain data be sorted out from the mere increase of training material (Sennrich et al., 2016a)? For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of Poncelas et al. (2018). To answer these questions, we have reimplemented several strategies to use monolingual data in NMT and have run experiments on two language pairs in a very controlled setting (see § 2). Our main results (see § 4 and § 5) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT. Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (P"
W18-6315,skadins-etal-2014-billions,0,0.0298502,"Missing"
W18-6315,W17-3204,0,0.0289306,"through back-translation - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement. 1 Introduction The new generation of Neural Machine Translation (NMT) systems is known to be extremely data hungry (Koehn and Knowles, 2017). Yet, most existing NMT training pipelines fail to fully take advantage of the very large volume of monolingual source and/or parallel data that is often available. Making a better use of data is particularly critical in domain adaptation scenarios, where parallel adaptation data is usually assumed to be small in comparison to out-of-domain parallel data, or to in-domain monolingual texts. This situation sharply contrasts with the previous generation of statistical MT engines (Koehn, 2010), which could seamlessly integrate very large amounts of nonparallel documents, usually with a large posi"
W18-6315,D14-1025,0,0.0390857,"Missing"
W18-6315,P14-5010,0,0.0028429,"he same length are more frequent. (ii) automatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall τ of source-target alignments (Birch and Osborne, 2010): for FrenchEnglish the respective numbers are 0.048 (natural) and 0.018 (artificial); for GermanEnglish 0.068 and 0.053. Using more mono4 Stupid Back-Translation We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. 3 Parses were automatically computed with CoreNLP (Manning et al., 2014). 147 (a) (b) (c) (d) Figure 2: Properties of pseudo-English data obtained with backtrans-nmt from French. The synthetic source contains shorter sentences (a) and slightly simpler syntax (b). The vocabulary growth wrt. an increasing number of observed sentences (c) and the token-type correlation (d) suggest that the natural source is lexically richer. random monotonic BLEU 32.08 33.52 test-07 BEER 62.98 63.75 CTER 50.78 49.51 BLEU 32.66 33.73 test-08 BEER 62.86 63.59 CTER 49.99 48.91 BLEU 23.05 32.16 newstest-14 BEER CTER 55.38 58.51 61.75 48.64 Table 4: Selection strategies for BT data (Engli"
W18-6315,W16-2342,0,0.0213024,"the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, finetuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upperbound of the translation performance and is denoted below as natural. Our baseline and topline results are in Table 2, where we measure translation performance using BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) (higher is better) and characTER (Wang et al., 2016) (smaller is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today’s best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests. Table 1: Size of parallel corpora 2 Experimental Setup 2.1 In-domain and out-of-domain data We are mostly interested with the following training s"
W18-6315,P02-1040,0,0.108003,"in corpus mixed with an equal amount of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10. Since BPE units are selected based only on the out-of-domain statistics, finetuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training. This system defines an upperbound of the translation performance and is denoted below as natural. Our baseline and topline results are in Table 2, where we measure translation performance using BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) (higher is better) and characTER (Wang et al., 2016) (smaller is better). As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today’s best system, but still represent serious baselines for these datasets. Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests. Table 1: Size of parallel corpora 2 Experimental Setup"
W18-6315,D16-1160,0,0.0500484,"s well as pivot-based artificial data; and (Poncelas et al., 2018), which studies the effects of increasing the size of BT data. The studies of Crego and Senellart (2016); Park et al. (2017) also consider forward translation and Chinea-Rios et al. (2017) expand these results to domain adaptation scenarios. Our results are complementary to these earlier studies. As shown above, many alternatives to BT exist. The most obvious is to use target LMs (Domhan and Hieber, 2017; Gulcehre et al., 2017), as we have also done here; but attempts to improve the encoder using multi-task learning also exist (Zhang and Zong, 2016). This investigation is also related to recent attempts to consider supplementary data with a valid target side, such as multi-lingual NMT (Firat et al., 2016), where source texts in several languages are fed in the same encoder-decoder architecture, with partial sharing of the layers. This is another realistic scenario where additional resources can be used to selectively improve parts of the model. Round trip training is another important source of inspiration, as it can be viewed as a way to use BT to perform semi-unsupervised (Cheng et al., 2016) or unsupervised (He et al., 2016) training"
W18-6315,E17-3017,0,0.0798103,"Missing"
W18-6433,W15-1844,0,0.0116739,". We collected sentences containing a coreference link involving a personal pronoun (it) or a relative pronoun (that, which, who, whom, whose). The base sentence remains unchanged. In order to generate the variant, the antecedent noun of the pronoun is then changed to a synonym using WordNet (Miller, 1995): • Czech: MorphoDiTa (Strakov´a et al., 2014) • German: SMOR (Schmid et al., 2004) • Personal pronoun: This cat is cute and I love it. → This dog is cute and I love it. • Finnish: The finnish-analyze-words script7 provided by the Language Bank of Finland8 and based on the Omorfi morphology (Pirinen, 2015) and the HFST toolkit (Lind´en et al., 2011) • Relative pronoun: The woman who left was angry. → The man who left was angry. In the output of the MT system, we are then able to locate the antecedent of the pronoun by looking for the only noun that differs between the base and variant translations (namely, the translation of cat/woman in the base and dog/man in the variant). Finally, we check whether the noun and personal • English: MorphoDiTa (Strakov´a et al., 2014) 7 http://urn.fi/urn:nbn:fi: lb-2018041701 8 https://www.kielipankki.fi/ 549 pronoun bear the same gender.9 We also check number"
W18-6433,W17-4705,1,0.757878,".fi francois.yvon@limsi.fr Abstract about systems performance than just one overall number (even if it correlates well with human judgement). Evaluation metrics that focus on various aspects of the translation, such as syntax or morphology, rather than on general translation quality, have thus seen renewed interest. This interest has spurred the inclusion of additional test suites into the WMT 2018 news translation task. Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics. In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-toCzech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018’s participants for these language pairs. Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure. 1 Burlot and Yvon (2017, B&Y in the following) present a test suite for evaluating the morphological competence of machine translation systems. They provide a set of sentence pairs in the source language that differ by one"
W18-6433,schmid-etal-2004-smor,0,0.0605379,"Missing"
W18-6433,P16-1162,0,0.017792,"th feature types, the variants are created through some type of transformation that is supposed to be invariant with respect to target morphology. For the consistency features, this transformation is semantic (based on the hyponymy relation), whereas it is morphological for the stability features. Rare word features In the early days of NMT, translation of out-ofvocabulary words was virtually impossible and hampered the performance when compared with SMT. In recent years however, most systems have adopted an approach in which rare words are split into “subwords” during preprocessing (see e.g. Sennrich et al., 2016), such that any unknown word can be composed of various subword chunks during test time. Several subword chunking algorithms with various parameter settings can be used, but their respective performance differences are hard to assess as they typically concern low-frequency words with low impact on general translation quality. Therefore, we introduce two features that specifically deal with low-frequency items. These features are language-independent and do not require the use of a morphological analyzer. For the first feature, we identify large numbers (at least 3 digits) in the English source"
W18-6433,P05-1045,0,0.0339138,"set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of a contrast pair only differ in one word (or phrase) and across one feature at a time. Type B contrasts contain somewhat more complicated substitutions that are mainly evaluated in terms of agreement. For example, a contrast pair con"
W18-6433,P14-5003,0,0.0577653,"Missing"
W18-6433,tiedemann-2012-parallel,0,0.0291093,"For instance, the English expression apple juice in the base translates into the German compound Apfelsaft. We modify the word apple and obtain orange juice, which translates into Orangensaft. In the MT output we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where th"
W18-6433,P07-2045,1,0.0112429,"ut we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where the verb should be located at the end of the clause. Using CoreNLP annotations, we focus on specific English conjunctions that lead to a verb shift in German, like that → dass, because → weil, etc. In order to gen"
W18-6433,P14-5010,0,0.0375358,"generation: The Morpheval test suites 1. Collect a large number of short sentences (length &lt; 15 words) containing a source feature of interest. A Morpheval test suite according to B&Y consists of three aspects: • the definition of a set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of"
W19-4443,D18-1289,0,0.0673825,"Missing"
W19-4443,D17-1215,0,0.0204232,"mbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which gaps and distractors are carefully selected. One of the challenges of recent work on this issue is the development of realistic test sets, which can only be answered by a deep understanding of the text (Paperno et al., 2016; Xie et al., 2018), using for instance information regarding words partof-speech or syntactic role of the deleted word. 1 See however the work of Jia and Liang (2017) or Kaushik and Lipton (2018), who claim that quantitative progresses, measured on standard question answering tasks, mainly reflect an improvement in the ability of these systems to perform surface matches between questions and answers. 413 Cloze test Herring is a type of fish it swims closer to San Francisco than before. Herring is a favorite food of sea , so they might have followed those fish to . Sea lions might also like the pier because ... They have twin daughters named Barbara and Jenna family has a dog Barney and a named India. The Neil Armstrong was the first man to step onto the mo"
W19-4443,D18-1546,0,0.0134248,"and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which gaps and distractors are carefully selected. One of the challenges of recent work on this issue is the development of realistic test sets, which can only be answered by a deep understanding of the text (Paperno et al., 2016; Xie et al., 2018), using for instance information regarding words partof-speech or syntactic role of the deleted word. 1 See however the work of Jia and Liang (2017) or Kaushik and Lipton (2018), who claim that quantitative progresses, measured on standard question answering tasks, mainly reflect an improvement in the ability of these systems to perform surface matches between questions and answers. 413 Cloze test Herring is a type of fish it swims closer to San Francisco than before. Herring is a favorite food of sea , so they might have followed those fish to . Sea lions might also like the pier because ... They have twin daughters named Barbara and Jenna family has a dog Barney and a named India. The Neil Armstrong was the first man to step onto the moon . put an American flag up"
W19-4443,N04-1025,0,0.203757,"Missing"
W19-4443,P17-1168,0,0.0310053,"b) the training data. We acknowledge that it would also be possible to use other architectures for language models (e.g. n-grams models (Chen and Goodman, 1999)); to use character-based models (Sutskever et al., 2011; Kim et al., 2016) or subword-based models that could accommodate open vocabularies; or to train more sophisticated RNN models (Yang et al., 2018). It would likewise be possible to use more complex algorithms to predict missing words (e.g. by using left and right contexts), or even by testing text understanding systems that are more representative to the current state of-the-art (Dhingra et al., 2017; Yu et al., 2018). As a first step in this direction, we also experiment with a self-attentional language-model (AlRfou et al., 2018; Liu et al., 2018) using the Transformer architecture (Vaswani et al., 2017), from We stick here to a much simpler form of cloze testing, based on a uniform random strategy to select the deleted words. We leave for future work the use of more sophisticated methods specifically designed to generate difficult tests. In practice, each test document is automatically divided into N passages of the same size; in each passage, M positions are randomly selected uniforml"
W19-4443,E09-1027,0,0.0289832,"into a global score. This approach has recently been renewed by the use of supervised statistical learning methods capable of integrating into the prediction of readability a very large number of linguistic characteristics (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Vajjala and Meurers, 2012; Franc¸ois and Fairon, 2012; Vajjala and Meurers, 2014; Brunato et al., 2018) aimed at capturing readability indices at the lexical, syntactic, semantic and even discursive levels. It can be argued that these enhanced feature sets are able to take into account so-called cognitive factors (Feng et al., 2009). However, these approaches depend on the availability of texts annotated with their difficulty levels, which are often defined in relation to a particular task or readership. The elicitation of these annotations is a complex operation, which requires either the implementation of understanding measurement protocols on controlled populations, using for example cloze tests to evaluate understanding (Taylor, 1953; Oller Jr., 1973); or the work of highly qualified experts, at the risk of observing disagreements between annotators (Petersen and Ostendorf, 2009). They also require automatically extr"
W19-4443,D12-1043,0,0.528907,"Missing"
W19-4443,P16-1144,0,0.0233464,"ill in blanks in sentences from children’s stories, introducing the CBT (Children Book Tests) corpus. This technique has also been used in several previous studies (Bimbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which gaps and distractors are carefully selected. One of the challenges of recent work on this issue is the development of realistic test sets, which can only be answered by a deep understanding of the text (Paperno et al., 2016; Xie et al., 2018), using for instance information regarding words partof-speech or syntactic role of the deleted word. 1 See however the work of Jia and Liang (2017) or Kaushik and Lipton (2018), who claim that quantitative progresses, measured on standard question answering tasks, mainly reflect an improvement in the ability of these systems to perform surface matches between questions and answers. 413 Cloze test Herring is a type of fish it swims closer to San Francisco than before. Herring is a favorite food of sea , so they might have followed those fish to . Sea lions might also like th"
W19-4443,W18-0535,0,0.119666,"Source Weebit OSE Gaps 157K 8.5K Levels 5 3 Figure 1 displays the distribution of the FleschKincaid Grade Level (FKGL) for these two corpora. Complexity levels are distributed nearly as expected for the Weebit corpus apart from levels 3 and 4 where the latter seems simpler than foreseen, with large overlapping spans. It was already noted by (Vajjala and Meurers, 2012) that the actual readability level of each test was difficult to predict accurately based on the sole FKGL. OSE complexity levels are also in agreement with the Flesch-Kincaid index, and in agreement with the numbers reported in (Vajjala and Lucic, 2018); again we see a large overlap between levels for this index. Overall, OSE texts are somewhat more complex than Weebit’s, with OSE level 1 comparable in difficulty to Weebit level 4. Both sources were pre-processed with regular expressions to discard bits of text that were recurrent such as advertisement footers for companies (BBC, WeeklyReader, MetaMetrics) and material The reference annotations used to validate our method mostly come from two sources: Weebit (Vajjala and Meurers, 2012) and OneStopEnglish (OSE for short) (Vajjala and Lucic, 2018). The first identifies 5 levels of complexity i"
W19-4443,W12-2019,0,0.671267,"yvon}@limsi.fr Abstract ity (using the average sentence length as a proxy) and lexical complexity (average length in characters or syllables of words in a sentence). One of the most well-known measure along these lines is the Flesch-Kincaid readability index (Kincaid et al., 1975), which combines these two measures into a global score. This approach has recently been renewed by the use of supervised statistical learning methods capable of integrating into the prediction of readability a very large number of linguistic characteristics (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Vajjala and Meurers, 2012; Franc¸ois and Fairon, 2012; Vajjala and Meurers, 2014; Brunato et al., 2018) aimed at capturing readability indices at the lexical, syntactic, semantic and even discursive levels. It can be argued that these enhanced feature sets are able to take into account so-called cognitive factors (Feng et al., 2009). However, these approaches depend on the availability of texts annotated with their difficulty levels, which are often defined in relation to a particular task or readership. The elicitation of these annotations is a complex operation, which requires either the implementation of understand"
W19-4443,E17-2025,0,0.0221862,"e a drop-out of 0.1 for the embedding layer, 0.3 for the LSTM layers and 0.4 for the output layer ; the batch size is 64, and backpropagation context is 64. We borrow two additional techniques from Merity et al. (2018): the use of variable-length backpropagation sequences and the use of two regularization terms penalizing the parameters of the output layers: activation regularization and temporal activation regularization (see reference for details). The weight matrices at the output of LSTM after the linear projection and the word embedding are shared to avoid overfitting (Inan et al., 2017; Press and Wolf, 2017) and to reduce the number of parameters. This implementation is referred to as RNN below. We contrast our implementation with that of Merity et al. (2018)6 trained with the same parameter values as our RNN implementation, random seed value is 1882. This implementation is referred to as AWD below. Basic statistics regarding these models (size, test set perplexity) are in Table 2, suggesting that we were able to have a diverse set of models of varying strengths7 . Last, we also report results obtained with a selfattentional architecture, directly using the smaller version (context size is 768 an"
W19-4443,W14-1203,0,0.0158052,"length as a proxy) and lexical complexity (average length in characters or syllables of words in a sentence). One of the most well-known measure along these lines is the Flesch-Kincaid readability index (Kincaid et al., 1975), which combines these two measures into a global score. This approach has recently been renewed by the use of supervised statistical learning methods capable of integrating into the prediction of readability a very large number of linguistic characteristics (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Vajjala and Meurers, 2012; Franc¸ois and Fairon, 2012; Vajjala and Meurers, 2014; Brunato et al., 2018) aimed at capturing readability indices at the lexical, syntactic, semantic and even discursive levels. It can be argued that these enhanced feature sets are able to take into account so-called cognitive factors (Feng et al., 2009). However, these approaches depend on the availability of texts annotated with their difficulty levels, which are often defined in relation to a particular task or readership. The elicitation of these annotations is a complex operation, which requires either the implementation of understanding measurement protocols on controlled populations, us"
W19-4443,D16-1264,0,0.0415735,"text to directly evaluate its empirical readability, dispensing with the need to perform step (b). As a first step in this direction, we need to check whether that performance of machine comprehension systems should exhibit a form of dependency to the actual readability of the text: the simpler the text, the better they should be understood by a 2.2 Testing machine comprehension The measurement of automatic comprehension of texts is an old and difficult issue. By analogy with measures of human understanding, two main methods are commonly used: comprehension questions (Richardson et al., 2013; Rajpurkar et al., 2016) and cloze tests. The use of cloze tests to assess the performance of comprehension systems has for instance been proposed by Hill et al. (2016), which studies the ability of various neural models to fill in blanks in sentences from children’s stories, introducing the CBT (Children Book Tests) corpus. This technique has also been used in several previous studies (Bimbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which ga"
W19-4443,D18-1257,0,0.0199396,"nces from children’s stories, introducing the CBT (Children Book Tests) corpus. This technique has also been used in several previous studies (Bimbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which gaps and distractors are carefully selected. One of the challenges of recent work on this issue is the development of realistic test sets, which can only be answered by a deep understanding of the text (Paperno et al., 2016; Xie et al., 2018), using for instance information regarding words partof-speech or syntactic role of the deleted word. 1 See however the work of Jia and Liang (2017) or Kaushik and Lipton (2018), who claim that quantitative progresses, measured on standard question answering tasks, mainly reflect an improvement in the ability of these systems to perform surface matches between questions and answers. 413 Cloze test Herring is a type of fish it swims closer to San Francisco than before. Herring is a favorite food of sea , so they might have followed those fish to . Sea lions might also like the pier because ..."
W19-4443,D13-1020,0,0.0383586,"hension tests on a given text to directly evaluate its empirical readability, dispensing with the need to perform step (b). As a first step in this direction, we need to check whether that performance of machine comprehension systems should exhibit a form of dependency to the actual readability of the text: the simpler the text, the better they should be understood by a 2.2 Testing machine comprehension The measurement of automatic comprehension of texts is an old and difficult issue. By analogy with measures of human understanding, two main methods are commonly used: comprehension questions (Richardson et al., 2013; Rajpurkar et al., 2016) and cloze tests. The use of cloze tests to assess the performance of comprehension systems has for instance been proposed by Hill et al. (2016), which studies the ability of various neural models to fill in blanks in sentences from children’s stories, introducing the CBT (Children Book Tests) corpus. This technique has also been used in several previous studies (Bimbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Complet"
W19-4443,P05-1065,0,0.0868879,"e du rocher, F-75008 Paris, France {marc.benzahra,francois.yvon}@limsi.fr Abstract ity (using the average sentence length as a proxy) and lexical complexity (average length in characters or syllables of words in a sentence). One of the most well-known measure along these lines is the Flesch-Kincaid readability index (Kincaid et al., 1975), which combines these two measures into a global score. This approach has recently been renewed by the use of supervised statistical learning methods capable of integrating into the prediction of readability a very large number of linguistic characteristics (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Vajjala and Meurers, 2012; Franc¸ois and Fairon, 2012; Vajjala and Meurers, 2014; Brunato et al., 2018) aimed at capturing readability indices at the lexical, syntactic, semantic and even discursive levels. It can be argued that these enhanced feature sets are able to take into account so-called cognitive factors (Feng et al., 2009). However, these approaches depend on the availability of texts annotated with their difficulty levels, which are often defined in relation to a particular task or readership. The elicitation of these annotations is a complex operatio"
W19-4443,W12-2704,0,0.030003,"comprehension of texts is an old and difficult issue. By analogy with measures of human understanding, two main methods are commonly used: comprehension questions (Richardson et al., 2013; Rajpurkar et al., 2016) and cloze tests. The use of cloze tests to assess the performance of comprehension systems has for instance been proposed by Hill et al. (2016), which studies the ability of various neural models to fill in blanks in sentences from children’s stories, introducing the CBT (Children Book Tests) corpus. This technique has also been used in several previous studies (Bimbot et al., 2001; Zweig and Burges, 2012) to evaluate language models: the former reference establishes a direct link between perplexity and filling-the-gap tests; the latter introduces the Sentence Completion Challenge in which gaps and distractors are carefully selected. One of the challenges of recent work on this issue is the development of realistic test sets, which can only be answered by a deep understanding of the text (Paperno et al., 2016; Xie et al., 2018), using for instance information regarding words partof-speech or syntactic role of the deleted word. 1 See however the work of Jia and Liang (2017) or Kaushik and Lipton"
wisniewski-etal-2014-corpus,J11-4002,0,\N,Missing
wisniewski-etal-2014-corpus,P11-1022,0,\N,Missing
wisniewski-etal-2014-corpus,C08-1141,0,\N,Missing
wisniewski-etal-2014-corpus,vilar-etal-2006-error,0,\N,Missing
