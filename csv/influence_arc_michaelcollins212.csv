2020.emnlp-main.391,W13-3520,0,0.0754815,"Missing"
2020.emnlp-main.391,C04-1080,0,0.257443,"universal POS tags. We rely on off-the-shelf taggers to tag the source text prior to projecting the annotations as described next. POS Projection using Token and Type Constraints. To project the POS tags from the source to the target language, we use token and type constraints based on the mapping induced by the wordlevel alignments. The idea of using both token and type constraints was first introduced by T¨ackstr¨om et al. (2013). Type constraints define the set of POS tags a word type can receive. In a semisupervised leaning setup, type constraints can be obtained from an annotated corpus (Banko and Moore, 2004) or from a resource that serves as a POS lookup such as the Wiktionary 3 (Li et al., 2012; T¨ackstr¨om et al., 2013). For the extraction of type constraints in an unsupervised fashion, we follow the approach of (Buys and Botha, 2016), where we define a tag distribution for each word type on the target side by accumulating the counts of the different POS tags of the source-side tokens that align with the target-side tokens of that word type. The POS tags whose probability is equal to or greater than some threshold β constitute the type constraints of the underlying word type. As token constrain"
2020.emnlp-main.391,A00-1031,0,0.718325,"Missing"
2020.emnlp-main.391,J92-4003,0,0.540298,"ns into one or more high-resource Our first contribution is a robust approach for selecting training instances via cross-lingual annotation projection that exploits and expands all these best practices: coupling type and token constraints obtained in an unsupervised way, wordalignment confidence together with the density of the projected POS, and (optionally) multi-source projection (Sub-section 2.1). Our second contribution is a BiLSTM (Hochreiter and Schmidhuber, 1997) neural architecture that uses pre-trained contextualized word embeddings, affix embeddings and hierarchical Brown clusters (Brown et al., 1992). As contextualized embeddings, we show gains by exploiting the mul4820 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4820–4831, c November 16–20, 2020. 2020 Association for Computational Linguistics tilingual XML-R model (Conneau et al., 2019), while affix embeddings are particularly useful for morphologically-rich languages, and word clusters have been shown to be useful for non-neural POS tagging (Kupiec, 1992; T¨ackstr¨om et al., 2013; Owoputi et al., 2012). Moreover, in addition to the single-source setups, we propose an approach that utiliz"
2020.emnlp-main.391,P16-1184,0,0.231995,"tion on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Buys and Botha, 2016), word-alignment confidence (Duong et al"
2020.emnlp-main.391,P19-4007,0,0.0617345,"Missing"
2020.emnlp-main.391,D17-1078,0,0.0146932,"ng out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of the multilingual XLM-RoBERTa mo"
2020.emnlp-main.391,P11-1061,0,0.0190467,", affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al."
2020.emnlp-main.391,N19-1423,0,0.00647488,"eural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of the multilingual XLM-RoBERTa model, XLM-R (Conneau et al., 2019) 5 XLM-R is a transformer-based multilingual masked language model that is pre-trained on texts of 100 languages, and its performance is competitive with strong monolingual models when tested on a variety of NLP tasks. It also shows better performance than multilingual BERT, mBERT (Devlin et al., 2019), particularly for low-resource languages. We use the average of the embedding vectors of the first and last sub-tokens of each word to represent its pre-trained embeddings. It is worth noting that when using our architecture for a target language that is not present in the XLM-R model, one can consider training a custom XLM transformer-based model 6 given the availability of monolingual data and suitable computational resources, and thus our architecture is not limited to the languages available in the XLM-R model. The randomly initialized embeddings are learned as part of training the model."
2020.emnlp-main.391,P13-2112,0,0.11992,"hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Buys and Bot"
2020.emnlp-main.391,K16-1018,0,0.0350024,"Missing"
2020.emnlp-main.391,I05-1075,0,0.0804695,"tualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 20"
2020.emnlp-main.391,W19-1425,0,0.0326517,"Missing"
2020.emnlp-main.391,L18-1293,0,0.0468848,"Missing"
2020.emnlp-main.391,D12-1127,0,0.0407781,"Missing"
2020.emnlp-main.391,P16-1101,0,0.0378867,") (dS +aS ) Filtering out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of t"
2020.emnlp-main.391,D17-1036,0,0.0285607,"Missing"
2020.emnlp-main.391,mayer-cysouw-2014-creating,0,0.0294466,"2013), multi-source projection (Agi´c et al., 2016) and coverage (percentage of tokens covered by multisource projection) (Plank and Agi´c, 2018) have shown to lead to training instances of better quality. However, only one or two of these have been usually employed. Introduction Majority of world’s languages do not have annotated datasets even for the most simple NLP tasks such as part-of-speech (POS) tagging. However, efforts in documenting low-resource languages often contain translations, usually of religious text, into other high-resource languages. One such parallel corpus is the Bible (Mayer and Cysouw, 2014): 484 languages have a complete Bible translation, while 2551 have a part of the Bible translated. Our goal is to learn POS taggers for a diverse set of target languages in a truly low-resource scenario, where only a limited and possibly out-of-domain set of translations into one or more high-resource Our first contribution is a robust approach for selecting training instances via cross-lingual annotation projection that exploits and expands all these best practices: coupling type and token constraints obtained in an unsupervised way, wordalignment confidence together with the density of the p"
2020.emnlp-main.391,J03-1002,0,0.0206054,"o induce a neural POS tagger for a target language of interest without any direct supervision. Instead, we rely on parallel translations between the target and one or more source languages for which POS taggers are accessible. This section describes our approach: 1) cross-lingual annotation projection via word alignments to prepare the training instances of the target language, and 2) neural POS tagging for the target language. 2.1 Cross-Lingual Projection via Word Alignments Given sentence-aligned parallel data, we align the text of the source and target sides at the word level using GIZA++ (Och and Ney, 2003), while sentences of more than 80 tokens are eliminated. We construct bidirectional word alignments, by only considering the intersecting source-to-target and target-to-source alignments, and exclude the alignment points where the average of the alignment probabilities in the two directions is below some threshold α. Tagging of Source Languages. Since crosslingual projection requires a common POS tagset for all languages, we use the universal POS tagset of the Universal Dependencies (UD) project 2 , which consists of 17 universal POS tags. We rely on off-the-shelf taggers to tag the source tex"
2020.emnlp-main.391,pasha-etal-2014-madamira,1,0.816624,"Missing"
2020.emnlp-main.391,P16-2067,0,0.0217121,"core(S) = 2×(dS ×aS ) (dS +aS ) Filtering out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use t"
2020.emnlp-main.391,2020.acl-demos.14,0,0.0288331,"solate), Bulgarian (IE, Slavic), Finnish (Uralic, Finnic), Hindi (IE, Hindi), Indonesian (Austronesian, MalayoSumbawan), Lithuanian (IE, Baltic), Persian (IE, Iranian), Portuguese (IE, Romance), Telugu (Dravidian, South Central) and Turkish (Turkic, Southwestern). We use the multilingual parallel Bible corpus 10 (Christodouloupoulos and Steedman, 2015) as the source of our parallel data, where we perform the alignment on the verse and word levels. The Bible text is available in full for our source and target languages except Basque, where only the new testament is available. We use Stanza 11 (Qi et al., 2020) to tag the source-side text of the source languages except for Arabic, for which we apply MADAMIRA (Pasha et al., 2014) for performance gain. However, since MADAMIRA was trained on PTB tags and was not designed to follow the UD guidelines, we mapped the Arabic PTB tags into their UD cognates and manually corrected the analyses of the most frequent 2,500 Arabic POS and lemma pairs by selecting the most likely analysis for each. We evaluate our models in terms of POS accuracy on the test sets of the Universal Dependencies, UD v2.5 (Zeman et al., 2019) 12 . We also report our results on older ve"
2020.emnlp-main.391,W96-0213,0,0.77217,". Coupling both the randomly initialized embeddings and the pre-trained ones is essential when the domain of the training data is different from the one of the pre-trained embeddings, which is the case in our learning setup, where we use the Bible data for training, while the XLM-R model is trained on text from Wikipedia 7 and a CommonCrawl corpus (See Conneau et al. (2019) for more details). In addition to word embeddings, we use randomly initialized prefix and suffix n-gram character embeddings, where n is in {1, 2, 3, 4}, as the use of affix information has proved effective in POS tagging (Ratnaparkhi, 1996; Martins and Kreutzer, 4 We also experimented with BiLSTM+CRF, but the CRF layer did not improve the model, which is in line with previous research (Yang et al., 2018; Plank and Agi´c, 2018). 5 We get better results when using the XLM-R embeddings as features as opposed to performing fine tuning, where the latter is more suitable to sentence-level predictions. 6 https://github.com/facebookresearch/XLM 7 https://wikipedia.org 4822 POSn POS3 POS2 POS1 Custom Softmax Activation BiLSTM Encoding Layer h1 h2 h3 h1 h2 h3 h1 h2 h3 hn Backward Layer hn Forward Layer hn Word Clusters 1 Word Clusters 2"
2020.emnlp-main.391,W17-2213,0,0.0487393,"Missing"
2020.emnlp-main.391,Q13-1001,0,0.0605263,"Missing"
2020.emnlp-main.391,petrov-etal-2012-universal,0,0.135086,"Missing"
2020.emnlp-main.391,C18-1327,0,0.0124817,"ined embeddings, which is the case in our learning setup, where we use the Bible data for training, while the XLM-R model is trained on text from Wikipedia 7 and a CommonCrawl corpus (See Conneau et al. (2019) for more details). In addition to word embeddings, we use randomly initialized prefix and suffix n-gram character embeddings, where n is in {1, 2, 3, 4}, as the use of affix information has proved effective in POS tagging (Ratnaparkhi, 1996; Martins and Kreutzer, 4 We also experimented with BiLSTM+CRF, but the CRF layer did not improve the model, which is in line with previous research (Yang et al., 2018; Plank and Agi´c, 2018). 5 We get better results when using the XLM-R embeddings as features as opposed to performing fine tuning, where the latter is more suitable to sentence-level predictions. 6 https://github.com/facebookresearch/XLM 7 https://wikipedia.org 4822 POSn POS3 POS2 POS1 Custom Softmax Activation BiLSTM Encoding Layer h1 h2 h3 h1 h2 h3 h1 h2 h3 hn Backward Layer hn Forward Layer hn Word Clusters 1 Word Clusters 2 Word Clusters 3 Word Clusters n Affix Embeddings 1 Affix Embeddings 2 Affix Embeddings 3 Affix Embeddings n RI Word Embeddings 1 RI Word Embeddings 2 RI Word Embedding"
2020.emnlp-main.391,P19-1493,0,0.0149119,"mbination, further improves the tagging accuracy for most target languages. Our experiments, using limited and outof-domain parallel data, demonstrate significant improvements over previous work (both unsupervised and semi-supervised), even when comparing our single-source setups to other multi-source ones. We also investigate how much gold data is needed to develop supervised taggers comparable to our best unsupervised models. In addition, we show that cross-lingual annotation projection generalizes across languages of different typologies better than the zero-shot model-transfer approach by Pires et al. (2019). Finally, our tagging scripts and models are made publicly available 1 . 2 Approach Our goal is to induce a neural POS tagger for a target language of interest without any direct supervision. Instead, we rely on parallel translations between the target and one or more source languages for which POS taggers are accessible. This section describes our approach: 1) cross-lingual annotation projection via word alignments to prepare the training instances of the target language, and 2) neural POS tagging for the target language. 2.1 Cross-Lingual Projection via Word Alignments Given sentence-aligne"
2020.emnlp-main.391,H01-1035,0,0.373127,"ecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constra"
2020.emnlp-main.391,D18-1061,0,0.0281103,"Missing"
2020.emnlp-main.391,K17-3001,0,0.0587746,"Missing"
2020.tacl-1.30,W07-0809,0,0.124841,"Missing"
2020.tacl-1.30,N19-1423,0,0.138918,"Missing"
2020.tacl-1.30,P17-1171,0,0.0936515,"Missing"
2020.tacl-1.30,D18-1241,1,0.907474,"Missing"
2020.tacl-1.30,N19-1300,1,0.881003,"Missing"
2020.tacl-1.30,N19-1246,0,0.0748214,"Missing"
2020.tacl-1.30,D18-1269,0,0.126714,"(Kwiatkowski et al., 2019) and QuAC. Similarly, Lee et al. (2019) found that question answering datasets in which questions were written while annotators saw the 7 Or other roughly paragraph-like HTML element. Compare these information-seeking questions with carefully crafted reading comprehension or trivia questions that should have an unambiguous answer. There, expert question askers have a different purpose: to validate the knowledge of the potentially expert question answerer. 8 One approach to creating multilingual data is to translate an English corpus into other languages, as in XNLI (Conneau et al., 2018). However, the process of translation—including human translation—tends to introduce problematic artifacts to the output language such as preserving source-language word order as when translating from English to Czech (which allows flexible word order) or the use of more constrained language by translators (e.g., more formal). The result is that a corpus of so-called Translationese may be markedly different from purely native text (Lembersky et al., 2012; Volansky et al., 2013; Avner et al., 2014; Eetemadi and Toutanova, 2014; Rabinovich and Wintner, 2015; Wintner, 2016). Questions that origin"
2020.tacl-1.30,D14-1018,0,0.0360698,"Missing"
2020.tacl-1.30,Q19-1026,1,0.905969,"Missing"
2020.tacl-1.30,L18-1440,0,0.0399555,"ertain property such as location. Prior to these, several non-English multilingual question answering datasets have appeared, typically including one or two languages: These include DuReader (He et al., 2017) and DRCD (Shao et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al., 2018), Korean translations of SQuAD (Lee et al., 2018; Lim et al., 2019), a semi-automatic Italian translation of SQuAD (Croce et al., 2018), ARCD—an Arabic reading comprehension dataset (Mozannar et al., 2019), a Hindi-English parallel dataset in a SQuAD-like setting (Gupta et al., 2018), and a Chinese–English dataset focused on visual QA (Gao et al., 2015). The recent MLQA and XQuAD datasets also translate SQuAD in several languages (see Section 3.2). With the exception of DuReader, these sets also come with the same lexical overlap caveats as SQuAD. Outside of QA, XNLI (Conneau et al., 2018) has gained popularity for natural language understanding. However, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) can be modeled surprisingly well while ignoring the presumably critical premise (Poliak et al., 2018). While NLI stress tests have been created to mitigate thes"
2020.tacl-1.30,D17-1082,0,0.0674368,"Missing"
2020.tacl-1.30,P19-1612,0,0.026374,"esulting question-answer pairs avoid many typical artifacts of QA data creation such as high lexical overlap, which can be exploited by machine learning systems to artificially inflate task performance.8 We see this difference borne out in the leaderboards of datasets in each category: datasets where question writers saw the answer are mostly solved—for example, SQuAD (Rajpurkar et al., 2016, 2018) and CoQA (Reddy et al., 2019); datasets whose question writers did not see the answer text remain largely unsolved—for example, the Natural Questions (Kwiatkowski et al., 2019) and QuAC. Similarly, Lee et al. (2019) found that question answering datasets in which questions were written while annotators saw the 7 Or other roughly paragraph-like HTML element. Compare these information-seeking questions with carefully crafted reading comprehension or trivia questions that should have an unambiguous answer. There, expert question askers have a different purpose: to validate the knowledge of the potentially expert question answerer. 8 One approach to creating multilingual data is to translate an English corpus into other languages, as in XNLI (Conneau et al., 2018). However, the process of translation—includi"
2020.tacl-1.30,L18-1437,0,0.021435,"ements phrased syntactically as questions (Did you know that is the largest stringray?) are given as prompts to retrieve a noun phrase from an article. Kenter et al. (2018) locate a span in a document that provides information on a certain property such as location. Prior to these, several non-English multilingual question answering datasets have appeared, typically including one or two languages: These include DuReader (He et al., 2017) and DRCD (Shao et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al., 2018), Korean translations of SQuAD (Lee et al., 2018; Lim et al., 2019), a semi-automatic Italian translation of SQuAD (Croce et al., 2018), ARCD—an Arabic reading comprehension dataset (Mozannar et al., 2019), a Hindi-English parallel dataset in a SQuAD-like setting (Gupta et al., 2018), and a Chinese–English dataset focused on visual QA (Gao et al., 2015). The recent MLQA and XQuAD datasets also translate SQuAD in several languages (see Section 3.2). With the exception of DuReader, these sets also come with the same lexical overlap caveats as SQuAD. Outside of QA, XNLI (Conneau et al., 2018) has gained popularity for natural language understa"
2020.tacl-1.30,P17-1147,1,0.877645,"Missing"
2020.tacl-1.30,J12-4004,0,0.019813,"lly expert question answerer. 8 One approach to creating multilingual data is to translate an English corpus into other languages, as in XNLI (Conneau et al., 2018). However, the process of translation—including human translation—tends to introduce problematic artifacts to the output language such as preserving source-language word order as when translating from English to Czech (which allows flexible word order) or the use of more constrained language by translators (e.g., more formal). The result is that a corpus of so-called Translationese may be markedly different from purely native text (Lembersky et al., 2012; Volansky et al., 2013; Avner et al., 2014; Eetemadi and Toutanova, 2014; Rabinovich and Wintner, 2015; Wintner, 2016). Questions that originate in a different language may also differ in what is left underspecified or in what topics will be discussed. For example, in TYDI QA, one Bengali question asks What does sapodilla taste like?, referring to a fruit that is unlikely to be mentioned in an English corpus, presenting unique challenges for transfer learning. Each of these issues makes a translated corpus more English-like, potentially inflating the apparent gains of transfer-learning approa"
2020.tacl-1.30,P19-1227,0,0.0356955,"priming them to use similar words. 456 while MLQA’s use of a model-in-the-middle to match English answers to target language answers comes with some risks: (1) of selecting answers containing machine-translated Wikipedia content; and (2) of the dataset favoring models that are trained on the same parallel data or that use a similar multilingual model architecture. and minimal answers to ensure that they are acceptable.12 4 Related Work In addition to the various datasets discussed throughout Section 3, multilingual QA data has also been generated for very different tasks. For example, in XQA (Liu et al., 2019a) and XCMRC (Liu et al., 2019b), statements phrased syntactically as questions (Did you know that is the largest stringray?) are given as prompts to retrieve a noun phrase from an article. Kenter et al. (2018) locate a span in a document that provides information on a certain property such as location. Prior to these, several non-English multilingual question answering datasets have appeared, typically including one or two languages: These include DuReader (He et al., 2017) and DRCD (Shao et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al.,"
2020.tacl-1.30,D19-1169,0,0.0305626,"priming them to use similar words. 456 while MLQA’s use of a model-in-the-middle to match English answers to target language answers comes with some risks: (1) of selecting answers containing machine-translated Wikipedia content; and (2) of the dataset favoring models that are trained on the same parallel data or that use a similar multilingual model architecture. and minimal answers to ensure that they are acceptable.12 4 Related Work In addition to the various datasets discussed throughout Section 3, multilingual QA data has also been generated for very different tasks. For example, in XQA (Liu et al., 2019a) and XCMRC (Liu et al., 2019b), statements phrased syntactically as questions (Did you know that is the largest stringray?) are given as prompts to retrieve a noun phrase from an article. Kenter et al. (2018) locate a span in a document that provides information on a certain property such as location. Prior to these, several non-English multilingual question answering datasets have appeared, typically including one or two languages: These include DuReader (He et al., 2017) and DRCD (Shao et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al.,"
2020.tacl-1.30,Q15-1030,0,0.0193857,"lish corpus into other languages, as in XNLI (Conneau et al., 2018). However, the process of translation—including human translation—tends to introduce problematic artifacts to the output language such as preserving source-language word order as when translating from English to Czech (which allows flexible word order) or the use of more constrained language by translators (e.g., more formal). The result is that a corpus of so-called Translationese may be markedly different from purely native text (Lembersky et al., 2012; Volansky et al., 2013; Avner et al., 2014; Eetemadi and Toutanova, 2014; Rabinovich and Wintner, 2015; Wintner, 2016). Questions that originate in a different language may also differ in what is left underspecified or in what topics will be discussed. For example, in TYDI QA, one Bengali question asks What does sapodilla taste like?, referring to a fruit that is unlikely to be mentioned in an English corpus, presenting unique challenges for transfer learning. Each of these issues makes a translated corpus more English-like, potentially inflating the apparent gains of transfer-learning approaches. Two recent multilingual QA datasets have used this approach. MLQA (Lewis et al., 2019) includes 1"
2020.tacl-1.30,P18-2124,0,0.0453504,"r a Boolean answer, the annotator selects either YES or NO. If no such minimal answer is possible, then the annotators indicate this. answer text tend to be easily defeated by TF-IDF approaches that rely mostly on lexical overlap whereas datasets where question-writers did not know the answer benefited from more powerful models. Put another way, artificially easy datasets may favor overly simplistic models. Unseen answers provide a natural mechanism for creating questions that are not answered by the text since many retrieved articles indeed do not contain an appropriate answer. In SQuAD 2.0 (Rajpurkar et al., 2018), unanswerable questions were artificially constructed. 3.2 Why Not Translate? 3.1 The Importance of Unseen Answers Our question writers seek information on a topic that they find interesting yet somewhat unfamiliar. When questions are formed without knowledge of the answer, the questions tend to contain (a) underspecification of questions, such as What is sugar made from?—Did the asker intend a chemical formula or the plants it is derived from?—and (b) mismatches of the lexical choice and morphosyntax between the question and answer since the question writers are not cognitively primed to use"
2020.tacl-1.30,W19-4612,0,0.215279,"Missing"
2020.tacl-1.30,D16-1264,0,0.215591,"om?—and (b) mismatches of the lexical choice and morphosyntax between the question and answer since the question writers are not cognitively primed to use the same words and grammatical constructions as some unseen answer. The resulting question-answer pairs avoid many typical artifacts of QA data creation such as high lexical overlap, which can be exploited by machine learning systems to artificially inflate task performance.8 We see this difference borne out in the leaderboards of datasets in each category: datasets where question writers saw the answer are mostly solved—for example, SQuAD (Rajpurkar et al., 2016, 2018) and CoQA (Reddy et al., 2019); datasets whose question writers did not see the answer text remain largely unsolved—for example, the Natural Questions (Kwiatkowski et al., 2019) and QuAC. Similarly, Lee et al. (2019) found that question answering datasets in which questions were written while annotators saw the 7 Or other roughly paragraph-like HTML element. Compare these information-seeking questions with carefully crafted reading comprehension or trivia questions that should have an unambiguous answer. There, expert question askers have a different purpose: to validate the knowledge o"
2020.tacl-1.30,C18-1198,0,0.0703647,"Missing"
2020.tacl-1.30,Q19-1016,0,0.061721,"Missing"
2020.tacl-1.30,L18-1431,0,0.0364831,", multilingual QA data has also been generated for very different tasks. For example, in XQA (Liu et al., 2019a) and XCMRC (Liu et al., 2019b), statements phrased syntactically as questions (Did you know that is the largest stringray?) are given as prompts to retrieve a noun phrase from an article. Kenter et al. (2018) locate a span in a document that provides information on a certain property such as location. Prior to these, several non-English multilingual question answering datasets have appeared, typically including one or two languages: These include DuReader (He et al., 2017) and DRCD (Shao et al., 2018) in Chinese, French/Japanese evaluation sets for SQuAD created via translation (Asai et al., 2018), Korean translations of SQuAD (Lee et al., 2018; Lim et al., 2019), a semi-automatic Italian translation of SQuAD (Croce et al., 2018), ARCD—an Arabic reading comprehension dataset (Mozannar et al., 2019), a Hindi-English parallel dataset in a SQuAD-like setting (Gupta et al., 2018), and a Chinese–English dataset focused on visual QA (Gao et al., 2015). The recent MLQA and XQuAD datasets also translate SQuAD in several languages (see Section 3.2). With the exception of DuReader, these sets also c"
2020.tacl-1.30,N18-1101,0,0.0986364,"Missing"
2020.tacl-1.30,P17-1184,0,0.0590884,"Missing"
2020.tacl-1.30,C16-3005,0,0.0179249,"es, as in XNLI (Conneau et al., 2018). However, the process of translation—including human translation—tends to introduce problematic artifacts to the output language such as preserving source-language word order as when translating from English to Czech (which allows flexible word order) or the use of more constrained language by translators (e.g., more formal). The result is that a corpus of so-called Translationese may be markedly different from purely native text (Lembersky et al., 2012; Volansky et al., 2013; Avner et al., 2014; Eetemadi and Toutanova, 2014; Rabinovich and Wintner, 2015; Wintner, 2016). Questions that originate in a different language may also differ in what is left underspecified or in what topics will be discussed. For example, in TYDI QA, one Bengali question asks What does sapodilla taste like?, referring to a fruit that is unlikely to be mentioned in an English corpus, presenting unique challenges for transfer learning. Each of these issues makes a translated corpus more English-like, potentially inflating the apparent gains of transfer-learning approaches. Two recent multilingual QA datasets have used this approach. MLQA (Lewis et al., 2019) includes 12k SQuAD-like En"
2020.tacl-1.30,D15-1237,0,0.131285,"Missing"
2020.tacl-1.30,D18-1259,0,0.0658302,"Missing"
2020.tacl-1.30,Q18-1021,0,0.0971909,"Missing"
2020.tacl-1.30,D15-1075,0,\N,Missing
2020.tacl-1.30,D11-1034,0,\N,Missing
2020.tacl-1.30,W17-2623,0,\N,Missing
2021.deelio-1.3,D13-1160,0,0.0649286,"iever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 25–30 Online, June 10, 2021. ©2021 Association for Computational"
2021.deelio-1.3,D14-1067,0,0.0270901,"nto text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration f"
2021.deelio-1.3,P17-1171,0,0.0187296,"aseline setting. Variants of PPR do not improve over the text only baseline. BM25 PPR(Q) QI PPR(Q) WS PPR(Q) Shortest Path Fact R Ans R 19.1 33.0 31.2 51.0 29.8 28.8 25.2 40.0 DecAtt + DocReader BERTjoint BERTjoint * Traditional PPR QI PPR WS PPR Table 2: Answer Recall and Shortest Path Fact Recall metrics for the different Retrieval Methods. Traditional and QI PPR methods have very low recall and WS PPR method improves the recall significantly. Short F1 54.8 64.7 68.1 66.7 65.8 67.5 31.4 52.7 54.0 54.3 53.6 54.4 Table 3: Results on Full NQ. Baselines: DecAtt (Parikh et al., 2016), DocReader (Chen et al., 2017), and BertJoint (Alberti et al., 2019). *- our reimplementation. WS PPR improves over previous baseline on Short F1 and has comparable performance to BertJoint on Long F1. QA model in place of the KB retrieved facts. As the oracle setting uses gold KB links, this setting is tested on the FQ subset where such links exist and is called the Clean Oracle. To establish a harder upper bound setting, random facts about the question are added in addition to the oracle shortest path facts using PPR, forming a Noisy Oracle setting. 3.2 Long F1 contain the answer, we simply set the answer span to be the"
2021.deelio-1.3,N19-1423,0,0.0609056,"Missing"
2021.deelio-1.3,D19-1242,1,0.891701,"time. As a consequence injecting retrieved KB paths in a realistic QA setting like NQ yields only small, inconsistent improvements. To summarize our contributions, we (1) identify a new experimental subset of NQ that supports (2) the study of effectiveness of KB path-retrieval approaches. We also (3) describe a simple, modelagnostic method to using oracle KB paths that can significantly improve QA performance and evaluate PPR based path-retrieval methods. To our Existing work shows the benefits of integrating KBs with textual evidence for QA only on questions that are answerable by KBs alone (Sun et al., 2019). In contrast, real world QA systems often have to deal with questions that might not be directly answerable by KBs. Here, we investigate the effect of integrating background knowledge from KBs for the Natural Questions (NQ) task. We create a subset of the NQ data, Factual Questions (FQ), where the questions have evidence in the KB in the form of paths that link question entities to answer entities but still must be answered using text, to facilitate further research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based"
2021.deelio-1.3,D18-1455,1,0.870826,"rther research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work don"
2021.deelio-1.3,Q19-1026,1,0.838535,"ntities (nodes) w.r.t seed entities. Sun et al. (2018) present an improved PPR version, Question Informed (QI) PPR, to weigh relations which are semantically closer to the question higher. Specifically, they average the GLOVE (Pennington et al., 2014) embeddings to compute a relation vector v(R) from the relation surface form, and a question vector v(Q) from the question text, and use cosine similarity between them as edgeweights for PPR. For every node, the γ probability is multiplied by the edge-score to weigh entities along relevant paths higher. Dataset The Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) is a large scale QA dataset containing 307,373 training, 7,830 dev, and 7,842 test examples. Each example is a user query paired with Wikipedia documents annotated with a passage (long answer) answering the question and one or more short spans (short answer) containing the answer. The questions in NQ are not artificially constructed, making the NQ task more difficult (Lee et al., 2019). We use Sling (Ringgaard et al., 2017) (which uses an NP chunker and phrase table for linking entities to Wikidata) to entity link the questions and documents. To focus on knowledge-driven factoid question answ"
2021.deelio-1.3,P19-1219,0,0.0169232,"o KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 2"
2021.deelio-1.3,2020.deelio-1.5,0,0.0402008,"Missing"
2021.deelio-1.3,P19-1612,0,0.0165772,"milarity between them as edgeweights for PPR. For every node, the γ probability is multiplied by the edge-score to weigh entities along relevant paths higher. Dataset The Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) is a large scale QA dataset containing 307,373 training, 7,830 dev, and 7,842 test examples. Each example is a user query paired with Wikipedia documents annotated with a passage (long answer) answering the question and one or more short spans (short answer) containing the answer. The questions in NQ are not artificially constructed, making the NQ task more difficult (Lee et al., 2019). We use Sling (Ringgaard et al., 2017) (which uses an NP chunker and phrase table for linking entities to Wikidata) to entity link the questions and documents. To focus on knowledge-driven factoid question answering, we create a subset of NQ having relevant knowledge in the KB. Shortest paths between entities in KB is very often used as a proxy for gold knowledge linking questions to answer (Sun et al., 2019) and we use the same proxy in our setting. Specifically, we select questions whose short answers are entities in the KB and have a short path (up to 3 steps) from a question entity to an"
2021.deelio-1.3,D18-1242,0,0.0197972,"tems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Ar"
2021.deelio-1.3,D16-1244,0,0.103473,"Missing"
2021.deelio-1.3,D14-1162,0,0.0842017,"Missing"
A00-1041,W99-0613,1,0.551726,"y classifies capitalized words as intrinsically capitalized or not, where the alternatives to intrinsic capitalization are sentence-initial capitalization or capitalization in titles and headings. The extractor uses various heuristics, including whether the words under consideration appear unambiguously capitalized elsewhere in the document. 2.3 E n t i t y Classification The following types of entities were extracted as potential answers to queries. Person, Location, Organization, Other Proper names were classified into these categories using a classifier built using the method described in (Collins and Singer, 1999). 1 This is the only place where entity classification was actually done as a separate step from entity extraction. D a t e s Four-digit numbers starting with 1 . . . or 2 0 . . were taken to be years. Cass was used to extract more complex date expressions (such as Saturday, January 1st, 2000). Q u a n t i t i e s Quantities include bare numbers and numeric expressions&apos; like The Three Stooges, 4 1//2 quarts, 27~o. The head word of complex numeric expressions was identified (stooges, quarts or percent); these entities could then be later identified as good answers to How many questions such as"
D07-1015,E06-1011,0,0.0733024,"timization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapting the inside-outside algorithm to these models, partition functions and marginals can be computed for second-order projective structures, allowing log-linear and max-margin training to be applied via the framework developed in this paper. For higher-order non-projective parsing, however, computational complexity"
D07-1015,W07-2216,0,0.703877,"to a large-scale problem. We again show improved performance over the perceptron. The goal of our experiments is to give a rigorous comparative study of the marginal-based training algorithms and a highly-competitive baseline, the averaged perceptron, using the same feature sets for all approaches. We stress, however, that the purpose of this work is not to give competitive performance on the CoNLL data sets; this would require further engineering of the approach. Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneously by Smith and Smith (2007) and McDonald and Satta (2007); see Section 5 for more discussion. 2 2.1 Background Discriminative Dependency Parsing Dependency parsing is the task of mapping a sentence x to a dependency structure y. Given a sentence x with n words, a dependency for that sentence is a tuple (h, m) where h ∈ [0 . . . n] is the index of the head word in the sentence, and m ∈ [1 . . . n] is the index of a modifier word. The value h = 0 is a special root-symbol that may only appear as the head of a dependency. We use D(x) to refer to all possible dependencies for a sentence x: D(x) = {(h, m) : h ∈ [0 . . . n], m ∈ [1 . . . n]}. A dependency"
D07-1015,P05-1012,0,0.106724,"earl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a wellknown result in graph theory: Kirchhoff’s MatrixTree Theorem (Tutte, 1984). A na¨ıve application of the theorem yields O(n4 ) and O(n6 ) algorithms for computation of the partition func"
D07-1015,H05-1066,0,0.175733,"earl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a wellknown result in graph theory: Kirchhoff’s MatrixTree Theorem (Tutte, 1984). A na¨ıve application of the theorem yields O(n4 ) and O(n6 ) algorithms for computation of the partition func"
D07-1015,W06-2920,0,0.341918,"s, Bartlett et al. (2004) have provided a simple training algorithm, based on exponentiated-gradient (EG) updates, that requires computation of marginals and can thus be implemented within our framework. Both of these methods explicitly minimize the loss incurred when parsing the entire training set. This contrasts with the online learning algorithms used in previous work with spanning-tree models (McDonald et al., 2005b). We applied the above two marginal-based training algorithms to six languages with varying degrees of non-projectivity, using datasets obtained from the CoNLL-X shared task (Buchholz and Marsi, 2006). Our experimental framework compared three training approaches: log-linear models, max-margin models, and the averaged perceptron. Each of these was applied to both projective and non-projective parsing. Our results demonstrate that marginal-based training yields models which out141 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 141–150, Prague, June 2007. 2007 Association for Computational Linguistics Projective perform those trained using the averaged perceptron. In summary, the contributions of"
D07-1015,W06-2932,0,0.0181463,"raining-setting combination. The 3 Our algorithms also support labeled parsing (see Section 3.4). Initial experiments with labeled models showed the same trend that we report here for unlabeled parsing, so for simplicity we conducted extensive experiments only for unlabeled parsing. 4 The transformations were performed by running the projective parser with score +1 on correct dependencies and -1 otherwise: the resulting trees are guaranteed to be projective and to have a minimum loss with respect to the correct tree. Note that only the training sets were transformed. 5 It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features. Ara Dut Jap Slo Spa Tur Perceptron p np 71.74 71.84 77.17 78.83 91.90 91.78 78.02 78.66 81.19 80.02 71.22 71.70 Max-Margin p np 71.74 72.99 76.53 79.69 92.10 92.18 79.78 80.10 81.71 81.93 72.83 72.02 Log-Linear p np 73.11 73.67 76.23 79.55 91.68 91.49 78.24 79.66 81.75 81.57 72.26 72.62 Table 2: Test data results. The p and np columns show results with projective and non-projective training respectively. P E L Ara 71.74 72.99 73.67 Dut 78.83 79.69 79.55 Jap 91.78 92.18 91.49 Slo 78.66 80.10 79.66 Spa 81.19 81.93 81.57 Tur 71.70 72"
D07-1015,D07-1101,1,0.770405,"s work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapting the inside-outside algorithm to these models, partition functions and marginals can be computed for second-order projective structures, allowing log-linear and max-margin training to be applied via the framework developed in this paper. For higher-order non-projective parsing, however, computational complexity results (McDonald and Pereira, 2006; McDonald and Satta, 2007) indicate that exact solutions to the three inference problems of"
D07-1015,W04-2407,0,0.0142665,"les correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projective parsing. By adapti"
D07-1015,W02-1001,1,0.191066,"s that we report are for unlabeled dependency parsing.3 The non-projective models were trained on the CoNLL-X data in its original form. Since the projective models assume that the dependencies in the data are non-crossing, we created a second training set for each language where non-projective dependency structures were automatically transformed into projective structures. All projective models were trained on these new training sets.4 Our feature space is based on that of McDonald et al. (2005a).5 6.2 Results We performed experiments using three training algorithms: the averaged perceptron (Collins, 2002), log-linear training (via conjugate gradient descent), and max-margin training (via the EG algorithm). Each of these algorithms was trained using projective and non-projective methods, yielding six training settings per language. The different training algorithms have various meta-parameters, which we optimized on the validation set for each language/training-setting combination. The 3 Our algorithms also support labeled parsing (see Section 3.4). Initial experiments with labeled models showed the same trend that we report here for unlabeled parsing, so for simplicity we conducted extensive e"
D07-1015,dzeroski-etal-2006-towards,0,0.0167912,"Missing"
D07-1015,C96-1058,0,0.181095,"cy structure y ∗ (x; w) (see Eq. 1) can be computed. In this paper the motivation for solving Problems 2 and 3 arises from training algorithms for discriminative models. As we will describe in Section 4, both log-linear and max-margin models can be trained via methods that make direct use of algorithms for Problems 2 and 3. In the case of projective dependency structures (i.e., T (x) defined as Tps (x) or Tpm (x)), there are well-known algorithms for all three inference problems. Decoding can be carried out using Viterbistyle dynamic-programming algorithms, for example the O(n3 ) algorithm of Eisner (1996). Computation of the marginals and partition function can also be achieved in O(n3 ) time, using a variant of the inside-outside algorithm (Baker, 1979) applied to the Eisner (1996) data structures (Paskin, 2001). In the non-projective case (i.e., T (x) defined as s (x) or T m (x)), McDonald et al. (2005b) deTnp np scribe how the CLE algorithm (Chu and Liu, 1965; Edmonds, 1967) can be used for decoding. However, it is not possible to compute the marginals and partition function using the inside-outside algorithm. We next describe a method for computing these quantities in O(n3 ) time using mat"
D07-1015,P99-1069,0,0.0534465,"s y ∈ Tnp compute the partition function directly: construct a Laplacian matrix L(θ) for G0 and compute the minor L(0,0) (θ). Since this minor is also a determinant, the marginals can be obtained analogously to the single-root case. More concretely, this technique ˆ corresponds to defining the matrix L(θ) as Multiple Roots In the case of multiple roots, we can still compute the partition function and marginals efficiently. In fact, the derivation of this case is simpler than for single-root structures. Create an extended graph G0 145 4.1 Log-Linear Estimation In conditional log-linear models (Johnson et al., 1999; Lafferty et al., 2001), a distribution over parse trees for a sentence x is defined as follows: exp P (y |x; w) = nP o (h,m)∈y w · f (x, h, m) Z(x; w) (7) where Z(x; w) is the partition function, a sum over s (x), T m (x) or T m (x). Tps (x), Tnp p np We train the model using the approach described by Sha and Pereira (2003). Assume that we have a training set {(xi , yi )}N i=1 . The optimal parameters are taken to be w∗ = argminw L(w) where L(w) = −C convex function L(w). Let the margin for parse tree y on the i’th training example be defined as N X 1 log P (yi |xi ; w) + ||w||2 2 i=1 mi,y ("
D07-1015,P01-1042,0,0.0304567,"| . If obj has decreased 2 compared to last iteration, set η = η2 . Output: Parameter values w. Figure 2: The EG Algorithm for Max-Margin Estimation. The learning rate η is halved each time the dual objective function (see (Bartlett et al., 2004)) fails to increase. In our experiments we chose β = 9, which was found to work well during development of the algorithm. achieved using the inside-outside algorithm for projective structures, and the algorithms described in Section 3 for non-projective structures. 5 Related Work Global log-linear training has been used in the context of PCFG parsing (Johnson, 2001). Riezler et al. (2004) explore a similar application of log-linear models to LFG parsing. Max-margin learning 147 has been applied to PCFG parsing by Taskar et al. (2004b). They show that this problem has a QP dual of polynomial size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsoch"
D07-1015,N04-1013,0,0.0144885,"ecreased 2 compared to last iteration, set η = η2 . Output: Parameter values w. Figure 2: The EG Algorithm for Max-Margin Estimation. The learning rate η is halved each time the dual objective function (see (Bartlett et al., 2004)) fails to increase. In our experiments we chose β = 9, which was found to work well during development of the algorithm. achieved using the inside-outside algorithm for projective structures, and the algorithms described in Section 3 for non-projective structures. 5 Related Work Global log-linear training has been used in the context of PCFG parsing (Johnson, 2001). Riezler et al. (2004) explore a similar application of log-linear models to LFG parsing. Max-margin learning 147 has been applied to PCFG parsing by Taskar et al. (2004b). They show that this problem has a QP dual of polynomial size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004)"
D07-1015,N03-1028,0,0.040522,"In the case of multiple roots, we can still compute the partition function and marginals efficiently. In fact, the derivation of this case is simpler than for single-root structures. Create an extended graph G0 145 4.1 Log-Linear Estimation In conditional log-linear models (Johnson et al., 1999; Lafferty et al., 2001), a distribution over parse trees for a sentence x is defined as follows: exp P (y |x; w) = nP o (h,m)∈y w · f (x, h, m) Z(x; w) (7) where Z(x; w) is the partition function, a sum over s (x), T m (x) or T m (x). Tps (x), Tnp p np We train the model using the approach described by Sha and Pereira (2003). Assume that we have a training set {(xi , yi )}N i=1 . The optimal parameters are taken to be w∗ = argminw L(w) where L(w) = −C convex function L(w). Let the margin for parse tree y on the i’th training example be defined as N X 1 log P (yi |xi ; w) + ||w||2 2 i=1 mi,y (w) = The parameter C &gt; 0 is a constant dictating the level of regularization in the model. Since L(w) is a convex function, gradient descent methods can be used to search for the global minimum. Such methods typically involve repeated computation of the loss L(w) and gradient ∂L(w) ∂w , requiring efficient implementations of"
D07-1015,P05-1044,0,0.0141546,"ents, for example the set of all parse trees for a given sentence. Methods for summing over such structures include the inside-outside algorithm for probabilistic contextfree grammars (Baker, 1979), the forward-backward algorithm for hidden Markov models (Baum et al., 1970), and the belief-propagation algorithm for graphical models (Pearl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed usi"
D07-1015,D07-1014,0,0.706894,"plication of this algorithm to a large-scale problem. We again show improved performance over the perceptron. The goal of our experiments is to give a rigorous comparative study of the marginal-based training algorithms and a highly-competitive baseline, the averaged perceptron, using the same feature sets for all approaches. We stress, however, that the purpose of this work is not to give competitive performance on the CoNLL data sets; this would require further engineering of the approach. Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneously by Smith and Smith (2007) and McDonald and Satta (2007); see Section 5 for more discussion. 2 2.1 Background Discriminative Dependency Parsing Dependency parsing is the task of mapping a sentence x to a dependency structure y. Given a sentence x with n words, a dependency for that sentence is a tuple (h, m) where h ∈ [0 . . . n] is the index of the head word in the sentence, and m ∈ [1 . . . n] is the index of a modifier word. The value h = 0 is a special root-symbol that may only appear as the head of a dependency. We use D(x) to refer to all possible dependencies for a sentence x: D(x) = {(h, m) : h ∈ [0 . . . n], m"
D07-1015,W04-3201,0,0.551986,"probabilistic contextfree grammars (Baker, 1979), the forward-backward algorithm for hidden Markov models (Baum et al., 1970), and the belief-propagation algorithm for graphical models (Pearl, 1988). These algorithms compute marginal probabilities and partition functions, quantities which are central to many methods for the statistical modeling of complex structures (e.g., the EM algorithm (Baker, 1979; Baum et al., 1970), contrastive estimation (Smith and Eisner, 2005), training algorithms for CRFs (Lafferty et al., 2001), and training algorithms for max-margin models (Bartlett et al., 2004; Taskar et al., 2004a)). This paper describes inside-outside-style algorithms for the case of directed spanning trees. These structures are equivalent to non-projective dependency parses (McDonald et al., 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. Unlike the case for projective dependency structures, partition functions and marginals for non-projective trees cannot be computed using dynamic-programming methods such as the insideoutside algorithm. In this paper we describe how these quantities can be computed by adapting a"
D07-1015,W03-3023,0,0.0251568,"size, where the dual variables correspond to marginal probabilities of CFG rules. A similar QP dual may be obtained for max-margin projective dependency parsing. However, for nonprojective parsing, the dual QP would require an exponential number of constraints on the dependency marginals (Chopra, 1989). Nevertheless, alternative optimization methods like that of Tsochantaridis et al. (2004), or the EG method presented here, can still be applied. The majority of previous work on dependency parsing has focused on local (i.e., classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al., 2004; Y. Cheng, 2005). Non-local (i.e., classification of entire trees) training methods were used by McDonald et al. (2005a), who employed online learning. Dependency parsing accuracy can be improved by allowing second-order features, which consider more than one dependency simultaneously. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. Both authors give polytime algorithms for exact projectiv"
D07-1071,C04-1180,0,0.0148097,"ls hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sen"
D07-1071,P98-1025,0,0.0289435,"egrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sentences from childdirected speech, which differ significantly from the natural language interface queries we consider. Finally, there is work on"
D07-1071,J83-3001,0,0.468325,"order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information. Both of these approaches use sentences from childdirected speech, which differ significantly from the natural language interface queries we consider. Finally, there is work on manually developing parsing techniques to improve robustness (Carbonell and Hayes, 1983; Seneff, 1992). In contrast, our approach is integrated into a learning framework. 6 Experiments The main focus of our experiments is on the ATIS travel planning domain. For development, we used 4978 sentences, split into a training set of 4500 examples, and a development set of 478 examples. For test, we used the ATIS NOV93 test set which contains 448 examples. To create the annotations, we created a script that maps the original SQL annotations provided with the data to lambda-calculus expressions. He and Young (2006) previously reported results on the ATIS domain, using a learning approach"
D07-1071,W03-1013,0,0.00950692,"emoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Sisk"
D07-1071,W02-1001,1,0.0960636,", Λ)) as its output. The algorithm is online, in that it visits each example in turn, and updates both w and Λ if necessary. In Step 1 on each example, the input xi is parsed. If it is parsed correctly, the algorithm immediately moves to the next example. In Step 2, the algorithm temporarily introduces all lexical entries seen in GENLEX(xi , zi ), and finds the highest scoring parse that leads to the correct semantics zi . A small subset of GENLEX(xi , zi )—namely, only those lexical entries that are contained in the highest scoring parse—are added to Λ. In Step 3, a simple perceptron update (Collins, 2002) is performed. The hypothesis is parsed again with the new lexicon, and an update to the parameters w is made if the resulting parse does not have the correct logical form. This algorithm differs from the approach in ZC05 in a couple of important respects. First, the ZC05 algorithm performed learning of the lexicon Λ at each iteration in a batch method, requiring a pass over the entire training set. The new algorithm is fully online, learning both Λ and w in an example-by-example fashion. This has important consequences for the efficiency of the algorithm. Second, the parameter estimation meth"
D07-1071,H94-1010,0,0.484357,"at it allows a system to handle quite complex semantic effects, such as coordination or scoping phenomena. In particular, it allows us to leverage the considerable body of work on semantics within these formalisms, for example see Carpenter (1997). However, a grammar based on a formalism such as CCG can be somewhat rigid, and this can cause problems when a system is faced with spontaneous, unedited natural language input, as is commonly seen in natural language interface applications. For example, consider the sentences shown in figure 1, which were taken from the ATIS travel-planning domain (Dahl et al., 1994). These sentences exhibit characteristics which present significant challenges to the approach of ZC05. For ex678 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 678–687, Prague, June 2007. 2007 Association for Computational Linguistics ample, the sentences have quite flexible word order, and include telegraphic language where some words are effectively omitted. In this paper we describe a learning algorithm that retains the advantages of using a detailed grammar, but is highly effective in dealing"
D07-1071,P06-2034,0,0.0481911,"ns. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabili"
D07-1071,P99-1069,0,0.0363867,"linguistic phenomena. As we will see, they also turn out to be useful when modeling constructions with relaxed word order, as seen frequently in domains such as ATIS. In addition to the application and composition rules, we will also make use of type raising and coordination combinators. A full description of these combinators goes beyond the scope of this paper. Steedman (1996; 2000) presents a detailed description of CCG. 2.3 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows. Our models are similar to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). We will write x to denote a sentence, and y to denote a CCG parse for a sentence. We use GEN(x; Λ) to refer to all possible CCG parses for x under some CCG lexicon Λ. We will define f(x, y) ∈ Rd to be a d-dimensional feature–vector that represents a parse tree y paired with an input sentence x. In principle, f could include features that are sensitive to arbitrary substructures within the pair (x, y). We will define w ∈ Rd to be a parameter vector. The optimal parse for a sentence x under parameters w and lexicon Λ is then defined a"
D07-1071,P06-1115,0,0.736567,", would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to"
D07-1071,P06-1096,0,0.0126236,"Missing"
D07-1071,P96-1008,0,0.0448016,"emantic representations. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm t"
D07-1071,P06-2080,0,0.0657208,"Missing"
D07-1071,W00-1317,0,0.0402486,"april twenty second dallas to washington the latest nighttime departure one way argmax(λx.f light(x) ∧ f rom(x, dallas)∧ to(x, washington) ∧ month(x, april)∧ day number(x, 22) ∧ during(x, night)∧ one way(x), λy.depart time(y)) Figure 1: Three sentences from the ATIS domain. other derivations. The output from the learning algorithm is a combinatory categorial grammar (CCG), together with parameters that define a log-linear distribution over parses under the grammar. Experiments show that the approach gives high accuracy on two database-query problems, introduced by Zelle and Mooney (1996) and Tang and Mooney (2000). Introduction Recent work (Mooney, 2007; He and Young, 2006; Zettlemoyer and Collins, 2005) has developed learning algorithms for the problem of mapping sentences to underlying semantic representations. In one such approach (Zettlemoyer and Collins, 2005) (ZC05), the input to the learning algorithm is a training set consisting of sentences paired with lambda-calculus expressions. For instance, the training data might contain the following example: Sentence: list flights to boston Logical Form: λx.f light(x) ∧ to(x, boston) In this case the lambda-calculus expression denotes the set of all fli"
D07-1071,W04-3201,0,0.0334154,"be useful when modeling constructions with relaxed word order, as seen frequently in domains such as ATIS. In addition to the application and composition rules, we will also make use of type raising and coordination combinators. A full description of these combinators goes beyond the scope of this paper. Steedman (1996; 2000) presents a detailed description of CCG. 2.3 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows. Our models are similar to several other approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). We will write x to denote a sentence, and y to denote a CCG parse for a sentence. We use GEN(x; Λ) to refer to all possible CCG parses for x under some CCG lexicon Λ. We will define f(x, y) ∈ Rd to be a d-dimensional feature–vector that represents a parse tree y paired with an input sentence x. In principle, f could include features that are sensitive to arbitrary substructures within the pair (x, y). We will define w ∈ Rd to be a parameter vector. The optimal parse for a sentence x under parameters w and lexicon Λ is then defined as y ∗ (x) = arg max w · f(x, y) . y∈GEN(x;Λ) 680 Assuming su"
D07-1071,W99-0909,0,0.0931974,"emoyer and Collins (2005) was presented in section 2.4. He and Young (2005) describe an algorithm that learns a probabilistic push-down automaton that models hierarchical dependencies but can still be trained on a data set that does not have full treebank-style annotations. This approach has been integrated with a speech recognizer and shown to be robust to recognition errors (He and Young, 2006). There is also related work in the CCG literature. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexicons that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics but requires fully–specified CCG derivations in the training data. Bozsahin (1998) presents work on using CCG to model languages with free word order. In addition, there is related work that focuses on modeling child language learning. Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations. Villavi"
D07-1071,N06-1056,0,0.726929,"on learning to map sentences to underlying semantic representations. A wide variety 9 Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains. Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned. 684 of techniques have been considered including approaches based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques (Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al., 2005), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyer and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems. The approach of Zettlemoyer and Collins (2005) was presented in section 2.4. He"
D07-1071,C98-1025,0,\N,Missing
D07-1077,C04-1090,0,0.0253863,". (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering"
D07-1077,P06-1077,0,0.0336674,"e, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP LST NP PP PRN QP"
D07-1077,J96-1002,0,0.00714724,"hese parsers are typically of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describe an approach for Fre"
D07-1077,1992.tmi-1.8,0,0.438727,"38 parsers is that these parsers are typically of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describ"
D07-1077,W06-1606,0,0.216949,"at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We report results for the method on t"
D07-1077,J04-2003,0,0.0335531,"cally of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describe an approach for French, where reordering r"
D07-1077,P05-1033,0,0.766039,"ation achieved at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We report result"
D07-1077,P02-1040,0,0.114258,"r training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use BLEU scores as the performance measure in our evaluation (Papineni et al., 2002). Table 2 gives results for the baseline and reordered systems on both the development and test sets. As shown in the table, the reordering method is able to improve the BLEU scores by 1.29 points on the development set, and by 2.34 on the NIST 2006 set. 4.1 Frequency and Accuracy of Reordering Rules We collected statistics to evaluate how often and accurately the reordering rules are applied in the data. The accuracy is measured in terms of the percentage of rule applications that correctly reorder sentences. The vast majority of reordering errors are due to parsing mistakes. Table 3 summariz"
D07-1077,P05-1066,1,0.685642,"Missing"
D07-1077,P05-1034,0,0.120971,"n the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP"
D07-1077,P97-1003,1,0.1369,"be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use BLEU scores as the performance measure in our evaluation (Papineni et al., 2002). Table 2 gives results for the baseline and reordered systems on both the development and test sets. As shown in the table, the reordering method is able to improve the BLEU scores by 1.29 points on the development set, and by 2.34 on"
D07-1077,W02-1001,1,0.0378144,"lit into two sets of roughly equal sizes: a tuning set of 2347 sentences is used for optimizing various parameters using minimum error training (also using the MOSES toolkit), and a development set of 2320 sentences is used for various analysis experiments. We report results on the NIST 2006 evaluation data. A series of processing steps are needed before the reordering rules can be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tok"
D07-1077,N04-1032,0,0.038956,"f 2347 sentences is used for optimizing various parameters using minimum error training (also using the MOSES toolkit), and a development set of 2320 sentences is used for various analysis experiments. We report results on the NIST 2006 evaluation data. A series of processing steps are needed before the reordering rules can be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use"
D07-1077,N04-4026,0,0.436662,"al. (2005) also describe an approach for German, concentrating on reordering German clauses, which have quite different word order from clauses in English. Our approach is most similar to that of Collins et al. (2005). Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding). The MOSES phrase-based system that we use has a relatively simple reordering model which has a fixed penalty for reordering moves in the decoder. More sophisticated models include reordering parameters that are sensitive to lexical information (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). The model of Chiang (2005) employs a synchronous context-free grammar to allow hierarchical approaches to reordering. The syntaxbased models of Yamada and Knight (2001) and Marcu et al. (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada"
D07-1077,P05-1067,0,0.039507,"ild a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DN"
D07-1077,C04-1073,0,0.855106,"362 32 Vassar Street, Room G-484 2 Buccleuch Place, 5BP 2L2 Cambridge, MA 02139, USA Cambridge, MA 02139, USA Edinburgh, EH8 9LW, UK wangc@csail.mit.edu mcollins@csail.mit.edu pkoehn@inf.ed.ac.uk Abstract is then applied to the resulting parse tree, with the goal of transforming the source language sentence into a word order that is closer to that of the target language. The reordering process is used to preprocess both the training and test data used within an existing SMT system. Reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). This paper describes a syntactic reordering approach for translation from Chinese to English. Figure 1 gives an example illustrating some of the differences in word order between the two languages. The example shows a Chinese sentence whose literal translation in English is: Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We descri"
D07-1077,2006.amta-papers.8,0,0.0614087,"ly allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP LST NP PP PRN QP UCP VP adjective phra"
D07-1077,P01-1067,0,0.8385,"ent that the French delegation achieved at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We"
D07-1077,P07-2045,1,0.131262,"fective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules. this is French delegation at Winter Olympics on achieve DEC best accomplishment and where a natural translation would be this is the best accomplishment that the French delegation achieved at the Winter Olympics 1 Introduction Syntactic reordering approaches are an effective method for handling systematic differences in word order between source and target"
D07-1077,H05-1021,0,0.02585,"describe an approach for German, concentrating on reordering German clauses, which have quite different word order from clauses in English. Our approach is most similar to that of Collins et al. (2005). Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding). The MOSES phrase-based system that we use has a relatively simple reordering model which has a fixed penalty for reordering moves in the decoder. More sophisticated models include reordering parameters that are sensitive to lexical information (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). The model of Chiang (2005) employs a synchronous context-free grammar to allow hierarchical approaches to reordering. The syntaxbased models of Yamada and Knight (2001) and Marcu et al. (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and"
D07-1077,2005.iwslt-1.8,1,\N,Missing
D09-1021,W06-1606,0,0.29494,"ouple of respects: first, these criticisms is initially seen to the left of take, but after the adjunction this order is reversed; second, and more unusually, the treelet for seriously has been skipped over, with the result that the German words translated at this point (diese, kritik, and nehmen) form a non-contiguous sequence. More generally, we will allow any two 1 Introduction Syntax-based models for statistical machine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information i"
D09-1021,P96-1023,0,0.0611987,"here wm and sm are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules"
D09-1021,P05-1012,0,0.0634496,"the administration must be able to respond more quickly in future try hes gibt ⇒ there isi a correct alignment would be h(1, 1), (2, 2)i, specifying that there is aligned to es, and is is aligned to gibt (note that in many, but not all, cases ai = bi , i.e., a target language word is aligned to a single source language word). The alignment information in s-phrases will be useful in tying syntactic dependencies created in the target language to positions in the source language string. In particular, we will consider discriminative models (analogous to models for dependency parsing, e.g., see (McDonald et al., 2005)) that estimate the probability of targetlanguage dependencies conditioned on properties of the source-language string. Alignments may be derived in a number of ways; in our method we directly use phrase entries proposed by a phrasebased system. Specifically, for each target word ei in a phrase entry hf1 . . . fn , e1 . . . em i for a training example, we find the smallest5 phrase entry in the same training example that includes ei on the target side, and is a subset of f1 . . . fn on the source side; the word ei is then aligned to the subset of source language words in this “minimal” phrase."
D09-1021,W08-2102,1,0.612555,"s; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information in the target language. As one example, when translating the sentence wir m¨ussen auch diese kritik ernst nehmen from German into English, the following sequence of syntactic phrasal entries might be used (we show each English syntactic fragment above its associated German sub-string): 1 Note that in the above example each English phrase consists of a completely connected syntactic structure; this is not, however, a required constraint, see section 3.2 for discussion. 200 Proceeding"
D09-1021,2003.mtsummit-papers.6,0,0.408892,"Missing"
D09-1021,P01-1017,0,0.357788,"cisms seriously VP must ADVP take ADVP also wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these criticisms and take to be combined to form a new structure with the sub-string take these criticisms. This step in the derivation is neces"
D09-1021,P04-1083,0,0.0266659,"s of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation define"
D09-1021,P08-1023,0,0.0611245,"ous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in the Penn treebank. 201 S es gibt"
D09-1021,2006.amta-papers.15,0,0.018273,"tate (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in th"
D09-1021,P08-1009,0,0.0088223,"t tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in the Penn treeban"
D09-1021,P03-1021,0,0.102403,"ctively. In addition we can define (am , bm ) to be the start and end indices of the words in the foreign string to which the word wm is aligned; this information can be recovered because the s-phrase qm contains alignment information for all target words in the phrase, including wm . Similarly, we can define (ah , bh ) to be alignment information for the head word wh . Finally, we can define ρ to be a binary flag specifying whether or not the adjunction operation involves reordering (in the take criticism example, this flag is set to true, because the order in En7 In practice, MERT training (Och, 2003) will be used to train relative weights for the different model components. 204 i = 1 . . . N from our training data as follows: for each pair of target-language words (wm , wh ) seen in the training data, we can extract associated spines (sm , sh ) from the relevant parse tree, and also extract a label y indicating whether or not a head-modifier dependency is seen between the two words in the parse tree. Given an s-phrase in the training example that includes wm , we can extract alignment information (am , bm ) from the sphrase; we can extract similar information (ah , bh ) for wh . The end r"
D09-1021,P05-1033,0,0.542334,"rect English word order, and is novel in a couple of respects: first, these criticisms is initially seen to the left of take, but after the adjunction this order is reversed; second, and more unusually, the treelet for seriously has been skipped over, with the result that the German words translated at this point (diese, kritik, and nehmen) form a non-contiguous sequence. More generally, we will allow any two 1 Introduction Syntax-based models for statistical machine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented"
D09-1021,P02-1040,0,0.106951,"ce of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM"
D09-1021,P05-1066,1,0.383303,"the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM) Given Qi , define Qi,j for j = 1 . . . n to be the subset of items in Qi which have their j’th bit equal to one (i.e., have the j’th source language word translated). Define Q′i,j to be the N highest scoring elements in Qi,j . Then BEAM(Qi ) = ∪nj=1"
D09-1021,P97-1003,1,0.609084,"rce language) sentence. ADJP DT man sub-strings above their associated sequence of treelets.4 Figure 1: A training example consisting of an English (tarbe VP Figure 2: Example syntactic phrase entries. We show GerNP es gibt keine hierarchie der diskriminierung SG NP PP discrimination VP NP S VP NP hierarchie der ADJP able SG to S NP VP there is To give a more formal description of how syntactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al., 2008). This procedure uses the head finding rules of (Collins, 1997). The resulting derivation consists of a TAG spine for each word seen in the sentence, together with a set of adjunction operations which each involve a modifier spine and a head spine. Given an English string e = e1 . . . en , with an associated parse tree t, the syntactic structure associated with a substring ek . . . el (e.g., there is) is then defined as follows: VP be ⇒ VP respond In this case the treelet for to respond sister-adjoins into the treelet for be able. This operation introduces a bi-lexical dependency between the modifier word to and the head word able. • For each word in the"
D09-1021,P05-1034,0,0.0975814,"the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-"
D09-1021,P95-1021,0,0.361151,"Missing"
D09-1021,P08-1066,0,0.195012,"d AFNLP pora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than stating reordering rules at the level of source or target language parse trees, we capture reordering phenomena using a discriminative dependency model. Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information (although see (Charniak et al., 2003; Shen et al., 2008) for related approaches). tree fragments to be combined during the translation process, irrespective of the reorderings which are introduced, or the non-projectivity of the parsing operations that are required. The use of flexible parsing operations raises two challenges that will be a major focus of this paper. First, these operations will allow the model to capture complex reordering phenomena, but will in addition introduce many spurious possibilities. Inspired by work in discriminative dependency parsing (e.g., (McDonald et al., 2005)), we add probabilistic constraints to the model through"
D09-1021,P03-2041,0,0.186779,"the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction o"
D09-1021,J97-3002,0,0.123383,"are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be lea"
D09-1021,P01-1067,0,0.215065,"and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the"
D09-1021,N03-1017,0,0.541898,"m German to English show improvements over phrase-based systems, both in terms of BLEU scores and in human evaluations. we NP VP these criticisms seriously VP must ADVP take ADVP also wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these cr"
D09-1021,P05-1059,0,0.0157149,"ord and spine, pos is the position in the head spine that is being adjoined into, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2"
D09-1021,W04-3250,0,0.0866349,"05). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the π-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. Definition 4 (BEAM) Given Qi , define Qi,j for j = 1 . . . n to be the subset of items in Qi which have their j’th bit equal to one (i.e., have the j’th source language word translated"
D09-1021,W06-3119,0,0.0519657,"to, and σ is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from cor2 We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of s"
D09-1021,2005.mtsummit-papers.11,0,0.00537231,"ng representations with a record of “pending” treelets which have not yet been included in a derivation. It is also possible to enforce the π-constituent constraint during decoding, as well as a constraint that ensures that reordering operations do not “break apart” English sub-strings within s-phrases that have multiple treelets (for example, for the s-phrase in figure 2, we ensure that there is no remains as a contiguous sequence of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koe"
D09-1021,D08-1076,0,\N,Missing
D09-1058,D07-1101,1,0.74923,"dency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that inThis paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning app"
D09-1058,P99-1065,1,0.699382,"at the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the genera"
D09-1058,C96-1058,0,0.336642,"describes how the parameters θj,a are trained on unlabeled data. Given parameters θj,a , we can simply define the functions q1 . . . qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent feat"
D09-1058,D07-1015,1,0.813809,". qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η −"
D09-1058,P08-1068,1,0.098982,"for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approac"
D09-1058,E06-1011,0,0.589255,".mit.edu Abstract supervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension"
D09-1058,W07-2216,0,0.0658637,"enerative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . (6) This corresponds to a MA"
D09-1058,P05-1012,0,0.451267,"d l is the label of the dependency. We use h = 0 for the root of the sentence. We assume access to a set of labeled training examples, {xi , yi }N i=1 , and in addition a set of unlabeled examples, {x0i }M i=1 . In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: 2.2 X (1) w · f (x, h, m, l) (h,m,l)∈y Here f (x, h, m, l) is a feature vector representing the dependency (h, m, l) in the context of the sentence x (see for example (McDonald et al., 2005a)). In this paper we extend the definition of g(x, y) to include features that are induced from unlabeled data. Specifically, we define g(x, y) = X The Generative Models We now describe how the generative models q1 . . . qk are defined, and how they are induced from unlabeled data. These models make direct use of the feature-vector definition f (x, y) used in the original, fully supervised, dependency parser. The first step is to partition the d features in f (x, y) into k separate feature vectors, r1 (x, y) . . . rk (x, y) (with the result that f is the concatenation of the k feature vectors"
D09-1058,H05-1066,0,0.116818,"Missing"
D09-1058,W06-1615,0,0.174516,"Missing"
D09-1058,J92-4003,0,0.385752,"parameters (w1 , v1 , q1 ). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Second-order Parsing Models Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech 1 We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi , y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 Corp"
D09-1058,W06-2920,0,0.027869,"Missing"
D09-1058,W96-0213,0,0.382393,"ther created through random sampling or by using a predefined subset of document IDs from the labeled training data. mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown"
D09-1058,D07-1070,0,0.0431948,"Missing"
D09-1058,D07-1014,0,0.0889514,"obabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . ("
D09-1058,P08-1076,1,0.72351,"upervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more spe"
D09-1058,P08-1061,0,0.0411284,"Missing"
D09-1058,W03-3023,0,0.20564,"ent, test data (labeled data sets) and unlabeled data used in our experiments parameter-estimation method for the second-order parsing model. In particular, we perform the following optimizations on each update t = 1, ..., T for re-estimating w and v: min ||w(t+1) − w(t) ||+ ||v(t+1) − v(t) || ˆ ) ≥ L(yi , y ˆ) s.t. S(xi , yi ) − S(xi , y ˆ = arg maxy S(xi , y) + L(yi , y), y as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2 , giving a total of 1,796,379 sentences and 43,380,315 tokens. The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,"
D09-1058,J93-2004,0,\N,Missing
D09-1058,D07-1096,0,\N,Missing
D10-1001,W08-2102,1,0.637361,"Missing"
D10-1001,D07-1101,0,0.0662954,", and simple additive updates to the Lagrange multipliers enforcing agreement between the two models. 4.2 Integrating Two Lexicalized Parsers Our second example problem is the integration of a phrase-structure parser with a higher-order dependency parser. The goal is to add higher-order features to phrase-structure parsing without greatly increasing the complexity of inference. First, we define an index set for second-order unlabeled projective dependency parsing. The secondorder parser considers first-order dependencies, as well as grandparent and sibling second-order dependencies (e.g., see Carreras (2007)). We assume that Idep is an index set containing all such dependencies (for brevity we omit the details of this index set). For convenience we define an extended index set that makes explicit use of first-order dependencies, I 0 dep = Idep ∪ Ifirst , where 5.1 Marginal Polytopes For a finite set Y, define the set of all distributions Ifirst = {(i, j) : i ∈ {0 . . . n}, j ∈ {1 . . . n}, i 6= j} |Y |: α ≥ over y P elements in Y as ∆ = {α ∈ R Here (i, j) represents a dependency with head wi 0, y∈Y αy = 1}. Each α ∈ ∆ gives a vector of P and modifier wj (i = 0 corresponds to the root sym- margina"
D10-1001,W02-1001,1,0.467871,"POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find Model 1 Koo08 Baseline DD Combination Precision 88.4 89.9 91.0 Recall 87.8 89.6 90.4 F1 88.1 89.7 90.7 Dep 91.4 93.3 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 100 y = arg max (f1 (y) + γf2 (y)) y∈Y (11) where Y is the set of all lexicalized phrase-structure trees for the sentence x; f1 (y) is the score (log probability) under Model 1; f2 (y) is the score under Koo et al. (2008) for the dependency structure implied by y; and γ &gt; 0 is a parameter dictating the relative weight of the"
D10-1001,J03-4003,1,0.175967,"e; we use it in this paper. In our experiments we found that in the vast majority of cases, case 1 applies, after a small number of iterations; see the next section for more details. 7 Proposition 6.1 The algorithm in figure 1 is an instantiation of the algorithm in figure 4,8 with X1 = conv(Y), X2 = conv(Z), and the matrices E and F defined to be binary matrices specifying the constraints µ(i, t) = ν(i, t) for all (i, t) ∈ Iuni . 8 Recovering the LP Solution Experiments 7.1 Integrated Phrase-Structure and Dependency Parsing Our first set of experiments considers the integration of Model 1 of Collins (2003) (a lexicalized phrasestructure parser, from here on referred to as Model (k) (k) (k) (k) We have that θ1 · x1 + θ2 · x2 = L(u(k) , x1 , x2 ) = (k) (k) (k) L(u ), where the last equality is because x1 and x2 are de(k) (k) fined by the respective arg max’s. Thus, (x1 , x2 ) and u(k) are primal and dual optimal. 10 The resulting fractional solution can be projected back to the set Q, see (Smith and Eisner, 2008; Martins et al., 2009). 9 Itn. Dep POS 1 43.5 58.7 2 20.1 15.4 3 10.2 6.3 4 4.9 3.6 5-10 14.0 10.3 11-20 5.7 3.8 20-50 1.4 0.8 ** 0.4 1.1 Table 1: Convergence results for Section 23 of th"
D10-1001,J85-1006,0,0.942677,"ion of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. Introduction Dynamic programming algorithms have been remarkably useful for inference in many NLP problems. Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP."
D10-1001,P08-1068,1,0.250264,"tional solution can be projected back to the set Q, see (Smith and Eisner, 2008; Martins et al., 2009). 9 Itn. Dep POS 1 43.5 58.7 2 20.1 15.4 3 10.2 6.3 4 4.9 3.6 5-10 14.0 10.3 11-20 5.7 3.8 20-50 1.4 0.8 ** 0.4 1.1 Table 1: Convergence results for Section 23 of the WSJ Treebank for the dependency parsing and POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find Model 1 Koo08 Baseline DD Combination Precision 88.4 89.9 91.0 Recall 87.8 89.6 90.4 F1 88.1 89.7 90.7 Dep 91.4 93.3 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 100 y = arg max"
D10-1001,D10-1125,1,0.627932,"Missing"
D10-1001,P09-1039,0,0.450406,"n loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper."
D10-1001,H05-1066,0,0.312236,"ficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2)"
D10-1001,P04-1035,0,0.00645042,"tleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1 The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and depende"
D10-1001,W06-1616,0,0.42633,"tion (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper. We take some care to set"
D10-1001,D08-1016,0,0.6547,"(Wainwright 2 et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solver"
D10-1001,W04-3201,0,0.0365589,"raints To make the connection to linear programming, in Eq. 6 specify that for each production of the form we first introduce the idea of marginal polytopes in 6 For any finite set Y, conv(Y) can be expressed as {µ ∈ section 5.1. In section 5.2, we give a precise statem R : Aµ ≤ b} where A is a matrix of dimension p × m, and ment of the LP relaxations that are being solved b ∈ Rp (see, e.g., Korte and Vygen (2008), pg. 65). The value by the example algorithms, making direct use of for p depends on the set Y, and can be exponential in size. 7 marginal polytopes. In section 6 we will prove that Taskar et al. (2004) describe the same set of constraints, but without proof of correctness or reference to Martin et al. (1990). the example algorithms solve these LP relaxations. 5 X ∀r ∈ I 0 , µr ≥ 0 ; X 0 ∀r ∈ Itag , νr ≥ 0 ; µ(X → Y Z, 1, k, n) = 1 (5) ν((X, Y ) → Z, 3) = 1 X,Y,Z∈T X,Y,Z∈N k=1...(n−1) ∀X ∈ N , ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) 6= (1, n): X X µ(X → Y Z, i, k, j) = µ(Y → Z X, k, i − 1, j) Y,Z∈N Y,Z∈N k=i...(j−1) k=1...(i−1) X + µ(Y → X Z, i, j, k) (6) Y,Z∈N k=(j+1)...n X,Z∈N X,Z∈N k=1...(i−1) Y,Z∈T Y,Z∈T ∀X ∈ T , ∀i ∈ {3 . . . n − 2}: X X ν((Y, Z) → X, i) = ν((X, Y ) → Z, i + 2) Y,Z∈T"
D10-1001,W00-1308,0,0.0345805,"Missing"
D10-1001,J93-2004,0,\N,Missing
D10-1125,P96-1023,0,0.143953,"Missing"
D10-1125,W06-2920,0,0.536662,"lso related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the m"
D10-1125,D07-1101,0,0.397968,"ments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but deco"
D10-1125,W02-1001,1,0.273285,"some feature vector definition φ(x, y|i ). In the bigram sibling models in our experiments, we assume that φ(x, y|i ) = p+1 X k=1 φL (x, i, lk−1 , lk ) + q+1 X φR (x, i, rk−1 , rk ) k=1 where as before l1 . . . lp and r1 . . . rq are left and right modifiers under y|i , and where φL and φR are feature vector definitions. In the grandparent models in our experiments, we use a similar definition with feature vectors φL (x, i, k ∗ , lk−1 , lk ) and φR (x, i, k ∗ , rk−1 , rk ), where k ∗ is the parent for word i under y|i . We train the model using the averaged perceptron for structured problems (Collins, 2002). Given the i’th example in the training set, (x(i) , y (i) ), the perceptron updates are as follows: • z ∗ = argmaxy∈Z w · φ(x(i) , y) • If z ∗ 6= y (i) , w = w +φ(x(i) , y (i) )−φ(x(i) , z ∗ ) The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. O"
D10-1125,D07-1015,1,0.681145,"2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2 Note also that the feature vectors φ and inner products w ·φ 1294 The strategy for choosing step sizes αk is described in Appendix A,"
D10-1125,D08-1017,0,0.0390796,". Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1 : UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2 : UAS when testing on the CoNLL-08 test"
D10-1125,P09-1039,0,0.787813,"d formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective"
D10-1125,E06-1011,0,0.82283,"s most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the"
D10-1125,W07-2216,0,0.11176,"for arc-factored models can be accomplished using directed minimum-weight spanning tree (MST) algorithms. The resulting parsing algorithms have the following properties: • They are efficient and easy to implement, relying on standard dynamic programming and MST algorithms. • They provably solve a linear programming (LP) relaxation of the original decoding problem. Introduction Non-projective dependency parsing is useful for many languages that exhibit non-projective syntactic structures. Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higher"
D10-1125,H05-1066,0,0.845802,"Missing"
D10-1125,P08-1108,0,0.0583026,"k. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1 : UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2 : UAS when t"
D10-1125,W06-1616,0,0.411692,"up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al."
D10-1125,D10-1001,1,0.539821,"els, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. 1289 Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be I = {(i, j) : i ∈ {0 . . . n}, j ∈ {1 . . . n}, i 6= j}. A dependency parse is a vector y = {y(i, j) : (i, j) ∈ I}, where y(i, j) = 1 if a dependency with head word i and modifier j is in the parse, 0 otherwise. We use i = 0 for the root symbol"
D10-1125,D08-1016,0,0.904744,"ur work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. 1289 Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be I = {("
D10-1125,D07-1014,0,0.0291793,"task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y (k) , z (k) ) within 5,000 iterations—we simply take the parse y (k) with the maximum value of f (y (k) ) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2 Note also that the feature vectors φ and inner products w ·φ 1294 The strategy for choosing step sizes αk is descr"
D10-1125,W08-2121,0,\N,Missing
D11-1003,J93-2003,0,0.0567809,"a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorith"
D11-1003,P01-1030,0,0.199739,"ecoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorithm for TSP. Och et al. (2001) propose an A*"
D11-1003,N03-1017,0,0.643861,"position, a special case of Lagrangian relaxation, has been applied to inference problems in NLP (Koo et al., 2010; Rush et al., 2010), and also to Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cut"
D11-1003,2005.mtsummit-papers.11,0,0.0438912,"Missing"
D11-1003,D10-1125,1,0.111225,"sentences of lengths 11-15 words takes on average 2707.8 seconds. 26 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 26–37, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics covered exact solutions for the type of phrase-based models used in MOSES. 2 Related Work Lagrangian relaxation is a classical technique for solving combinatorial optimization problems (Korte and Vygen, 2008; Lemar´echal, 2001). Dual decomposition, a special case of Lagrangian relaxation, has been applied to inference problems in NLP (Koo et al., 2010; Rush et al., 2010), and also to Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbase"
D11-1003,H05-1021,0,0.0462353,"Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorithm for TSP. Och et al. (2001) propose an A* search algorithm for IBM model 4, and test on sentence lengths up to 14 words. Other work (Kumar and Byrne, 2005; Blackwood et al., 2009) has considered variants of phrase-based models with restrictions on reordering that allow exact, polynomial time decoding, using finite-state transducers. The idea of incrementally adding constraints to 27 tighten a relaxation until it is exact is a core idea in combinatorial optimization. Previous work on this topic in NLP or machine learning includes work on inference in Markov random fields (Sontag et al., 2008); work that encodes constraints using finitestate machines (Tromble and Eisner, 2006); and work on non-projective dependency parsing (Riedel and Clarke, 200"
D11-1003,W99-0604,0,0.222672,"Missing"
D11-1003,W01-1408,0,0.063141,"tudied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorithm for TSP. Och et al. (2001) propose an A* search algorithm for IBM model 4, and test on sentence lengths up to 14 words. Other work (Kumar and Byrne, 2005; Blackwood et al., 2009) has considered variants of phrase-based models with restrictions on reordering that allow exact, polynomial time decoding, using finite-state transducers. The idea of incrementally adding constraints to 27 tighten a relaxation until it is exact is a core idea in combinatorial optimization. Previous work on this topic in NLP or machine learning includes work on inference in Markov random fields (Sontag et al., 2008); work that encodes constrain"
D11-1003,P03-1021,0,0.0082644,"e dynamic programming is in general intractable. Instead, people commonly use heuristic search methods such as beam search for decoding. However, these methods have no guarantee of returning the highest scoring translation. 4 A Decoding Algorithm based on Lagrangian Relaxation We now describe a decoding algorithm for phrasebased translation, based on Lagrangian relaxation. 3 The language model score usually includes a word insertion score that controls the length of translations. The relative weights of the g(p) and h(e(y)) terms, and the value for η, are typically chosen using MERT training (Och, 2003). 28 We first describe a dynamic program for decoding which is efficient, but which relaxes the y(i) = 1 constraints described in the previous section. We then describe the Lagrangian relaxation algorithm, which introduces Lagrange multipliers for each constraint of the form y(i) = 1, and uses a subgradient algorithm to minimize the dual arising from the relaxation. We conclude with theorems describing formal properties of the algorithm, and with an example run of the algorithm. 4.1 An Efficient Dynamic Program As described in the previous section, our goal is to find the optimal translation y"
D11-1003,P02-1040,0,0.11519,"Missing"
D11-1003,W06-1616,0,0.0705109,"(Kumar and Byrne, 2005; Blackwood et al., 2009) has considered variants of phrase-based models with restrictions on reordering that allow exact, polynomial time decoding, using finite-state transducers. The idea of incrementally adding constraints to 27 tighten a relaxation until it is exact is a core idea in combinatorial optimization. Previous work on this topic in NLP or machine learning includes work on inference in Markov random fields (Sontag et al., 2008); work that encodes constraints using finitestate machines (Tromble and Eisner, 2006); and work on non-projective dependency parsing (Riedel and Clarke, 2006). 3 The Phrase-based Translation Model This section establishes notation for phrase-based translation models, and gives a definition of the decoding problem. The phrase-based model we use is the same as that described by Koehn et al. (2003), as implemented in MOSES (Koehn et al., 2007). The input to a phrase-based translation system is a source-language sentence with N words, x1 x2 . . . xN . A phrase table is used to define the set of possible phrases for the sentence: each phrase is a tuple p = (s, t, e), where (s, t) are indices representing a contiguous span in the source-language sentence"
D11-1003,N09-2002,0,0.193839,"different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorithm for TSP. Och et al. (2001) propose an A* search algorithm for IBM model 4, and test on sentence lengths up to 14 words. Other work (Kumar and Byrne, 2005; B"
D11-1003,P11-1008,1,0.454849,"uistics covered exact solutions for the type of phrase-based models used in MOSES. 2 Related Work Lagrangian relaxation is a classical technique for solving combinatorial optimization problems (Korte and Vygen, 2008; Lemar´echal, 2001). Dual decomposition, a special case of Lagrangian relaxation, has been applied to inference problems in NLP (Koo et al., 2010; Rush et al., 2010), and also to Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM"
D11-1003,D10-1001,1,0.31557,"ths 11-15 words takes on average 2707.8 seconds. 26 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 26–37, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics covered exact solutions for the type of phrase-based models used in MOSES. 2 Related Work Lagrangian relaxation is a classical technique for solving combinatorial optimization problems (Korte and Vygen, 2008; Lemar´echal, 2001). Dual decomposition, a special case of Lagrangian relaxation, has been applied to inference problems in NLP (Koo et al., 2010; Rush et al., 2010), and also to Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algori"
D11-1003,D08-1016,0,0.0467021,"rgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics covered exact solutions for the type of phrase-based models used in MOSES. 2 Related Work Lagrangian relaxation is a classical technique for solving combinatorial optimization problems (Korte and Vygen, 2008; Lemar´echal, 2001). Dual decomposition, a special case of Lagrangian relaxation, has been applied to inference problems in NLP (Koo et al., 2010; Rush et al., 2010), and also to Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency re"
D11-1003,J03-1005,0,0.0557656,"akis et al., 2007; Sontag et al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem ("
D11-1003,W06-3602,0,0.0388239,"al., 2008). Earlier work on belief propagation (Smith and Eisner, 2008) is closely related to dual decomposition. Recently, Rush and Collins (2011) describe a Lagrangian relaxation algorithm for decoding for syntactic translation; the algorithmic construction described in the current paper is, however, very different in nature to this work. Beam search stack decoders (Koehn et al., 2003) are the most commonly used decoding algorithm for phrase-based models. Dynamic-programmingbased beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advan"
D11-1003,N06-1054,0,0.041809,"for IBM model 4, and test on sentence lengths up to 14 words. Other work (Kumar and Byrne, 2005; Blackwood et al., 2009) has considered variants of phrase-based models with restrictions on reordering that allow exact, polynomial time decoding, using finite-state transducers. The idea of incrementally adding constraints to 27 tighten a relaxation until it is exact is a core idea in combinatorial optimization. Previous work on this topic in NLP or machine learning includes work on inference in Markov random fields (Sontag et al., 2008); work that encodes constraints using finitestate machines (Tromble and Eisner, 2006); and work on non-projective dependency parsing (Riedel and Clarke, 2006). 3 The Phrase-based Translation Model This section establishes notation for phrase-based translation models, and gives a definition of the decoding problem. The phrase-based model we use is the same as that described by Koehn et al. (2003), as implemented in MOSES (Koehn et al., 2007). The input to a phrase-based translation system is a source-language sentence with N words, x1 x2 . . . xN . A phrase table is used to define the set of possible phrases for the sentence: each phrase is a tuple p = (s, t, e), where (s, t) a"
D11-1003,P09-1038,0,0.309221,"search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). Several works attempt exact decoding, but efficiency remains an issue. Exact decoding via integer linear programming (ILP) for IBM model 4 (Brown et al., 1993) has been studied by Germann et al. (2001), with experiments using a bigram language model for sentences up to eight words in length. Riedel and Clarke (2009) have improved the efficiency of this work by using a cutting-plane algorithm, and experimented with sentence lengths up to 30 words (again with a bigram LM). Zaslavskiy et al. (2009) formulate the phrase-based decoding problem as a traveling salesman problem (TSP), and take advantage of existing exact and approximate approaches designed for TSP. Their translation experiment uses a bigram language model and applies an approximate algorithm for TSP. Och et al. (2001) propose an A* search algorithm for IBM model 4, and test on sentence lengths up to 14 words. Other work (Kumar and Byrne, 2005; Blackwood et al., 2009) has considered variants of phrase-based models with restrictions on reordering that allow exact, polynomial time decoding, using finite-state transducers. The i"
D11-1003,P07-2045,0,\N,Missing
D12-1019,P12-1024,1,0.832772,"and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al. (2011) also extends Hsu et al. (2008) to latent variable dependency trees like us but under the restrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility an"
D12-1019,P99-1059,0,0.349064,"Missing"
D12-1019,P08-1068,1,0.290812,"s a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions on the smallest singular value of the HMM parameters. The main intuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and a"
D12-1019,E12-1042,0,0.291709,"t Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics ing which has one hidden node for each word in the sentence, like the one shown in Figure 1 and work out the details for the parameter estimation of the corresponding spectral learning model. At a very high level, the parameter estimation of our model involves collecting unigram, bigram and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al"
D12-1019,J94-2001,0,0.170465,"Missing"
D12-1019,P08-2054,0,0.0194386,"is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1 Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 Proceedings of the 2012 Joint"
D12-1019,P06-1055,0,0.0305203,"tuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1 Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205"
D12-1019,W96-0213,0,0.037909,"ents both first and second order parsers and is trained using MIRA (Crammer et al., 2006) and used the standard baseline features as described in McDonald (2006). We tested our methods on the English Penn Treebank (Marcus et al., 1993). We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7 http://nlp.cs.lth.se/software/treebank_ converter/ et al., 2008), we used the POS tagger by Ratnaparkhi (1996) trained on the full training data to provide POS tags for development and test sets and used 10way jackknifing to generate tags for the training set. As is common practice we stripped our sentences of all the punctuation. We evaluated our approach on sentences of all lengths. 4.2 Details of spectral learning For the spectral learning phase, we need to just collect word counts from the training data as described above, so there are no tunable parameters as such. However, we need to have access to an attribute dictionary U which contains a k dimensional representation for each word in the corpu"
D12-1019,J93-2004,0,\N,Missing
D12-1019,J05-1003,1,\N,Missing
D12-1019,P02-1062,1,\N,Missing
D12-1131,W06-2920,0,0.155602,"e WSJ PennTreebank (Marcus et al., 1993) and the QuestionBank (QTB) (Judge et al., 2006). In the WSJ → QTB scenario, we train on sections 2-21 of the WSJ and test on the entire QTB (4000 questions). In the QTB → WSJ scenario, we train on the entire QTB and test on section 23 of the WSJ. Data for Lightly Supervised Training For all English experiments, our data was taken from the WSJ PennTreebank: training sentences from Section 0, development sentences from Section 22, and test sentences from Section 23. For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set (Buchholz and Marsi, 2006) with training data taken from the official training files. We trained the sentence-level models with 50-500 sentences. To verify the robustness of our results, our test sets consist of the official test sets augmented with additional sentences from the official training files such that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, δ1 , δ2 , and δ3 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the"
D12-1131,P04-1056,0,0.0717296,"formation with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann a"
D12-1131,D10-1003,0,0.0232755,"ere used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. 3 where u is a vector in R|I(x) |. In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1 , . . . , wn ). We skip the formal details here. We next introduce notation for Markov random fields (MRFs) (Koller and Friedman, 2009). An MRF consists of an undire"
D12-1131,W03-0407,0,0.0205689,"Missing"
D12-1131,N09-1068,0,0.00939211,"in adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic st"
D12-1131,P05-1045,0,0.0245567,"el algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Ba"
D12-1131,P10-2036,0,0.0269225,"Missing"
D12-1131,J04-3001,0,0.022584,"ised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, how"
D12-1131,P06-1063,0,0.110247,"Missing"
D12-1131,D10-1125,1,0.938178,"S tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Bel"
D12-1131,I05-1006,0,0.05248,"(Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistic"
D12-1131,J93-2004,0,0.053186,"Missing"
D12-1131,P08-2026,0,0.0106814,"manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper."
D12-1131,P06-1043,0,0.055918,"and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a majo"
D12-1131,N10-1004,0,0.0836437,"s (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic pr"
D12-1131,H05-1066,0,0.802359,"ncy parse is a vector y = {y(m, h) : (m, h) ∈ I(x)} where y(m, h) = 1 if m is a modifier of the head word h. We define the set Y(x) ⊂ {0, 1}|I(x)| to be the set of all valid dependency parses for a sentence x. In this work, we use projective dependency parses, but the method also applies to the set of nonprojective parse trees. Additionally, we have a scoring function f : Y(x) → R. The optimal parse y ∗ for a sentence x is given by, y ∗ = arg maxy∈Y(x) f (y). This sentencelevel decoding problem can often be solved efficiently. For example in commonly used projective dependency parsing models (McDonald et al., 2005), we can compute y ∗ efficiently using variants of the Viterbi algorithm. For this work, we make the assumption that we have an efficient algorithm to find the argmax of X f (y) + u(m, h)y(m, h) = f (y) + u · y (m,h)∈I(x) 1436 {((i, j), li , lj ) : (i, j) ∈ E, li ∈ Li , lj ∈ Lj } A label assignment in the MRF is a binary vector z with z(i, l) = 1 if the label l is selected at node i and z((i, j), li , lj ) = 1 if the labels li , lj are selected for the nodes i, j. In applications such as parsing and POS tagging, some of the label assignments are not allowed. For example, in dependency parsing"
D12-1131,P07-1078,1,0.381082,"ce-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used b"
D12-1131,D08-1050,0,0.0198208,"06) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phe"
D12-1131,D10-1001,1,0.955163,"and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifo"
D12-1131,D07-1111,0,0.0147533,"aper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in huma"
D12-1131,D08-1016,0,0.0219949,"ake a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the"
D12-1131,E03-1008,0,0.364995,"on. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS ta"
D12-1131,D10-1017,0,0.0145414,"co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We compleme"
D12-1131,N03-1033,0,0.0205525,"Missing"
D13-1022,D11-1003,1,0.428041,"us the language model score of the target sentence f (p) = n X i=1 |u|+1 ω(q(pi ), r(pi )) + X σ(ui−1 , ui ) i=0 where u is the sequence of words in Σ formed by concatenating the phrases r(p1 ) . . . r(pn ), with boundary cases u0 = &lt;s&gt; and u|u|+1 = &lt;/s&gt;. 212 Crucially for a derivation to be valid it must satisfy an additional condition: it must translate every source word exactly once. The decoding problem for phrase-based translation is to find the highestscoring derivation satisfying this property. We can represent this decoding problem as a constrained hypergraph using the construction of Chang and Collins (2011). The hypergraph weights encode the translation and language model scores, and its structure ensures that the count of source words translated is |w|, i.e. the length of the source sentence. Each vertex will remember the preceding target-language word and the count of source words translated so far. The hypergraph, which for this problem is also a directed graph, takes the following form. • Vertices v ∈ V are labeled (c, u) where c ∈ {1 . . . |w|} is the count of source words translated and u ∈ Σ is the last target-language word produced by a partial hypothesis at this vertex. Additionally the"
D13-1022,J10-3008,0,0.0412784,"Missing"
D13-1022,P10-4002,0,0.063735,"Missing"
D13-1022,P01-1030,0,0.0773916,"dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ"
D13-1022,W05-1506,0,0.0400969,"m invokes the black-box function, P RUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2 For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 214 1: procedure B EAM S EARCH(θ, τ, lb, β) 2: ubs ← O UTSIDE(θ, τ ) 3: opt ← true 4: π[v, sig] ← −∞ for all v ∈ V, sig ∈ R|b| 5: π[v, 0] ← 0 for all v ∈ T 6: for e ∈ E in topological order do 7: hhv2 , . . . , v|v |i, v1 i ← e 8: for sig (2) . . . sig (|v|) ∈ S IGS(v2 , . . . , v|v |) do 9: sig ← Aδ(e) + |v| X sig (i) i=2 |v| X s ← θ(e) + π[vi , sig (i) ] i=2   s &gt; π[v1 , sig] ∧ 11: if  C HECK(sig) ∧  then s + ubs[v1 ] ≥ lb 12: π[v1 , sig] ← s 13: if P RUNE(π, v1 , sig, β) then opt ← false 0 14: lb ← π[1, c] + τ 0 15: return  lb , opt (V, E, θ, τ ) hypergraph with weights  (A, b) matr"
D13-1022,P07-1019,0,0.0433886,"6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) ha"
D13-1022,D10-1027,0,0.12585,"ween computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with"
D13-1022,E09-1044,0,0.0470736,"Missing"
D13-1022,J99-4005,0,0.0468421,"peredge constraints. Define the set of constrained hyperpaths as X 0 = {x ∈ X : Ax = b} 1 The purpose of the offset will be clear in later sections. For this section, the value of τ can be taken as 0. where we have a constraint matrix A ∈ R|b|×|E| and vector b ∈ R|b |encoding |b |constraints. The optimal constrained hyperpath is x∗ = arg maxx∈X 0 θ&gt; x + τ . Note that the constrained hypergraph search problem may be NP-Hard. Crucially this is true even when the corresponding unconstrained search problem is solvable in polynomial time. For instance, phrase-based decoding is known to be NP-Hard (Knight, 1999), but we will see that it can be expressed as a polynomial-sized hypergraph with constraints. Example: Phrase-Based Machine Translation Consider translating a source sentence w1 . . . w|w |to a target sentence in a language with vocabulary Σ. A simple phrase-based translation model consists of a tuple (P, ω, σ) with • P; a set of pairs (q, r) where q1 . . . q|q |is a sequence of source-language words and r1 . . . r|r| is a sequence of target-language words drawn from the target vocabulary Σ. • ω : R|P |; parameters for the translation model mapping each pair in P to a real-valued score. • σ :"
D13-1022,N03-1017,0,0.0690447,"exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Michael Collins Department of Computer Science, Columbia University, New York, NY 10027, USA mcollins@cs.columbia.edu • It utilizes well-studied algorithms and extends off-the-shelf beam search decoders. • Empirically it is very fast, results show that it is 3.5 times faster than an optimized incremental constraint-based solver. Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search"
D13-1022,koen-2004-pharaoh,0,0.0241844,"Missing"
D13-1022,H05-1021,0,0.0704157,"work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1) , βk ) if opt then return lb(k) return maxk∈{1...K} lb(k) procedure O PT B EAM(α, β) λ(0) ← 0 lb(0) ← −∞ for k in 1 . . . K do λ(k) , ub(k) , opt ← LRROUND(αk , λ(k−1) ) if opt then return ub(k) θ0 ← θ − A&gt; λ(k) τ 0 ← τ + λ(k)&gt; b lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1"
D13-1022,W01-1408,0,0.0463385,"oximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EA"
D13-1022,N09-2002,0,0.0231604,"hm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ f"
D13-1022,D12-1067,0,0.114093,"l beam search: staged and alternating. Staged runs Lagrangian relaxation to find the optimal λ, uses λ to compute upper bounds, and then repeatedly runs beam search with pruning sequence β1 . . . βk . Alternating switches between running a round of Lagrangian relaxation and a round of beam search with the updated λ. If either produces a certificate it returns the result. and Collins (2011) and Rush and Collins (2011) develop Lagrangian relaxation-based approaches for exact machine translation. Apart from translation decoding, this paper is closely related to work on column generation for NLP. Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems. The latter work also gives conditions for when beam search-style decoding is optimal. 7 Results To evaluate the effectiveness of optimal beam search for translation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the speed and optimality of these decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We tes"
D13-1022,P11-1008,1,0.32464,"{x ∈ X : Ax = 1 }, and the best derivation under this phrase-based translation model has score maxx∈X 0 θ&gt; x + τ . Figure 2.2 shows an example hypergraph with constraints for translating the sentence les pauvres sont demunis into English using a simple set of phrases. Even in this small example, many of the possible hyperpaths violate the constraints and correspond to invalid derivations. Example: Syntax-Based Machine Translation Syntax-based machine translation with a language model can also be expressed as a constrained hypergraph problem. For the sake of space, we omit the definition. See Rush and Collins (2011) for an indepth description of the constraint matrix used for syntax-based translation. 213 3 A Variant of Beam Search This section describes a variant of the beam search algorithm for finding the highest-scoring constrained hyperpath. The algorithm uses three main techniques: (1) dynamic programming with additional signature information to satisfy the constraints, (2) beam pruning where some, possibly optimal, hypotheses are discarded, and (3) branch-andbound-style application of upper and lower bounds to discard provably non-optimal hypotheses. Any solution returned by the algorithm will be"
D13-1022,J03-1005,0,0.0103242,"improve the lower bound lb. This motivates the alternating algorithm O PTB EAM shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. E"
D13-1022,W06-3602,0,0.072549,"checks. The algorithm invokes the black-box function, P RUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2 For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 214 1: procedure B EAM S EARCH(θ, τ, lb, β) 2: ubs ← O UTSIDE(θ, τ ) 3: opt ← true 4: π[v, sig] ← −∞ for all v ∈ V, sig ∈ R|b| 5: π[v, 0] ← 0 for all v ∈ T 6: for e ∈ E in topological order do 7: hhv2 , . . . , v|v |i, v1 i ← e 8: for sig (2) . . . sig (|v|) ∈ S IGS(v2 , . . . , v|v |) do 9: sig ← Aδ(e) + |v| X sig (i) i=2 |v| X s ← θ(e) + π[vi , sig (i) ] i=2   s &gt; π[v1 , sig] ∧ 11: if  C HECK(sig) ∧  then s + ubs[v1 ] ≥ lb 12: π[v1 , sig] ← s 13: if P RUNE(π, v1 , sig, β) then opt ← false 0 14: lb ← π[1, c] + τ 0 15: return  lb , opt (V, E, θ, τ ) hypergrap"
D13-1022,P06-1098,0,0.0269831,"lgorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set β for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phra"
D13-1022,P09-1038,0,0.0179732,"ed on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang 217 procedure O PT B EAM S TAGED(α, β) λ, ub, opt ←L AGRANGIAN R ELAXATION (α) if opt then return ub θ 0 ← θ − A&gt; λ τ 0 ← τ + λ&gt; b lb(0) ← −∞ for k in 1 . . . K do lb(k) , opt ← B EAM S EARCH(θ0 , τ 0 , lb(k−1) , β"
D13-1022,P07-2045,0,\N,Missing
D13-1022,J07-2003,0,\N,Missing
D13-1164,P05-1033,0,0.0811568,"et al., 2010)). (Here t(f |e) are the translation parameters of the model, and d(i|j) are the distortion parameters; the product is non-linear, effectively introducing nonconvexity into the problem.) Introduction The IBM translation models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is"
D13-1164,J07-3002,0,0.400818,"Missing"
D13-1164,P09-1104,0,0.0227423,"Och and Ney, 2003) give a systematic comparison of several alignment models in the literature. (Moore, 2004) gives a detailed study of IBM Model 1, showing various steps that can be used to improve its performance. (Ganchev et al., 2010) describes a method based on posterior regularization that incorporates additional constraints within the EM algorithm for estimation of IBM models. All of these approaches are unsupervised, in that they do not require labeled alignment data; however several authors have considered supervised models (e.g., see (Lacoste-Julien et al., 2006; Taskar et al., 2005; Haghighi et al., 2009)). The focus of the current paper is on unsupervised learning; the unsupervised variants described above all make use of non1575 convex objective functions during training, with the usual problems with multiple local maxima. 3 The IBM Model 1 and Model 2 Optimization Problems In this section we give a brief review of IBM Models 1 and 2, and the optimization problems arising from these models. The standard approach for optimization within these models is the EM algorithm. Throughout this section, and the remainder of the paper, we assume that our set of training examples is (e(k) , f (k) ) for"
D13-1164,P12-2060,0,0.0182009,"n models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computationally intractible problem. Typi• We describe an optimization algorithm for the relaxed objective, based on a combination of stochastic subgradient methods with the exponentiated-gradient (EG) algorithm (Kivinen and War"
D13-1164,N06-1014,0,0.82572,"to the problem.) Introduction The IBM translation models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computationally intractible problem. Typi• We describe an optimization algorithm for the relaxed objective, based on a combination of stochastic subgradient methods with the ex"
D13-1164,W06-1606,0,0.0854778,". (Here t(f |e) are the translation parameters of the model, and d(i|j) are the distortion parameters; the product is non-linear, effectively introducing nonconvexity into the problem.) Introduction The IBM translation models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computat"
D13-1164,D10-1004,0,0.0235195,"on of IBM Model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates. Our approach gives the same level of alignment accuracy as IBM Model 2. 1 Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu • We introduce a convex relaxation of IBM Model 2. At a very high level, the relaxation is derived by replacing the product t(fj |ei ) × d(i|j) with a relaxation that is commonly used in the linear programming literature (e.g., see (Bertsimas, 1997; Bertsimas and Tsitsiklis, 1997; Martins et al., 2010)). (Here t(f |e) are the translation parameters of the model, and d(i|j) are the distortion parameters; the product is non-linear, effectively introducing nonconvexity into the problem.) Introduction The IBM translation models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005;"
D13-1164,W03-0300,0,0.349224,"using the I2CR-2 optimization problem combined with the Input: Define E, F , L, M , (e(k) , f (k) , lk , mk ) for k = 1 . . . n, D(e) for e ∈ E as in Section 3. An integer B specifying the batch size. An integer S specifying the number of passes over the data. A step size γ &gt; 0. A parameter λ &gt; 0 used in the definition of log0 . 2: Parameters: • A parameter t(f |e) for each e ∈ E, f ∈ D(e). • A parameter d(i|j) for each i ∈ [L]0 , j ∈ [M ]. 3: Definitions: 1: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 6.1 Data Sets We use data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Michalcea and Pederson, 2003). As a first dataset, we use the Canadian Hansards bilingual corpus, with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (note that we use a randomly chosen lk X (k) (k) subset of the original training set of 1.1 million senR(j, k) = λ + t(fj |ei ) tences, similar to the setting used in (Moore, 2004)). i=0 The development and test data have been manually l k X (k) (k) Q(j, k) = λ + min{t(fj |ei ), d(i|j)} aligned at the word level, annotating alignments between source and target words in the corpus as eii=0 ther “sure” ("
D13-1164,C96-2141,0,0.965295,"1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computationally intractible problem. Typi• We describe an optimization algorithm for the relaxed objective, based on a combination of stochastic subgradient methods with the exponentiated-gradient (EG) algorithm (Kivinen and Warmuth, 1997; Beck and"
D13-1164,J03-1002,0,0.227525,"ced for full translation, they are now mainly used to derive alignments which are then used by phrase-based and other modern SMT systems. Since the original IBM models were introduced, many variants have been introduced in the literature. (Vogel et al., 1996) introduced a model, sometimes referred to as IBM 2.5, which uses a parameterization that is similar to a hidden Markov model, and which allows the value of each alignment variable to be conditioned on a previous alignment variable. (Liang et al., 2006) describe a method that explicitly incorporates agreement preferences during training. (Och and Ney, 2003) give a systematic comparison of several alignment models in the literature. (Moore, 2004) gives a detailed study of IBM Model 1, showing various steps that can be used to improve its performance. (Ganchev et al., 2010) describes a method based on posterior regularization that incorporates additional constraints within the EM algorithm for estimation of IBM models. All of these approaches are unsupervised, in that they do not require labeled alignment data; however several authors have considered supervised models (e.g., see (Lacoste-Julien et al., 2006; Taskar et al., 2005; Haghighi et al., 2"
D13-1164,P08-1066,0,0.0493626,"Missing"
D13-1164,H05-1010,0,0.0331057,"es during training. (Och and Ney, 2003) give a systematic comparison of several alignment models in the literature. (Moore, 2004) gives a detailed study of IBM Model 1, showing various steps that can be used to improve its performance. (Ganchev et al., 2010) describes a method based on posterior regularization that incorporates additional constraints within the EM algorithm for estimation of IBM models. All of these approaches are unsupervised, in that they do not require labeled alignment data; however several authors have considered supervised models (e.g., see (Lacoste-Julien et al., 2006; Taskar et al., 2005; Haghighi et al., 2009)). The focus of the current paper is on unsupervised learning; the unsupervised variants described above all make use of non1575 convex objective functions during training, with the usual problems with multiple local maxima. 3 The IBM Model 1 and Model 2 Optimization Problems In this section we give a brief review of IBM Models 1 and 2, and the optimization problems arising from these models. The standard approach for optimization within these models is the EM algorithm. Throughout this section, and the remainder of the paper, we assume that our set of training examples"
D13-1164,P11-2081,0,0.228151,"roduction The IBM translation models (Brown et al., 1993) have been tremendously important in statistical machine translation (SMT). The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models (Koehn, 2008) and syntax-based translation systems (e.g., (Chiang, 2005; Marcu et al., 2006)). Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g., (Moore, 2004; Liang et al., 2006; Toutanova and Galley, 2011; Riley and Gildea, 2012; Vogel et al., 1996)). Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex. Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computationally intractible problem. Typi• We describe an optimization algorithm for the relaxed objective, based on a combination of stochastic subgradient methods with the exponentiated-gradient (EG) al"
D13-1164,P01-1067,0,0.213772,"Missing"
D13-1164,P12-1033,0,0.235673,"Missing"
D13-1164,J93-2003,0,\N,Missing
D13-1164,N06-1015,0,\N,Missing
D13-1164,P04-1066,0,\N,Missing
D15-1023,N13-1073,0,0.0933506,"Missing"
D15-1023,J07-3002,0,0.443498,"Missing"
D15-1023,W03-0300,0,0.674801,"iments (k) (k) we picked α(fj , ei ) = d(i|j, lk , mk ) or 1; we hold λ to a constant of either 16 or 0 and do not estimate this variable (λ = 16 can be chosen by cross validation on a small trial data set). 6 6.1 where (k) and more association, respectively. Additionally, we make use of positional constants like those of the IBM Model 2 distortions given by Experiments Data Sets For our alignment experiments, we used a subset of the Canadian Hansards bilingual corpus with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (Michalcea and Pederson, 2003). As a second validation corpus, we considered a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sentence pairs, and a test set of 248 sentence pairs (Michalcea and Pederson, 2003). 6.2 Methodology Below we report results in both AER (lower is better) and F-Measure (higher is better) (Och and Ney, 2003) for the English → French translation direction. To declare a better model we have to settle on an alignment measure. Although the relationship between AER/F-Measure and translation quality varies (Dyer et al., 2013), there are some positive experiments (Fraser an"
D15-1023,C96-2141,0,0.843318,"Missing"
D15-1023,J03-1002,0,0.113966,"a(k) ,j,k (t(fj |e (k) )) a j j are constants gotten in the E step. This optimization step is very similar to the regular Model 1 M step since the β drops down using log tβ = β log t; the exact same count-based method can be applied. The details of this algorithm are in Fig. 3. 5 Choosing α and β The performance of our new model will rely heav(k) (k) (k) (k) ily on the choice of α(ei , fj ), β(ei , fj ) ∈ (0, 1) we use. In particular, we could make β depend on the association between the words, or the words’ positions, or both. One classical measure of word association is the dice coefﬁcient (Och and Ney, 2003) given by dice(e, f ) = 2c(e, f ) . c(e) + c(f ) In the above, the count terms c are the number of training sentences that have either a particular word or a pair of of words (e, f ). As with the other choices we explore, the dice coefﬁcient is a fraction between 0 and 1, with 0 and 1 implying less    1 (l+1)Z(j,l,m) :i=0 (l+1)Z(j,l,m) : i �= 0 j i le−λ |l − m | In the above, Z(j, l, m) is the partition function discussed in (Dyer et al., 2013). The previous measures all lead to potential candidates for β(e, f ), we have t(f |e) ∈ (0, 1), and we want to enlarge competing values when decodin"
D15-1023,D13-1164,1,0.863907,"Missing"
D15-1023,P11-2081,0,0.581065,"is a constant that can be ignored. Notation. Throughout this paper, for any positive integer N , we use [N ] to denote {1 . . . N } and [N ]0 to denote {0 . . . N }. We denote by Rn+ the set of nonnegative n dimensional vectors. We denote by [0, 1]n the n−dimensional unit cube. 2 (k) (k) lk � t(fj |ei ) t(f |e) ≥ 0 (1) � t(f |e) = 1 (2) ∀e ∈ E, f ∈D(e) Objective: Maximize n m l k k � 1 �� (k) (k) log t(fj |ei ) n k=1 j=1 (3) i=0 with respect to the t(f |e) parameters. Figure 1: The IBM Model 1 Optimization Problem. While IBM Model 1 is concave optimization problem, it is not strictly concave (Toutanova and Galley, 2011). Therefore, optimization methods for IBM Model 1 (speciﬁcally, the EM algorithm) are typically only guaranteed to reach a global maximum of the objective function (see the Appendix for a simple example contrasting convex and strictly convex functions). In particular, although the objective cost is the same for any optimal solution, the translation quality of the solutions is not ﬁxed and will still depend on the initialization of the model (Toutanova and Galley, 2011). 3 A Strictly Concave IBM Model 1 We now detail a very simple method to make IBM Model 1 strictly concave with a unique optima"
D15-1023,P12-1033,0,0.14635,"Missing"
D15-1023,J93-2003,0,\N,Missing
D15-1039,D12-1001,0,0.109799,"Missing"
D15-1039,P09-1042,0,0.427082,"Missing"
D15-1039,J92-4003,0,0.19102,"Missing"
D15-1039,D11-1005,0,0.186757,"Missing"
D15-1039,Q13-1033,0,0.0129254,"ng on how POS constraints are used. As a second evaluation method, we can test the accuracy of a model trained on the P100 data. The benefit of the soft-matching POS definition is clear. The hard match definition harms performance, presumably because it reduces the number of sentences used to train the model. Throughout the rest of this paper, we use the soft POS constraints in all projection algorithms.3 3.4 to operate in a “constrained” mode, where it returns the highest scoring parse that is consistent with a given subset of dependencies. This can be achieved via zero-cost dynamic oracles (Goldberg and Nivre, 2013). We assume the following definitions: • TRAIN(D) is a function that takes a set of dependency structures D as input, and returns a model θ as its output. The dependency structures are assumed to be full trees: that is, they correspond to fully projected trees with the root symbol as their root. • CDECODE(P, θ) is a function that takes a set of partial dependency structures P, and a model θ as input, and as output returns a set of full trees D. It achieves this by constrained decoding of the sentences in P under the model θ, where for each sentence we use beam search to search for the highest"
D15-1039,W02-1001,1,0.363337,"Missing"
D15-1039,P15-1133,0,0.0429848,"Missing"
D15-1039,K15-1012,0,0.0985919,"these are the highest accuracy parsing results for an approach that makes no use of treebank data for the language of interest. 2 Related Work A number of researchers have considered the problem of projecting linguistic annotations from the source to the target language in a parallel corpus (Yarowsky et al., 2001; Hwa et al., 2005; 1 The original paper of (McDonald et al., 2011) does not use the Google universal treebank, however (Ma and Xia, 2014) reimplemented the model and report results on the Google universal treebank. 329 rett et al., 2012; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015; Zhang and Barzilay, 2015). These results are somewhat below the performance of (Ma and Xia, 2014).2 3 3.2 We now describe various sets of projected dependencies. We use D to denote the set of all dependencies in the source language: these dependencies are the result of parsing the English side of the translation data using a supervised parser. Each dependency (l, k, h, m) ∈ D is a four-tuple as described above, with l = e. We will use P to denote the set of all projected dependencies from the source to target language. The set P is constructed from D and the alignment variables Ak,j as follo"
D15-1039,P15-1119,0,0.278156,"Missing"
D15-1039,P13-1028,0,0.0759333,"Missing"
D15-1039,N09-1012,0,0.060374,"Missing"
D15-1039,H05-1066,0,0.390752,"Missing"
D15-1039,N12-1015,0,0.0377659,"Missing"
D15-1039,D11-1006,0,0.584661,"Missing"
D15-1039,P13-2017,0,0.247408,"Missing"
D15-1039,P04-1061,0,0.144059,"Missing"
D15-1039,P12-1066,0,0.230396,"acy on this data. To the best of our knowledge these are the highest accuracy parsing results for an approach that makes no use of treebank data for the language of interest. 2 Related Work A number of researchers have considered the problem of projecting linguistic annotations from the source to the target language in a parallel corpus (Yarowsky et al., 2001; Hwa et al., 2005; 1 The original paper of (McDonald et al., 2011) does not use the Google universal treebank, however (Ma and Xia, 2014) reimplemented the model and report results on the Google universal treebank. 329 rett et al., 2012; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015; Zhang and Barzilay, 2015). These results are somewhat below the performance of (Ma and Xia, 2014).2 3 3.2 We now describe various sets of projected dependencies. We use D to denote the set of all dependencies in the source language: these dependencies are the result of parsing the English side of the translation data using a supervised parser. Each dependency (l, k, h, m) ∈ D is a four-tuple as described above, with l = e. We will use P to denote the set of all projected dependencies from the source to target language. The set P is constructed fr"
D15-1039,2005.mtsummit-papers.11,0,0.288567,"cal evidence that dense structures give particularly high accuracy for their projected dependencies. Currently on leave at Google Inc. New York. 328 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 328–338, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. The political priorities must be set by this House Die politischen Priorit¨aten m¨ussen von diesem Parlament und den and the MEPs . ROOT Europaabgeordneten abgesteckt werden . ROOT Figure 1: An example projection from English to German in the EuroParl data (Koehn, 2005). The English parse tree is the output from a supervised parser, while the German parse tree is projected from the English parse tree using translation alignments from GIZA++. Ganchev et al., 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011; Ma and Xia, 2014). The projected annotations are then used to train a model in the target language. This prior work involves various innovations such as the use of posterior regularization (Ganchev et al., 2009), the use of entropy regularization and parallel guidance (Ma and Xia, 2014), the use of a simple method to transfer delexicalized parsers acros"
D15-1039,N15-1067,0,0.0367614,"Missing"
D15-1039,D13-1204,0,0.0248819,"Missing"
D15-1039,P14-1126,0,0.778616,"and Michael Collins∗ Department of Computer Science, Columbia University New York, NY 10027, USA {rasooli,mcollins}@cs.columbia.edu Abstract Guo et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015)), in an effort to reduce or eliminate the need for annotated training examples. Unfortunately the accuracy of these methods generally lags quite substantially behind the performance of fully supervised approaches. This paper describes novel methods for the transfer of syntactic information between languages. As in previous work (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014), our goal is to induce a dependency parser in a target language of interest without any direct supervision (i.e., a treebank) in the target language: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source languages. We can then use alignments induced using tools such as GIZA++ (Och and Ney, 2000), to transfer dependencies from the source language(s) to the target language (example projections are shown in Figure 1). A target language parser is then trained on the projected dependencies. Our contributions a"
D15-1039,W09-1104,0,0.172279,"Missing"
D15-1039,N13-1126,0,0.442928,"Missing"
D15-1039,W14-1614,0,0.512721,"Missing"
D15-1039,W15-1824,0,0.353582,"Missing"
D15-1039,K15-1008,0,0.142561,"Missing"
D15-1039,H01-1035,0,0.85456,"Missing"
D15-1039,D15-1213,0,0.504844,"t accuracy parsing results for an approach that makes no use of treebank data for the language of interest. 2 Related Work A number of researchers have considered the problem of projecting linguistic annotations from the source to the target language in a parallel corpus (Yarowsky et al., 2001; Hwa et al., 2005; 1 The original paper of (McDonald et al., 2011) does not use the Google universal treebank, however (Ma and Xia, 2014) reimplemented the model and report results on the Google universal treebank. 329 rett et al., 2012; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015; Zhang and Barzilay, 2015). These results are somewhat below the performance of (Ma and Xia, 2014).2 3 3.2 We now describe various sets of projected dependencies. We use D to denote the set of all dependencies in the source language: these dependencies are the result of parsing the English side of the translation data using a supervised parser. Each dependency (l, k, h, m) ∈ D is a four-tuple as described above, with l = e. We will use P to denote the set of all projected dependencies from the source to target language. The set P is constructed from D and the alignment variables Ak,j as follows: Our Approach This secti"
D15-1039,P11-2033,0,0.0374753,"nces in P under the model θ, where for each sentence we use beam search to search for the highest scoring projective full tree that is consistent with the dependencies in P. The Training Procedure • TOP(D, θ) takes as input a set of full trees D, and a model θ. It returns the top m highest scoring trees in D (in our experiments we used m = 200, 000), where the score for each tree is the perceptron-based score normalized by the sentence length. Thus we return the We now describe the training procedure used in our experiments. We use a perceptron-trained shift-reduce parser, similar to that of (Zhang and Nivre, 2011). We assume that the parser is able 3 The hard constraint is also used by Ma and Xia (2014). 331 POS Constraints No Restriction Hard match Soft match #sen 968k 927k 904k P Acc. 74.0 80.1 80.0 dense #sen Acc. 65k 81.4 26k 88.0 52k 84.9 P100 #sen Acc. 23k 83.0 8k 90.1 18k 85.8 Train on P100 69.5 68.0 70.6 Table 1: Statistics showing the accuracy for various definitions of projected trees: see §3.2 for definitions of P, P100 etc. Columns labeled “Acc.” show accuracy when the output of a supervised German parser is used as gold standard data. Columns labeled “#sen” show number of sentences. “dense"
D15-1039,J93-2004,0,\N,Missing
D16-1051,N13-1073,0,0.0609315,"Missing"
D16-1051,J07-3002,0,0.0757843,"Missing"
D16-1051,W04-3250,0,0.177454,"Missing"
D16-1051,N06-1014,0,0.0610078,"s 1 and 2, the Viterbi alignment splits easily (Koehn, 2008). For the HMM, dynamic programming is used (Vogel et al., 1996). Although it is non-convex and thus its initialization is important, the HMM is the last alignment model in the classical setting that has an exact EM procedure (Och and Ney, 2003): from IBM Model 3 onwards heuristics are used within the expectation and maximization steps of each model’s associated EM procedure. 3 Input: Define E, F , (e(k) , f (k) , lk , mk ) for k = 1 . . . n, D(e) for e ∈ E as in Section 2. detail this here. We are using the roughly same structure as (Liang et al., 2006) and (Dyer et al., 2013): the distortions and emissions of our model are parametrized by forcing the model to concentrate its alignments on the diagonal. 3.1 Distortion Parameters for IBM2 Let λ > 0. For the IBM Model 2 distortions we set the NULL word probability as d(0|j, l, m) = p0 , 1 where p0 = l+1 and note that this will generally depend on the target sentence length within a bitext training pair that we are considering. For i 6= 0 which satisfies we set i j (1 − p0 )e−λ |l − m | d(i|j, l, m) = , Zλ (j, l, m) where Zλ (j, l, m) is a normalization constant as in (Dyer et al., 2013). Disto"
D16-1051,W06-1606,0,0.0489401,"non-convex models. In particular, we initialize, the GIZA++ HMM with IBM Model 2, IBM Model 2 with IBM Model 1, and IBM2-HMM and IBM Model 3 with IBM Model 2 preceded by Model 1. Lastly, for FastAlign, we initialized all parameters uniformly since this empirically was a more favorable initialization, as discussed in (Dyer et al., 2013). We measure the performance of the models in terms of Precision, Recall, F-Measure, and AER using only sure alignments in the definitions of the first 538 three metrics and sure and possible alignments in the definition of AER , as in (Simion et al., 2013) and (Marcu et al., 2006). For our experiments, we report results in both AER (lower is better) and F-Measure (higher is better) (Och and Ney, 2003). Table 1 shows the alignment summary statistics for the 447 sentences present in the Hansard test data. We present alignments quality scores using either the FastAlign IBM Model 2, the GIZA++ HMM, and our model and its relaxation using either the “HMM” or “Joint” decoding. First, we note that in deciding the decoding style for IBM2-HMM, the HMM method is better than the Joint method. We expected this type of performance since HMM decoding introduces positional dependance"
D16-1051,W03-0300,0,0.405115,"ptimal alignment that yields the highest probability alignment through generating technique p1 and p2 . This method of decoding is a lot like the HMM style and also relies 537 MJoint (j − 1, i0 ) = max {d(i0 |i, l)QJoint (j − 1, i)} , i=0 ∀ 2 ≤ j ≤ m, ∀ i0 ∈ [l]0 . The alignment results gotten by decoding with this method is labelled “Joint” in Table 1. 9 Experiments In this section we describe experiments using the IBM2-HMM optimization problem combined with the EM algorithm for parameter estimation. 9.1 Data Sets We use data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Michalcea and Pederson, 2003). We use the Canadian Hansards bilingual corpus, with 743,989 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (note that we use a randomly chosen subset of the original training set of 1.1 million sentences, similar to the setting used in (Moore, 2004)). The development and test data have been manually aligned at the word level, annotating alignments between source and target words in the corpus as either “sure” (S) or “possible” (P ) alignments, as described in (Och and Ney, 2003). As is standard, we lower-cased all words before"
D16-1051,C96-2141,0,0.79652,"model which combines the structure of the HMM and IBM Model 2 and show that its performance is very close to that of the HMM. There are several reasons why such a result would be of value (for more on this, see (Simion et al., 2013) and (Simion et al., 2015a), for example). Introduction The IBM translation models are widely used in modern statistical translation systems. Typically, one seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure. Among the IBM Models, perhaps the most elegant is the HMM model (Vogel et al., 1996). The HMM is the last model whose expectation step is ∗ Currently at Google. † Currently on leave at Google. • The main goal of this work is not to eliminate highly non-convex models such as the HMM entirely but, rather, to develop a new, powerful, convex alignment model and thus push the boundary of these theoretically justified techniques further. Building on the work of (Simion et al., 2015a), we derive a convex relaxation for the new model and show that its performance is close to that of the HMM. Although it does not beat the HMM, the new convex model improves upon the standard IBM Model"
D16-1051,J03-1002,0,0.228685,"enefits greatly from being seeded by the HMM (Och and Ney, 2003). In this paper we make the following contributions: Among the alignment models used in statistical machine translation (SMT), the hidden Markov model (HMM) is arguably the most elegant: it performs consistently better than IBM Model 3 and is very close in performance to the much more complex IBM Model 4. In this paper we discuss a model which combines the structure of the HMM and IBM Model 2. Using this surrogate, our experiments show that we can attain a similar level of alignment quality as the HMM model implemented in GIZA++ (Och and Ney, 2003). For this model, we derive its convex relaxation and show that it too has strong performance despite not having the local optima problems of non-convex objectives. In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013). 1 Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu • We derive a new alignment model which combines the structure of the HMM and IBM Model 2 and show that its performance i"
D16-1051,D13-1164,1,0.204435,"cal optima problems of non-convex objectives. In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013). 1 Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu • We derive a new alignment model which combines the structure of the HMM and IBM Model 2 and show that its performance is very close to that of the HMM. There are several reasons why such a result would be of value (for more on this, see (Simion et al., 2013) and (Simion et al., 2015a), for example). Introduction The IBM translation models are widely used in modern statistical translation systems. Typically, one seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure. Among the IBM Models, perhaps the most elegant is the HMM model (Vogel et al., 1996). The HMM is the last model whose expectation step is ∗ Currently at Google. † Currently on leave at Google. • The main goal of this work is not to eliminate highly non-convex models such as the HMM entirely but,"
D16-1051,D15-1023,1,0.165029,"-convex objectives. In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013). 1 Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu • We derive a new alignment model which combines the structure of the HMM and IBM Model 2 and show that its performance is very close to that of the HMM. There are several reasons why such a result would be of value (for more on this, see (Simion et al., 2013) and (Simion et al., 2015a), for example). Introduction The IBM translation models are widely used in modern statistical translation systems. Typically, one seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure. Among the IBM Models, perhaps the most elegant is the HMM model (Vogel et al., 1996). The HMM is the last model whose expectation step is ∗ Currently at Google. † Currently on leave at Google. • The main goal of this work is not to eliminate highly non-convex models such as the HMM entirely but, rather, to develop a new,"
D16-1051,P11-2081,0,0.0567955,"Missing"
D16-1051,P12-1033,0,0.0402542,"Missing"
D16-1051,N06-1015,0,\N,Missing
D16-1051,E14-4035,1,\N,Missing
D17-1157,D11-1003,1,0.845651,"Missing"
D17-1157,P05-1066,1,0.751053,"Missing"
D17-1157,D13-1176,0,0.0468392,"e in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have until recently been a stateof-the-art method for statistical machine translation, and Moses (Koehn et al., 2007) is one of the most used phrase-based translation systems. Moses uses a beam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right (Koehn et al., 2003). Neural machine translation systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which have given impressive improvements over phrase-based systems, also typically use models and decoders that construct the target-language string in strictly leftto-right order. Recently, Chang and Collins (2017) proposed a phrase-based decoding algorithm that processes the source-language string in strictly left-to-right order. Reordering is implemented by maintaining multiple sub-strings in the target-language, with phrases being used to extend these sub-strings by various operations (see Section 2 for a full description). With a fixed distorti"
D17-1157,2005.mtsummit-papers.11,0,0.110895,"Moses BLEU time 17.56 3m32s 26.69 7m37s 32.03 4m01s 23.73 3m37s 31.45 4m20s 28.41 3m36s 25.16 5m56s 31.05 3m31s 31.34 3m02s 20.95 2m40s Figure 3: Comparison of beam search under the new decoding algorithm and the Moses decoder. We show the BLEU score and the decoding time of three beam search based decoding methods. section describes experiments comparing beam search under the new algorithm to the method of Koehn et al. (2003). Throughout this section we refer to the algorithm of Chang and Collins (2017) as the “new” decoding algorithm. Data. We use the Europarl parallel corpus (Version 7)3 (Koehn, 2005) for all language pairs except for Vietnamese-English (vi-en). For CzechEnglish (cs-en), we use the Newstest2015 as the development set and Newstest2016 as the test set. For European languages other than Czech, we use the development and test set released for the Shared Task of WPT 20054 . For vi-en, we use the IWSLT’15 data. 3.1 Search Space with a Bigram Model We first analyze the properties of the algorithm by running the exact decoding algorithm with a bigram language model and a fixed distortion limit of four, with no pruning. In Figure 2, we plot the number of transitions computed versus"
D17-1157,P07-2045,0,0.00546709,"ht? An Empirical Comparison of Two Phrase-Based Decoding Algorithms Michael Collins∗ Google Research, New York mjcollins@google.com Yin-Wen Chang Google Research, New York yinwen@google.com Abstract This paper describes an empirical study of the phrase-based decoding algorithm proposed by Chang and Collins (2017). The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have until recently been a stateof-the-art method for statistical machine translation, and Moses (Koehn et al., 2007) is one of the most used phrase-based translation systems. Moses uses a beam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right (Koehn et al., 2003). Neural machine translation systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which have given impressive improveme"
D17-1157,N03-1017,0,0.349494,"arch, New York mjcollins@google.com Yin-Wen Chang Google Research, New York yinwen@google.com Abstract This paper describes an empirical study of the phrase-based decoding algorithm proposed by Chang and Collins (2017). The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have until recently been a stateof-the-art method for statistical machine translation, and Moses (Koehn et al., 2007) is one of the most used phrase-based translation systems. Moses uses a beam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right (Koehn et al., 2003). Neural machine translation systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which have given impressive improvements over phrase-based systems, also typically use models and decoders that construct the target"
D17-1157,J04-4002,0,0.11154,"lins@google.com Yin-Wen Chang Google Research, New York yinwen@google.com Abstract This paper describes an empirical study of the phrase-based decoding algorithm proposed by Chang and Collins (2017). The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have until recently been a stateof-the-art method for statistical machine translation, and Moses (Koehn et al., 2007) is one of the most used phrase-based translation systems. Moses uses a beam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right (Koehn et al., 2003). Neural machine translation systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which have given impressive improvements over phrase-based systems, also typically use models and decoders that construct the target-language string in"
D17-1157,Q17-1005,1,0.803469,"-art method for statistical machine translation, and Moses (Koehn et al., 2007) is one of the most used phrase-based translation systems. Moses uses a beam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right (Koehn et al., 2003). Neural machine translation systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which have given impressive improvements over phrase-based systems, also typically use models and decoders that construct the target-language string in strictly leftto-right order. Recently, Chang and Collins (2017) proposed a phrase-based decoding algorithm that processes the source-language string in strictly left-to-right order. Reordering is implemented by maintaining multiple sub-strings in the target-language, with phrases being used to extend these sub-strings by various operations (see Section 2 for a full description). With a fixed distortion limit on reordering, ∗ On leave from Columbia University. the time complexity of the algorithm is linear in terms of sentence length, and is polynomial time in other factors. Chang and Collins (2017) present the algorithm and give a proof of its time comple"
D17-1157,N16-1111,0,\N,Missing
D18-1405,J96-1002,0,0.0949345,"a and language modeling showing the effectiveness and trade-offs of both methods. 1 Introduction This paper considers parameter estimation in conditional models of the form p(y|x; ✓) = exp (s(x, y; ✓)) Z(x; ✓) (1) where s(x, y; ✓) is the unnormalized score of label y in conjunction with input x under parameters ✓, Y Pis a finite set of possible labels, and Z(x; ✓) = y2Y exp (s(x, y; ✓)) is the partition function for input x under parameters ✓. It is hard to overstate the importance of models of this form in NLP. In log-linear models, including both the original work on maximum-entropy models (Berger et al., 1996), and later work on conditional random fields (Lafferty et al., 2001), ⇤ † Part of this work done at Google. Work done at Google. the scoring function s(x, y; ✓) = ✓ · f (x, y) where f (x, y) 2 Rd is a feature vector, and ✓ 2 Rd are the parameters of the model. In more recent work on neural networks the function s(x, y; ✓) is a nonlinear function. In Word2Vec the scoring function is s(x, y; ✓) = ✓x · ✓y0 where y is a word in the context of word x, and ✓x 2 Rd and ✓y0 2 Rd are “inside” and “outside” word embeddings x and y. In many NLP applications the set Y is large. Maximum likelihood estimat"
D18-1405,J93-2004,0,0.0631423,"the feature space X . To make this clear, we fix n = 16000, d = 4, my = 100, K = 32 and experiment with mx = 100, 200, 300, 400. The results are summarized on the right panel of figure 2. As |X |increases, the KL divergence will grow while the performance of NCE-ranking/MLE is independent of |X |. Without the x-dependent bias term for NCE-binary, the KL divergence will be much higher due to lack of consistency (0.19, 0.21, 0.24, 0.26 respectively). 5.2 Language Modeling We evaluate the performance of the two NCE algorithms on a language modeling problem, using the Penn Treebank (PTB) dataset (Marcus et al., 1993). We choose (Zaremba et al., 2014) as the benchmark where the conditional distribution is modeled by two-layer LSTMs and the parameters are estimated by MLE (note that the current state-of-the-art is (Yang et al., 2018)). Zaremba et al. (2014) implemented 3 model configurations: “Small” , “Medium” and “Large”, which have 200, 650 and 1500 units per layer respectively. We follow their setup (model size, unrolled steps, dropout ratio, etc) but train the model by maximiz3705 MLE NCE K = 200 K = 400 K = 800 K = 1600 reg-MLE reg-Ranking (K = 1600) reg-Binary (K = 1600) Small 111.5 Ranking Binary 11"
D19-1219,N18-1038,0,0.0982743,"Missing"
D19-1219,P19-1506,1,0.799802,"is trained on ImageNet, and so it might be very good at recognizing objects, but might be unable to recognize human expressions and activities. Our models could have correctly answered the question in Example 4 if they were able to recognize smiles. Similarly our models could have ruled out the incorrect answer they picked for the question in Example 5 if they were able to see that both people in the picture are sitting down and are not moving. 6 Related Work Modeling visual contexts can aid in learning useful sentence representations (Kiela et al., 2017) and even in training language models (Ororbia et al., 2019). This paper takes these more general ideas to a downstream task that requires modeling of visual input. Similar to B2T2, VideoBERT (Sun et al., 2019) jointly processes video frames and text tokens with a Transformer architecture. However, VideoBERT cannot answer questions, nor does the model consider bounding boxes. Our B2T2 model is similar to the Bottom-Up Top-Down attention model (Anderson et al., 2018) in how bounding boxes generated at preprocessing time are attended to by the VQA model. “BottomUp” refers to the idea of attending from the text to the bounding boxes of objects detected in"
D19-1219,P18-1238,0,0.0831393,"Missing"
E14-1048,W09-1119,0,0.157746,"Missing"
E14-1048,W99-0613,1,0.404172,"¯1 ), . . . , (¯ xn , z¯n ) where x ¯ i = ΦT 1 xi ∈ k T k R , z¯i = Φ2 zi ∈ R , ∀i ∈ {1, 2, . . . , n}. Φ1 , Φ2 are chosen to maximize the correlation between x ¯i and z¯i , ∀i ∈ {1, 2, . . . , n}. Consider the setting where we have a label for the data point along with it’s two views and either view is sufficient to make accurate predictions. Kakade and Foster (2007) and Sridharan and Kakade (2008) give strong theoretical guarantees when the lower dimensional embeddings from CCA are used for predicting the label of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 • Previous two predictions yi−1 and yi−2 . The effectiveness of the dictionaries are evaluated by adding dictionary matches as features along with the baseline features (Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. We also compared the quality of the candidate ph"
E14-1048,W03-0430,0,0.114689,"“the” and “virus” in the simple pattern “the ... virus” during a single pass over the unlabeled document collection. This noisy list had a lot of virus names such as influenza, human immunodeficiency and Epstein-Barr along with phrases that are not virus names, like mutant, same, new, and so on. A similar rule like “the ... disease” did not give a good coverage of disease names since it is not the common way of how diseases are mentioned in publications. So we took a different approach CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1 , w2 , . . . , wn ) where wi , ∀i ∈ {1, 2, . . . , n} are words in the sentence. The output is a sequence of tags y1 , y2 , . . . , yn where yi ∈ {B, I, O}, ∀i ∈ {1, 2, . . . , n}. B is the tag given to the first word in a named entity, I is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER"
E14-1048,W12-2411,0,0.156848,"ctionary of named entities by removing the noisy candidates from the list obtained in the first step. This is done by learning a classifier using the lower dimensional, real-valued CCA (Hotelling, 1935) embeddings of the candidate phrases as features and training it using a small number of labeled examples. The classifier we use is a binary SVM which predicts whether a candidate phrase is a named entity or not. We compare our method to a widely used semisupervised algorithm based on co-training (Blum and Mitchell, 1998). The dictionaries are first evaluated on virus (GENIA, 2003) and disease (Dogan and Lu, 2012) NER by using them directly in dictionary based taggers. We also give results comparing the dictionaries produced by the two semi-supervised approaches with dictionaries that are compiled manually. The effectiveness of the dictionaries are also measured by injecting dictionary matches as features in a Conditional Random Field (CRF) based tagger. The results indicate that our approach with minimal supervision produces dictionaries that are comparable to dictionaries compiled manually. Finally, we also compare the quality of the candidate phrase embeddings with word embeddings (Dhillon et al., 2"
E14-1048,W06-2809,0,0.122547,"Missing"
E14-1048,W04-1221,0,0.0136618,"ment collection. This noisy list had a lot of virus names such as influenza, human immunodeficiency and Epstein-Barr along with phrases that are not virus names, like mutant, same, new, and so on. A similar rule like “the ... disease” did not give a good coverage of disease names since it is not the common way of how diseases are mentioned in publications. So we took a different approach CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1 , w2 , . . . , wn ) where wi , ∀i ∈ {1, 2, . . . , n} are words in the sentence. The output is a sequence of tags y1 , y2 , . . . , yn where yi ∈ {B, I, O}, ∀i ∈ {1, 2, . . . , n}. B is the tag given to the first word in a named entity, I is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: • Curr"
E14-1048,D07-1073,0,\N,Missing
E14-4035,J07-3002,0,0.3121,"Missing"
E14-4035,W04-3250,0,0.167178,"(Simion et al., 2013). We assume that our set of training examples is (e(k) , f (k) ) for k = 1 . . . n, where e(k) is the k’th English sentence and f (k) is the k’th French sentence. The k’th English sentence is a sequence of (k) (k) words e1 . . . elk where lk is the length of the k’th (k) English sentence, and each ei ∈ E; similarly (k) (k) the k’th French sentence is a sequence f1 . . . fmk (k) (k) where each fj ∈ F . We define e0 for k = 1 . . . n to be a special NULL word (note that E contains the NULL word). IBM Model 2 is detailed in several sources such as (Simion et al., 2013) and (Koehn, 2004). The convex and non-convex objectives of respectively IBM Model 1 and 2 can be found in (Simion 180 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 180–184, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2013). For I2CR-2, the convex relaxation of IBM Model 2, the objective is given by (k) (k) lk n mk X t(fj |ei ) 1 XX 0 log 2n (L + 1) j=1 i=0 In previous experiments, Simion et al. (Simion et al., 2013) were comparing I2CR-2 and IBM Model 2 using the standard alignment formula de"
E14-4035,N06-1014,0,0.153055,"xa (t(fj |e(k) a ) min {t(fj |ea ), d(a|j)}) . (1) (2) Experiments In this section we describe experiments using the I2CR-2 optimization problem combined with the stochastic EG algorithm (Simion et al., 2013) for parameter estimation. The experiments conducted here use a similar setup to those in (Simion et al., 2013). We first describe the data we use, and then describe the experiments we ran. 4.1 Alignment models have been compared using methods other than Viterbi comparisons; for example, Simion et al. (2013) use IBM Model 2’s optimal rule given by (see below) Eq. 2 to compare models while Liang et al. (2006) use posterior decoding. Here, we derive and use I2CR-2’s Viterbi alignment. To get the Viterbi alignment of a pair (e(k) , f (k) ) using I2CR-2 we need to find a(k) = (k) (k) (a1 , . . . , amk ) which yields the highest probability p(f (k) , a(k) |e(k) ). Referring to the I2CR-2 objective, this corresponds to finding a(k) that maximizes log 4 (k) = argmaxa (t(fj |e(k) a )d(a|j)) . Data Sets We use data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Michalcea and Pederson, 2003). We use the Canadian Hansards bilingual corpus, with 247,878 English-French sentence pairs as tr"
E14-4035,W06-1606,0,0.819649,"rive the Viterbi alignment for I2CR-2 and compare it directly with that of IBM Model 2. Previously, Simion et al. (2013) had compared IBM Model 2 and I2CR-2 by using IBM Model 2’s Viterbi alignment rule, which is not necessarily the optimal alignment for I2CR-2. Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu We show that by comparing I2CR-2 with IBM Model 2 by using each model’s optimal Viterbi alignment the convex model consistently has a higher F-Measure. F-Measure is an important metric because it has been shown to be correlated with BLEU scores (Marcu et al., 2006). Notation. We adopt the notation introduced in (Och and Ney, 2003) of having 1m 2n denote the training scheme of m IBM Model 1 EM iterations followed by initializing Model 2 with these parameters and running n IBM Model 2 EM iterations. The n notation EGm B 2 means that we run m iterations of I2CR-2’s EG algorithm (Simion et al., 2013) with batch size of B, initialize IBM Model 2 with I2CR2’s parameters, and then run n iterations of Model 2’s EM. 2 The IBM Model 1 and 2 Optimization Problems In this section we give a brief review of IBM Models 1 and 2 and the convex relaxation of Model 2, I2C"
E14-4035,W03-0300,0,0.820119,"le, Simion et al. (2013) use IBM Model 2’s optimal rule given by (see below) Eq. 2 to compare models while Liang et al. (2006) use posterior decoding. Here, we derive and use I2CR-2’s Viterbi alignment. To get the Viterbi alignment of a pair (e(k) , f (k) ) using I2CR-2 we need to find a(k) = (k) (k) (a1 , . . . , amk ) which yields the highest probability p(f (k) , a(k) |e(k) ). Referring to the I2CR-2 objective, this corresponds to finding a(k) that maximizes log 4 (k) = argmaxa (t(fj |e(k) a )d(a|j)) . Data Sets We use data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Michalcea and Pederson, 2003). We use the Canadian Hansards bilingual corpus, with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (note that we use a randomly chosen subset of the original training set of 1.1 million sentences, similar to the setting used in (Moore, 2004)). The development and test data have been manually aligned at the word level, annotating alignments between source and target words in the corpus as either “sure” (S) or “possible” (P ) alignments, as described in (Och and Ney, 2003). As a second data set, we used the RomanianEngli"
E14-4035,C96-2141,0,0.779107,"Missing"
E14-4035,J03-1002,0,0.203235,"hat of IBM Model 2. Previously, Simion et al. (2013) had compared IBM Model 2 and I2CR-2 by using IBM Model 2’s Viterbi alignment rule, which is not necessarily the optimal alignment for I2CR-2. Clifford Stein Columbia University IEOR Department New York, NY, 10027 cs2035@columbia.edu We show that by comparing I2CR-2 with IBM Model 2 by using each model’s optimal Viterbi alignment the convex model consistently has a higher F-Measure. F-Measure is an important metric because it has been shown to be correlated with BLEU scores (Marcu et al., 2006). Notation. We adopt the notation introduced in (Och and Ney, 2003) of having 1m 2n denote the training scheme of m IBM Model 1 EM iterations followed by initializing Model 2 with these parameters and running n IBM Model 2 EM iterations. The n notation EGm B 2 means that we run m iterations of I2CR-2’s EG algorithm (Simion et al., 2013) with batch size of B, initialize IBM Model 2 with I2CR2’s parameters, and then run n iterations of Model 2’s EM. 2 The IBM Model 1 and 2 Optimization Problems In this section we give a brief review of IBM Models 1 and 2 and the convex relaxation of Model 2, I2CR-2 (Simion et al., 2013). The standard approach in training parame"
E14-4035,D13-1164,1,0.504729,"bi alignment for the convex relaxation of IBM Model 2 and show that it leads to better F-Measure scores than those of IBM Model 2. 1 Introduction The IBM translation models are widely used in modern statistical translation systems. Unfortunately, apart from Model 1, the IBM models lead to non-convex objective functions, leading to methods (such as EM) which are not guaranteed to reach the global maximum of the log-likelihood function. In a recent paper, Simion et al. introduced a convex relaxation of IBM Model 2, I2CR-2, and showed that it has performance on par with the standard IBM Model 2 (Simion et al., 2013). In this paper we make the following contributions: • We explore some applications of I2CR-2. In particular, we show how this model can be used to seed IBM Model 2 and compare the speed/performance gains of our initialization under various settings. We show that initializing IBM Model 2 with a version of I2CR-2 that uses large batch size yields a method that has similar run time to IBM Model 1 initialization and at times has better performance. • We derive the Viterbi alignment for I2CR-2 and compare it directly with that of IBM Model 2. Previously, Simion et al. (2013) had compared IBM Model"
E14-4035,P11-2081,0,0.219607,"Missing"
E14-4035,P12-1033,0,0.362575,"Missing"
E14-4035,J93-2003,0,\N,Missing
E14-4035,N06-1015,0,\N,Missing
E14-4035,P04-1066,0,\N,Missing
H05-1064,P99-1069,0,0.0706373,"ain to many words; for instance, singular nouns such as bond, market, and bank all receive the same domain. The behavior of the model then emulates a clustering operation. Figure 2 shows the single–variable and pairwise features used in our experiments. The features are shown with hidden variables corresponding to word–specific hidden values, such as shares1 or bought3 . In our experiments, we made use of features such as those in Figure 2 in combination with the following four definitions of the hidden–value 3 We also performed some experiments using the conjugate gradient descent algorithm (Johnson et al., 1999). However, we did not find a significant difference between the performance of either method. Since stochastic gradient descent was faster and required less memory, our final experiments used the stochastic gradient method. frag replacements S(bought) VP(bought) VBD(bought) NP(shares) PP(in) NP(yesterday) bought 1.5m shares in heavy trading yesterday Single–variable features generated for (shares 1 ) = (shares1 ) (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares1 , (shares"
H05-1064,P05-1010,0,0.219037,"en et al., 2004), and generalizations of support– vector machines (Shen and Joshi, 2003). There have been several previous approaches to parsing using log–linear models and hidden variables. Riezler et al. (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) 508 treebank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f -structure. Clark and Curran (2004) present an alternative CCG parsing approach that divides each CCG parse into a dependency structure (observed) and a derivation (hidden). More recently, Matsuzaki et al. (2005) introduce a probabilistic CFG augmented with hidden information at each nonterminal, which gives their model the ability to tailor itself to the task at hand. The form of our model is closely related to that of Quattoni et al. (2005), who describe a hidden–variable model for object recognition in computer vision. The approaches of Riezler et al., Clark and Curran, and Matsuzaki et al. are similar to our own work in that the hidden variables are exponential in number and must be handled with dynamic– programming techniques. However, they differ from our approach in the definition of the hidden"
H05-1064,N04-1043,0,0.0200308,"Missing"
H05-1064,W00-1320,0,0.055282,"ynamic–programming approaches to training. Finally, these approaches use Viterbi or other approximations during decoding, something our model can avoid (see section 6.2). In some instantiations, our model effectively clusters words into categories. Our approach differs from standard word clustering in that the clustering criteria is directly linked to the reranking objective, whereas previous word–clustering approaches (e.g. Brown et al. (1992) or Pereira et al. (1993)) have typically leveraged distributional similarity. In other instantiations, our model establishes word– sense distinctions. Bikel (2000) has done previous work on incorporating the WordNet hierarchy into a generative parsing model; however, this approach requires data with word–sense annotations whereas our model deals with word–sense ambiguity through unsupervised discriminative training. 3 The Hidden–Variable Model In this section we describe a hidden–variable model based on conditional log–linear models. Each sentence si for i = 1 . . . n in our training data has a set of ni candidate parse trees ti,1 , . . . , ti,ni , which are the output of an N –best baseline parser. Each candidate parse has an associated F –measure scor"
H05-1064,P05-1022,0,0.159575,"e techniques described generalize naturally to NLP structures other than parse trees. 1 Introduction A number of recent approaches in statistical NLP have focused on reranking algorithms. In reranking methods, a baseline model is used to generate a set of candidate output structures for each input in training or test data. A second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline. Reranking approaches have given improvements in accuracy on a number of NLP problems including parsing (Collins, 2000; Charniak and Johnson, 2005), machine translation (Och and Ney, 2002; Shen et al., 2004), information extraction (Collins, 2002), and natural language generation (Walker et al., 2001). The success of reranking approaches depends critically on the choice of representation used by the reranking model. Typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to a feature–vector representation. Previous work has generally relied on two approaches to representation: explicitly hand–crafted features (e.g., in Charniak and Johnson (2005)) or features defined through kernels (e.g., see Collins"
H05-1064,W03-1022,0,0.0399931,"in. Part–of–Speech (Clustering) The part–of– speech tag of each word is split into five sub–values. In Figure 2, the word shares would be assigned the domain {NNS1 , . . . , NNS5 }, and the word bought would have the domain {VBD1 , . . . , VBD5 }. Highest Nonterminal (Clustering) The highest nonterminal to which each word propagates as a headword is split into five sub–values. In Figure 2 the word bought yields domain {S1 , . . . , S5 }, while in yields {PP1 , . . . , PP5 }. Supersense (Pre–Built Ontology) We borrow the idea of using WordNet lexicographer filenames as broad “supersenses” from Ciaramita and Johnson (2003). For each word, we split each of its 511 supersenses into three sub–supersenses. If no supersenses are available, we fall back to splitting the part–of–speech into five sub–values. For example, shares has the supersenses noun.possession, noun.act and noun.artifact, which yield the domain {noun.possession 1 , noun.act 1 , noun.artifact 1 , . . . noun.possession 3 , noun.act 3 , noun.artifact 3 }. On the other hand, in does not have any WordNet supersenses, so it is assigned the domain {IN1 , . . . , IN5 }. 4.2 The Final Feature Sets We created eight feature sets by combining the four hidden–va"
H05-1064,P04-1014,0,0.014663,"luding conditional log– linear models (Ratnaparkhi et al., 1994; Johnson et al., 1999), boosting methods (Collins, 2000), variants of the perceptron algorithm (Collins, 2002; Shen et al., 2004), and generalizations of support– vector machines (Shen and Joshi, 2003). There have been several previous approaches to parsing using log–linear models and hidden variables. Riezler et al. (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) 508 treebank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f -structure. Clark and Curran (2004) present an alternative CCG parsing approach that divides each CCG parse into a dependency structure (observed) and a derivation (hidden). More recently, Matsuzaki et al. (2005) introduce a probabilistic CFG augmented with hidden information at each nonterminal, which gives their model the ability to tailor itself to the task at hand. The form of our model is closely related to that of Quattoni et al. (2005), who describe a hidden–variable model for object recognition in computer vision. The approaches of Riezler et al., Clark and Curran, and Matsuzaki et al. are similar to our own work in tha"
H05-1064,P02-1034,1,0.703278,", 2005), machine translation (Och and Ney, 2002; Shen et al., 2004), information extraction (Collins, 2002), and natural language generation (Walker et al., 2001). The success of reranking approaches depends critically on the choice of representation used by the reranking model. Typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to a feature–vector representation. Previous work has generally relied on two approaches to representation: explicitly hand–crafted features (e.g., in Charniak and Johnson (2005)) or features defined through kernels (e.g., see Collins and Duffy (2002)). This paper describes a new method for the representation of NLP structures within reranking approaches. We build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses). Our method involves a hidden–variable model, where the hidden variables correspond to an assignment of words to either clusters or word–senses. Lexical items are automatically assigned their hidden values using unsupervised learning within a discri"
H05-1064,P02-1038,0,0.0100876,"structures other than parse trees. 1 Introduction A number of recent approaches in statistical NLP have focused on reranking algorithms. In reranking methods, a baseline model is used to generate a set of candidate output structures for each input in training or test data. A second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline. Reranking approaches have given improvements in accuracy on a number of NLP problems including parsing (Collins, 2000; Charniak and Johnson, 2005), machine translation (Och and Ney, 2002; Shen et al., 2004), information extraction (Collins, 2002), and natural language generation (Walker et al., 2001). The success of reranking approaches depends critically on the choice of representation used by the reranking model. Typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to a feature–vector representation. Previous work has generally relied on two approaches to representation: explicitly hand–crafted features (e.g., in Charniak and Johnson (2005)) or features defined through kernels (e.g., see Collins and Duffy (2002)). This paper describes"
H05-1064,P93-1024,0,0.078513,"the most similar). In addition, these three approaches don’t use reranking, so their features must be restricted to local scope in order to allow dynamic–programming approaches to training. Finally, these approaches use Viterbi or other approximations during decoding, something our model can avoid (see section 6.2). In some instantiations, our model effectively clusters words into categories. Our approach differs from standard word clustering in that the clustering criteria is directly linked to the reranking objective, whereas previous word–clustering approaches (e.g. Brown et al. (1992) or Pereira et al. (1993)) have typically leveraged distributional similarity. In other instantiations, our model establishes word– sense distinctions. Bikel (2000) has done previous work on incorporating the WordNet hierarchy into a generative parsing model; however, this approach requires data with word–sense annotations whereas our model deals with word–sense ambiguity through unsupervised discriminative training. 3 The Hidden–Variable Model In this section we describe a hidden–variable model based on conditional log–linear models. Each sentence si for i = 1 . . . n in our training data has a set of ni candidate pa"
H05-1064,P02-1035,0,0.0163758,"we describe generalize naturally to other NLP structures such as strings or labeled sequences. We discuss this point further in Section 6.1. 2 Related Work Various machine–learning methods have been used within reranking tasks, including conditional log– linear models (Ratnaparkhi et al., 1994; Johnson et al., 1999), boosting methods (Collins, 2000), variants of the perceptron algorithm (Collins, 2002; Shen et al., 2004), and generalizations of support– vector machines (Shen and Joshi, 2003). There have been several previous approaches to parsing using log–linear models and hidden variables. Riezler et al. (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) 508 treebank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f -structure. Clark and Curran (2004) present an alternative CCG parsing approach that divides each CCG parse into a dependency structure (observed) and a derivation (hidden). More recently, Matsuzaki et al. (2005) introduce a probabilistic CFG augmented with hidden information at each nonterminal, which gives their model the ability to tailor itself to the task at hand. The form of our model is clos"
H05-1064,W03-0402,0,0.0207207,"eranking approach described in Collins (2000). Although the experiments in this paper are focused on parsing, the techniques we describe generalize naturally to other NLP structures such as strings or labeled sequences. We discuss this point further in Section 6.1. 2 Related Work Various machine–learning methods have been used within reranking tasks, including conditional log– linear models (Ratnaparkhi et al., 1994; Johnson et al., 1999), boosting methods (Collins, 2000), variants of the perceptron algorithm (Collins, 2002; Shen et al., 2004), and generalizations of support– vector machines (Shen and Joshi, 2003). There have been several previous approaches to parsing using log–linear models and hidden variables. Riezler et al. (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) 508 treebank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f -structure. Clark and Curran (2004) present an alternative CCG parsing approach that divides each CCG parse into a dependency structure (observed) and a derivation (hidden). More recently, Matsuzaki et al. (2005) introduce a probabilistic CFG augmented with hidden information"
H05-1064,N04-1023,0,0.0194473,"han parse trees. 1 Introduction A number of recent approaches in statistical NLP have focused on reranking algorithms. In reranking methods, a baseline model is used to generate a set of candidate output structures for each input in training or test data. A second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline. Reranking approaches have given improvements in accuracy on a number of NLP problems including parsing (Collins, 2000; Charniak and Johnson, 2005), machine translation (Och and Ney, 2002; Shen et al., 2004), information extraction (Collins, 2002), and natural language generation (Walker et al., 2001). The success of reranking approaches depends critically on the choice of representation used by the reranking model. Typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to a feature–vector representation. Previous work has generally relied on two approaches to representation: explicitly hand–crafted features (e.g., in Charniak and Johnson (2005)) or features defined through kernels (e.g., see Collins and Duffy (2002)). This paper describes a new method for th"
H05-1064,N01-1003,0,0.0619119,"on reranking algorithms. In reranking methods, a baseline model is used to generate a set of candidate output structures for each input in training or test data. A second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline. Reranking approaches have given improvements in accuracy on a number of NLP problems including parsing (Collins, 2000; Charniak and Johnson, 2005), machine translation (Och and Ney, 2002; Shen et al., 2004), information extraction (Collins, 2002), and natural language generation (Walker et al., 2001). The success of reranking approaches depends critically on the choice of representation used by the reranking model. Typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to a feature–vector representation. Previous work has generally relied on two approaches to representation: explicitly hand–crafted features (e.g., in Charniak and Johnson (2005)) or features defined through kernels (e.g., see Collins and Duffy (2002)). This paper describes a new method for the representation of NLP structures within reranking approaches. We build on the intuition that"
H05-1064,J93-2004,0,\N,Missing
H05-1064,J03-4003,1,\N,Missing
H05-1064,J05-1003,1,\N,Missing
H05-1064,J92-4003,0,\N,Missing
H05-1064,P02-1062,1,\N,Missing
H05-1100,P05-1022,0,0.0214825,"Missing"
H05-1100,C02-1126,0,0.0511068,"Missing"
H05-1100,P99-1065,1,0.938214,"Missing"
H05-1100,W02-1001,1,0.0820937,"7 80.8 83.1 82.0 83.9 83.4 85.1 84.4 85.2 85.0 86.3 85.9 Table 4: Results after running the morphological and reranking models on test data. Row 1 is our baseline model. Row 2 is the morphological model that scored highest during development. Row 3 gives the accuracy of the reranking approach, when applied to n-best output from the model in Row 2. 5.1 The Effects of Morphology In our first experiments, we trained over 50 models, incorporating different morphological information into each in the way described in Section 3.1. Prior to running the parsers, we trained the POS tagger described in (Collins, 2002). The output from the tagger was used to assign a POS label for unknown words. We only attempted to parse sentences under 70 words in length. Table 3 describes some of the models we tried during development and gives results for each. Our baseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tagset containing almost no morphological information. The morphological models we show are meant to be representative of both the highest-scoring models and the performance of various morphological features. For instance, we found that, in"
H05-1100,J05-1003,1,0.052157,"hich will most likely be estimated reliably by the model. 3.2 The Reranking Model In the reranking model, we use an n-best version of the morphologically-rich parser to generate a number of candidate parse trees for each sentence in training and test data. These parse trees are then represented through a combination of the log probability under the initial model, together with a large number of global features. A reranking model uses the information from these features to derive a new ranking of the n-best parses, with the hope of improving upon the baseline model. Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. There are a variety of methods for training the parameters of the model. In this work, we use the algorithm described in (Bartlett et al., 2004), which applies the large-margin training criterion of support vector machines (Cortes and Vapnik, 1995) to the reranking problem. The motivation for the reranking model is that a wide variety of features, which can essentially be sensitive to arbitrary context in the parse trees, can be incorporated into the model. In our wor"
H05-1100,P03-1013,0,0.0556942,"Missing"
H05-1100,P05-1039,0,0.109257,"Missing"
H05-1100,J98-4004,0,0.0197957,"Missing"
H05-1100,P03-1056,0,0.0575827,"Missing"
H05-1100,moreno-etal-2000-treebank,0,0.068584,"Missing"
H05-1100,J03-4003,1,\N,Missing
H05-1100,P05-1038,0,\N,Missing
J03-4003,J94-4005,0,0.0829586,"babilistic model of an incremental parser, with good results in terms of both parse accuracy on the treebank and also perplexity scores for language modeling. Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervis"
J03-4003,W00-1320,0,0.129919,"Missing"
J03-4003,A97-1029,0,0.0671339,"s associated with the nonterminal, and ∆ is the distance measure) into the product PL1 (Li (lti ), c, p |P, H, w, t, ∆, LC) × PL2 (lwi |Li , lti , c, p, P, H, w, t, ∆, LC) These two probabilities are then smoothed separately. Eisner (1996b) originally used POS tags to smooth a generative model in this way. In each case the final estimate is e = λ1 e1 + (1 − λ1 )(λ2 e2 + (1 − λ2 )e3 ) where e1 , e2 , and e3 are maximum-likelihood estimates with the context at levels 1, 2, and 3 in the table, and λ1 , λ2 and λ3 are smoothing parameters, where 0 ≤ λi ≤ 1. We use the smoothing method described in Bikel et al. (1997), which is derived from a method described in Witten and Bell (1991). First, say that the most specific estimate e1 = nf11 ; that is, f1 is the value of the denominator count in the relative frequency estimate. Second, define u1 to be the number of distinct outcomes seen in the f1 events in training data. The variable u1 can take any value from one to f1 inclusive. Then we set f1 λ1 = f1 + 5u1 f 2 Analogous definitions for f2 and u2 lead to λ2 = f2 +5u . The coefficient five was chosen 2 to maximize accuracy on the development set, section 0 of the treebank (in practice it was found that any v"
J03-4003,H91-1060,0,0.100614,"Missing"
J03-4003,H92-1026,0,0.105337,"n this example, and in the examples in the rest of the article, for brevity we omit the part-of-speech tags associated with words, writing, for example S(bought) rather than S(bought,VBD). We emphasize that throughout the models in this article, each word is always paired with its part of speech, either when the word is generated or when the word is being conditioned upon. 3.1.1 Adding Distance to the Model. In this section we first describe how the model can be extended to be “history-based.” We then show how this extension can be utilized in incorporating “distance” features into the model. Black et al. (1992) originally introduced history-based models for parsing. Equations (3) and (4) of the current article made the independence assumption that each modifier is generated independently of the others (i.e., that the modifiers are generated independently of everything except P, H, and h). In general, however, the probability of generating each modifier could depend on any function of the previous modifiers, head/parent category, and headword. Moreover, if the top-down derivation order is fully specified, then the probability of generating a modifier can be conditioned on any structure that has been"
J03-4003,A00-2031,0,0.0925059,"bilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develop"
J03-4003,P01-1010,0,0.117361,"hrough a maximum-entropy-inspired model. The use of additional features gives clear improvements in performance. Collins (2000) shows similar improvements through a quite different model based on boosting approaches to reranking (Freund et al. 1998). An initial model—in fact Model 2 described in the current article—is used to generate N-best output. The reranking approach attempts to rerank the N-best lists using additional features that are not used in the initial model. The intention of this approach is to allow greater flexibility in the features that can be included in the model. Finally, Bod (2001) describes a very different approach (a DOP approach to parsing) that gives excellent results on treebank parsing, comparable to the results of Charniak (2000) and Collins (2000). 8.1 Comparison to the Model of Charniak (1997) We now give a more detailed comparison of the models in this article to the parser of Charniak (1997). The model described in Charniak (1997) has two types of parameters: 1. Lexical-dependency parameters. Charniak’s dependency parameters are similar to the L2 parameters of section 5.1. Whereas our parameters are PL2 (lwi |Li , lti , c, p, P, H, w, t, ∆, LC) Charniak’s pa"
J03-4003,P93-1035,0,0.0923397,"inivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000"
J03-4003,A00-2018,0,0.816101,"e Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develop"
J03-4003,P01-1017,0,0.107015,"djoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account. Collins et al. (1999) describe how the models in the current article were applied to parsing Czech. Charniak (2000) describes a parsing model that also use"
J03-4003,P98-1035,0,0.0751833,"Missing"
J03-4003,P00-1058,0,0.108372,"shawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a si"
J03-4003,W95-0103,1,0.11577,"xical dependencies approximate this notion of plausibility. Implicitly, we would be just as well off (maybe even better off) if statistics were calculated between items at the predicate-argument level, with no reference to structure. The distance preferences under this interpretation are just a way of mitigating sparse-data problems: When the lexical statistics are too sparse, then falling back on some structural preference is not ideal, but is at least better than chance. This hypothesis is suggested by previous work on specific cases of attachment ambiguity such as PP attachment (see, e.g., Collins and Brooks 1995), which has showed that models will perform better given lexical statistics, and that a straight structural preference is merely a fallback. But some examples suggest this is not the case: that, in fact, many sentences have several equally semantically plausible analyses, but that structural preferences 619 Computational Linguistics Volume 29, Number 4 distinguish strongly among them. Take the following example (from Pereira and Warren 1980): (4) John was believed to have been shot by Bill. Surprisingly, this sentence has two analyses: Bill can be the deep subject of either believed or shot. Y"
J03-4003,P96-1025,1,0.180448,"H, and h). In general, however, the probability of generating each modifier could depend on any function of the previous modifiers, head/parent category, and headword. Moreover, if the top-down derivation order is fully specified, then the probability of generating a modifier can be conditioned on any structure that has been previously generated. The remainder of this article assumes that the derivation order is depth-first: that is, each modifier recursively generates the subtree below it before the next modifier is generated. (Figure 3 gives an example that illustrates this.) The models in Collins (1996) showed that the distance between words standing in head-modifier relationships was important, in particular, that it is important to capture a preference for right-branching structures (which almost translates into a preference for dependencies between adjacent words) and a preference for dependencies not to cross a verb. In this section we describe how this information can be incorporated into model 1. In section 7.2, we describe experiments that evaluate the effect of these features on parsing accuracy. Figure 3 A partially completed tree derived depth-first. “????” marks the position of th"
J03-4003,P97-1003,1,0.350496,"Institute of Technology, 545 Technology Square, Cambridge, MA 02139. E-mail: mcollins@ai.mit.edu. c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 4 et al. (1992). In a history-based model, a parse tree is represented as a sequence of decisions, the decisions being made in some derivation of the tree. Each decision has an associated probability, and the product of these probabilities defines a probability distribution over possible derivations. We first describe three parsing models based on this approach. The models were originally introduced in Collins (1997); the current article1 gives considerably more detail about the models and discusses them in greater depth. In Model 1 we show one approach that extends methods from probabilistic context-free grammars (PCFGs) to lexicalized grammars. Most importantly, the model has parameters corresponding to dependencies between pairs of headwords. We also show how to incorporate a “distance” measure into these models, by generalizing the model to a history-based approach. The distance measure allows the model to learn a preference for close attachment, or right-branching structures. In Model 2, we extend th"
J03-4003,P99-1065,1,0.300767,"Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account. Collins et al. (1999) describe how the models in the current article were applied to parsing Czech. Charniak (2000) describes a parsing model that also uses Markov processes to generate rules. The model takes into account much additional context (such as previously generated modifiers, or nonterminals higher in the parse trees) through a maximum-entropy-inspired model. The use of additional features gives clear improvements in performance. Collins (2000) shows similar improvements through a quite different model based on boosting approaches to reranking (Freund et al. 1998). An initial model—in fact Model 2 descri"
J03-4003,C96-1058,0,0.262918,", c, p, P, H, w, t, ∆, LC Li , lti , c, p, P, H, t, ∆, LC lti 5. Practical Issues 5.1 Parameter Estimation Table 1 shows the various levels of back-off for each type of parameter in the model. Note that we decompose PL (Li (lwi , lti ), c, p |P, H, w, t, ∆, LC) (where lwi and lti are the word and POS tag generated with nonterminal Li , c and p are the coord and punc flags associated with the nonterminal, and ∆ is the distance measure) into the product PL1 (Li (lti ), c, p |P, H, w, t, ∆, LC) × PL2 (lwi |Li , lti , c, p, P, H, w, t, ∆, LC) These two probabilities are then smoothed separately. Eisner (1996b) originally used POS tags to smooth a generative model in this way. In each case the final estimate is e = λ1 e1 + (1 − λ1 )(λ2 e2 + (1 − λ2 )e3 ) where e1 , e2 , and e3 are maximum-likelihood estimates with the context at levels 1, 2, and 3 in the table, and λ1 , λ2 and λ3 are smoothing parameters, where 0 ≤ λi ≤ 1. We use the smoothing method described in Bikel et al. (1997), which is derived from a method described in Witten and Bell (1991). First, say that the most specific estimate e1 = nf11 ; that is, f1 is the value of the denominator count in the relative frequency estimate. Second,"
J03-4003,W02-1009,0,0.0922708,"generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Mil"
J03-4003,P99-1059,0,0.193913,"rithm The parsing algorithm for the models is a dynamic programming algorithm, which is very similar to standard chart parsing algorithms for probabilistic or weighted grammars. The algorithm has complexity O(n5 ), where n is the number of words in the string. In practice, pruning strategies (methods that discard lower-probability constituents in the chart) can improve efficiency a great deal. The appendices of Collins (1999) give a precise description of the parsing algorithms, an analysis of their computational complexity, and also a description of the pruning methods that are employed. See Eisner and Satta (1999) for an O(n4 ) algorithm for lexicalized grammars that could be applied to the models in this paper. Eisner and Satta (1999) also describe an O(n3 ) algorithm for a restricted class of lexicalized grammars; it is an open question whether this restricted class includes the models in this article. 6. Results The parser was trained on sections 2–21 of the Wall Street Journal portion of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) (approximately 40,000 sentences) and tested on section 23 (2,416 sentences). We use the PARSEVAL measures (Black et al. 1991) to compare performance: La"
J03-4003,W01-0521,0,0.119322,"Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account. Collins et al. (1999) describe how the models in the current article were applied to parsing Czech. Charniak (2000) describes a parsing model that also uses Markov processes to generate rules. The model takes into account much"
J03-4003,1997.iwpt-1.13,0,0.18404,"branching representation; (a ) our modification of the Penn Treebank style to differentiate recursive and nonrecursive NPs (in some sense NPB is a bar 1 structure and NP is a bar 2 structure). 1. Transform training data trees into the new representation and train the model. 2. Recover parse trees in the new representation when running the model over test data sentences. 3. Convert the test output back into the treebank representation for scoring purposes. As long as there is a one-to-one mapping between the treebank and the new representation, nothing is lost in making such a transformation. Goodman (1997) and Johnson (1997) both suggest this strategy. Goodman (1997) converts the treebank into binary-branching trees. Johnson (1997) considers conversion to a number of different representations and discusses how this influences accuracy for nonlexicalized PCFGs. The models developed in this article have tacitly assumed the Penn Treebank style of annotation and will perform badly given other representations (for example, binary-branching trees). This section makes this point more explicit, describing exactly what annotation style is suitable for the models and showing how other annotation styles w"
J03-4003,P97-1062,0,0.105037,"cribe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word sense information into the parsing process. Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recov"
J03-4003,P91-1030,0,0.0914038,"heir application to parsing the treebank. Chelba and Jelinek (1998) describe an incremental, history-based parsing approach that is applied to language modeling for speech recognition. History-based approaches were introduced to parsing in Black et al. (1992). Roark (2001) describes a generative probabilistic model of an incremental parser, with good results in terms of both parse accuracy on the treebank and also perplexity scores for language modeling. Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “"
J03-4003,H94-1052,0,0.0756659,"Missing"
J03-4003,C94-1024,0,0.115618,"ndently of one another often leads to incorrect parses. (See Figure 6 for examples.) 3.2.1 Identifying Complements and Adjuncts in the Penn Treebank. We add the -C suffix to all nonterminals in training data that satisfy the following conditions: 1. 598 The nonterminal must be (1) an NP, SBAR, or S whose parent is an S; (2) an NP, SBAR, S, or VP whose parent is a VP; or (3) an S whose parent is an SBAR. Collins Head-Driven Statistical Models for NL Parsing 2. The nonterminal must not have one of the following semantic tags: ADV, VOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP. See Marcus et al. (1994) for an explanation of what these tags signify. For example, the NP Last week in figure 2 would have the TMP (temporal) tag, and the SBAR in (SBAR because the market is down) would have the ADV (adverbial) tag. 3. The nonterminal must not be on the RHS of a coordinated phrase. For example, in the rule S → S CC S, the two child Ss would not be marked as complements. In addition, the first child following the head of a prepositional phrase is marked as a complement. 3.2.2 Probabilities over Subcategorization Frames. Model 1 could be retrained on training data with the enhanced set of nonterminal"
J03-4003,W02-1002,0,0.109473,"estimation techniques that they use: maximum-entropy modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al. 1994 and Magerman 1995). A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers. We give examples in which this method leads to the parameters’ unnecessarily fragmenting the training data in some cases or ignoring important context in other cases. Similar observations have been made in the context of tagging problems using maximum-entropy models (Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002). We first analyze the model of Magerman (1995) through three common examples of ambiguity: PP attachment, coordination, and appositives. In each case a word sequence S has two competing structures, T1 and T2 , with associated decision sequences d1 , . . . , dn  and e1 , . . . , em , respectively. Thus the probability of the two structures can be written as P(T1 |S) =  P(di |d1 . . . di−1 , S) i=1...n P(T2 |S) =  P(ei |e1 . . . ei−1 , S) i=1...m It will be useful to isolate the decision between the two structures to a single probability term. Let the value j be the minimum value of i suc"
J03-4003,P95-1037,0,0.446494,"ria in Booth and Thompson (1973), it can be shown that convergence of rule probabilities implies that the distribution over trees will converge to that of the underlying PCFG, at least when Kullback-Liebler divergence or the infinity norm is taken to be the measure of distance between the two distributions. Thanks to Tommi Jaakkola and Nathan Srebro for discussions on this topic. 3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999). The rules are a modified version of a head table provided by David Magerman and used in the parser described in Magerman (1995). 592 Collins Head-Driven Statistical Models for NL Parsing Internal rules TOP → S S → NP NP VP NP → JJ NN NP → NNP VP → VBD NP NP → NNP Lexical rules JJ → Last NN → week NNP → IBM VBD → bought NNP → Lotus Figure 1 A nonlexicalized parse tree and a list of the rules it contains. Internal Rules: TOP S(bought,VBD) NP(week,NN) NP(IBM,NNP) VP(bought,VBD) NP(Lotus,NNP) → → → → → → S(bought,VBD) NP(week,NN) JJ(Last,JJ) NNP(IBM,NNP) VBD(bought,VBD) NNP(Lotus,NNP) Lexical Rules: JJ(Last,JJ) NN(week,NN) NNP(IBM,NNP) VBD(bought,VBD) NNP(Lotus,NN) → → → → → Last week IBM bought Lotus NP(IBM,NNP) NN(week,"
J03-4003,H94-1020,0,0.097311,"nerated independently of one another often leads to incorrect parses. (See Figure 6 for examples.) 3.2.1 Identifying Complements and Adjuncts in the Penn Treebank. We add the -C suffix to all nonterminals in training data that satisfy the following conditions: 1. 598 The nonterminal must be (1) an NP, SBAR, or S whose parent is an S; (2) an NP, SBAR, S, or VP whose parent is a VP; or (3) an S whose parent is an SBAR. Collins Head-Driven Statistical Models for NL Parsing 2. The nonterminal must not have one of the following semantic tags: ADV, VOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP. See Marcus et al. (1994) for an explanation of what these tags signify. For example, the NP Last week in figure 2 would have the TMP (temporal) tag, and the SBAR in (SBAR because the market is down) would have the ADV (adverbial) tag. 3. The nonterminal must not be on the RHS of a coordinated phrase. For example, in the rule S → S CC S, the two child Ss would not be marked as complements. In addition, the first child following the head of a prepositional phrase is marked as a complement. 3.2.2 Probabilities over Subcategorization Frames. Model 1 could be retrained on training data with the enhanced set of nonterminal"
J03-4003,J93-2004,0,0.097441,"rror rate (number of incorrect trees) on newly drawn test examples. Hence if the data are generated by a PCFG, and there are enough training examples for the maximum-likelihood estimates to converge to the true values, then this parsing method will be optimal. In practice, these assumptions cannot be verified and are arguably quite strong, but these limitations have not prevented generative models from being successfully applied to many NLP and speech tasks. (See Collins [2002] for a discussion of other ways of conceptualizing the parsing problem.) In the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), which is the source of data for our experiments, the rules are either internal to the tree, where LHS is a nonterminal and RHS is a string of one or more nonterminals, or lexical, where LHS is a part-of-speech tag and RHS is a word. (See Figure 1 for an example.) 2.2 Lexicalized PCFGs A PCFG can be lexicalized3 by associating a word w and a part-of-speech (POS) tag t with each nonterminal X in the tree. (See Figure 2 for an example tree.) The PCFG model can be applied to these lexicalized rules and trees in exactly the same way as before. Whereas before the nonterminals were simple (for exam"
J03-4003,A00-2030,0,0.0973597,"02) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules. Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account. Collins et al. (1999) describe how the models in the current article were applied to parsing Czech. Charniak (2000) describes a parsing model that also uses Markov processes to generate rules. The model takes into account much additional context (such as previously generated modifiers, or nonterminals higher in the parse trees) through a maximum-entropy-inspired mo"
J03-4003,W96-0213,0,0.328623,"2 to maximize accuracy on the development set, section 0 of the treebank (in practice it was found that any value in the range 2–5 gave a very similar level of performance). 5.2 Unknown Words and Part-of-Speech Tagging All words occurring less than six times14 in training data, and words in test data that have never been seen in training, are replaced with the UNKNOWN token. This allows the model to handle robustly the statistics for rare or new words. Words in test data that have not been seen in training are deterministically assigned the POS tag that is assigned by the tagger described in Ratnaparkhi (1996). As a preprocessing step, the 14 In Collins (1999) we erroneously stated that all words occuring less than five times in training data were classified as “unknown.” Thanks to Dan Bikel for pointing out this error. 606 Collins Head-Driven Statistical Models for NL Parsing tagger is used to decode each test data sentence. All other words are tagged during parsing, the output from Ratnaparkhi’s tagger being ignored. The POS tags allowed for each word are limited to those that have been seen in training data for that word (any tag/word pairs not seen in training would give an estimate of zero in"
J03-4003,W97-0301,0,0.107955,"treebank annotated in a different style: In this case we simply recommend transforming the trees into flat, one-levelper-X-bar-level trees before training the model, as in the three-step procedure outlined above. Other models in the literature are also very likely to be sensitive to annotation style. Charniak’s (1997) models will most likely perform quite differently with binarybranching trees (for example, his current models will learn that rules such as VP → V SG PP are very rare, but with binary-branching structures, this context sensitivity will be lost). The models of Magerman (1995) and Ratnaparkhi (1997) use contextual predicates that would most likely need to be modified given a different annotation style. Goodman’s (1997) models are the exception, as he already specifies that the treebank should be transformed into his chosen representation, binary-branching trees. 7.3.1 Representation Affects Structural, not Lexical, Preferences. The alternative representations in Figures 16 and 17 have the same lexical dependencies (providing that the binary-branching structures are centered about the head of the phrase, as in the examples). The difference between the representations involves structural p"
J03-4003,C92-2065,0,0.0767913,"ier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in"
J03-4003,J01-2004,0,0.116586,"next two sections we give a detailed comparison of the models in this article to the lexicalized PCFG model of Charniak (1997) and the history-based models of Jelinek et al. (1994), Magerman (1995), and Ratnaparkhi (1997). For discussion of additional related work, chapter 4 of Collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998. Of particular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al. 1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman 1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001). Eisner (1996a, 1996b) describes several dependency-based models that are also closely related to the models in this article. Collins (1996) also describes a dependency-based model applied to treebank parsing. Goodman (1997) describes probabilistic feature grammars and their application to parsing the treebank. Chelba and Jelinek (1998) describe an incremental, history-based parsing approach that is applied to language modeling for speech recognition. History-based approaches were introduced to parsing in Black et al. (1992). Roark (2001) describes a generative probabilistic model of an incre"
J03-4003,C92-2066,0,0.0868969,"s of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current arti"
J03-4003,1993.iwpt-1.20,0,0.0818079,"vance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all"
J03-4003,A92-1014,0,0.107003,"lack et al. (1992). Roark (2001) describes a generative probabilistic model of an incremental parser, with good results in terms of both parse accuracy on the treebank and also perplexity scores for language modeling. Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity. For work that uses lexical relations for parse disambiguation— all with very promising results—see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexical"
J03-4003,J92-1004,0,0.129063,"models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative “supertagging” model for tree-adjoining grammars. See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training. Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article. Finally, note that not all machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems. In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel 628 Collins Head-Driven Statistical Models for NL Parsing (2000) has developed generative statistical models that integrate word"
J03-4003,W98-1206,0,\N,Missing
J03-4003,H93-1047,0,\N,Missing
J03-4003,C98-1035,0,\N,Missing
J03-4003,P96-1023,0,\N,Missing
J03-4003,P93-1005,0,\N,Missing
J03-4003,W01-1802,1,\N,Missing
J05-1003,J97-4005,0,0.0331977,"nvolving around one million parse trees and over 500,000 features. The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds. The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the ‘‘naive’’ implementation). The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-line"
J05-1003,W03-1019,0,0.0223334,"Missing"
J05-1003,J96-1002,0,0.0173535,"Missing"
J05-1003,H92-1026,0,0.0187531,"imental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from the"
J05-1003,A00-2018,0,0.331711,"tion, the improved algorithm requires total computation that is equivalent to a mere 37.1 passes over the training set. This is a saving of a factor of 2,692 over the naive algorithm. Table 4 shows the value of Savingsða,bÞ for various values of ða,bÞ. It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected. Even so, there are still savings of a factor of almost 50 in the early stages of the method. 6. Related Work 6.1 History-Based Models with Complex Features Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997). The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods). Our features are in many ways similar to those of Charniak (2000). The model in Charniak (2000) is quite different, however. The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratn"
J05-1003,P97-1003,1,0.521324,"sses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X YY given training examples ðxi , yi Þ, where xi Z X , yi Z Y. We define GENðxÞÎY"
J05-1003,W02-1001,1,0.209985,"scribe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question. Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and T"
J05-1003,P02-1034,1,0.501895,"Missing"
J05-1003,N03-1014,0,0.00628668,"more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X YY given training examples ðxi , yi Þ, where xi Z X , yi Z Y. We define GENðxÞÎY to be the set of candid"
J05-1003,P99-1069,0,0.078602,"ar history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X YY given training examples ðxi , yi Þ, where xi Z X , yi Z Y. We define GENðxÞÎY to be the set of candidates for a given input x. In the parsing problem x is a sentence, and 1 Note, however, that log-linear models which employ regularization methods instead of feature selection—see, for example, Johnson et al. (1999) and Lafferty, McCallum, and Pereira (2001)—are likely to be comparable in terms of efficiency to our feature selection approach. See section 6.3 for more discussion. 27 Computational Linguistics Volume 31, Number 1 GENðxÞ is a set of candidate trees for that sentence. A particular characteristic of the problem is the complexity of GENðxÞ : GENðxÞ can be very large, and each member of GENðxÞ has a rich internal structure. This contrasts with ‘‘typical’’ classification problems in which GENðxÞ is a fixed, small set, for example, f1; þ1g in binary classification problems. In probabilistic appro"
J05-1003,W02-2018,0,0.0263705,"gLoss(a¯ ) with respect to a¯ , for example, generalized or improved iterative scaling (Berger, Della Pietra, and 4 It might seem to be a restriction to have the hyperplane passing through the origin of the space. However if a constant ‘‘bias’’ feature hmþ1 ðxÞ  1 for all x is added to the representation, a hyperplane passing through the origin in this new space is equivalent to a hyperplane in general position in the original m-dimensional space. 31 Computational Linguistics Volume 31, Number 1 Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient methods (Malouf 2002). In the next section we describe feature selection methods, as described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra, and Lafferty (1997). Once the parameters a¯ are estimated on training examples, the output for an example x is the most likely label under the model, arg max Pðy j x, a¯ Þ  arg yZY max yZf1,þ1g yFðx, a¯ Þ  signðFðx, a¯ ÞÞ ð5Þ where as before, sign ðzÞ  1 if z  0, sign ðzÞ  1 otherwise. Thus we see that the logistic regression model implements a hyperplane classifier. In boosting, a different loss function is used, namely, ExpLoss(a¯ )"
J05-1003,J93-2004,0,0.0366926,"Missing"
J05-1003,P02-1038,0,0.143927,"Missing"
J05-1003,W97-0301,0,0.304514,"the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X YY given training examples ðxi , yi Þ, where x"
J05-1003,P02-1035,0,0.0530129,"he additional features can in principle be any predicates over sentence/tree pairs. Evidence from the initial loglikelihood and the additional features is combined using a linear model. Parameter estimation becomes a problem of learning how to combine these different sources of information. The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and Och and Ney (2002). Section 4.1 gives a formal definition of the reranking problem. Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2. Section 4.3 describes a general approach to feature selection methods with these loss functions. Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function. 4.1 Problem Definition We use the"
J05-1003,W04-3223,0,0.411764,"aximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the"
J05-1003,N03-1028,0,0.162604,"Missing"
J05-1003,W03-1012,0,0.063498,"Missing"
J05-1003,N01-1003,0,0.00564306,"Missing"
J05-1003,W03-1020,0,0.167244,"he earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems;"
J05-1003,J03-4003,1,\N,Missing
J05-1003,P93-1005,0,\N,Missing
J05-1003,P02-1062,1,\N,Missing
J99-3006,P97-1003,1,0.724665,"Missing"
J99-3006,1997.iwpt-1.13,0,0.0265317,"Missing"
J99-3006,J93-2004,0,0.0251222,"Missing"
J99-3006,J93-2006,0,0.0356777,"Missing"
N13-1015,H91-1060,0,0.284786,"e a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the model on section 22 (development data) at different iteration numbers. Table 1 shows that a peak level of accuracy is reached for all values of m, other than m = 8, at iteration 20–30, with som"
N13-1015,W08-2102,1,0.366224,"Missing"
N13-1015,P05-1022,0,0.193929,"Missing"
N13-1015,P12-1024,1,0.782914,"iments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the data where parameter values are cal"
N13-1015,J03-4003,1,0.155015,"Missing"
N13-1015,D12-1019,1,0.792489,"Missing"
N13-1015,P96-1024,0,0.481709,", 2, . . . n}. 149 The parsing problem is to take a sentence as input, and produce a skeletal tree as output. A standard method for parsing with L-PCFGs is as follows. First, for a given input sentence x1 . . . xn , for any triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n, the marginal µ(a, i, j) is defined as X p(t) (1) µ(a, i, j) = t:(a,i,j)∈t where the sum is over all skeletal trees t for x1 . . . xn that include non-terminal a spanning words xi . . . xj . A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find P arg maxt (a,i,j)∈t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is"
N13-1015,P12-1046,0,0.0332165,"Missing"
N13-1015,P03-1054,0,0.0369327,"D∗ N. • The two-level and three-level rule fragments above the foot node. In the above example these features would be VP V S NP D∗ NP N VP V NP D∗ N • The label of the foot node, together with the label of its parent. In the above example this is (D, NP). • The label of the foot node, together with the label of its parent and grandparent. In the above example this is (D, NP, VP). • The part of speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of 4 We use the English head rules from the Stanford parser (Klein and Manning, 2003). 152 the foot node. In the above example this is N. • The width of the span to the left of the foot node, paired with the label of the foot node. • The width of the span to the right of the foot node, paired with the label of the foot node. Scaling of features. The features defined above are almost all binary valued features. We scale the features in the following way. For each feature φi (t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be s M φi (t) × count(i) + κ where κ is a smoothing ter"
N13-1015,E12-1042,0,0.573193,"Missing"
N13-1015,J93-2004,0,0.0490646,"ds to the lowest probability parse being output under the model). We suspect that this is because in some cases a dominant parameter has had its sign flipped due to sampling error; more theoretical and empirical work is required in fully understanding this issue. 4 Experiments In this section we describe parsing experiments using the L-PCFG estimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the init"
N13-1015,D10-1004,0,0.0243085,"example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the mo"
N13-1015,P05-1010,0,0.673932,"uarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a"
N13-1015,N07-1051,0,0.036876,"Missing"
N13-1015,P06-1055,0,0.331212,"lexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the d"
N13-1052,H91-1060,0,0.485743,"for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,i,j)∈τ Here we write (a, i, j) ∈ τ if nonterminal a spans words xi · · · xj in the parse tree τ . Inputs: Sentence x1 · · ·"
N13-1052,W03-3005,0,0.0380165,"s of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section int"
N13-1052,N06-1022,0,0.0230182,"ncluding beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for"
N13-1052,P12-1024,1,0.92836,"approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section introduces the special representation for probabilistic context-free grammars that we adopt in this paper, along with the decoding algorithm that we investigate. For an integer i ≥ 1, we let [i] = {1, 2, . . . ,"
N13-1052,W05-1504,0,0.0275876,"ee grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on"
N13-1052,P96-1024,0,0.442285,"Bayes-Risk Decoding Let z = x1 · · · xN be some input sentence; we write T (z) to denote the set of all possible trees for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,"
N13-1052,P03-1054,0,0.0240128,"up of a factor of 4.75 for Arabic (r = 140) and 6.5 for English (r = 260) while retaining similar performance. Perhaps more surprising is that using the tensor approximation actually improves performance in several cases. We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar, and therefore leads to better generalization than a vanilla maximum likelihood estimate. We include results for a more complex model for Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (Klein and Manning, 2003). This grammar includes 2,188 binary rules. Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average) with an F1 measure of 64.43. Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F1 measure of 64.05. The probability of Pa sentence z under a PCFG is defined as p(z) = τ ∈T (z) p(τ ), and can be approximated using the algorithm in Section 4.3, running in time O(rN 3 + rmN 2 ). Of theoretical interest, we discuss here a time O(rN 3 + r2 N 2 ) algorithm, which is more convenient when r &lt; m. Observe that in Eq. (3) vector"
N13-1052,J93-2004,0,0.0421226,"Missing"
N13-1052,P05-1010,0,0.121189,"s algorithm does not enforce that the output parse trees are included in the tree language of the PCFG, that is, certain combinations of children and parent nonterminals may violate the rules in the grammar. In our experiments we departed from this, and changed Goodman’s algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1. The reason this is important for our experiments is that we binarize the grammar prior to parsing, and we need to enforce the links between the split nonterminals (in the binarized grammar) that refer to the same syntactic category. See Matsuzaki et al. (2005) for more details about the binarization scheme we used. This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar (figure 1). However, empirically, it is the insideoutside algorithm which takes most of the time to compute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulatio"
N13-1052,J00-1003,0,0.0675288,"mproves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get significant improvements in parsing speed. 1 Introduction The problem of speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters"
N13-4005,E12-1042,0,\N,Missing
N13-4005,J92-4003,0,\N,Missing
N13-4005,P06-1055,0,\N,Missing
N13-4005,P05-1010,0,\N,Missing
N13-4005,W99-0613,1,\N,Missing
N13-4005,D12-1019,1,\N,Missing
N13-4005,N13-1015,1,\N,Missing
N13-4005,W97-0309,0,\N,Missing
N13-4005,P96-1024,0,\N,Missing
N19-1300,Q19-1026,1,0.809429,"s a result, they can be used to construct highly inferential reading comprehension datasets that have the added benefit of being directly related to the practical end-task of answering user yes/no questions. Yes/No questions do appear as a subset of some existing datasets (Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018). However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks,"
N19-1300,D15-1075,0,0.265278,"Missing"
N19-1300,P17-1152,0,0.0881736,"Missing"
N19-1300,D18-1241,0,0.0608786,"Missing"
N19-1300,L18-1269,0,0.0405572,"Missing"
N19-1300,N18-2017,0,0.0305137,"consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find su"
N19-1300,D17-1215,0,0.0358614,"), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find such questions often qu"
N19-1300,P17-1147,0,0.100114,"Missing"
N19-1300,P19-1334,0,0.0296322,"Missing"
N19-1300,D18-1260,0,0.0376092,"Missing"
N19-1300,L18-1008,0,0.0292594,"Missing"
N19-1300,D16-1244,0,0.0948182,"Missing"
N19-1300,N18-1202,1,0.559304,"e detail: Our Recurrent model follows a standard recurrent plus attention architecture for text-pair classification (Wang et al., 2018). It embeds the premise/hypothesis text using fasttext word vectors (Mikolov et al., 2018) and learned character vectors, applies a shared bidirectional LSTM to both parts, applies co-attention (Parikh et al., 2016) to share information between the two parts, applies another bi-LSTM to both parts, pools the result, and uses the pooled representation to predict the final class. See Appendix A.2 for details. Our Recurrent +ELMo model uses the language model from Peters et al. (2018) to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors. Our OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from Radford et al. (2018), which has been pretrained as a language model on the Books corpus (Zhu et al., 2015). Our BERTL model fine-tunes the 24 layer 1024 dimensional transformer from Devlin et al. (2018), which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia. We fine-tune the BERTL and the OpenAI GPT models using the optimizers recommende"
N19-1300,W18-5441,0,0.0742579,"Missing"
N19-1300,P18-2124,0,0.099408,"Missing"
N19-1300,D18-1233,0,0.064836,"Missing"
N19-1300,W18-5446,0,0.39751,"ning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing. Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018). We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few othe"
N19-1300,Q18-1021,0,0.0822727,"Missing"
N19-1300,N18-1101,0,0.16728,"Missing"
N19-1300,D18-1259,0,0.0909819,"Missing"
N19-1300,D18-1009,0,0.09126,"Missing"
N19-1385,W17-0401,0,0.342044,"Missing"
N19-1385,P15-2044,0,0.0541729,"Missing"
N19-1385,Q16-1022,0,0.143598,"Missing"
N19-1385,Q16-1031,0,0.014945,"an ours in all languages. The recent work by Wang and Eisner (2018b) reorders delexicalized treebanks of part-of-speech sequences in order to make it more similar to the target language of interest. The latter work is similar to our work in terms of using reordering. Our work is more sophisticated by using a full-fledged parsing model with automatic part-of-speech tags and every accessible dataset such as projected trees and multiple source treebanks as well as cross-lingual word embeddings for all languages. Previous work (T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 20"
N19-1385,J16-2001,0,0.035641,"Missing"
N19-1385,D11-1005,0,0.295996,"Missing"
N19-1385,W02-1001,1,0.225315,"ch and Ney, 2003). Following Rasooli and Collins (2015), we obtain intersected alignments and apply soft POS consistency to filter potentially incorrect alignments. We use the Wikipedia dump data to extract monolingual data for the languages in order to train monolingual embeddings. We follow the method of Rasooli and Collins (2017) to use the extracted dictionaries from the Bible and monolingual text from Wikipedia to create cross-lingual word embeddings. We use the UDPipe pretrained models (Straka and Strakov´a, 2017) to tokenize Wikipedia, and a reimplementation of the Perceptron tagger of Collins (2002)4 to achieve automatic POS tags trained on the training data of the Universal Dependencies corpus (Nivre et al., 2017). We use word2vec (Mikolov et al., 2013)5 to achieve embedding vectors both in monolingual and cross-lingual settings. Supervised Parsing Models We trained our supervised models on the union of all datasets in a language to obtain a supervised model for each language. It is worth noting that there are two major changes that we make to the neural parser of Dozat and Manning (2016) in our implementation6 using the Dynet library (Neubig et al., 2017): first, we add a one-layer cha"
N19-1385,C16-1298,0,0.0445548,"Missing"
N19-1385,P81-1022,0,0.561231,"Missing"
N19-1385,K15-1012,0,0.0619803,"at they have tried is significantly worse than ours in all languages. The recent work by Wang and Eisner (2018b) reorders delexicalized treebanks of part-of-speech sequences in order to make it more similar to the target language of interest. The latter work is similar to our work in terms of using reordering. Our work is more sophisticated by using a full-fledged parsing model with automatic part-of-speech tags and every accessible dataset such as projected trees and multiple source treebanks as well as cross-lingual word embeddings for all languages. Previous work (T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation pro"
N19-1385,D12-1001,0,0.0342334,"nks of part-of-speech sequences in order to make it more similar to the target language of interest. The latter work is similar to our work in terms of using reordering. Our work is more sophisticated by using a full-fledged parsing model with automatic part-of-speech tags and every accessible dataset such as projected trees and multiple source treebanks as well as cross-lingual word embeddings for all languages. Previous work (T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c"
N19-1385,C96-1058,0,0.118278,"Missing"
N19-1385,P09-1042,0,0.0596258,"mmar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016) is another approach in parser transfer. In this approach, supervised dependencies are projected through word alignments and then used as training data. Similar to previous work (Rasooli and Collins, 2017), we make use of a combination of projected dependencies from annotation projection in addition to partially translated source treebanks. One other approach is treebank translation (Tiedemann et al., 2014) for which a statistical machine translation system is used to translate source"
N19-1385,P15-1119,0,0.333291,"s significantly worse than ours in all languages. The recent work by Wang and Eisner (2018b) reorders delexicalized treebanks of part-of-speech sequences in order to make it more similar to the target language of interest. The latter work is similar to our work in terms of using reordering. Our work is more sophisticated by using a full-fledged parsing model with automatic part-of-speech tags and every accessible dataset such as projected trees and multiple source treebanks as well as cross-lingual word embeddings for all languages. Previous work (T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al"
N19-1385,N16-1121,0,0.269067,"Missing"
N19-1385,P14-1126,0,0.115401,"-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016) is another approach in parser transfer. In this approach, supervised dependencies are projected through word alignments and then used as training data. Similar to previous work (Rasooli and Collins, 2017), we make use of a combination of projected dependencies from annotation projection in addition to partially translated source treebanks. One other approach is treebank translation (Tiedemann et al., 2014) for which a statistical machine translation system is used to translate source treebanks to the target language. These"
N19-1385,D11-1006,0,0.114822,"Missing"
N19-1385,P12-1066,0,0.371314,"Missing"
N19-1385,J03-1002,0,0.02476,"In other words, if the source and target languages do not have the same dominant dependency direction for r and the dominant direction of the target language is the reverse of the current direction, we change the direction of that dependency. Reordering multiple dependencies in a gold standard tree then results in a reordering of the full tree, as for example in the transformation from Figure 1a to Figure 1b. 3848 1. Extracting reordering mappings from alignments: We first extract intersected word alignments for each source-target sentence pair. This is done by running the Giza++ alignments (Och and Ney, 2003) in both directions. We ignore sentence pairs that more than half of the source words do not get alignment. We create a new mapping µ(i) = (i) (i) µ1 . . . µsi that maps each index 1 ≤ j ≤ si in the original source sentence to a unique in(i) dex 1 ≤ µj ≤ si in the reordered sentence. 2. Parsing source sentences: We parse each source sentence using the supervised parser of the source language. We use the mapping µ(i) to come up with a reordered tree for each sentence. In cases for which the number of non-projective arcs in the projected tree increase compared to the original tree, we do not use"
N19-1385,P18-1142,0,0.124374,"Missing"
N19-1385,D15-1039,1,0.953849,"esentations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016) is another approach in parser transfer. In this approach, supervised dependencies are projected through word alignments and then used as training data. Similar to previous work (Rasooli and Collins, 2017), we make use of a combination of projected dependencies from annotation projection in addition to partially translated source treebanks. One other approach is treebank translation (Tiedemann et al., 2014) for which a statistical machine translation system is used to translate source treebanks to the target language. These models need a large amount"
N19-1385,Q17-1020,1,0.8163,"sequences in order to make it more similar to the target language of interest. The latter work is similar to our work in terms of using reordering. Our work is more sophisticated by using a full-fledged parsing model with automatic part-of-speech tags and every accessible dataset such as projected trees and multiple source treebanks as well as cross-lingual word embeddings for all languages. Previous work (T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016) is another ap"
N19-1385,P15-2040,0,0.165141,"Missing"
N19-1385,K17-3009,0,0.0688577,"Missing"
N19-1385,N13-1126,0,0.493249,"Missing"
N19-1385,N12-1052,0,0.313858,"Missing"
N19-1385,W14-1614,0,0.350262,"Missing"
N19-1385,Q16-1035,0,0.145365,"(T¨ackstr¨om et al., 2012; Duong et al., 2015; Guo et al., 2015, 2016; Ammar et al., 2016) has considered using cross-lingual word representations. A number of authors (Durrett et al., 2012; Rasooli and Collins, 2017) have used cross-lingual dictionaries. We also make use of cross-lingual word representations and dictionaries in this paper. We use the automatically extracted dictionaries from the Bible to translate words in the source treebanks to the target language. One other line of research in the delexicalized transfer approach is creating a synthetic treebank (Tiedemann and Agi´c, 2016; Wang and Eisner, 2016, 2018b). Annotation projection (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016) is another approach in parser transfer. In this approach, supervised dependencies are projected through word alignments and then used as training data. Similar to previous work (Rasooli and Collins, 2017), we make use of a combination of projected dependencies from annotation projection in addition to partially translated source treebanks. One other approach is treebank translation (Tiedemann et al., 2014) for whi"
N19-1385,Q17-1011,0,0.377722,"Missing"
N19-1385,Q18-1046,0,0.523878,"Missing"
N19-1385,D18-1163,0,0.803584,"Missing"
N19-1385,P15-1032,1,0.795418,"t is worth noting that there are two major changes that we make to the neural parser of Dozat and Manning (2016) in our implementation6 using the Dynet library (Neubig et al., 2017): first, we add a one-layer character BiLSTM to represent the character information for each word. The final character representation is obtained by concatenating the forward representation of the last character and the backward representation of the first character. The concatenated vector is summed with the randomly initialized as well as fixed pre-trained cross-lingual word embedding vectors. Second, inspired by Weiss et al. (2015), we maintain the moving average parameters to obtain more robust parameters at decoding time. We excluded the following languages from the set of source languages for annotation projection due to their low supervised accuracy: Estonian, Hungarian, Korean, Latin, Lithuanian, Latvian, Turkish, Ukrainian, Vietnamese, and Chinese. Baseline Transfer Models We use two baseline models: 1) Annotation projection: This model only trains on the projected dependencies. 2) Annotation projection + direct transfer: To speed up training, we sample at most thousand sentences from each treebank, comprising a t"
N19-1385,H01-1035,0,0.249714,"ojected through word alignments and then used as training data. Similar to previous work (Rasooli and Collins, 2017), we make use of a combination of projected dependencies from annotation projection in addition to partially translated source treebanks. One other approach is treebank translation (Tiedemann et al., 2014) for which a statistical machine translation system is used to translate source treebanks to the target language. These models need a large amount of parallel data for having an accurate translation system. Using the Bible data goes back to the work of Diab and Finch (2000) and Yarowsky et al. (2001). Recently there has been more interest in using the Bible data for different tasks, due to its availability for many languages (Christodouloupoulos and Steedman, 2014; Agi´c et al., 2015, 2016; Rasooli ¨ and Collins, 2017). Previous work (Ostling and Tiedemann, 2017) has shown that the size of the Bible dataset does not provide a reliable machine 3846 translation model. Previous work in the context of machine translation (Bisazza and Federico, 2016; Daiber et al., 2016) presumes the availability of a parallel data that is often much larger than the Bible data. 3 punct nmod case det ROOT obj d"
N19-1385,I08-3008,0,0.479101,"Missing"
N19-1385,D15-1213,0,0.539668,"Missing"
P02-1034,P01-1010,0,0.0115226,"Missing"
P02-1034,W98-1118,0,0.0454451,"Missing"
P02-1034,A00-2018,0,0.113092,"Missing"
P02-1034,W02-1001,1,0.312859,"Missing"
P02-1034,W96-0214,0,0.203624,"ire 1999) for its application to image classification, for example. This paper describes how the perceptron and voted perceptron algorithms can be used for parsing and tagging problems. Crucially, the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. It might seem paradoxical to be able to efficiently learn and apply a model with an exponential number of features.1 The key to our algorithms is the 1 Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. Nigel Duffy iKuni Inc., 3400 Hillview Ave., Building 5, Palo Alto, CA 94304. nigeduff@cs.ucsc.edu “kernel” trick ((Cristianini and Shawe-Taylor 2000) discuss kernel methods at length). We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms. This leads to polynomial time2 algorithms for training and applying the perceptron. The kernels we describe are related to the kernels over discrete structures in (Haussler"
P02-1034,P99-1069,0,0.00810652,"Missing"
P02-1034,J02-1005,0,0.0888882,"polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set. The method uses a similar recursion to the common sub-trees recursion described in this paper. Goodman’s method still leaves exact parsing under the model intractable (because of the need to sum over multiple derivations underlying the same tree), but he gives an approximation to finding the most probable tree, which can be computed efficiently. From a theoretical point of view, it is difficult to find motivation for the parameter estimation methods used by (Bod 1998) – see (Johnson 2002) for discussion. In contrast, the parameter estimation methods in this paper have a strong theoretical basis (see (Cristianini and Shawe-Taylor 2000) chapter 2 and (Freund & Schapire 1999) for statistical theory underlying the perceptron). For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). (Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. (Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, an"
P02-1034,J93-2004,0,0.0401565,"h  criterion of )f¿  . Finally, the definition of  can , be  modified to:, ,    VU .  If labels at  ´ e are different,  , , , Else  W G U .ÁÀ   )f¿        G U .ÁÀ  )¿ ,      # ¸W G ¥Â, ¸   µ ""   µ ""¶ ## , where   ,  are the words at  and respectree kernel. tively. This inner product implicitly includes features which track word features, and thus can make better use of sparse data. 6 Experiments 6.1 Parsing Wall Street Journal Text We used the same data set as that described in (Collins 2000). The Penn Wall Street Journal treebank (Marcus et al. 1993) was used as training and test data. Sections 2-21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set. Of the 40,000 training sentences, the first 36,000 were used to train the perceptron. The remaining 4,000 sentences were used as development data, and for tuning parameters of the algorithm. Model 2 of (Collins 1999) was used to parse both the training and test data, producing multiple hypotheses for each sentence. In order to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sentence chunks, e"
P02-1034,W96-0213,0,\N,Missing
P02-1034,J03-4003,1,\N,Missing
P02-1034,P02-1062,1,\N,Missing
P02-1062,W98-1118,0,0.0623136,"Missing"
P02-1062,P02-1034,1,0.652652,"Missing"
P02-1062,W02-1001,1,0.515,"Missing"
P02-1062,P99-1069,0,0.0207413,"ta. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Coll"
P02-1062,J97-4005,0,\N,Missing
P02-1062,W96-0213,0,\N,Missing
P02-1062,N01-1003,0,\N,Missing
P02-1062,J03-4003,1,\N,Missing
P04-1007,P03-1006,1,0.526898,"atures Φi for i = 1 . . . d implemented by D. The second concerns the choice of parameters αi for i = 0 . . . d which assign weights to the n-gram features as well as the baseline feature Φ0 . Before describing methods for training a discriminative language model using perceptron and CRF algorithms, we give a little more detail about the structure of D, focusing on how n-gram language models can be implemented with finite-state techniques. 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions (Allauzen et al., 2003). Every string accepted by such an automaton has a single path through the automaton, and the weight of the string is the sum of the weights of the transitions in that path. In such a representation, every state in the automaton represents an n-gram history h, e.g. wi−2 wi−1 , and there are transitions leaving the state for every word wi such that the feature hwi has a weight. There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state. This failure transit"
P04-1007,W02-1001,1,0.422692,"eter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach. In the next section of the paper we describe how global linear models can be applied to speech recognition. In particular, we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques. 2.1 Global linear models We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y. We assume the following components: (1) Training examples (xi , yi ) for i = 1 . . . N . (2) A function GEN which enumerates a set of candidates GEN(x) for an input x. (3) A representation Φ mapping each (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . (4) A parameter vector α ¯ ∈ Rd . The components GEN, Φ and α ¯ define a mapping from an input x to an output F (x) through F (x) = argmax Φ(x, y) · α ¯ (1) y∈GEN(x) P where Φ(x, y) · α ¯ is the inner product s αs Φs (x, y). The learning task is to set the parameter val"
P04-1007,P99-1069,1,0.599285,"training algorithm in decoding heldout and test examples in our experiments. Say α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We no"
P04-1007,W02-2018,0,0.0613897,"ctive function: LLR (α) ¯ = N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ − i=1 ||α|| ¯ 2 2σ 2 (3) The value σ dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are α ¯ ∗ = argmaxα¯ LLR (¯ α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR . There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α ¯ as input, and in turn returns LLR (¯ α) as well as the gradient of LLR at α ¯ . The derivative of the objective function with respect to a parameter αs at parameter values α ¯ is   N X X ∂LLR αs Φs (xi , yi ) − = pα ¯ (y|xi )Φs (xi , y)− 2 ∂αs σ i=1 (4) y∈GEN(xi ) Note that LLR (¯ α) is a convex function, so that there is a globally optimal solution and the optimization method 2 will find it. The use of the Gaussian prior term ||¯ α ||/2σ 2 in the objective fun"
P04-1007,W03-0430,0,0.0804031,"r vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of training examples 2 Note"
P04-1007,N03-1028,0,0.0263195,"α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of tr"
P04-1015,W02-1001,1,0.645823,"al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). In this approach the training and decoding problems are very closely related – the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative mod"
P04-1015,P02-1036,0,0.018976,"Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1 Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existing baseline model with reasonable performance. Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search. This paper explores an alternative approach to parsing, based on the perceptron training algorithm"
P04-1015,C00-1052,1,0.543517,"gh a tree transform; next, we limit the left-child chains consisting of more than two non-terminal categories to those actually observed in the training data more than once. Left-child chains of length less than or equal to two are all those observed in training data. As a practical matter, the set of leftchild chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech (POS) tags T for x. Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b). This transform is only applied to left-recursive productions, i.e. productions of the form A → Aγ. The transformed trees look as in figure 3. The transform has the benefit of dramatically reducing the number of left-child chains, without unduly disrupting the immediate dominance relationships that provide features for the model. The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . Table 1 presents the number of left-child chains of length greater than 2 in sections 2"
P04-1015,P99-1069,0,0.0137957,"pproach. In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model). To this end, a number of recently proposed methods allow a model to incorporate “arbitrary” global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N"
P04-1015,J98-4004,0,0.00832404,"Missing"
P04-1015,P02-1035,0,0.00810474,"tly proposed methods allow a model to incorporate “arbitrary” global features of candidate analyses or parses. Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002). A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases1 . For example, Johnson et al. (1999) and Riezler et al. (2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1 Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. Brian Roark AT&T Labs - Research roark@research.att.com presupposes that there is an existi"
P04-1015,J01-2004,1,0.690269,"this approach the training and decoding problems are very closely related – the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made. Thus the main complexity of the method is isolated to the decoding problem. We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model. The same search method is used in both training and decoding. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. We also describe several refinements to the training algorithm, and demonstrate their impact on convergence properties of the method. Finally, we describe training the perceptron model with the negative log probability given by the generative model as another feature. This provides the perceptron algorithm with a better starting point, leading to large improve"
P04-1015,J97-4005,0,\N,Missing
P04-1015,J05-1003,1,\N,Missing
P04-1015,P02-1034,1,\N,Missing
P05-1063,P01-1017,0,0.142246,"channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably m"
P05-1063,P98-1035,0,0.216145,"4; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tag"
P05-1063,W02-1001,1,0.587262,"d-out data. In (2) Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector α ¯ using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jeline"
P05-1063,J91-3004,0,0.0204762,"Missing"
P05-1063,P99-1069,0,0.0134391,"ion of this input. We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is X X L(¯ α) = log P (si |ai , α) ¯ −C αj2 (4) i where P (si |ai , α ¯) = Input: A parameter specifying the number of iterations over the training set, T . A value for the first parameter, α. A feature-vector representation Φ(a, w) ∈ Rd . Training examples (ai , wi ) for i = 1 . . . m. An n-best list GEN(ai ) for each training utterance. We take si to be the member of GEN(ai ) which has the lowest WER when compared to wi . Initialization: Set α1 = α, and αj = 0 for j = 2 . . . d. Algorithm: For t = 1 . . . T"
P05-1063,P04-1007,1,0.593855,", Φ(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w). The optimal string under our new model is defined as w∗ = arg max (β log Pl (w) + hα, ¯ Φ(a, w)i+ w log Pa (a|w)) where the arg max is taken over all strings in the 1000-best list, and where α ¯ ∈ Rd is a parameter vector specifying the “weight” for each feature in Φ (note that we define hx, yi to be the inner, or dot 1 where β &gt; 0 is some value that reflects the relative importance of the language model; β is typically chosen by optimization on held-out data. In (2) Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector α ¯ using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a ve"
P05-1063,J01-2004,1,0.898369,"hods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the mo"
P05-1063,N03-1028,0,0.0152592,"αi ¯ . e w∈GEN(a ) i Here, each si is the member of GEN(ai ) which has lowest WER with respect to the target transcription wi . The first term in L(¯ α) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parameter values. C is a constant that dictates the relative weighting given to the two terms. The optimal parameters are defined as α ¯ ∗ = arg max L(¯ α) α ¯ We refer to these models as global conditional loglinear models (GCLMs). Each of these algorithms has advantages. A number of results—e.g., in Sha and Pereira (2003) and Roark et al. (2004b)—suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method. However the perceptron converges very quickly, often in just a few passes over the training set—in comparison GCLM’s can take tens or hundreds of gradient calculations before convergence. In addition, the perceptron can be used as an effective feature selection technique, in that 510 at each training example it only increments features seen on si or yi , effectively ignoring all other features seen on members of GEN(ai ). For example, in the experiments in Roark et al"
P05-1063,P94-1011,0,0.0172554,"ion 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is con"
P05-1063,J95-2002,0,0.0243637,"Missing"
P05-1063,W02-1031,0,0.409897,"ry similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations Φ(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modifi"
P05-1063,P02-1025,0,0.169167,"g method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit"
P05-1063,W03-1021,0,0.0551928,"tice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic depende"
P05-1063,J03-4003,1,\N,Missing
P05-1063,C98-1035,0,\N,Missing
P05-1063,N04-1021,0,\N,Missing
P05-1066,P96-1023,0,0.155377,"e-based systems currently represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the re"
P05-1066,N04-1035,0,0.654328,"ranslation from French to English, where reordering rules are acquired automatically. The reordering rules in their approach operate at the level of context-free rules in the parse tree. Our method differs from that of (Xia and McCord, 2004) in a couple of important respects. First, we are considering German, which arguably has more challenging word order phenonema than French. German has relatively free word order, in contrast to both English and French: for example, there is considerable flexibility in terms of which phrases can appear in the first position in a clause. Second, Xia et. al’s (2004) use of reordering rules stated at the context-free level differs from ours. As one example, in our approach we use a single transformation that moves an infinitival verb to the first position in a verb phrase. Xia et. al’s approach would require learning of a different rule transformation for every production of the form VP => .... In practice the German parser that we are using creates relatively “flat” structures at the VP and clause levels, leading to a huge number of context-free rules (the flatness is one consequence of the relatively free word order seen within VP’s and clauses in Germa"
P05-1066,P03-1011,0,0.0444847,"he–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the su"
P05-1066,J96-1002,0,0.0208231,"2004) was due to the addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet). This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study). (Niessen and Ney, 2004) describe a method that combines morphologically–split verbs in German, and also reorders questions in English and German. Our method goes beyond this approach"
P05-1066,N04-1014,0,0.370674,"translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side"
P05-1066,W04-3250,1,0.336068,"here itself is translated by the reordered model. We then define 7@    7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores."
P05-1066,2003.mtsummit-papers.6,0,0.0395528,"ehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Och et al., 2004) was due to the addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet)."
P05-1066,P03-1040,1,0.323197,"guages. One class of approaches make use of “bitext” grammars which simultaneously parse both the source and target languages. Another class of approaches make use of syntactic information in the target language alone, effectively transforming the translation problem into a parsing problem. Note that these models have radically different structures and parameterizations from phrase–based models for SMT. As yet, these systems have not shown significant gains in accuracy in comparison to phrase-based systems. Reranking methods have also been proposed as a method for using syntactic information (Koehn and Knight, 2003; Och et al., 2004; Shen et al., 2004). In these approaches a baseline system is used to generate -best output. Syntactic features are then used in a second model that reranks the -best lists, in an attempt to improve over the baseline approach. (Koehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Oc"
P05-1066,N03-1017,1,0.133673,"actors, including a cost of skipping over a phrase of length 4 (i.e., Ihnen die entsprechenden Anmerkungen) in the German string. 534 The ability to penalise “skips” of this type, and the potential to model multi-word phrases, are essentially the main strategies that the phrase-based system is able to employ when modeling differing word-order across different languages. In practice, when training the parameters of an SMT system, for example using the discriminative methods of (Och, 2003), the cost for skips of this kind is typically set to a very high value. In experiments with the system of (Koehn et al., 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e., have skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. In summary, phrase-based systems have relatively limited potential to model word-order differences between different languages. The reordering stage described in this paper attempts to modify the source language (e.g., German) in such a way that its word order is very similar to that seen in the target language (e.g., English). In an ideal approach, the resulting tr"
P05-1066,1993.eamt-1.1,0,0.0697396,"7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores.    0 538  that indicates whether the translafunction  tions have improved or not under the"
P05-1066,W02-1018,0,0.0417534,"Missing"
P05-1066,P02-1040,0,0.11539,"Missing"
P05-1066,P04-1083,0,0.0268447,"istical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on"
P05-1066,N04-1023,0,0.170201,"f “bitext” grammars which simultaneously parse both the source and target languages. Another class of approaches make use of syntactic information in the target language alone, effectively transforming the translation problem into a parsing problem. Note that these models have radically different structures and parameterizations from phrase–based models for SMT. As yet, these systems have not shown significant gains in accuracy in comparison to phrase-based systems. Reranking methods have also been proposed as a method for using syntactic information (Koehn and Knight, 2003; Och et al., 2004; Shen et al., 2004). In these approaches a baseline system is used to generate -best output. Syntactic features are then used in a second model that reranks the -best lists, in an attempt to improve over the baseline approach. (Koehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Och et al., 2004) was due to the additio"
P05-1066,J04-2003,0,0.0291282,"addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet). This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study). (Niessen and Ney, 2004) describe a method that combines morphologically–split verbs in German, and also reorders questions in English and German. Our method goes beyond this approach in several respects, fo"
P05-1066,P03-1021,0,0.14218,"e same time absorbing “aushaendigen” from the German string. The cost of this decoding step will involve a number of factors, including a cost of skipping over a phrase of length 4 (i.e., Ihnen die entsprechenden Anmerkungen) in the German string. 534 The ability to penalise “skips” of this type, and the potential to model multi-word phrases, are essentially the main strategies that the phrase-based system is able to employ when modeling differing word-order across different languages. In practice, when training the parameters of an SMT system, for example using the discriminative methods of (Och, 2003), the cost for skips of this kind is typically set to a very high value. In experiments with the system of (Koehn et al., 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e., have skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. In summary, phrase-based systems have relatively limited potential to model word-order differences between different languages. The reordering stage described in this paper attempts to modify the source language (e.g., German) in such a way"
P05-1066,N04-1021,0,0.183195,"their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side of the translatio"
P05-1066,W99-0604,0,0.107073,"Missing"
P05-1066,J97-3002,0,0.887209,"currently represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting pa"
P05-1066,C04-1073,0,0.813091,"key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of t"
P05-1066,P01-1067,0,0.9072,"represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reo"
P05-1066,2004.tmi-1.9,0,0.0405392,"s translated by the reordered model. We then define 7@    7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores.    0 538  that indicate"
P05-1066,C00-2163,0,\N,Missing
P05-1066,J93-2003,0,\N,Missing
P05-1066,P03-1013,0,\N,Missing
P08-1068,J92-4003,0,0.69795,"argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). 2.2 Brown clustering algorithm In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest"
P08-1068,W06-2920,0,0.790504,"le two-stage semi-supervised approach. First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner. We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features. To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research. However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure. Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications. In general, semi-supervis"
P08-1068,D07-1101,1,0.741046,"ured classification approach to dependency parsing. For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y ∈ Y(x) decomposes into a set of “parts” r ∈ y. In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model. In higher-order parsing models, the parts can consist of interactions between more than two words. For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio “plays”, “Elianti”, and “.” in Figure 1. The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio “*”, “plays”, and “Haag” in Figure 1. These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve"
P08-1068,P99-1065,1,0.578037,"Missing"
P08-1068,W02-1001,1,0.166689,"he Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures. To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times. To select the number 6 We used Joakim Nivre’s “Penn2Malt” conversion tool (http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Dependency labels were obtained via the “Malt” hard-coded setting. 7 For computational reasons, we removed a single 249-word sentence from Section 0. 8 That is, we tagged each fold with the tagger trained on the other 9 folds. 9 We ensured that the sentences of the Penn Treebank were excluded from the text used for the clustering. 10 Following Collins et al. (1999), we used a coarsened v"
P08-1068,W05-1505,0,0.00418844,"Missing"
P08-1068,H05-1064,1,0.179121,"ithout parts of speech is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and th"
P08-1068,J93-2004,0,0.0496562,"parsing applications. In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled data to facilitate the utilization of the supervised corpus, increasing the performance of the model in absolute terms. Second, given a fixed target performance level, we might wish to use unlabeled data to reduce the amount of annotated data necessary to reach this target. We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3). By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4). The remainder of this paper is divided as follows: 595 Proceedings of ACL-08: HLT, pages 595–603, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics root nmod sbj 0 p obj 000 * Ms. Haag plays Elian"
P08-1068,P05-1010,0,0.186274,"ined by using clusters without parts of speech is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributi"
P08-1068,N06-1020,0,0.813326,"when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of “backed-off” version of the baseline features. However, our work is focused on discriminative learning as opposed to generative models. Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice. However, their approach depends on the usage of a high-quality parse reranker, whereas the method described here simply augments the features of an existing parser. Note that our two approaches are compatible in that we could also design a reranker and apply self-training techniques on top of the clusterbased features. 6 Conclusions In this paper, we have presented a simple but effective semi-supervis"
P08-1068,E06-1011,0,0.83386,"s ideal candidates for the application of coarse word proxies such as word clusters. In this paper, we take a part-factored structured classification approach to dependency parsing. For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y ∈ Y(x) decomposes into a set of “parts” r ∈ y. In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model. In higher-order parsing models, the parts can consist of interactions between more than two words. For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio “plays”, “Elianti”, and “.” in Figure 1. The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio “*”, “plays”, and “Haag” in Figure 1. These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters"
P08-1068,P05-1012,0,0.839755,"ons allow dependency parsers to obtain a limited form of context-sensitivity. Given a factorization of dependency structures into parts, we restate dependency parsing as the fol596 PARSE(x; w) = argmax X w · f (x, r) y∈Y(x) r∈y Above, we have assumed that each part is scored by a linear model with parameters w and featuremapping f (·). For many different part factorizations and structure domains Y(·), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). 2.2 Brown clustering algorithm In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text con"
P08-1068,H05-1066,0,0.714491,"Missing"
P08-1068,N04-1043,0,0.873543,"truct a new cluster-based feature mapping for a discriminative learner. We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features. To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research. However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure. Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications. In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled"
P08-1068,P05-1013,0,0.0486766,"Missing"
P08-1068,P06-1055,0,0.409463,"is close to the performance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased featur"
P08-1068,W96-0213,0,0.520557,"the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24). The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures. To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/te"
P08-1068,P07-1080,0,0.00712696,"rmance of the baseline features. 5 Related Work As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Our research, however, applies this technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a"
P08-1068,W05-1516,0,0.0119333,"technique to dependency parsing rather than named-entity recognition. In this paper, we have focused on developing new representations for lexical information. Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand. Crucially, however, these methods do not exploit unlabeled data when learning their representations. Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task. Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of “backed-off” version of the baseline features. However, our work is focused on discriminative learning as opposed to generative models. Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to"
P08-1068,W03-3023,0,0.953733,"es in a wide range of parsing configurations, including first-order and second-order parsers, and labeled and unlabeled parsers.5 3 As in Brown et al. (1992), we limit the clustering algorithm so that it recovers at most 1,000 distinct bit-strings; thus full bit strings are not equivalent to word forms. 4 We used N = 800 for all experiments in this paper. 5 In an “unlabeled” parser, we simply ignore dependency label information, which is a common simplification. 598 The English experiments were performed on the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24). The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for"
P08-1068,D07-1096,0,\N,Missing
P09-1110,C04-1021,0,0.0129951,". Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, Figure 2: An online learning algorithm. 8 Featu"
P09-1110,C04-1180,0,0.0365058,"Missing"
P09-1110,J83-3001,0,0.444479,"of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the"
P09-1110,W03-1013,0,0.0196546,"Missing"
P09-1110,W02-1001,1,0.066962,"binator rules. For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. A/B : f B : g B : g AB : f ⇒ ⇒ A :"
P09-1110,W00-1317,0,0.020042,"i,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse"
P09-1110,H94-1010,0,0.2237,"Missing"
P09-1110,W04-3201,0,0.0261329,"For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. A/B : f B : g B : g AB : f ⇒ ⇒ A : f (g) A : f (g) (>) ("
P09-1110,W05-0602,0,0.0419396,"zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It r"
P09-1110,H94-1039,0,0.0190751,"from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, Figu"
P09-1110,P99-1069,0,0.0094594,"nstruct parse trees according to a set of combinator rules. For example, consider the functional application combinators:3 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms π for wj and many ways of modifying each π to create a final logical form zj . Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form π and all of the steps that map π to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test example"
P09-1110,W99-0909,0,0.183667,"Missing"
P09-1110,P07-1121,0,0.592714,"ectness) • Let d∗ = arg maxd∈GEN(wi,j ;C) θ · φ(d) . • If L(d∗ ) = zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully sup"
P09-1110,P96-1008,0,0.863406,"φ(d) . • If L(d∗ ) = zi,j , go to Step 3. Step 2: (Update parameters) • Let d0 = arg maxd∈GEN(wi,j ,zi,j ;C) θ · φ(d) . • Set θ = θ + φ(d0 ) − φ(d∗ ) . Step 3: (Update context) • Append zi,j to the current context C. 9 Output: Estimated parameters θ. There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning repre"
P09-1110,D07-1071,1,0.89181,"light(x) ∧ f rom(x, bos) ∧ to(x, phi) ∧ during(x, morning) constructed from the variable x, where the subexpression aircraf t(x) = y has been removed because it contains the free variable y. 7 Learning Figure 2 details the complete learning algorithm. Training is online and error-driven. Step 1 parses the current sentence in context. If the optimal logical form is not correct, Step 2 finds the best derivation that produces the labeled logical form7 and does an additive, perceptron-style parameter update. Step 3 updates the context. This algorithm is a direct extension of the one introduced by Zettlemoyer and Collins (2007). It maintains the context but does not have the lexical induction step that was previously used. Elaboration Expressions Finally, E(z) is a set of elaboration expressions constructed Sj−1 from a logical form z. We define E(C) = i=1 E(zi ). E(z) is defined by enumerating the places where embedded variables are found in z. For each logical variable x and each coordination (conjunction or disjunction) in the scope of x, a new expression is created by defining a function λf.z 0 where z 0 has the function f (x) added to the appropriate coordination. This procedure would 6 7 A lambda-calculus expre"
P10-1001,W06-2922,0,0.0596945,"en m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8 7.1 Features for third-order parsing Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: S CORE PART(x,"
P10-1001,J98-4004,0,0.0169842,"projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 6 Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best. Eisner (2000) defines dependency parsing models where each word has a set o"
P10-1001,W08-2102,1,0.823705,"and validation data. Pass = %dependencies surviving the beam in training data, Orac = maximum achievable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P (h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001 × maxh0 P (h0 , m |x). Table 1 provides information on the behavior of the pruning method. Averaged perceptron training There are a wide variety of parameter estimation methods for"
P10-1001,D07-1101,0,0.201071,"is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the 1 For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). 2 http://groups.csail.mit.edu/nlp/dpo3/ 1 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (a) = h * e + h m m e Insiders must report purchases and sales immediately (b) Figure 1: An example dependency structure. h m + h r r+1 m Figure 2: The dynamic-programming structures and derivations of the Eisner (2000) algorithm. Complete spans are depicted as triangles and incomplete spa"
P10-1001,D07-1015,1,0.595774,"we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10 For Czech, we used the first character of the tag; for English, we used the first two characters, except PRP and PRP$. 11 For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12 Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 7.4 8 Main results Parser McDonald et al. (2005a,2005b) McDonald and Pereira (2006) Koo et al. (2008), standard Model 1 Model 2 Koo et al. (2008), semi-sup† Suzuki et al. (2009)† Carreras et al. (2008)† Eng 90.9 91.5 92.02 93.04 92.93 93.16 93.79 93.5 Cze 84.4 85.2 86.13 87.38 87.37 87.13 88.05 Parser Model 0 Carreras (2007) emulation Model 1 Model 1, no-3rd Model 2 Mode"
P10-1001,P05-1022,0,0.688812,"vable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P (h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001 × maxh0 P (h0 , m |x). Table 1 provides information on the behavior of the pruning method. Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Tas"
P10-1001,P08-1068,1,0.726548,"ord in the relationship, and a modifier, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the 1 For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). 2 http://groups.csail.mit.edu/nlp/dpo3/ 1 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (a) = h * e + h m m e Insiders must report purchases and sales immediately (b) Figure 1: An example dependency structure. h m + h r r+1 m Figure 2: The dynamic-programming structures and derivations of the Eisner (2000) algorithm. Complete spans are depicted as triangles and incomplete spans as trapezoids. For brevit"
P10-1001,W02-1001,1,0.128366,"ry sentence x in training, validation, and test data we compute the marginal probability P (h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001 × maxh0 P (h0 , m |x). Table 1 provides information on the behavior of the pruning method. Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select paTable 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 9 For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996)"
P10-1001,J93-2004,0,0.0425643,"ature mappings fdep , fsib , and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features. For example, fdep contains lexicalized “in-between” features that depend on the head and modifier words as well as a word lying in between the two; in contrast, previous work has generally defined in-between features for POS tags only. As another example, our Parsing experiments In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is 6 For English, we extracted dependencies using Joakim Nivre’s Penn2Malt tool with standard head rules (Yamada and Matsumoto, 2003); for Czech, we “projectivized” the training data by finding best-match projective trees. 7 For Czech, the PDT has a predefined split; for English, we split the Sections as: 2–21 training, 22 validation, 23 test. 8 As in previous work, English evaluation ignores any token whose gold-standard POS tag i"
P10-1001,C96-1058,0,0.934034,"For brevity, we elide the right-headed versions. e + Figure 4: The dynamic-programming structures and derivations of Model 0. For brevity, we elide the right-headed versions. Note that (c) and (d) differ from (a) and (b) only in the position of g. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3 ) time and O(n2 ) space (Eisner, 1996; McAllester, 1999). 3.2 g (b) + s = (a) m still defined by a span and split point, so the parser requires O(n3 ) time and O(n2 ) space. 4 Second-order sibling factorization New third-order parsing algorithms In this section we describe our new third-order dependency parsing algorithms. Our overall method is characterized by the augmentation of each span with a “grandparent” index: an index external to the span whose role will be made clear below. This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers"
P10-1001,E06-1011,0,0.208605,"ndencies: directed arcs between a head, which is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the 1 For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). 2 http://groups.csail.mit.edu/nlp/dpo3/ 1 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (a) = h * e + h m m e Insiders must report purchases and sales immediately (b) Figure 1: An example dependency structure. h m + h r r+1 m Figure 2: The dynamic-programming structures and derivations of the Eisner (2000) algorithm. Complete spans are depicte"
P10-1001,W07-2216,0,0.212058,"” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 6 Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of inf"
P10-1001,P05-1012,0,0.914996,".g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n). the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 6 Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on t"
P10-1001,H05-1066,0,0.89475,"Missing"
P10-1001,W06-2933,0,0.256072,"Missing"
P10-1001,N07-1051,0,0.0629076,", Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P (h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001 × maxh0 P (h0 , m |x). Table 1 provides information on the behavior of the pruning method. Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We cho"
P10-1001,W96-0213,0,0.027593,"99; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select paTable 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 9 For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10 For Czech, we used the first character of the tag; for English, we used the first two characters, except PRP and PRP$. 11 For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for t"
P10-1001,P08-1066,0,0.0128421,"rther research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008). Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2 Ablation studies In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3). Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy su"
P10-1001,D07-1014,0,0.0615769,"ch, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10 For Czech, we used the first character of the tag; for English, we used the first two characters, except PRP and PRP$. 11 For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12 Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 7.4 8 Main results Parser McDonald et al. (2005a,2005b) McDonald and Pereira (2006) Koo et al. (2008), standard Model 1 Model 2 Koo et al. (2008), semi-sup† Suzuki et al. (2009)† Carreras et al. (2008)† Eng 90.9 91.5 92.02 93.04 92.93 93.16 93.79 93.5 Cze 84.4 85.2 86.13 87.38 87.37 87.13 88.05 Parser Model 0 Carreras (2007) e"
P10-1001,D09-1058,1,0.547507,"r, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the 1 For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). 2 http://groups.csail.mit.edu/nlp/dpo3/ 1 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (a) = h * e + h m m e Insiders must report purchases and sales immediately (b) Figure 1: An example dependency structure. h m + h r r+1 m Figure 2: The dynamic-programming structures and derivations of the Eisner (2000) algorithm. Complete spans are depicted as triangles and incomplete spans as trapezoids. For brevity, we elide the symmetric right-headed ve"
P10-1001,W03-3023,0,0.904071,"(g, h, m) is scored only when m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8 7.1 Features for third-order parsing Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as:"
P11-1008,P05-1033,0,0.0940162,"then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a p"
P11-1008,J10-3008,0,0.0990845,"Missing"
P11-1008,P07-1019,0,0.0829747,"Missing"
P11-1008,D10-1027,0,0.0729844,". In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the source language string. Second, transdu"
P11-1008,E09-1044,0,0.107895,"Missing"
P11-1008,D10-1125,1,0.155243,"binatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integr"
P11-1008,H05-1021,0,0.0385627,"ccurate estimates of the number of search errors for cube pruning. 2 Related Work A variety of approximate decoding algorithms have been explored for syntax-based translation systems, including cube-pruning (Chiang, 2007; Huang and Chiang, 2007), left-to-right decoding with beam search (Watanabe et al., 2006; Huang and Mi, 2010), and coarse-to-fine methods (Petrov et al., 2008). Recent work has developed decoding algorithms based on finite state transducers (FSTs). Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al. (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). Approximate search methods are used for more complex reordering models or grammars. The FST algorithms are shown to produce higher scoring solutions than cube-pruning on a large proportion of examples. Lagrangian relaxation is a classical technique in combinatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual funct"
P11-1008,A00-2023,0,0.0413351,"ing, largely because of the cost of integrating an n-gram language model into the search process. Exact dynamic programming algorithms for the problem are well known (Bar-Hillel et al., 1964), but are too expensive to be used in practice.2 Previous work on decoding for syntax-based SMT has therefore been focused primarily on approximate search methods. This paper describes an efficient algorithm for exact decoding of synchronous grammar models for translation. We avoid the construction of (Bar-Hillel 1 This problem is also relevant to other areas of statistical NLP, for example NL generation (Langkilde, 2000). 2 E.g., with a trigram language model they run in O(|E|w6 ) time, where |E |is the number of edges in the hypergraph, and w is the number of distinct lexical items in the hypergraph. 2. Application of an all-pairs shortest path algorithm to a directed graph derived from the weighted hypergraph. The size of the derived directed graph is linear in the size of the hypergraph, hence this step is again efficient. Informally, the first decoding algorithm incorporates the weights and hard constraints on translations from the synchronous grammar, while the second decoding algorithm is used to integr"
P11-1008,W06-1606,0,0.0294161,", for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the"
P11-1008,D08-1012,0,0.0770549,"Missing"
P11-1008,W06-1616,0,0.257948,"Missing"
P11-1008,D10-1001,1,0.249468,"al technique in combinatorial optimization (Korte and Vygen, 2008). Lagrange multipliers are used to add linear constraints to an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The secon"
P11-1008,P08-1066,0,0.0204753,"subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph is created as follows: first, the source side of the synchronous grammar is used to create a parse forest over the source language st"
P11-1008,D08-1016,0,0.0604628,"o an existing problem that can be solved using a combinatorial algorithm; the resulting dual function is then minimized, for example using subgradient methods. In recent work, dual decomposition—a special case of Lagrangian relaxation, where the linear constraints enforce agreement between two or more models—has been applied to inference in Markov random fields (Wainwright et al., 2005; Komodakis et al., 2007; Sontag et al., 2008), and also to inference problems in NLP (Rush et al., 2010; Koo et al., 2010). There are close connections between dual decomposition and work on belief propagation (Smith and Eisner, 2008). 73 3 Background: Hypergraphs Translation with many syntax-based systems (e.g., (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008; Huang and Mi, 2010)) can be implemented as a two-step process. The first step is to take an input sentence in the source language, and from this to create a hypergraph (sometimes called a translation forest) that represents the set of possible translations (strings in the target language) and derivations under the grammar. The second step is to integrate an n-gram language model with this hypergraph. For example, in the system of (Chiang, 2005), the hypergraph"
P11-1008,N06-1054,0,0.269276,"Missing"
P11-1008,P06-1098,0,0.145069,"Missing"
P11-1008,N09-2002,0,\N,Missing
P11-1008,P01-1030,0,\N,Missing
P11-1008,N09-1049,0,\N,Missing
P11-1008,J07-2003,0,\N,Missing
P11-5006,P11-1043,0,0.022125,"man man man man man g (z1 ) > g (z2 ) Identifying word tags notation: identify the tag labels selected by each model • Ys (i, t) = 1 when the tagger for sentence s at position i selects tag t • z(s, i, t) = 1 when the constraint assigns at sentence s position i the tag t example: a parse and tagging with Y1 (5, N) = 1 and z(1, 5, N) = 1 He saw an American man The smart man stood outside Y man man z man Combined optimization goal: arg max Y ∈Y,z∈Z F (Y ) + g (z) such that for all s = 1 . . . m, i = 1 . . . n, t ∈ T , Ys (i, t) = z(s, i, t) Algorithm step-by-step [Animation] Combined alignment (DeNero and Macherey, 2011) setup: assume separate models trained for English-to-French and French-to-English alignment problem: find an alignment that maximizes the score of both models with soft agreement example: • HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity) English-to-French alignment define: • • • Y is set of all possible English-to-French alignments y ∈ Y is a valid alignment f (y ) scores of the alignment example: HMM alignment 1 3 2 4 6 5 The1 ugly2 dog3 has4 red5 fur6 French-to-English alignment define: • • • Z is set of all possible French-to-English align"
P11-5006,D10-1125,1,0.825095,"complexity focus: decoding problem for natural language tasks y ∗ = arg max f (y ) y motivation: • richer model structure often leads to improved accuracy • exact decoding for complex models tends to be intractable Decoding tasks many common problems are intractable to decode exactly high complexity • combined parsing and part-of-speech tagging (Rush et al., 2010) • “loopy” HMM part-of-speech tagging • syntactic machine translation (Rush and Collins, 2011) NP-Hard • symmetric HMM alignment (DeNero and Macherey, 2011) • phrase-based translation • higher-order non-projective dependency parsing (Koo et al., 2010) in practice: • approximate decoding methods (coarse-to-fine, beam search, cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models) Motivation cannot hope to find exact algorithms (particularly when NP-Hard) aim: develop decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality • show that for practical instances, these algorithms often yield exact solutions • provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us"
P11-5006,D10-1001,1,0.891786,"Missing"
P11-5006,D08-1016,0,0.0817424,"Missing"
P11-5006,P11-1008,1,\N,Missing
P11-5006,D11-1003,1,\N,Missing
P12-1024,P96-1024,0,0.812379,"rm for calculation of p(r1 . . . rN ). 1. For a given p(r1 . . . rN ). s-tree r1 . . . rN , calculate 2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities X µ(a, i, j) = p(τ ) τ ∈T (x):(a,i,j)∈τ for each non-terminal a ∈ N , for each (i, j) such that 1 ≤ i ≤ j ≤ N . Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) ∈ τ if nonterminal a spans words xi . . . xj in the parse tree τ . The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 . . . xN , the parsing algorithm of Goodman (1996) can be used to find X arg max µ(a, i, j) τ ∈T (x) (a,i,j)∈τ This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for anPinput sentence, p(x) = P τ ∈T (x) p(τ ), as p(x) = a∈I µ(a, 1, N ). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 5 Tensor Form of the Inside-Outside Algori"
P12-1024,E12-1042,0,0.119777,"The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-PCFG formalism used in this paper. An L-PCFG is a 5-tuple (N , I, P, m, n) where: • N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of non-terminals into two subsets. • [m] is the set of possi"
P12-1024,P05-1010,0,0.93,"ar value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of the training examples down"
P12-1024,P92-1017,0,0.508816,"tion for Computational Linguistics, pages 223–231, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-"
P12-1024,P06-1055,0,0.878323,"in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of t"
P12-1024,J98-4004,0,\N,Missing
P12-1024,J93-2004,0,\N,Missing
P12-1024,P97-1003,1,\N,Missing
P12-1024,P03-1054,0,\N,Missing
P14-1099,J93-2004,0,0.050709,"initializer for the EM algorithm for LPCFGs. Two-step estimation methods such as these are well known in statistics; there are guarantees for example that if the first estimator is consistent, and the second step finds the closest local maxima of the likelihood function, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described m EM Spectral Pivot Pivot+EM 8 86.69 40 85.60 83.56 86.83 2 sec. 16 88.32 30 87.77 86.00 88.14 6 22 24 88.35 30 88.53 86.87 88.64 2 32 88.56 20 88.82 86.40 88.55 2 sec. 23 87.76 88.05 85.83 88.03 Table 1: Results on the development data (section 22) and test d"
P14-1099,P05-1010,0,0.140712,"imated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like E"
P14-1099,P06-1055,0,0.708959,"ees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates o"
P14-1099,W97-0309,0,0.146933,"o latent variable models. We apply the matrix decomposition algorithm to a co-occurrence matrix that can be estimated directly from a training set consisting of parse trees without latent annoThe algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one “pivot” feature. This assumption is similar to the “pivot word” assumption used by Arora et al. (2013) and Arora et al. (2012) in the context of learning topic models. We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of Saul and Pereira (1997). A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in"
P14-1099,P12-1024,1,0.893218,"the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates of the original parameters of an LPCFG, but, unlike EM, does not suffer from problems of local optima. The algorithm relies on two key ideas: 1) A matrix decomposition algorithm (section 5) which is Papplicable to matrices Q of the form Qf"
P14-1099,N13-1015,1,0.945585,"eloped provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method leads to problems with negative or unnormalized probablities; the method does not give parameters"
P14-1099,D12-1019,1,0.844255,"convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Ling"
P14-1139,P06-1002,0,0.0974445,"e bidirectional models with soft-penalties to explicitly permit these violations. A Proof of NP-Hardness We can show that the bidirectional alignment problem is NP-hard by reduction from the trav50 0 50 1000 50 100 150 200 250 iteration 300 350 400 (a) The best dual and the best primal score, relative to the optimal score, averaged over all sentence pairs. The best primal curve uses a feasible greedy algorithm, whereas the intersection curve is calculated by taking the intersection of x and y. 1.0 relative search space size ble 2 also compares the models in terms of phraseextraction accuracy (Ayan and Dorr, 2006). We use the phrase extraction algorithm described by DeNero and Klein (2010), accounting for possible links and  alignments. CONS performs better than each of the directional models, but worse than the best D&M model. Finally we consider the impact of constraint addition, pruning, and use of a lower bound. Table 3 gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. Figure 7(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph compares the use of the greedy algorithm f"
P14-1139,J93-2003,0,0.121881,"Missing"
P14-1139,E09-1020,0,0.0164391,"e heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries"
P14-1139,P10-1147,1,0.888202,"Missing"
P14-1139,P11-1043,1,0.881064,"for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 (Brown et al., 1993) or hidden Markov model (HMM) based aligners (Vogel et al., 1996). Several authors have proposed bidirectional models that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult. In this paper, we describe a novel Lagrangianrelaxation based decoder for the bidirectional model proposed by DeNero and Macherey (2011), with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but We begin in Section 2 by formally describing the directional word alignment problem. Section 3 describes a preliminary bidirectional model using full agreement constraints and a Lagrangian relaxation-based solver. Section 4 modifies this model to include adjacency constraints. Section 5 describes an extension to the relaxed algorithm to explicitly enforce constraints, and Section 6 gives a pruning method for improving the efficiency of the algorithm. Experim"
P14-1139,P08-1112,0,0.187295,"Missing"
P14-1139,N06-1015,0,0.118406,"I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in that it does not track the last"
P14-1139,N06-1014,0,0.294672,"d decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental"
P14-1139,H05-1011,0,0.0783886,"MM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in t"
P14-1139,W99-0604,0,0.548881,"Note that unlike an alignment from Y multiple words may be aligned in a column and words may transition from nonaligned positions. Note that for both of these models we can solve the optimization problem exactly using the standard Viterbi algorithm for HMM decoding. The first can be solved in O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this m"
P14-1139,C96-2141,0,0.757889,"ng problem. Given a sentence e of length |e |= I and a sentence f of length |f |= J, our goal is to find the best bidirectional alignment between the two sentences under a given objective function. Before turning to the model of interest, we first introduce directional word alignment. 2.1 Word Alignment In the e→f word alignment problem, each word in e is aligned to a word in f or to the null word . This alignment is a mapping from each index i ∈ [I] to an index j ∈ [J]0 (where j = 0 represents alignment to ). We refer to a single word alignment as a link. A first-order HMM alignment model (Vogel et al., 1996) is an HMM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and wei"
P14-1139,P09-1104,1,0.884523,"Missing"
P14-1139,N03-1017,0,0.289841,"O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this model to adjacent agreement. 3.1 Y0 = ∗ x ,y = arg max f (x) + g(y) s.t. x∈X ,y∈Y x(i, j) = y(i, j) ∀i ∈ [I], j ∈ [J]  y : y(0, 0) = 1, P y(i, j) = Ii0 =0 y(i0 , i, j) ∀i ∈ [I]0 , j ∈ [J] Figure 2(b) shows a possible y ∈ Y 0 and a valid unchained structure. To form the Lagrangian dual with relaxe"
P15-1032,D09-1087,0,0.0207389,"Missing"
P15-1032,E12-1009,0,0.0813111,"Missing"
P15-1032,P06-1063,0,0.191628,"Missing"
P15-1032,C10-1011,0,0.0209909,"Missing"
P15-1032,P08-1068,1,0.672703,"quires a feature-vector definition φ that maps a sentence x together with a configuration c to a feature vector φ(x, c) ∈ Rd . There is a one-to-one mapping between configurations c and decision sequences 3.3 Incorporating Unlabeled Data Given the high capacity, non-linear nature of the deep network we hypothesize that our model can 1 If the gold parse tree stays within the beam until the end of the sentence, conventional perceptron updates are used. 326 4 be significantly improved by incorporating more data. One way to use unlabeled data is through unsupervised methods such as word clusters (Koo et al., 2008); we follow Chen and Manning (2014) and use pretrained word embeddings to initialize our model. The word embeddings capture similar distributional information as word clusters and give consistent improvements by providing a good initialization and information about words not seen in the treebank data. Experiments In this section we present our experimental setup and the main results of our work. 4.1 Experimental Setup We conduct our experiments on two English language benchmarks: (1) the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and (2) a more comprehen"
P15-1032,W06-2920,0,0.0478166,"Missing"
P15-1032,P14-1043,0,0.205475,"Missing"
P15-1032,J93-2004,0,0.0558856,"Missing"
P15-1032,D14-1082,0,0.790216,"l Network Transition-Based Parsing David Weiss Chris Alberti Michael Collins Slav Petrov Google Inc New York, NY {djweiss,chrisalberti,mjcollins,slav}@google.com Abstract perceptron training algorithm can greatly improve accuracy. Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers (Zhang and Nivre, 2011), and only by incorporating graph-based scoring functions were Bohnet and Kuhn (2012) able to exceed the accuracy of graph-based approaches. In contrast to these carefully hand-tuned approaches, Chen and Manning (2014) recently presented a neural network version of a greedy transition-based parser. In their model, a feedforward neural network with a hidden layer is used to make the transition decisions. The hidden layer has the power to learn arbitrary combinations of the atomic inputs, thereby eliminating the need for hand-engineered features. Furthermore, because the neural network uses a distributed representation, it is able to model lexical, part-of-speech (POS) tag, and arc label similarities in a continuous space. However, although their model outperforms its greedy hand-engineered counterparts, it i"
P15-1032,P13-2109,0,0.0322801,"Missing"
P15-1032,D13-1129,0,0.0762893,"Missing"
P15-1032,N06-1020,0,0.256666,"Missing"
P15-1032,P04-1015,1,0.226241,"eriments. 3.2 v(y j ) · φ(x, y1 . . . y j−1 ). j=1 Thus each decision y j receives a score: where λ is a regularization hyper-parameter over the hidden layer parameters (we use λ = 10−4 in all experiments) and j sums over all decisions and configurations {y j , c j } extracted from gold parse trees in the dataset. The specific update rule we apply at iteration t is as follows: gt = µgt−1 − ∆L(Θt ), m X Structured Perceptron Training Given the hidden representations, we now describe how the perceptron can be trained to utilize these representations. The perceptron algorithm with early updates (Collins and Roark, 2004) requires a feature-vector definition φ that maps a sentence x together with a configuration c to a feature vector φ(x, c) ∈ Rd . There is a one-to-one mapping between configurations c and decision sequences 3.3 Incorporating Unlabeled Data Given the high capacity, non-linear nature of the deep network we hypothesize that our model can 1 If the gold parse tree stays within the beam until the end of the sentence, conventional perceptron updates are used. 326 4 be significantly improved by incorporating more data. One way to use unlabeled data is through unsupervised methods such as word cluster"
P15-1032,de-marneffe-etal-2006-generating,0,0.0608748,"Missing"
P15-1032,P15-1033,0,0.237047,"course easier to parse, having an average length of 15 words, compared to 24 words for the tune set overall. However, because we only use these sentences to extract individual transition decisions, the shorter length does not seem to hurt their utility. We generate 107 tokens worth of new parses and use this data in the backpropagation stage of training. 327 Method UAS LAS Beam Graph-based Bohnet (2010) 92.88 90.71 Martins et al. (2013) 92.89 90.55 Zhang and McDonald (2014) 93.22 91.02 n/a n/a n/a Transition-based ? Zhang and Nivre (2011) Bohnet and Kuhn (2012) Chen and Manning (2014) S-LSTM (Dyer et al., 2015) Our Greedy Our Perceptron 93.00 93.27 91.80 93.20 93.19 93.99 90.95 91.19 89.60 90.90 91.18 92.05 32 40 1 1 1 8 Tri-training ? Zhang and Nivre (2011) Our Greedy Our Perceptron 92.92 90.88 93.46 91.49 94.26 92.41 32 1 8 Method Transition-based ? Zhang and Nivre (2011) Bohnet and Kuhn (2012) Our Greedy Our Perceptron (B=16) 91.15 91.69 91.21 92.25 Tri-training ? Zhang and Nivre (2011) Our Greedy Our Perceptron (B=16) 91.46 85.51 91.36 91.82 86.37 90.58 92.62 87.00 93.05 85.24 85.33 85.41 86.44 92.46 92.21 90.61 92.06 Table 2: Final Treebank Union test set results. We report LAS only for brevity"
P15-1032,P04-1013,0,0.163874,"the architecture and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1 Input layer Given a parse configuration c (consisting of a stack s and a buffer b), we extract a rich set of discrete features which w"
P15-1032,W04-0308,0,0.397798,"and other retrospective experiments. One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1"
P15-1032,N06-2015,0,0.0159683,"Missing"
P15-1032,J08-4003,0,0.235391,"knowledge is the best accuracy on Stanford Dependencies to date. We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. 1 Introduction Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention. Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages (Buchholz and Marsi, 2006; Nivre et al., 2007; McDonald et al., 2013) and the efficiency of dependency parsers. Transition-based parsers (Nivre, 2008) have been shown to provide a good balance between efficiency and accuracy. In transition-based parsing, sentences are processed in a linear left to right pass; at each position, the parser needs to choose from a set of possible actions defined by the transition strategy. In greedy models, a classifier is used to independently decide which transition to take based on local features of the current parse configuration. This classifier typically uses hand-engineered features and is trained on individual transitions extracted from the gold transition sequence. While extremely fast, these greedy mo"
P15-1032,N03-1033,0,0.211668,"Missing"
P15-1032,P06-1055,1,0.177457,", but with a more diverse dataset for training and evaluation. Following Vinyals et al. (2015), we use (in addition to the WSJ), the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006). We train on the union of each corpora’s training set and test on each domain separately. We refer to this setup as the “Treebank Union” setup. In our semi-supervised experiments, we use the corpus from Chelba et al. (2013) as our source of unlabeled data. We process it with the BerkeleyParser (Petrov et al., 2006), a latent variable constituency parser, and a reimplementation of ZPar (Zhang and Nivre, 2011), a transition-based parser with beam search. Both parsers are included as baselines in our evaluation. We select the first 107 tokens for which the two parsers agree as additional training data. For our tri-training experiments, we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser. This increases POS However, obtaining more training data is even more important than a good initialization. One potential way to obtain additional training dat"
P15-1032,W03-3023,0,0.871614,"Missing"
P15-1032,D10-1069,1,0.495674,"Missing"
P15-1032,W97-0301,0,0.0405813,"Missing"
P15-1032,D09-1058,1,0.921961,"Missing"
P15-1032,D08-1059,0,0.780514,"Missing"
P15-1032,P14-2107,0,0.0889251,"Missing"
P15-1032,P11-2112,0,0.0631238,"Missing"
P15-1032,P11-2033,0,0.723082,"he activations from all layers as features in the structured perceptron. Using the probability estimates directly is very similar to Ratnaparkhi (1997), where a maximum-entropy model was used to model the distribution over possible actions at each parser state, and beam search was used to search for the highest probability parse. A known problem with beam search in this setting is the label-bias problem. Table 5 shows the impact of using structured perceptron training over using the softmax function during beam search as a function of the beam size used. For reference, our reimplementation of Zhang and Nivre (2011) is trained equivalently for each setting. We also show the impact on beam size when tri-training is used. Although the beam does marginally improve accuracy for the softmax model, much greater gains are achieved when perceptron training is used. Increasing hidden units yields large gains. For these experiments, we fixed the embedding sizes Dword = 64, Dtag = Dlabel = 32 and tried increasing and decreasing the dimensionality of the hidden layers on a logarthmic scale. Improvements in accuracy did not appear to saturate even with increasing the number of hidden units by an order of magnitude, t"
P15-1032,D07-1099,0,0.0367885,"and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1 Input layer Given a parse configuration c (consisting of a stack s and a buffer b), we extract a rich set of discrete features which we feed into the neural netw"
P15-1032,P15-1117,0,0.197062,"input source: words, POS tags, and arc labels. The full set of features is given in Table 2. The features extracted for each group are represented as a sparse F ⇥ V matrix X, where V is the size of the vocabulary of the feature group and F is the number of features: the value of element Xf v is 1 if the f ’th feature takes on value v. We produce three input matrices: Xword for words features, Xtag for POS tag features, and Xlabel for arc labels. For all feature groups, we add additional special tends this line of work to the setting of inexact search with beam decoding for dependency parsing; Zhou et al. (2015) concurrently explored a similar approach using a structured probabilistic 2 Neural Network Model ranking objective. Dyer et al. (2015) concurrently In this section, we describe the architecture of our model, which is summarized in figure 2. NoteShort-Term that developed the Stack Long Memory we separate the embedding processing to a distinct (S-LSTM) architecture, which “embedding layer” for clarity of presentation. Our does incorporate model is based upon that of Chen and Manning recurrent architecture and look-ahead, and which yields comparable accuracy on the Penn Treebank to our greedy mo"
P15-1032,D07-1096,0,\N,Missing
P15-1032,W07-2218,0,\N,Missing
P15-1124,D14-1082,0,0.00594471,"beddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 7.1 Experiments Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and Goldberg (2014a) for a de"
P15-1124,J92-4003,0,0.589011,"ce matrices E[XX > ] and E[Y Y > ] are diagonal. This follows from our definition of the word and context variables as one-hot encodings since E[Xw Xw0 ] = 0 for w 6= w0 and E[Yc Yc0 ] = 0 for c 6= c0 . With these observations and the binary definition of (X, Y ), each entry in Ω now has a simple closed-form solution: P (Xw = 1, Yc = 1) Ωw,c = p P (Xw = 1)P (Yc = 1) Using CCA for parameter estimation In a less well-known interpretation of Eq. (4), CCA is seen as a parameter estimation algorithm for a language model (Stratos et al., 2014). This model is a restricted class of HMMs introduced by Brown et al. (1992), henceforth called the Brown model. In this section, we extend the result of Stratos et al. (2014) and show that its correctness is preserved under certain element-wise data transformations. 4.1 Importantly, the model makes the following additional assumption: Assumption 4.1 (Brown assumption). For each word type w ∈ [n], there is a unique hidden state H(w) ∈ [m] such that o(w|H(w)) > 0 and o(w|h) = 0 for all h 6= H(w). In other words, this model is an HMM in which observation states are partitioned by hidden states. Thus a sequence of N words w1 . . . wN ∈ [n]N QN has probability π(H(w1 )) ×"
P15-1124,N13-1015,1,0.150368,"imensions Dev Test 90.04 84.40 92.49 88.75 92.27 88.87 92.51 88.08 92.25 89.27 92.88 89.28 91.49 87.16 92.44 88.34 92.63 88.78 50 dimensions Dev Test 90.04 84.40 92.49 88.75 92.91 89.67 92.73 88.88 92.53 89.37 92.94 89.01 91.58 86.80 92.83 89.21 93.11 89.32 Table 4: NER F1 scores when word embeddings are added as features to the baseline (—). Our proposed method gives the best result among spectral methods and is competitive to other popular word embedding techniques. This work suggests many directions for future work. Past spectral methods that involved CCA without data transformation (e.g., Cohen et al. (2013)) may be revisited with the square-root transformation. Using CCA to induce representations other than word embeddings is another important future work. It would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the varianceweighted objective in Eq. (6). Even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions. Acknowledgments We thank Omer Levy, Yoav Goldberg, and David Belanger for helpful discussions. This work was made possible by a research grant from Bloomberg’s Knowledge Engineer"
P15-1124,D14-1162,0,0.139314,"= 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β ∈ {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: if t = — if t = log if t = two-thirds if t = sqrt 4. Define v(w) ∈ Rm to be the w-th row of U Σβ normalized to have unit 2-norm. Figure 2: A template for spectral word embedding methods.   No scaling t ∈ {—, log, sqrt}, s = — . This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI)  t = —, s = ppmi . Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: P #(w, c) c #(c)α pˆ(w, c) log = log pˆ(w)ˆ pα (c) #(w)#(c)α Typically negative values are thresholded to 0 to keep Ω sparse. Levy and Goldberg (2014b) observed th"
P15-1124,P14-1130,0,0.00968514,"to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 7.1 Experiments Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and"
P15-1124,W14-1618,0,0.652131,"ethods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computa"
P15-1124,Q15-1016,0,0.192846,"Note that the choice of α only affects methods that make use of the context distribution (s ∈ {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β > 0 since the best-fit subspace for the rows of Ω is given by U Σ. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β ∈ {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: if t = — if t = log if t = two-thirds if t = sqrt 4. Define v(w) ∈ Rm to be the w-th row of U Σβ normalized to have unit 2-norm. Figure 2: A template for spectral word embedding methods.   No scaling t ∈ {—, log, sqrt}, s = — . This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive poin"
P16-1231,P99-1070,0,0.206492,"G that are not in PL . The proof that PG * PL gives a clear illustration of the label bias problem. Proof that PL ⊆ PG : We need to show that for any locally normalized distribution pL , we can construct a globally normalized model pG such 6 Smith and Johnson (2007) cite Michael Collins as the source of the example underlying the proof. Note that the theorem refers to conditional models of the form p(d1:n |x1:n ) with global or local normalization. Equivalence (or non-equivalence) results for joint models of the form p(d1:n , x1:n ) are quite different: for example results from Chi (1999) and Abney et al. (1999) imply that weighted context-free grammars (a globally normalized joint model) and probabilistic context-free grammars (a locally normalized joint model) are equally expressive. (7) Note that the input x2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed. Now consider a globally normalized model, where the scores ρ(d1:i−1 , di , x1:i ) are defined as follows. Define T as the set {(A, B), (B, C), (A, D), (D, E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a, A), (b, B), (c, C), (b, D), (e, E)} o"
P16-1231,D15-1159,1,0.642693,"Missing"
P16-1231,D15-1041,0,0.15485,"Missing"
P16-1231,D12-1133,0,0.21927,"Missing"
P16-1231,D15-1042,0,0.0431093,"Missing"
P16-1231,Q13-1033,0,0.121645,"Missing"
P16-1231,D14-1082,0,0.651136,"oduced impressive results on some of the classic NLP tasks such as part-ofspeech tagging (Ling et al., 2015), syntactic parsing (Vinyals et al., 2015) and semantic role labeling (Zhou and Xu, 2015). One might speculate that it is the recurrent nature of these models that enables these results. In this work we demonstrate that simple feedforward networks without any recurrence can achieve comparable or better accuracies than LSTMs, as long as they are globally normalized. Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approxi"
P16-1231,J99-1004,0,0.253294,"tributions in PG that are not in PL . The proof that PG * PL gives a clear illustration of the label bias problem. Proof that PL ⊆ PG : We need to show that for any locally normalized distribution pL , we can construct a globally normalized model pG such 6 Smith and Johnson (2007) cite Michael Collins as the source of the example underlying the proof. Note that the theorem refers to conditional models of the form p(d1:n |x1:n ) with global or local normalization. Equivalence (or non-equivalence) results for joint models of the form p(d1:n , x1:n ) are quite different: for example results from Chi (1999) and Abney et al. (1999) imply that weighted context-free grammars (a globally normalized joint model) and probabilistic context-free grammars (a locally normalized joint model) are equally expressive. (7) Note that the input x2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed. Now consider a globally normalized model, where the scores ρ(d1:i−1 , di , x1:i ) are defined as follows. Define T as the set {(A, B), (B, C), (A, D), (D, E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a, A), (b, B), ("
P16-1231,P04-1015,1,0.818985,"on system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss. In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it—a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, synt"
P16-1231,de-marneffe-etal-2006-generating,0,0.00968088,"Missing"
P16-1231,P15-1030,0,0.068148,"ks. We apply our approach to POS tagging, syntactic dependency parsing, and sentence compression. While directly optimizing the global model defined by Eq. (5) works well, we found that training the model in two steps achieves the same precision much faster: we first pretrain the network using the local objective given in Eq. (4), and then perform additional training steps using the global objective given in Eq. (6). We pretrain all layers except the softmax layer in this way. We purposefully abstain from complicated hand engineering of input features, which might improve performance further (Durrett and Klein, 2015). We use the training recipe from Weiss et al. (2015) for each training stage of our model. Specifically, we use averaged stochastic gradient descent with momentum, and we tune the learning rate, learning rate schedule, momentum, and early stopping time using a separate held-out corpus for each task. We tune again with a different set of hyperparameters for training with the global objective. 4.1 Part of Speech Tagging Part of speech (POS) tagging is a classic NLP task, where modeling the structure of the output is important for achieving state-of-the-art performance. 2446 Data & Evaluation. W"
P16-1231,P15-1033,0,0.18042,"Missing"
P16-1231,P04-1013,0,0.655478,"Missing"
P16-1231,N06-2015,0,0.0782403,"Missing"
P16-1231,P06-1063,0,0.0228459,"Missing"
P16-1231,P14-1130,0,0.0317446,"Missing"
P16-1231,D15-1176,0,0.0214013,"Missing"
P16-1231,J93-2004,0,0.0799101,"Missing"
P16-1231,P13-2109,0,0.15041,"Missing"
P16-1231,P09-1040,0,0.0569419,"arameters θ(d) . We next describe how softmax-style normalization can be performed at the local or global level. 2.2 Global vs. Local Normalization In the Chen and Manning (2014) style of greedy neural network parsing, the conditional probability distribution over decisions dj given context d1:j−1 is defined as p(dj |d1:j−1 ; θ) = exp ρ(d1:j−1 , dj ; θ) , (1) ZL (d1:j−1 ; θ) where ZL (d1:j−1 ; θ) = X exp ρ(d1:j−1 , d0 ; θ). d0 ∈A(d1:j−1 ) 1 http://github.com/tensorflow/models/tree/master/syntaxnet http://www.tensorflow.org 3 Note that this is not true for the swap transition system defined in Nivre (2009). 2 4 It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. 2443 Each ZL (d1:j−1 ; θ) is a local normalization term. The probability of a sequence of decisions d1:n is pL (d1:n ) = n Y p(dj |d1:j−1 ; θ) j=1 P exp nj=1 ρ(d1:j−1 , dj ; θ) Qn . = j=1 ZL (d1:j−1 ; θ) (2) Beam search can be used to attempt to find the maximum of Eq. (2) with respect to d1:n . The additive scores used in beam search are the logsoftmax of each decision, ln p(dj |d1:j−1 ; θ), not the raw scores ρ(d1:j−1 , dj ; θ)."
P16-1231,J07-4003,0,0.381027,"the beam throughout decoding, a gradient step is performed using Bn , the beam at the end of decoding. 3 The Label Bias Problem Intuitively, we would like the model to be able to revise an earlier decision made during search, when later evidence becomes available that rules out the earlier decision as incorrect. At first glance, it might appear that a locally normalized model used in conjunction with beam search or exact search is able to revise earlier decisions. However the label bias problem (see Bottou (1991), Collins (1999) pages 222-226, Lafferty et al. (2001), Bottou and LeCun (2005), Smith and Johnson (2007)) means that locally normalized models often have a very weak ability to revise earlier decisions. This section gives a formal perspective on the label bias problem, through a proof that globally normalized models are strictly more expressive than locally normalized models. The theorem was originally proved5 by Smith and Johnson (2007). 5 More precisely Smith and Johnson (2007) prove the theorem for models with potential functions of the form ρ(di−1 , di , xi ); the generalization to potential functions of the form ρ(d1:i−1 , di , x1:i ) is straightforward. 2444 The example underlying the proo"
P16-1231,Q16-1014,0,0.0921649,"Missing"
P16-1231,P15-1113,0,0.262839,"Missing"
P16-1231,P15-1032,1,0.843195,"ce d1 . . . dj . We assume that there is a one-to-one mapping between decision sequences d1:j−1 and states sj : that is, we essentially assume that a state encodes the entire history of decisions. Thus, each state can be reached by a unique decision sequence from s† .4 We will use decision sequences d1:j−1 and states interchangeably: in a slight abuse of notation, we define ρ(d1:j−1 , d; θ) to be equal to ρ(s, d; θ) where s is the state reached by the decision sequence d1:j−1 . The scoring function ρ(s, d; θ) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al. (2015), we define it via a feedforward neural network as ρ(s, d; θ) = φ(s; θ(l) ) · θ(d) . Transition System Given an input x, most often a sentence, we define: • A set of states S(x). • A special start state s† ∈ S(x). • A set of allowed decisions A(s, x) for all s ∈ S(x). • A transition function t(s, d, x) returning a new state s0 for any decision d ∈ A(s, x). We will use a function ρ(s, d, x; θ) to compute the score of decision d in state s for input x. The vector θ contains the model parameters and we assume that ρ(s, d, x; θ) is differentiable with respect to θ. In this"
P16-1231,K15-1015,0,0.059669,"Missing"
P16-1231,P14-2107,0,0.07694,"Missing"
P16-1231,P15-1109,0,0.0181463,"Missing"
P16-1231,P15-1117,0,0.518116,"nd feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss. In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it—a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, syntactic dependency par"
P16-1231,J03-4003,1,\N,Missing
P16-1231,N03-1014,0,\N,Missing
P19-1620,N18-2092,0,0.0850556,"Step (4)). We train a separate model on labeled QA data for each of the first three steps, and then apply the models in sequence on a large number of unlabeled text passages. We show that pretraining on synthetic data generated through this procedure provides us with significant improvements on two challenging datasets, SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), achieving a new state of the art on the latter. 2 Related Work Question generation is a well-studied task in its own right (Heilman and Smith, 2010; Du et al., 2017; Du and Cardie, 2018). Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 2016) or backtranslation (Sennrich et al., 2016) across both translati"
P19-1620,P18-1177,0,0.0764053,"Missing"
P19-1620,P17-1123,0,0.11622,"e finally emit (C, Q, A) as a new synthetic training example (Step (4)). We train a separate model on labeled QA data for each of the first three steps, and then apply the models in sequence on a large number of unlabeled text passages. We show that pretraining on synthetic data generated through this procedure provides us with significant improvements on two challenging datasets, SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), achieving a new state of the art on the latter. 2 Related Work Question generation is a well-studied task in its own right (Heilman and Smith, 2010; Du et al., 2017; Du and Cardie, 2018). Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 20"
P19-1620,D18-1045,0,0.103814,"o improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 2016) or backtranslation (Sennrich et al., 2016) across both translation directions improves the quality of translation models. Back-translation, which adds synthetically generated parallel data as training examples, was an inspiration for this work, and has led to state-of-the-art results in both the supervised (Edunov et al., 2018) and the unsupervised settings (Lample et al., 2018). Lewis and Fan (2019) model the joint distribution of questions and answers given a context and use this model directly, whereas our work uses generative models to generate synthetic data to be used for pretraining. Combining these two approaches could be an area of fruitful future work. 3 Model Given a dataset of contexts, questions, and answers: {(c(i) , q (i) , a(i) ) : i = 1, . . . , N }, we train three models: (1) answer extraction: p(a|c; θA ), (2) question generation: p(q|c, a; θQ ), and (3) question answering: p(a|c, q; θA0 ). We use"
P19-1620,N10-1086,0,0.149806,"nd C. If A and A0 match we finally emit (C, Q, A) as a new synthetic training example (Step (4)). We train a separate model on labeled QA data for each of the first three steps, and then apply the models in sequence on a large number of unlabeled text passages. We show that pretraining on synthetic data generated through this procedure provides us with significant improvements on two challenging datasets, SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), achieving a new state of the art on the latter. 2 Related Work Question generation is a well-studied task in its own right (Heilman and Smith, 2010; Du et al., 2017; Du and Cardie, 2018). Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learni"
P19-1620,Q19-1026,1,0.831215,"Missing"
P19-1620,D18-1549,0,0.0242854,"low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 2016) or backtranslation (Sennrich et al., 2016) across both translation directions improves the quality of translation models. Back-translation, which adds synthetically generated parallel data as training examples, was an inspiration for this work, and has led to state-of-the-art results in both the supervised (Edunov et al., 2018) and the unsupervised settings (Lample et al., 2018). Lewis and Fan (2019) model the joint distribution of questions and answers given a context and use this model directly, whereas our work uses generative models to generate synthetic data to be used for pretraining. Combining these two approaches could be an area of fruitful future work. 3 Model Given a dataset of contexts, questions, and answers: {(c(i) , q (i) , a(i) ) : i = 1, . . . , N }, we train three models: (1) answer extraction: p(a|c; θA ), (2) question generation: p(q|c, a; θQ ), and (3) question answering: p(a|c, q; θA0 ). We use BERT (Devlin et al., 2018)∗ to model each of these"
P19-1620,P18-2124,0,0.266134,"e an extractive short answer A (Step (1) in Table 1). In Step (2), we generate a question Q conditioned on A and C, then (Step (3)) predict the extractive answer A0 conditioned on Q and C. If A and A0 match we finally emit (C, Q, A) as a new synthetic training example (Step (4)). We train a separate model on labeled QA data for each of the first three steps, and then apply the models in sequence on a large number of unlabeled text passages. We show that pretraining on synthetic data generated through this procedure provides us with significant improvements on two challenging datasets, SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), achieving a new state of the art on the latter. 2 Related Work Question generation is a well-studied task in its own right (Heilman and Smith, 2010; Du et al., 2017; Du and Cardie, 2018). Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gol"
P19-1620,P16-1009,0,0.332634,"Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 2016) or backtranslation (Sennrich et al., 2016) across both translation directions improves the quality of translation models. Back-translation, which adds synthetically generated parallel data as training examples, was an inspiration for this work, and has led to state-of-the-art results in both the supervised (Edunov et al., 2018) and the unsupervised settings (Lample et al., 2018). Lewis and Fan (2019) model the joint distribution of questions and answers given a context and use this model directly, whereas our work uses generative models to generate synthetic data to be used for pretraining. Combining these two approaches could be an a"
P19-1620,P17-1096,0,0.0460476,"etic training example (Step (4)). We train a separate model on labeled QA data for each of the first three steps, and then apply the models in sequence on a large number of unlabeled text passages. We show that pretraining on synthetic data generated through this procedure provides us with significant improvements on two challenging datasets, SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), achieving a new state of the art on the latter. 2 Related Work Question generation is a well-studied task in its own right (Heilman and Smith, 2010; Du et al., 2017; Du and Cardie, 2018). Yang et al. (2017) and Dhingra et al. (2018) both use generated 6168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–6173 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics question-answer pairs to improve a QA system, showing large improvements in low-resource settings with few gold labeled examples. Validating and improving the accuracy of these generated QA pairs, however, is relatively unexplored. In machine translation, modeling consistency with dual learning (He et al., 2016) or backtranslation (Sennrich et al., 2"
P96-1025,J93-1002,0,0.123483,"Missing"
P96-1025,A88-1019,0,0.40289,"Missing"
P96-1025,W95-0103,1,0.251043,"Missing"
P96-1025,H94-1052,0,0.346051,"Missing"
P96-1025,E91-1004,0,0.0155069,"Missing"
P96-1025,J93-2004,0,0.0818639,"Missing"
P96-1025,P92-1017,0,0.00758901,"Missing"
P96-1025,W96-0213,0,0.0288257,"Missing"
P96-1025,H91-1060,0,0.034756,"Missing"
P96-1025,J93-1005,0,\N,Missing
P96-1025,P95-1037,0,\N,Missing
P96-1025,1991.iwpt-1.22,0,\N,Missing
P97-1003,P96-1025,1,0.685761,"Missing"
P97-1003,C96-1058,0,0.0362766,"Missing"
P97-1003,H94-1052,0,0.00835927,"Missing"
P97-1003,J93-2004,0,0.10422,"Missing"
P97-1003,H91-1060,0,0.291297,"Missing"
P97-1003,W96-0213,0,0.386196,"Missing"
P97-1003,H94-1020,0,\N,Missing
P97-1003,P96-1023,0,\N,Missing
P97-1003,P95-1037,0,\N,Missing
P97-1003,P96-1008,0,\N,Missing
P99-1065,P96-1025,1,0.861179,"Missing"
P99-1065,P97-1003,1,0.830175,"Missing"
P99-1065,C96-1058,0,0.248701,"Missing"
P99-1065,P98-1080,0,0.083156,"Missing"
P99-1065,H94-1052,0,0.00800501,"Missing"
P99-1065,P95-1037,0,0.240954,"Missing"
P99-1065,J93-2004,0,0.0317856,"Missing"
P99-1065,W97-0301,0,0.0228427,"Missing"
P99-1065,J03-4003,1,\N,Missing
P99-1065,C98-1077,1,\N,Missing
Q16-1010,D15-1138,0,0.00679455,"atkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vecto"
Q16-1010,P02-1041,0,0.0927099,"(Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on te"
Q16-1010,P14-1091,0,0.0366925,"Missing"
Q16-1010,P14-1133,0,0.232833,"Missing"
Q16-1010,Q15-1039,0,0.656552,"Missing"
Q16-1010,D13-1160,0,0.418417,"za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-arg"
Q16-1010,D14-1067,0,0.304718,"in and obtains the best result to date. Interestingly, D EP T REE outperforms S IMPLE G RAPH in this case. We attribute this to the small training set and larger lexical variation of Free917. The structural features of the graph-based representations seem highly beneficial in this case. 6.3 Error Analysis We categorized 100 errors made by D EP L AMBDA (+C +E) on the WebQuestions development set. In 43 cases the correct answer is present in the beam, 136 Method Cai and Yates (2013) Berant et al. (2013) Kwiatkowski et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bao et al. (2014) Bordes et al. (2014) Yao (2015) Yih et al. (2015) (FB API) Bast and Haussmann (2015) Berant and Liang (2015) Yih et al. (2015) (Y&C) Free917 Accuracy WebQuestions Average F1 59.0 62.0 68.0 – 68.5 – – – – 76.4 – – – 35.7 – 33.0 39.9 37.5 39.2 44.3 48.4 49.4 49.7 52.5 This Work D EP T REE S IMPLE G RAPH CCGG RAPH (+ C + E) D EP L AMBDA (+ C + E) 53.2 43.7 73.3 78.0 40.4 48.5 48.6 50.3 Table 3: Question-answering results on the WebQuestions and Free917 test sets. but ranked below an incorrect answer (e.g., for where does volga river start, the annotated gold answer is Valdai Hills, which is ranked second, with Russi"
Q16-1010,C04-1180,1,0.158689,"There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment."
Q16-1010,P13-1042,0,0.356088,"Missing"
Q16-1010,P15-1127,1,0.568655,"was sworn into office when john f kennedy was assassinated ), we do not have a special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from"
Q16-1010,W09-3726,0,0.0740413,"it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus, mimicking the stru"
Q16-1010,W02-1001,1,0.101306,"∈ &lt;n denotes the features for the pair of ungrounded and grounded graphs. Note that for a given query there may be multiple ungrounded graphs, primarily due to the optional use of the CON TRACT operation.3 The feature function has access to the ungrounded and grounded graphs, to the question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See Section 5.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002; Fre3 Another source of ambiguity may be a lexical item having multiple lambda-calculus entries; in our rules this only arises when analyzing count expressions such as how many. und and Schapire, 1999). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt + Φ(u+ , g + , q, K) − Φ(ˆ u, gˆ, q, K) , where (u+ , g + ) denotes the pair of gold ungrounded and grounded graphs for q. Since we do not have direct access to these gold graphs, we instead rely on the set of oracle graphs, OK,A (q), as a proxy: (u+ , g + ) = arg max θt · Φ(u, g, q, K) , (u,g)∈OK,A (q) where OK,A (q) is def"
Q16-1010,P01-1019,0,0.126066,"nd Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph"
Q16-1010,C04-1026,0,0.10619,"tics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus,"
Q16-1010,P15-1026,0,0.238247,"et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grounded to Freebase by learning from question-answer pairs. E"
Q16-1010,P14-1134,0,0.0159776,"gh an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting represent"
Q16-1010,E03-1030,0,0.118015,"araphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to"
Q16-1010,P09-1069,0,0.0392655,"entation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to ra"
Q16-1010,P15-1143,0,0.0199683,"ntic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grou"
Q16-1010,N10-1145,0,0.0444525,"ource semantic representation and the target application’s representation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion"
Q16-1010,D12-1069,0,0.104733,"cquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances ove"
Q16-1010,Q15-1019,0,0.0603458,"s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparable method that generates ungrounded logical forms using"
Q16-1010,D10-1119,1,0.364959,"tructures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of"
Q16-1010,D13-1161,1,0.949182,"lambda-calculus expression and the relabeled s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparab"
Q16-1010,D14-1107,1,0.663912,"XPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connected to an edge. After an edge is grounded, the entity type nodes connected to it are grounded in turn, before the next edge is processed. To restrict the search, if two beam items correspond to the same grounded graph, the one with the lower score is discarded. A beam size of 100 was used in all experiments. Features. We use the f"
Q16-1010,P11-1060,0,0.141153,"plication’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally der"
Q16-1010,N15-1114,0,0.0108635,"representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TA"
Q16-1010,P14-5010,0,0.00310002,"presentation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use eight handcrafted part-of-speech patterns to identify entity span candidates. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.5 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. Finally, we generate ungrounded graphs for the top 10 paths through the lattice and treat the final entity disambiguation as part of the semantic parsing problem. 4 5 http://github.com/sivareddyg/graph-parser http://developers.google.com/freebase/ Representation -C -E -C +E"
Q16-1010,P13-2109,0,0.018844,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since our approach uses dependency trees as input, we hypothesize that it will generalize better to domains that are well covered by dependency parsers than methods that induce semantic grammars from scratch. The system that maps a dependency tree to its logical form (hencefo"
Q16-1010,H05-1066,0,0.0191937,"Missing"
Q16-1010,P14-1041,0,0.019399,"special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al.,"
Q16-1010,N15-1077,0,0.0234129,"ith approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given q"
Q16-1010,P15-1146,0,0.0242314,"Missing"
Q16-1010,P13-1092,0,0.00889818,"nt problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential"
Q16-1010,Q14-1030,1,0.452199,"mbda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of t"
Q16-1010,N15-1118,0,0.0159445,"Missing"
Q16-1010,N15-1040,0,0.0128228,"epresentations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and a"
Q16-1010,N06-1056,0,0.0904852,"trongest result to date on Free917 and competitive results on WebQuestions. 1 Disney acquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et"
Q16-1010,P07-1121,0,0.0158316,"converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provid"
Q16-1010,P15-1049,0,0.0484602,"Missing"
Q16-1010,P14-1090,0,0.408064,"Missing"
Q16-1010,N13-1106,0,0.0699006,"Missing"
Q16-1010,N15-3014,0,0.0687989,"iginal dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0 . If a word is a question word, an additional TARGET predicate is attached to its entity node. S IMPLE G RAPH. This representation has a single event to which all entities in the question are connected by the predicate arg1 . An additional TARGET node is connected to the event by the predicate arg0 . This is similar to the template representation of Yao (2015) and Bast and Haussmann (2015). Note that this cannot represent any compositional structure. CCGG RAPH. Finally, we compare to the CCGbased semantic representation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use"
Q16-1010,P15-1128,0,0.133944,"into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since ou"
Q16-1010,P14-2107,0,0.0173573,".0 73.4 (c) Accuracy 42.6 48.2 46.5 48.8 42.6 48.2 48.9 50.4 D EP T REE S IMPLE G RAPH CCGG RAPH D EP L AMBDA Table 1: Oracle statistics and accuracies on the Web21.3 40.9 68.3 69.3 21.3 40.9 69.4 71.3 Table 2: Oracle statistics and accuracies on the Free917 Questions development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connect"
Q16-1010,J90-1001,0,\N,Missing
Q16-1010,D15-1198,0,\N,Missing
Q16-1018,N10-1083,0,0.408007,"Missing"
Q16-1018,J92-4003,0,0.507952,"g by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., “the” is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of BergKirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words. 1 In this work, we tackle unsupervised POS tagging with HMMs whose structure is deliberately suitable for POS tagging. These HMMs impose an assumption that each hidden state is associated with an observation state (“anchor word”) that can appear under no other state. For this reason, we denote this class of restricted HMMs by anchor HMMs. Such an assumption is relatively benign for POS tagging; it is reasonable to assume that"
Q16-1018,D10-1056,0,0.734643,"sed POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a la"
Q16-1018,P14-1099,1,0.833172,"In particular, we can multiply Ω by any rank-m projection matrix Π ∈ Rd×m on the right side: if Ω satisfies the properties in Proposition 4.1, then so does ΩΠ with m-dimensional rows (ΩΠ)x = E[YI Π|XI = x] Since rank(Ω) = m, a natural choice of Π is the projection onto the best-fit m-dimensional subspace of the row space of Ω. We mention that previous works on the NMFlearning framework have employed various projection methods, but they do not examine relative merits of their choices. For instance, Arora et al. (2013) simply use random projection, which is convenient for theoretical analysis. Cohen and Collins (2014) use a projection based on canonical correlation analysis (CCA) without further exploration. In contrast, we give a full comparison of valid construction methods and find that the choice of Ω is crucial in practice. 4.4.3 Construction of Ω for the Brown Model We can formulate an alternative way to construct a valid Ω when the model is further restricted to be a Brown model. Since every observation is an anchor, Ox ∈ Rm has a single nonzero entry for every x. Thus the rows defined by Ωx = Ox / ||Ox ||(an indicator vector for the unique hidden state of x) form Input: bigram probabilities B, unig"
Q16-1018,P11-1061,0,0.0316239,"universal tags are used (Table 2). Many past works on POS induction predate the introduction of the universal tagset by Petrov et al. (2012) and thus report results with fine-grained tags. More recent works adopt the universal tagset but 255 LOG - LINEAR Accuracy 62.6 (1.1) 65.6 67.2 67.7 74.9 (1.5) Table 7: Many-to-one accuracy on the English data with 45 original tags. We use the same setting as in Table 2. For BW and LOG - LINEAR, we report the mean and the standard deviation (in parentheses) of 10 random restarts run for 1,000 iterations. they leverage additional resources. For instance, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) use parallel data to project POS tags from a supervised source language. Li et al. (2012) use tag dictionaries built from Wiktionary. Thus their results are not directly comparable to ours.4 7 Conclusion We have presented an exact estimation method for learning anchor HMMs from unlabeled data. There are several directions for future work. An important direction is to extend the method to a richer family of models such as log-linear models or neural networks. Another direction is to further generalize the method to handle a wider class of HMMs by relaxing the anch"
Q16-1018,N06-1041,0,0.564643,"find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform po"
Q16-1018,D07-1031,0,0.459756,"e this is of course not true in practice, we find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance"
Q16-1018,D12-1127,0,0.0613514,"Missing"
Q16-1018,P08-1100,0,0.0230141,"idden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a large body of work points to the inappropriateness of simple generative models for unsupervised induction of linguistic structure 251 (Merialdo, 1994; Smith and Eisner, 2005b; Liang and Klein, 2008). Consequently, many works focus on using more expressive models such as log-linear models (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010) and Markov random fields (MRF) (Haghighi and Klein, 2006). These models are shown to deliver good performance even though learning is approximate. Thus one may question the value of having a consistent estimator for A-HMMs and Brown models in this work: if the model is wrong, what is the point of learning it accurately? However, there is also ample evidence that HMMs are competitive for unsupervised POS induction when they incorporate domain-specif"
Q16-1018,P13-2017,0,0.0556415,"Missing"
Q16-1018,J94-2001,0,0.857781,"many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a large body of work points to the inappropriateness of simple generative models for unsupervised induction of linguistic structure 251 (Merialdo, 1994; Smith and Eisner, 2005b; Liang and Klein, 2008). Consequently, many works focus on using more expressive models such as log-linear models (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010) and Markov random fields (MRF) (Haghighi and Klein, 2006). These models are shown to deliver good performance even though learning is approximate. Thus one may question the value of having a consistent estimator for A-HMMs and Brown models in this work: if the model is wrong, what is the point of learning it accurately? However, there is also ample evidence that HMMs are competitive for unsupervised"
Q16-1018,petrov-etal-2012-universal,0,0.11375,"Missing"
Q16-1018,P05-1044,0,0.0970804,"word given its tag. While this is of course not true in practice, we find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their medio"
Q16-1018,P15-1124,1,0.905872,"Missing"
Q16-1018,Q13-1001,0,0.0551433,"Missing"
Q17-1005,D14-1131,0,0.0135703,"on models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version of this paper states the complexity of decoding with a distortion limit as O(I 3 2d ) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I ) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this. 60 ing salesman algorithms for phrase-based decoding. Chang and Collins (2011) describe an exact method based on Lagrangian relaxation. Aziz et al. (2014) describe a coarse-to-fine approach. These algorithms all have exponential time runtime (in the length of the sentence) in the worst case. Galley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages. The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases. However, the algorithms differ in several important ways: Galley and Manning (2010) make use of bit string coverage vectors, giving an"
Q17-1005,D11-1003,1,0.943987,"hard distortion limit. Various methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version of this paper states the complexity of decoding with a distortion limit as O(I 3 2d ) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I ) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this. 60 ing salesman algorithms for phrase-based decoding. Chang and Collins (2011) describe an exact method based on Lagrangian relaxation. Aziz et al. (2014) describe a coarse-to-fine approach. These algorithms all have exponential time runtime (in the length of the sentence) in the worst case. Galley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages. The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases. However, the algorithms differ in several important ways: G"
Q17-1005,W12-3125,0,0.062196,"sed models with a hard distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that"
Q17-1005,P08-2007,0,0.0277851,"are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit. Various methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version of this paper states the complexity of decoding with a distortion limit as O(I"
Q17-1005,C10-2033,0,0.0213296,"with decoding phrase-based models with a hard distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phras"
Q17-1005,D08-1089,0,0.0335219,"rithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit. Various methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version of this paper states the complexity of decoding with"
Q17-1005,N10-1140,0,0.0234291,"distortion limit as O(I 3 2d ) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I ) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this. 60 ing salesman algorithms for phrase-based decoding. Chang and Collins (2011) describe an exact method based on Lagrangian relaxation. Aziz et al. (2014) describe a coarse-to-fine approach. These algorithms all have exponential time runtime (in the length of the sentence) in the worst case. Galley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages. The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases. However, the algorithms differ in several important ways: Galley and Manning (2010) make use of bit string coverage vectors, giving an exponential number of possible states; in contrast to our approach, the translations are not formed in strictly left-to-right ordering on the source side. 3 Backgro"
Q17-1005,J99-4005,0,0.60919,"the insight that decoding with a hard distortion limit is related to the bandwidth-limited traveling salesman problem (BTSP) (Lawler et al., 1985). The algorithm is easily amenable to beam search. It is quite different from previous methods for decoding of phrase-based models, potentially opening up a very different way of thinking about decoding algorithms for phrasebased models, or more generally for models in statistical NLP that involve reordering. Decoding of phrase-based translation models in the general case is known to be NPcomplete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lhd+1 ) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any sour"
Q17-1005,N03-1017,0,0.427468,"osing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lhd+1 ) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models. 1 Introduction Phrase-based translation models (Koehn et al., 2003; Och and Ney, 2004) are widely used in statistical machine translation. The decoding problem for phrase-based translation models is known to be difficult: the results from Knight (1999) imply that in the general case decoding of phrase-based translation models is NP-complete. The complexity of phrase-based decoding comes from reordering of phrases. In practice, however, various constraints on reordering are often imposed in phrase-based translation systems. A common constraint is a “distortion limit”, which places a hard constraint on how far phrases can move. The complexity of decoding with"
Q17-1005,P07-2045,0,0.0424352,"t (1999) proves that decoding of word-to-word translation models is NP-complete, assuming that there is no hard limit on distortion, through a reduction from the traveling salesman problem. Phrasebased models are more general than word-to-word models, hence this result implies that phrase-based decoding with unlimited distortion is NP-complete. Phrase-based systems can make use of both reordering constraints, which give a hard “distortion limit” on how far phrases can move, and reordering models, which give scores for reordering steps, often penalizing phrases that move long distances. Moses (Koehn et al., 2007b) makes use of a distortion limit, and a decoding algorithm that makes use 59 Transactions of the Association for Computational Linguistics, vol. 5, pp. 59–71, 2017. Action Editor: Holger Schwenk. Submission batch: 10/2016; Revision batch: 11/2016; Published 2/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. of bit-strings representing which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case"
Q17-1005,H05-1021,0,0.0485095,"distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of find"
Q17-1005,E09-1061,0,0.0305784,"der two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the gene"
Q17-1005,J04-4002,0,0.377934,"int is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lhd+1 ) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models. 1 Introduction Phrase-based translation models (Koehn et al., 2003; Och and Ney, 2004) are widely used in statistical machine translation. The decoding problem for phrase-based translation models is known to be difficult: the results from Knight (1999) imply that in the general case decoding of phrase-based translation models is NP-complete. The complexity of phrase-based decoding comes from reordering of phrases. In practice, however, various constraints on reordering are often imposed in phrase-based translation systems. A common constraint is a “distortion limit”, which places a hard constraint on how far phrases can move. The complexity of decoding with such a distortion li"
Q17-1005,N04-4026,0,0.0649068,"idered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion. A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit. Various methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version o"
Q17-1005,J97-3002,0,0.535764,"which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length. The current paper is concerned with decoding phrase-based models with a hard distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn"
Q17-1005,P09-1038,0,0.0268877,"of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008). DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit. Various methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel1 An earlier version of this paper states the complexity of decoding with a distortion limit as O(I 3 2d ) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I ) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this. 60 ing salesman algorithms for phrase-based decoding. Chang and Collins (2011) describe an exact method based on Lagrangian relaxation. Aziz et al. (2014) describe a coarse-to-fine approach."
Q17-1005,P03-1019,0,0.0666567,"on batch: 10/2016; Revision batch: 11/2016; Published 2/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. of bit-strings representing which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length. The current paper is concerned with decoding phrase-based models with a hard distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded"
Q17-1005,C04-1030,0,0.0603178,"ion batch: 11/2016; Published 2/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. of bit-strings representing which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length. The current paper is concerned with decoding phrase-based models with a hard distortion limit. Various other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012). Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transdu"
Q17-1005,2005.iwslt-1.8,0,\N,Missing
Q17-1020,Q16-1031,0,0.129847,"al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the state-of-the-"
Q17-1020,C10-3010,0,0.030501,"nlabeled dependency attachment for different POS tags as head for three groups of languages for the universal dependencies experiments in Table 9: G1 (languages with UAS ≥ 80), G2 (languages with 70 ≤ UAS < 80), G3 (languages with UAS < 70). The rows are sorted by frequency in the G1 languages. pus (see Figure 1) is closely related to Duong et al. (2015a); Gouws and Søgaard (2015); and Wick et al. (2015). Our work has made use of dictionaries that are automatically extracted from bilingual corpora. An alternative approach would be to use hand-crafted translation lexicons, for example, PanLex (Baldwin et al., 2010) (e.g. see Duong et al. (2015b)), which covers 1253 language varieties, Google translate (e.g., see Ammar et al. (2016c)), or Wiktionary (e.g., see Durrett et al. (2012) for an approach that uses Wiktionary for cross-lingual transfer). These resources are potentially very rich sources of information. Future work should investigate whether they can give improvements in performance. 7 Conclusions We have described a method for cross-lingual syntactic transfer that is effective in a scenario where a large amount of translation data is not available. We have introduced a simple, direct method for"
Q17-1020,J92-4003,0,0.138633,"ained using the averaged structured perceptron (Collins, 2002). We assume that the feature vector φ(x, y) is the concatenation of three feature vectors: • φ(p) (x, y) is an unlexicalized set of features. Each such feature may depend on the part-ofspeech (POS) tag of words in the sentence, but does not depend on the identity of individual words in the sentence. • φ(c) (x, y) is a set of cluster features. These features require access to a dictionary that maps each word in the sentence to an underlying cluster identity. Clusters may, for example, be learned using the Brown clustering algorithm (Brown et al., 1992). The features may make use of cluster identities in combination with POS tags. • φ(l) (x, y) is a set of lexicalized features. Each such feature may depend directly on word identities in the sentence. These features may also depend on part-of-speech tags or cluster information, in conjunction with lexical information. Appendix A has a complete description of the features used in our experiments. 2.2 This section gives a description of the underlying parsing models used in our experiments, the data 1 The parser code is available at https://github. com/rasoolims/YaraParser/tree/transfer. 280 Th"
Q17-1020,P15-1038,0,0.0232677,"elftraining experiments. We train our cross-lingual clustering with the off-the-shelf-tool6 from Stratos et al. (2015). We set the window size to 2 with a cluster size of 500.7 Parsing Model We use the k-beam arc-eager dependency parser of Rasooli and Tetreault (2015), which is similar to the model of Zhang and Nivre (2011). We modify the parser such that it can use both monolingual and cross-lingual word cluster features. The parser is trained using the the maximum violation update strategy (Huang et al., 2012). We use three epochs of training for all experiments. We use the DEPENDABLE Tool (Choi et al., 2015) to calculate significance tests on several of the comparisons (details are given in the captions to tables 5, 6, and 9). cient Greek, Basque, Catalan, Galician, Gothic, Irish, Kazakh, Latvian, Old Church Slavonic, and Tamil). We also excluded Arabic, Hebrew, Japanese and Chinese, as these languages have tokenization and/or morphological complexity that goes beyond the scope of this paper. Future work should consider these languages. 5 https://github.com/percyliang/ brown-cluster 6 https://github.com/karlstratos/singular 7 Usually the original Brown clusters are better features for parsing but"
Q17-1020,D11-1005,0,0.318073,"Missing"
Q17-1020,W02-1001,1,0.346474,"1 2 Background sets used, and a baseline approach based on delexicalized parsing models. 2.1 We assume that the parsing model is a discriminative linear model, where given a sentence x, and a set of candidate parses Y(x), the output from the model is y ∗ (x) = arg max θ · φ(x, y) y∈Y(x) where θ ∈ Rd is a parameter vector, and φ(x, y) is a feature vector for the pair (x, y). In our experiments we use the shift-reduce dependency parser of Rasooli and Tetreault (2015), which is an extension of the approach in Zhang and Nivre (2011). The parser is trained using the averaged structured perceptron (Collins, 2002). We assume that the feature vector φ(x, y) is the concatenation of three feature vectors: • φ(p) (x, y) is an unlexicalized set of features. Each such feature may depend on the part-ofspeech (POS) tag of words in the sentence, but does not depend on the identity of individual words in the sentence. • φ(c) (x, y) is a set of cluster features. These features require access to a dictionary that maps each word in the sentence to an underlying cluster identity. Clusters may, for example, be learned using the Brown clustering algorithm (Brown et al., 1992). The features may make use of cluster iden"
Q17-1020,K15-1012,0,0.451448,"ic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotatio"
Q17-1020,D15-1040,0,0.111747,"ic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotatio"
Q17-1020,D12-1001,0,0.203699,"u Abstract for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the dens"
Q17-1020,P09-1042,0,0.15784,"al word-embeddings—that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016; Ammar et al., 2016b). These cross-lingual representations are usually learned from parallel translation data. We show results of several methods (Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016b) in Table 7 of this paper. The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Delexicalized Bible Europarl prec./rec. f1 prec./rec. f1 prec./rec. f1 10.6 57.2/62.7 59.8 67.1/71.8 69.4 70.3/73.8 72.0 10.6 65.5/69.1 67.2 75.3/77.4 76.3 75.9/79.2 77.6 9.5 72.5/75.6 74.0 84.3/86.3 85.3 86.6/89.8 88.2 9.1 83.7/ 59.9 69.8 87.3/70.2 77.8 89.0/73.0 80.2 8.0 69.7/60.0 64.5 82.1/77.5 79.7 83.0/78.1 80.5 7.0 76.9/72.3 74.5 83.0/78.7 80.8 80.9/77.9 79.4 4.8 69.3/70.4 69.8 85.0/85.1 85.0 83.8/85.8 84.8 4.6 67.8/55.3 60.9 70.7/55.2 62.0 75.0/63.0 68.5 4.5 60.8/80.3 69.2 64.0/84.9 73.0 68.4/86.6 76.5 4.1 65.9/61.9 6"
Q17-1020,N15-1157,0,0.0183027,"6 33.3 39.7 30.0 26.7 51.7 44.2 rec. 71.0 66.5 54.8 49.1 49.1 54.7 25.8 8.3 52.2 25.3 2.2 42.7 13.5 36.8 10.2 43.0 f1 69.0 65.1 56.0 55.9 56.0 58.3 32.6 13.8 53.7 36.8 4.2 41.1 18.7 30.9 17.0 43.6 Table 12: Precision, recall and f-score of unlabeled dependency attachment for different POS tags as head for three groups of languages for the universal dependencies experiments in Table 9: G1 (languages with UAS ≥ 80), G2 (languages with 70 ≤ UAS < 80), G3 (languages with UAS < 70). The rows are sorted by frequency in the G1 languages. pus (see Figure 1) is closely related to Duong et al. (2015a); Gouws and Søgaard (2015); and Wick et al. (2015). Our work has made use of dictionaries that are automatically extracted from bilingual corpora. An alternative approach would be to use hand-crafted translation lexicons, for example, PanLex (Baldwin et al., 2010) (e.g. see Duong et al. (2015b)), which covers 1253 language varieties, Google translate (e.g., see Ammar et al. (2016c)), or Wiktionary (e.g., see Durrett et al. (2012) for an approach that uses Wiktionary for cross-lingual transfer). These resources are potentially very rich sources of information. Future work should investigate whether they can give improve"
Q17-1020,P15-1119,0,0.524818,"; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments sh"
Q17-1020,N12-1015,0,0.0509328,"Missing"
Q17-1020,P07-2045,0,0.00801455,"Missing"
Q17-1020,2005.mtsummit-papers.11,0,0.236744,"of cross-lingual syntactic transfer with limited resources of monolingual and translation data. Specifically, we use the Bible corpus of Christodouloupoulos and Steedman (2014) as a source of translation data, and Wikipedia as a source of monolingual data. We deliberately limit ourselves to the use of Bible translation data because it is available for a very broad set of languages: the data from Christodouloupoulos and Steedman (2014) includes data from 100 languages. The Bible data contains a much smaller set of sentences (around 24,000) than other translation corpora, for example Europarl (Koehn, 2005), which has around 2 million sentences per language pair. This makes it a considerably more challenging corpus to work with. Similarly, our choice of Wikipedia as the source of monolingual data is motivated by the availability of Wikipedia data in a very broad set of languages. 279 Transactions of the Association for Computational Linguistics, vol. 5, pp. 279–293, 2017. Action Editor: Yuji Matsumoto. Submission batch: 5/2016; Revision batch: 10/2016; 2/2017; Published 8/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. We introduce a set of simple b"
Q17-1020,N16-1121,0,0.730382,"T1 . . . Tm created using the lexicalization method of §3.2. We add all trees in these treebanks to the set P100 of full trees used to initialize the method of Rasooli and Collins (2015). In addition we make use of the representations φ(p) , φ(c) and φ(l) , throughout the learning process. 4 Experiments This section first describes the experimental settings, then reports results. 4.1 Data and Tools Data In the first set of experiments, we consider 7 European languages studied in several pieces of previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016a; Lacroix et al., 2016). More specifically, we use the 7 European languages in the Google universal treebank (v.2; standard data) (McDonald et al., 2013). As in previous work, gold part-of-speech tags are used for evaluation. We use the concatenation of the treebank training sentences, Wikipedia data and the Bible monolingual sentences as our monolingual raw text. Table 3 shows statistics for the monolingual data. We use the Bible from Christodouloupoulos and Steedman (2014), which includes data for 100 languages, as the source of translations. We also conduct experiments with the Europarl data (both with the origin"
Q17-1020,P14-1126,0,0.612416,"y-driven approach of Rasooli and Collins (2015) as follows: consider the treebanks T1 . . . Tm created using the lexicalization method of §3.2. We add all trees in these treebanks to the set P100 of full trees used to initialize the method of Rasooli and Collins (2015). In addition we make use of the representations φ(p) , φ(c) and φ(l) , throughout the learning process. 4 Experiments This section first describes the experimental settings, then reports results. 4.1 Data and Tools Data In the first set of experiments, we consider 7 European languages studied in several pieces of previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016a; Lacroix et al., 2016). More specifically, we use the 7 European languages in the Google universal treebank (v.2; standard data) (McDonald et al., 2013). As in previous work, gold part-of-speech tags are used for evaluation. We use the concatenation of the treebank training sentences, Wikipedia data and the Bible monolingual sentences as our monolingual raw text. Table 3 shows statistics for the monolingual data. We use the Bible from Christodouloupoulos and Steedman (2014), which includes data for 100 languages, as the source o"
Q17-1020,E17-1022,0,0.066113,"Missing"
Q17-1020,N06-1020,0,0.182531,"Missing"
Q17-1020,D11-1006,0,0.449841,"Missing"
Q17-1020,P13-2017,0,0.529728,"each language i ∈ {1 . . . m}. Part-of-speech (POS) data. We have handannotated POS data for all languages L1 . . . Lm+1 . We assume that the data uses a universal POS set that is common across all languages. Monolingual data. We have monolingual, raw text for each of the (m + 1) languages. We use Di to refer to the monolingual data for the ith language. Translation data. We have translation data for all language pairs. We use Bi,j to refer to translation data for the language pair (i, j) where i, j ∈ {1 . . . (m + 1)} and i 6= j. In our main experiments we use the Google universal treebank (McDonald et al., 2013) as our source language treebanks2 (this treebank provides universal dependency relations and POS tags), Wikipedia data as our monolingual data, and the Bible from Christodouloupoulos and Steedman (2014) as the source of our translation data. In additional experiments we use the Europarl corpus as a source of translation data, in order to measure the impact of using the smaller Bible corpus. 2.3 A Baseline Approach: Delexicalized Parsers with Self-Training Given the data assumption of a universal POS set, the feature vectors φ(p) (x, y) can be shared across languages. A simple approach is then"
Q17-1020,P12-1066,0,0.378524,"d, for three groups of languages for the universal dependencies experiments in Table 9: G1 (languages with UAS ≥ 80), G2 (languages with 70 ≤ UAS < 80), G3 (languages with UAS < 70). The rows are sorted by frequency in the G1 languages. approach described in this paper is a simple form of treebank translation, where we use a word-to-word translation model. In spite of its simplicity, it is an effective approach. A number of authors have considered incorporating universal syntactic properties, such as dependency order, by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016a). Selective sharing of syntactic properties is complementary to our work. We used a very limited form of selective sharing, through the WALS properties, in our baseline approach. More recently, Wang and Eisner (2016) have developed a synthetic treebank as a universal treebank to help learn parsers for new languages. Mart´ınez Alonso et al. (2017) try a very different approach in cross-lingual transfer by using a ranking approach. A number of authors (T¨ackstr¨om et al., 2012; Guo et al., 2015; Guo et al., 2016) have intr"
Q17-1020,J03-1002,0,0.0101398,"Missing"
Q17-1020,D15-1039,1,0.859707,"Missing"
Q17-1020,P15-2040,0,0.153434,"Missing"
Q17-1020,E17-1021,0,0.0559612,"Missing"
Q17-1020,P15-1124,1,0.898266,"Missing"
Q17-1020,N12-1052,0,0.226978,"Missing"
Q17-1020,N13-1126,0,0.542422,"Missing"
Q17-1020,W14-1614,0,0.385343,"Missing"
Q17-1020,W15-1824,0,0.0328584,"Limited Resources Mohammad Sadegh Rasooli and Michael Collins∗ Department of Computer Science, Columbia University New York, NY 10027, USA {rasooli,mcollins}@cs.columbia.edu Abstract for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a"
Q17-1020,Q16-1035,0,0.166533,"of treebank translation, where we use a word-to-word translation model. In spite of its simplicity, it is an effective approach. A number of authors have considered incorporating universal syntactic properties, such as dependency order, by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016a). Selective sharing of syntactic properties is complementary to our work. We used a very limited form of selective sharing, through the WALS properties, in our baseline approach. More recently, Wang and Eisner (2016) have developed a synthetic treebank as a universal treebank to help learn parsers for new languages. Mart´ınez Alonso et al. (2017) try a very different approach in cross-lingual transfer by using a ranking approach. A number of authors (T¨ackstr¨om et al., 2012; Guo et al., 2015; Guo et al., 2016) have introduced methods that learn cross-lingual representations that are then used in syntactic transfer. Most of these approaches introduce constraints to a clustering or embedding algorithm that encourage words that are translations of each other to have similar representations. Our method of de"
Q17-1020,K15-1008,0,0.0518575,"ds (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (201"
Q17-1020,I08-3008,0,0.431145,"Missing"
Q17-1020,D15-1213,0,0.192948,"nnotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T¨ackstr¨om et al., 2012; Durrett et al., 2012; Duong et al., 2015a; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016a). We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Raso"
Q17-1020,P11-2033,0,0.125177,"each of our languages. All numbers are in millions. Brown Clustering Algorithm We use the off-theshelf Brown clustering tool5 (Liang, 2005) to train monolingual Brown clusters with 500 clusters. The monolingual Brown clusters are used as features over lexicalized values created in φ(l) , and in selftraining experiments. We train our cross-lingual clustering with the off-the-shelf-tool6 from Stratos et al. (2015). We set the window size to 2 with a cluster size of 500.7 Parsing Model We use the k-beam arc-eager dependency parser of Rasooli and Tetreault (2015), which is similar to the model of Zhang and Nivre (2011). We modify the parser such that it can use both monolingual and cross-lingual word cluster features. The parser is trained using the the maximum violation update strategy (Huang et al., 2012). We use three epochs of training for all experiments. We use the DEPENDABLE Tool (Choi et al., 2015) to calculate significance tests on several of the comparisons (details are given in the captions to tables 5, 6, and 9). cient Greek, Basque, Catalan, Galician, Gothic, Irish, Kazakh, Latvian, Old Church Slavonic, and Tamil). We also excluded Arabic, Hebrew, Japanese and Chinese, as these languages have t"
Q19-1026,D15-1075,0,0.0508562,"ent set also appear in the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for t"
Q19-1026,P17-1171,0,0.275889,"Missing"
Q19-1026,D18-1241,0,0.0611339,"choice of supporting facts is somewhat subjective. They set high human upper bounds by selecting, for each example, the score maximizing partition of four annotations into one prediction and three references. The reference labels chosen by this maximization are not representative of the reference labels in HotpotQA’s evaluation set, and it is not clear that the upper bounds are achievable. A more robust approach is to keep the evaluation distribution fixed, and calculate an acheivable upper bound by approximating the expectation over annotations—as we have done for NQ in Section 5. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer. QuAC also prevents the questioner from seeing the evidence text. Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ. In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text. 3 Task Definition and Data Collection Natural Questions contains (question, wikipedia page, long answer, short answer) quadruples where: the question see"
Q19-1026,P18-1078,0,0.217148,"(P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and the super-annotator upper bound. The human performances marked with † are evaluated on a sample of five annotations from the 25-way annotated data introduced in Section 5. To address (ii), we tried adding special NULL passages to represent the lack of answer. However, we achieved better performance by training on the subset of questions with answers and then only predicting those answers whose scores exceed a threshold. With these two modifications, we are able to apply Document-QA to NQ. We follow Clark and Gardner (2018) in pruning documents down to the set of passages that have highest TFIDF similarity with the question. Under this approach, we consider the top 16 passages as long answers. We consider short answers containing up to 17 words. We train Document-QA for 30 epochs with batches containing 15 examples. The post hoc score threshold is set to 3.0. All of these values were chosen on the basis of development set performance. 6.1 Untrained Baselines NQ’s long answer selection task admits several untrained baselines. The first paragraph of a Wikipedia page commonly acts as a summary of the most important"
Q19-1026,D17-1082,0,0.109935,"annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should n"
Q19-1026,W18-2605,0,0.0359237,"ions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and ans"
Q19-1026,D18-1260,0,0.0463566,"variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the ins"
Q19-1026,C92-2082,0,0.0914034,"elines and tooling divide the annotation task into three conceptual stages, where all three stages are completed by a single annotator in succession. The decision flow through these is illustrated in Figure 2 and the instructions given to annotators are summarized below. Question Identification: Contributors determine whether the given question is good or bad. A good question is a fact-seeking question that can be answered with an entity or explanation. A bad question is ambigous, incomprehensible, 3 We pre-define the set of categorical noun phrases used in 4 and 5 by running Hearst patterns (Hearst, 1992) to find a broad set of hypernyms. Part of speech tags and entities are identified using Google’s Cloud NLP API: https://cloud. google.com/natural-language. 456 2) k -way annotations (with k = 25) on a subset of the data. Post hoc evaluation of non-null answers leads directly to a measure of annotation precision. As is common in information-retrieval style problems such as long-answer identification, measuring recall is more challenging. However, we describe how 25-way annotated data provide useful insights into recall, particularly when combined with expert judgments. dependent on clear false"
Q19-1026,D16-1241,0,0.0190295,"nts to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of deciding whether a pa"
Q19-1026,P16-1144,0,0.031038,"to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of"
Q19-1026,D17-1215,0,0.0458559,"questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with free"
Q19-1026,P17-1147,0,0.26169,"ssues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduc"
Q19-1026,P02-1040,0,0.108627,"asoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading co"
Q19-1026,N18-1101,0,0.0325746,"n the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for the NQ task. This system"
Q19-1026,P18-2124,0,0.322524,"answer (s) can be a span or set of spans (typically entities) within l that answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as"
Q19-1026,D16-1264,0,0.822388,"chine translation, speech recognition, and image recognition. One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches. A second major factor has been the existence of large quantities of training data for these systems. Open-domain question answering (QA) is a benchmark task in natural language understanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU. Several pieces of recent work have introduced QA data sets (e.g., Rajpurkar et al., 2016; Reddy et al., 2018). However, in contrast to tasks where it is relatively easy to gather naturally occurring examples,1 the definition of a suitable QA task, and the development of a methodology for annotation and evaluation, is challenging. Key issues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensure annotation quality; and the metrics used for evaluation. For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper. This paper introduces Natu"
Q19-1026,D15-1237,0,0.10531,"D data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discus"
Q19-1026,D18-1259,0,0.167869,"answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Se"
Q19-1026,D13-1020,0,0.136104,"quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sent"
Q19-1026,D16-1244,1,\N,Missing
Q19-1026,Q18-1023,0,\N,Missing
W01-1802,J97-4005,0,0.0749254,"Missing"
W01-1802,P99-1069,0,0.0623764,"Missing"
W02-1001,W02-1001,1,0.148081,"Missing"
W02-1001,J95-4004,0,0.081156,"Missing"
W02-1001,P02-1034,1,0.173507,"Missing"
W02-1001,J93-2004,0,0.0980755,"Missing"
W02-1001,W95-0107,0,0.0745956,"Missing"
W02-1001,W96-0213,0,\N,Missing
W02-1001,P02-1062,1,\N,Missing
W02-2236,W01-0520,1,0.885176,"Missing"
W02-2236,E99-1025,1,0.821222,"Missing"
W02-2236,2000.iwpt-1.9,1,0.941088,"Chen*, Srinivas Bangalore*, Michael Collins*, and Owen Rambowt *AT&T Labs-Research, t University ofPennsylvania {jchen,srini,mcollins}@research.att.com,rarnbow@unagi.cis.upenn.edu 1. Introduction As shown by Srinivas (1997), standard n-gram modeling may be used to perfonn supertag disambiguation with accuracy that is adequate for partial parsing, but in general not sufficient for füll parsing. A serious problem is that n-gram modeling usually considers a very small, fixed context and does not perfonn weil with large tag sets, such as those generated by automatic grammar extraction (Xia, 1999; Chen and Vijay-Shanker, 2000; Chlang, 2000). As an alternative, Chen, Bangalore and Vijay-Shanker (1999) introduce class-based supertagging. An example of class tagging is n-best trigram-based supertagging, which assigns to each word the top n most likely supertags as detennined by an n-gram supertagging model. Class-based supertagging can be performed much more accurately than supertagging with only a small increase in ambiguity. In a second phase, the most likely candidate from the class is chosen. In this paper, we investigate an approach to such a choice based on reranking a set of candidate supertags and their confi"
W02-2236,P00-1058,0,0.0381466,"g has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (M"
W02-2236,P97-1003,1,0.826017,"Missing"
W02-2236,J93-2004,0,0.022848,"Missing"
W02-2236,W96-0213,0,0.360487,"Missing"
W02-2236,W00-2027,0,0.029134,"r Output We claim that supertagging is a viable option to explore for use as a preprocessing step in order to speed up füll parsing. In order to substantiate this claim, we perform exploratory experiments that show the relationship between n-best supertagging and parsing performance. Using the grammar that is described in Section 2.2, we train n-best supertaggers on Sections 02-21 of the Perut Treebank. For each supertagger, we supertag Section 22, which consists of about 40,100 words in 1,700 sentences. We then feed the resulting output through the LEM parser, a head-driven TAG chart parser (Sarkar, 2000). Given an input sentence and a grammar, this parser either outputs nothing, or a packed derivation forest of every parse that can be assigned to the sentence by the grammar. lt does not retum partial parses. The results of these experiments are shown in Table 1. The input to the parser can be the output of either a 1, 2, or 4-best supertagger. lt can also be sentences where each word is associated with all of the supertags with that word's part of speech, as detennined by a trigram part of speech tagger. This is labeled as &quot;POS-tag&quot; in the table. Lastly, it can simply be sentences where each"
W02-2236,N01-1023,0,0.045366,"rks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz, 1993). lt contains 3964 tree frames (non-lexicalized elementary trees). The parameters of extraction are set as follows. Each tree frame contains nodes that are labeled using a label set similar to the XTAG (XTAG-Group, 2001) label set. Furthermore, tree frames are extracted corresponding to a &quot;moderate&quot; do"
W02-2236,C88-2121,0,0.206461,"Missing"
W02-2236,1997.iwpt-1.22,0,0.0396543,"hroughout this section, we describe the kinds oflinguistic resources that we use in all of our experiments and the kinds of notation that we will employ in the rest of this paper. 2.1. Supertagging Supertagging (Bangalore and Joshl, 1999) is the process of assigning the best TAG elementary tree, or supertag, to each word in the input sentence. lt performs the task of parsing disambiguation to such an extent that it may be characterized as providing an almost parse. There exist linear time approaches to supertagging, providing one promising route to linear time parsing disambiguation. However, Srinivas (1997) shows that standard n-grarn modeling may be used to perform supertagging with accuracy that is adequate for partial parsing, but not for füll parsing. On the other hand, n-gram modeling of supertagging has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Pr"
W02-2236,N01-1003,1,0.882532,"Missing"
W02-2236,W00-1208,0,0.0225814,"Missing"
W02-2236,W98-0143,0,0.0337296,"Missing"
W02-2236,J99-2004,1,\N,Missing
W02-2236,J03-4003,1,\N,Missing
W06-1628,P05-1066,1,0.50905,"Missing"
W06-1628,P05-1039,0,0.00789455,"o [np-sb significant legal, practical and economic differences] s-rc prels-sb die vp pp-mo 1 appr an pdat jenem nn tag pp-mo 2 appr in ne tschernobyl vvpp-hd gez¨undet vafin-hd wurde Paraphrase: which [pp-mo on that day] [pp-mo in chernobyl] released were A crucial step in our approach is the extraction of training examples from a translation corpus. Each training example consists of a German clause paired with an English AEP (see Figure 2). In our experiments, we used the Europarl corpus (Koehn, 2005). For each sentence pair from this data, we used a version of the German parser described by Dubey (2005) to parse the German component, and a version of the English parser described by Collins (1999) to parse the English component. To extract AEPs, we perform the following steps: English AEP STEM: be SPINE: SBAR-A IN that S NP-A VP V NP-A VOICE: SUBJECT: OBJECT: WH: MODALS: INFL: MOD1: MOD2: active 1 2 NULL has been null null NP and PP Alignment To align NPs and PPs, first all German and English nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence and aligned using GIZA++ (Och and Ney, 2003). Next, each NP in an English tree is aligned to an NP or PP i"
W06-1628,P03-2041,0,0.281283,"ve model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al. (2005) describe similar methods that make use of dependency representa"
W06-1628,N04-1014,0,0.0125371,"990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) des"
W06-1628,N03-1017,0,0.0583972,"e derived from the concept of an extended projection in lexicalized tree adjoining grammars (LTAG) (Frank, 2002), with the addition of alignment information that is based on work in synchronous LTAG (Shieber and Schabes, 1990). A key contribution of this paper is a method for learning to map German clauses to AEPs using a featurebased model with a perceptron learning algorithm. We performed experiments on translation from German to English on the Europarl data set. Evaluation in terms of both BLEU scores and human judgments shows that our system performs similarly to the phrase-based model of Koehn et al. (2003). This paper proposes a statistical, treeto-tree model for producing translations. Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, featurebased model for prediction of these targetlanguage syntactic structures—which we call aligned extended projections, or AEPs. An evaluation of the method on translation from German to English shows similar performance to the phrase-based model of Koehn et al. (2003). 1 Introduction Phrase-based approaches (Och and"
W06-1628,2005.mtsummit-papers.11,0,0.106381,"nd adja wirtschaftliche nn unterschiede Paraphrase: [pp-mo between the two pieces of legislation] exist so [np-sb significant legal, practical and economic differences] s-rc prels-sb die vp pp-mo 1 appr an pdat jenem nn tag pp-mo 2 appr in ne tschernobyl vvpp-hd gez¨undet vafin-hd wurde Paraphrase: which [pp-mo on that day] [pp-mo in chernobyl] released were A crucial step in our approach is the extraction of training examples from a translation corpus. Each training example consists of a German clause paired with an English AEP (see Figure 2). In our experiments, we used the Europarl corpus (Koehn, 2005). For each sentence pair from this data, we used a version of the German parser described by Dubey (2005) to parse the German component, and a version of the English parser described by Collins (1999) to parse the English component. To extract AEPs, we perform the following steps: English AEP STEM: be SPINE: SBAR-A IN that S NP-A VP V NP-A VOICE: SUBJECT: OBJECT: WH: MODALS: INFL: MOD1: MOD2: active 1 2 NULL has been null null NP and PP Alignment To align NPs and PPs, first all German and English nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence a"
W06-1628,P04-1083,0,0.0252628,"y Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the tar"
W06-1628,P96-1023,0,0.0405814,"y related to previous work on synchronous tree adjoining grammars (Shieber and Schabes, 1990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in"
W06-1628,J03-1002,0,0.00456293,"from this data, we used a version of the German parser described by Dubey (2005) to parse the German component, and a version of the English parser described by Collins (1999) to parse the English component. To extract AEPs, we perform the following steps: English AEP STEM: be SPINE: SBAR-A IN that S NP-A VP V NP-A VOICE: SUBJECT: OBJECT: WH: MODALS: INFL: MOD1: MOD2: active 1 2 NULL has been null null NP and PP Alignment To align NPs and PPs, first all German and English nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence and aligned using GIZA++ (Och and Ney, 2003). Next, each NP in an English tree is aligned to an NP or PP in the corresponding German tree in a way that is consistent with the word-alignment information. That is, the words dominated by the English node must be aligned only to words dominated by the German node, and vice versa. Note that if there is more than one German node that is consistent, then the one rooted at the minimal subtree is selected. STEM: be SPINE: S NP-A VP V NP-A VOICE: SUBJECT: OBJECT: WH: MODALS: INFL: MOD1: MOD2: MOD3: active “there” 3 NULL NULL are post-verb pre-sub null Clause alignment, and AEP Extraction The next"
W06-1628,P05-1034,0,0.139788,"etical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al. (2005) describe similar methods that make use of dependency representations. Syntactic parsers in the target language have been used as language models in translation, giving some improvement in accuracy (Charniak et al., 2001). The work of Gildea (2003) involves methods that make use of syntactic information in both the source and target languages. Other work has attempted to incorporate syntac3 A Translation Architecture Based on Aligned Extended Projections 3.1 Background: Extended Projections (EPs) Extended projections (EPs) play a crucial role in the lexicalized tree adjoining grammar (LTAG) (J"
W06-1628,N06-1032,0,0.0250239,"pproaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al. (2005) describe similar methods that make use of dependency representations. Syntactic parsers in the target language have been used as language models in translation, giving some improvement in accuracy (Charniak et al., 2001). The work of Gildea (2003) involves methods that make use of syntactic information in both"
W06-1628,W04-3312,0,0.0579035,"the sentence We know that the main obstacle has been the predictable resistance of manufacturers. tic information through reranking approaches applied to n-best output from phrase-based systems (Och et al., 2004). Another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover a word order that is closer to the target language (Collins et al., 2005; Xia and McCord, 2004). Our approach is closely related to previous work on synchronous tree adjoining grammars (Shieber and Schabes, 1990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight ("
W06-1628,C90-3045,0,0.627921,"Cowan MIT CSAIL Ivona Ku˘cerov´a MIT Linguistics Department Michael Collins MIT CSAIL brooke@csail.mit.edu kucerova@mit.edu mcollins@csail.mit.edu Abstract where the target-language parse tree is broken down into a sequence of clauses, and each clause is then translated separately. A central concept we introduce in the translation of clauses is that of an aligned extended projection (AEP). AEPs are derived from the concept of an extended projection in lexicalized tree adjoining grammars (LTAG) (Frank, 2002), with the addition of alignment information that is based on work in synchronous LTAG (Shieber and Schabes, 1990). A key contribution of this paper is a method for learning to map German clauses to AEPs using a featurebased model with a perceptron learning algorithm. We performed experiments on translation from German to English on the Europarl data set. Evaluation in terms of both BLEU scores and human judgments shows that our system performs similarly to the phrase-based model of Koehn et al. (2003). This paper proposes a statistical, treeto-tree model for producing translations. Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information fr"
W06-1628,P05-1033,0,0.0595757,"o account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al. (2005) describe similar methods that make use of dependency representations. Syntactic parsers in the target language have been used as language models in translation, giving some improvement in"
W06-1628,J97-3002,0,0.245519,"oach is closely related to previous work on synchronous tree adjoining grammars (Shieber and Schabes, 1990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation. Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG"
W06-1628,W02-1001,1,0.137805,"ct position exists in the SPINE). For any inF (x) = arg max y∈GEN(x) SCORE(x, y) (2) To decode with the model we use a beam-search method. The method incrementally builds an AEP in the decision order d1 , d2 , . . . , dN . At each point, a beam contains the top M highest–scoring partial paths for the first m decisions, where M is taken to be a fixed number. The score for any partial path is defined in Eq. 1. The ADVANCE function is used to specify the set of possible decisions that can extend any given path in the beam. To train the model, we use the averaged perceptron algorithm described by Collins (2002). This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark (2004).5 The perceptron algorithm is a convenient choice because it converges quickly — usually taking only a few iterations over the training set (Collins, 2002; Collins and Roark, 2004). 5.2 The Features of the Model The model’s features allow it to capture dependencies between the AEP and the German clause, as well as dependencies between different parts of the AEP itself. The features included in φ¯ 5 Future work may consider alternative algorithms, such as those described by D"
W06-1628,C04-1073,0,0.0119822,"NP-A been Figure 1: Extended projections for the verbs know and been, and for the noun obstacle. The EPs were taken from the parse tree for the sentence We know that the main obstacle has been the predictable resistance of manufacturers. tic information through reranking approaches applied to n-best output from phrase-based systems (Och et al., 2004). Another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover a word order that is closer to the target language (Collins et al., 2005; Xia and McCord, 2004). Our approach is closely related to previous work on synchronous tree adjoining grammars (Shieber and Schabes, 1990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002). A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP. Note also that our model may include features that take into account any part of the German clause. Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine tra"
W06-1628,P01-1067,0,0.249896,"se of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers. Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars. Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English. Yamada and Knight (2001) and Galley et al. (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al. (2005) describe similar methods that make use of dependency representations. Syntactic parsers in the target language have been used as language models in translation, giving some improvement in accuracy (Charniak et al., 2001). The work of Gildea (2003) involves methods that make use of syntactic information in both the source and target languages. Other work has attempted to incorporate syntac3 A Translation Architecture Based on Aligned Extended Projections 3.1 Backgr"
W06-1628,P04-1015,1,0.43475,"we use a beam-search method. The method incrementally builds an AEP in the decision order d1 , d2 , . . . , dN . At each point, a beam contains the top M highest–scoring partial paths for the first m decisions, where M is taken to be a fixed number. The score for any partial path is defined in Eq. 1. The ADVANCE function is used to specify the set of possible decisions that can extend any given path in the beam. To train the model, we use the averaged perceptron algorithm described by Collins (2002). This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark (2004).5 The perceptron algorithm is a convenient choice because it converges quickly — usually taking only a few iterations over the training set (Collins, 2002; Collins and Roark, 2004). 5.2 The Features of the Model The model’s features allow it to capture dependencies between the AEP and the German clause, as well as dependencies between different parts of the AEP itself. The features included in φ¯ 5 Future work may consider alternative algorithms, such as those described by Daum´e and Marcu (2005). 237 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 main verb any ver"
W06-1628,N04-1035,0,\N,Missing
W06-1628,J03-4003,1,\N,Missing
W06-1628,P03-1011,0,\N,Missing
W06-1628,J04-4002,0,\N,Missing
W06-1628,N04-1021,0,\N,Missing
W06-1628,W90-0102,0,\N,Missing
W08-2102,D07-1101,1,0.845906,"y a combination of the two structures in figure 2. hh,m,li∈D(y) Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine hi, ηi to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = hE, Di is a derivation, then: • S(y) is a set of sibling dependencies. Each sibling dependency is a tuple hh, m, l, si. For each hh, m, l, si ∈ S the tuple hh, m, li is an element of D; there is one member of S for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place). • G(y) is a set of grandparent dependencies of type 1. Each type 1 grandparent dependency is a tuple hh, m, l, gi. There is one member of G for every member of D. The additional information, the inde"
W08-2102,P05-1022,0,0.906598,"th in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigr"
W08-2102,A00-2018,0,0.991402,"hms). (2) Use of a lower-order model for pruning. The O(n4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic struc"
W08-2102,P04-1014,0,0.0340546,"ere is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependenc"
W08-2102,W07-1202,0,0.0117506,"r may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass pa"
W08-2102,P97-1003,1,0.899312,"ould calculate 4.2 Extracting Derivations from Parse Trees In the experiments in this paper, the following three-step process was used: (1) derivations were extracted from a training set drawn from the Penn WSJ treebank, and then used to train a parsing model; (2) the test data was parsed using the resulting model, giving a derivation for each test data sentence; (3) the resulting test-data derivations were mapped back to Penn-treebank style trees, using the method described in section 2.1. To achieve step (1), we first apply a set of headfinding rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract µ(x, h, m, hVP VBD NPi) = µ1 (x, h, m, hVPi) × µ2 (x, h, m, hVBDi) ×µ3 (x, h, m, hNPi) Training the three models, and calculating the marginals, now has a grammar constant equal 14 PPK07 FKM08 CH2000 CO2000 PK07 this paper CJ05 H08 CO2000(s24) this paper (s24) precision – 88.2 89.5 89.9 90.2 91.4 – – 89.6 91.1 recall – 87.8 89.6 89.6 89.9 90.7 – – 88.6 89.9 F1 88.3 88.0 89.6 89.8 90.1 91.1 91.4 91.7 89.1 90.5 α 10−4 10−5 10−6 CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, f"
W08-2102,W02-1001,1,0.264623,"ing such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y ∗ for an input x is y ∗ = arg max w · f (x, y) y∈Y(x) (1) where Y(x) is the set of possible labels for the input x; f (x, y) ∈ Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibi"
W08-2102,P08-1109,0,0.229764,"approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features. 1.1 Related Work Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak"
W08-2102,P08-1067,0,0.239331,"(x, h, m, hNPi) Training the three models, and calculating the marginals, now has a grammar constant equal 14 PPK07 FKM08 CH2000 CO2000 PK07 this paper CJ05 H08 CO2000(s24) this paper (s24) precision – 88.2 89.5 89.9 90.2 91.4 – – 89.6 91.1 recall – 87.8 89.6 89.6 89.9 90.7 – – 88.6 89.9 F1 88.3 88.0 89.6 89.8 90.1 91.1 91.4 91.7 89.1 90.5 α 10−4 10−5 10−6 CH2000, CO2000, PK07, CJ05 and H08 are results on section 23 of the Penn WSJ treebank, for the models of Petrov et al. (2007), Finkel et al. (2008), Charniak (2000), Collins (2000), Petrov and Klein (2007), Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) “s24” denotes results on section 24 of the treebank. s23 92.0 92.5 93.5 2nd stage oracle F1 speed 97.0 5:15 97.9 11:45 98.5 21:50 F1 91.1 91.6 92.0 Table 3: Effect of the beam size, controlled by α, on the performance of the parser on the development set (1,699 sentences). In each case α refers to the beam size used in both training and testing the model. “active”: percentage of dependencies that remain in the beam out of the total number of labeled dependencies (1,000 triple labels times 1,138,167 unl"
W08-2102,P99-1069,0,0.135563,"surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. 1 Introduction In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y ∗ for an input x is y ∗ = arg max w · f (x, y) y∈Y(x) (1) where Y(x) is the set of possible labels for the input x; f (x, y) ∈ Rd is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is t"
W08-2102,P08-1068,1,0.868119,"parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use o"
W08-2102,E06-1011,0,0.855269,"g and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar"
W08-2102,P05-1012,0,0.897191,"table”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 2.1 10 The TAG-Based Parsing Model Derivations (a) (b) S clear from context, we will use ηi to refer to the spine in E corresponding to the i’th word. • D is a set of n dependencies. Each dependency is a tuple hh, m, li. Here h is the index of the head-word of the dependency, corresponding to the spine ηh which contains a node that is being adjoined into. m is the index of the modifier-word of the dependency, corresponding to the spine ηm which is being adjoine"
W08-2102,N07-1051,0,0.440491,"model for pruning. The O(n4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n3 H) time where H ≪ G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser. Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006). ald et al., 2005). Dependency parsing can be implemented in O(n3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail i"
W08-2102,P07-1079,0,0.0224719,"ent parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however that the lower-order parser that we use to restrict the search space of the TAG-based parser is based on the work of McDonald et al. (2005). See also (Sagae et al., 2007) for a method that uses a dependency parser to restrict the search space of a more complex HPSG parser. 2.1 10 The TAG-Based Parsing Model Derivations (a) (b) S clear from context, we will use ηi to refer to the spine in E corresponding to the i’th word. • D is a set of n dependencies. Each dependency is a tuple hh, m, li. Here h is the index of the head-word of the dependency, corresponding to the spine ηh which contains a node that is being adjoined into. m is the index of the modifier-word of the dependency, corresponding to the spine ηm which is being adjoined into ηh . l is a label. The l"
W08-2102,H05-1102,0,0.049514,"rrors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon2 This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be “splittable”, allowing use of the efficient parsing algorithms of Eisner (2000). A derivation in our model is a pair hE, Di where E is a set of spines, and D is a set of dependencies 1 Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing. Our parser could, of course, be used as the first-stage parser in a reranking approach. 2 Note however t"
W08-2102,W04-3201,0,0.53722,"3 G) time, where n is the length of the sentence, and G is a grammar constant. The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing. As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work. The following ideas are central to our approach: (1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model. The formalism that we describe"
W08-2102,W03-3023,0,0.573252,"paper 1st stage active coverage 0.07 97.7 0.16 98.5 0.34 99.0 s24 91.0 91.7 92.5 dictionary listing the spines that have been seen with this POS tag in training data; during parsing we only allow spines that are compatible with this dictionary. (For test or development data, we used the part-of-speech tags generated by the parser of (Collins, 1997). Future work should consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.) Table 2: Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. 5 Experiments Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set. Sections 23 and 24 were used as test sets. The model was trained for 20 epochs with the averaged perceptron algorithm, w"
W08-2102,P00-1058,0,\N,Missing
W13-3507,P12-1024,1,0.909167,"g statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the “correct” model parameters. In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs. Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th train"
W13-3507,N13-1015,1,0.903398,"hidden state as well as the label. Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the expectation-maximization (EM) algorithm which 56 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 3.1 success for the R-HMM spectral algorithm. The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states. Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm. 2 We distinguish row vectors from column vectors when such distinction is necessary. We use a superscript > to denote the transpose operation. We write [n] to denote the set {1, 2, . . . , n} for any integer n ≥ 1. For any vector v ∈ Rm , diag(v) ∈ Rm×m is a diagonal matrix with entries v1 . . . vm . For any statement S, we us"
W13-3507,E12-1042,0,0.0933967,"le and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th training example consists of a sequence of ob(j) (j) servations x1 ...xN paired with a sequence of (j) (j) labels a1 ...aN and asked to predict the correct labels on a test set of observations. A common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a hidden Markov model (HMM). The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai−1 . This independence"
W13-3507,P05-1010,0,0.0362596,"or performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a,"
W13-3507,P92-1017,0,0.411969,"ning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of genera"
W13-3507,P06-1055,0,0.0566726,"his type are crucial for performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] a"
W13-3507,D07-1094,0,0.0373788,"ated models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of generating b ∈ [l] and h0 ∈"
W15-1511,N10-1083,0,0.0348946,"Missing"
W15-1511,J92-4003,0,0.766904,"Missing"
W15-1511,D10-1056,0,0.0697824,"Missing"
W15-1511,A88-1019,0,0.337735,"Missing"
W15-1511,W99-0613,1,0.628841,"Missing"
W15-1511,P11-1061,0,0.0827887,"Missing"
W15-1511,N13-1014,0,0.169427,"Missing"
W15-1511,N06-1041,0,0.102163,"Missing"
W15-1511,D07-1031,0,0.129676,"Missing"
W15-1511,N13-1139,0,0.152127,"Missing"
W15-1511,D12-1127,0,0.0295142,"Missing"
W15-1511,P13-2017,0,0.036997,"Missing"
W15-1511,N04-1043,0,0.292628,"types partition word types while imposing a first-order sequence structure on tag types. 2.2 Brown et al. (1992) model This class of restricted HMMs is precisely the model proposed by Brown et al. (1992)—henceforth the Brown model. A popular use of this model is agglomerative word clustering: the result is a hierarchy over word types, such as the one shown in Figure 1(a). In practice, each word type is represented as a bit-string indicating the path from the root. These bit-strings have been used as discrete (binary) features in various natural language tasks such as named-entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008). Recently, Stratos et al. (2014) showed that a variant of canonical correlation analysis (CCA) (Hotelling, 1936) can be used to provably recover the clusters under the Brown model. Under this method, each word is represented as an mdimensional vector where m is the number of hidden states in the model: see Figure 1(b) for illustration. This can be used as m real-valued features in discrminative models. Note that real-valued representations can reflect ambiguity (e.g., set in the illustration) which can be seen as a benefit over discrete representation"
W15-1511,W07-1516,0,0.066709,"Missing"
W15-1511,P05-1044,0,0.127464,"Missing"
W15-1511,Q13-1001,0,0.0540522,"Missing"
W15-1511,P95-1026,0,0.596539,"Missing"
W15-1511,P08-1068,1,\N,Missing
W95-0103,C94-2195,0,0.444591,"Missing"
W95-0103,J93-1005,0,0.414489,"Missing"
W95-0103,J93-2004,0,0.0322583,"Missing"
W95-0103,H94-1048,0,0.148966,"Missing"
W95-0103,H90-1056,0,\N,Missing
W98-1105,A97-1029,1,0.746842,"Missing"
W98-1105,W97-1002,0,0.0568981,"Missing"
W98-1105,A88-1019,0,0.0523595,"Missing"
W98-1105,M93-1023,0,0.225535,"Missing"
W98-1105,J93-2004,0,0.0348776,"Missing"
W98-1105,W96-0213,0,0.128165,"Missing"
W98-1105,M95-1014,0,\N,Missing
W98-1105,C92-3145,0,\N,Missing
W98-1105,P94-1013,0,\N,Missing
W99-0613,P96-1025,1,0.0689471,"Missing"
W99-0613,C92-2082,0,0.0838777,"Missing"
W99-0613,W97-0313,0,0.473378,"Missing"
W99-0613,P95-1026,0,0.835545,"Missing"
W99-0613,P99-1008,0,0.0522558,"Missing"
W99-0613,A97-1029,0,0.259686,"Missing"
