2020.acl-demos.6,lemnitzer-etal-2008-enriching,0,0.0419372,"Torrisi4 , and Roberto Navigli1,2 1 Sapienza NLP Group Department of Computer Science, Sapienza University of Rome 3 Department of Literature and Modern Cultures, Sapienza University of Rome 4 Babelscape, Italy firstname.lastname@uniroma1.it, lastname@babelscape.com 2 Abstract wide array of languages covered by LKBs like BabelNet1 (Navigli and Ponzetto, 2012), or the Open Multilingual WordNet (Bond and Foster, 2013). Moreover, it is widely acknowledged that the performance of a knowledge-based WSD system is strongly correlated with the structure of the LKB employed (Boyd-Graber et al., 2006; Lemnitzer et al., 2008; Navigli and Lapata, 2010; Ponzetto and Navigli, 2010). In fact, the knowledge available within LKBs reflects the fact that words can be linked via two types of semantic relations: paradigmatic relations – i.e. the most frequently encountered relations in LKBs – concern the substitution of lexical units, and determine to which level in a hierarchy a language unit belongs by semantic analogy with units similar to it; conversely, syntagmatic relations concern the positioning of such units, by linking elements belonging to the same hierarchical level (e.g., words), which appear in the same conte"
2020.acl-demos.6,E09-1005,0,0.130572,"nset to which the gloss refers to each of the synsets that have been tagged in the gloss itself. 3 Architecture of SyntagRank SyntagRank is a knowledge-based disambiguation system which uses the PPR algorithm to determine 3 In SyntagRank, a context is equivalent to a single whole sentence. Therefore, given an input paragraph made up of, say, three sentences, the system will perform the disambiguation task separately for each of these three sentences. 2 http://wordnetcode.princeton.edu/ glosstag.shtml 38 the most appropriate sense of a given word in context. This approach, already discussed by Agirre and Soroa (2009), is here presented in an optimized, rebuilt version, employing the LKBs described in Section 2.1 to achieve state-of-the-art knowledgebased performance across five languages: English, German, French, Spanish, and Italian. Our architecture (Figure 1) is composed of three main modules: (i) multilingual NLP pipeline, (ii) candidate retrieval, and (iii) disambiguator. 3.1 Multilingual NLP Pipeline In order to allow the user to provide an unprocessed text as input for SyntagRank to disambiguate, our system employs a multilingual NLP pipeline which preliminarily performs the functions of tokenizati"
2020.acl-demos.6,P14-5010,0,0.00443539,"Missing"
2020.acl-demos.6,R19-1015,1,0.863782,"tilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/). 1 Introduction In Natural Language Processing, Word Sense Disambiguation (WSD) is an open problem concerning lexical ambiguity. It is aimed at determining which sense – among a finite inventory of many – is evoked by a given word in context (Navigli, 2009). This challenge has been tackled by exploiting huge amounts of hand-annotated data in a supervised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing structured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the 1 https://babelnet.org/ 37 Proceedings of the 58th Annual Meetin"
2020.acl-demos.6,D19-1359,1,0.905464,"unit belongs by semantic analogy with units similar to it; conversely, syntagmatic relations concern the positioning of such units, by linking elements belonging to the same hierarchical level (e.g., words), which appear in the same context (e.g., a sentence). As a case in point, a paradigmatic relation exists, independently of a given context, between the words farmn and workplacen (where a farm is a type of workplace), whereas a syntagmatic relation is entertained between the words workv and farmn , e.g., in the sentence ‘her husband works in a farm as a labourer.’ In our most recent study (Maru et al., 2019, SyntagNet), we provided further evidence that the nature of LKBs impacts on system performance: the injection of syntagmatic relations – in the form of disambiguated pairs of co-occurring words – into an existing LKB biased towards paradigmatic knowledge enables knowledge-based systems to rival their supervised counterparts. To make the above results accessible to the research community, in this paper we introduce a Web interface and a RESTful API for SyntagRank, our multilingual WSD system, which applies the Exploiting syntagmatic information is an encouraging research focus to be pursued i"
2020.acl-demos.6,2020.acl-main.255,1,0.837648,"er-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/). 1 Introduction In Natural Language Processing, Word Sense Disambiguation (WSD) is an open problem concerning lexical ambiguity. It is aimed at determining which sense – among a finite inventory of many – is evoked by a given word in context (Navigli, 2009). This challenge has been tackled by exploiting huge amounts of hand-annotated data in a supervised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing structured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the 1 https://babelnet.org/ 37 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
2020.acl-demos.6,P13-1133,0,0.0756131,"Missing"
2020.acl-demos.6,H93-1061,0,0.824814,"execution times5 . Thus, the PPR vector for a precise context (i.e. an input sentence) is calculated simply by determining the weighted average of the pre-computed PPR vectors for each of its nodes6 . The weight factor p(w, s), for each candidate s associated with a content word w, is computed as follows: p(w, s) = 1 f reqws N ∗ |sensesw | (1) where N is the number of content words in the input sentence and sensesw is the set of sense candidates associated with w. Moreover, since the graph connectivity gets denser around most frequent senses (MFS) – according to their distribution in SemCor7 (Miller et al., 1993) –, and in view 5 All the pre-computed PPR vectors are stored in binary format, and are accessed via a memory-mapped file supported by a Least Recently Used (LRU) cache. 6 With regard to our PPR implementation details, we opted for a damping factor of 0.85. In addition, the algorithm performs a variable number of iterations (random walks) over the graph until reaching convergence, i.e. when the difference between the scores of any node computed at two successive iterations falls below a threshold of 10−4 . 7 SemCor is the largest, manually sense-annotated corpus of English, and is currently th"
2020.acl-demos.6,S15-2049,1,0.883015,"using syntagmatic information. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and http://api.syntagnet.org, respectively. Evaluation Acknowledgments In order to assess its performance, we tested SyntagRank on the five English all-words WSD evaluation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of SyntagRank in a multilingual setting, we used the German, Spanish, French and Italian annotations available in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets10 , which is accordant with the BabelNet API 4.0.1 graph and The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union’s Horizon 2020 research and innovation programme. This work was supported in part by the MIUR under the grant “Dipartimenti di eccellenza 20182022” of the Department o"
2020.acl-demos.6,Q14-1019,1,0.97103,"mmatically (accessible at http://api.syntagnet.org/). 1 Introduction In Natural Language Processing, Word Sense Disambiguation (WSD) is an open problem concerning lexical ambiguity. It is aimed at determining which sense – among a finite inventory of many – is evoked by a given word in context (Navigli, 2009). This challenge has been tackled by exploiting huge amounts of hand-annotated data in a supervised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing structured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the 1 https://babelnet.org/ 37 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 37–46 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Personali"
2020.acl-demos.6,S01-1001,0,0.793853,"this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic information. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and http://api.syntagnet.org, respectively. Evaluation Acknowledgments In order to assess its performance, we tested SyntagRank on the five English all-words WSD evaluation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of SyntagRank in a multilingual setting, we used the German, Spanish, French and Italian annotations available in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets10 , which is accordant with the BabelNet API 4.0.1 graph and The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union’s Horizon 202"
2020.acl-demos.6,S13-2040,1,0.878089,"or multilingual Word Sense Disambiguation using syntagmatic information. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and http://api.syntagnet.org, respectively. Evaluation Acknowledgments In order to assess its performance, we tested SyntagRank on the five English all-words WSD evaluation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of SyntagRank in a multilingual setting, we used the German, Spanish, French and Italian annotations available in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets10 , which is accordant with the BabelNet API 4.0.1 graph and The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union’s Horizon 2020 research and innovation programme. This work was supported in part by the MIUR under the grant “Dipartimenti"
2020.acl-demos.6,2020.acl-main.369,1,0.837903,"Stanford CoreNLP pipeline has full coverage, in order to perform the lemmatization for German, French, and Spanish, we use instead TreeTagger (Schmid, 1995). 39 Figure 2: User interface of SyntagRank when the Italian language is selected and the sentence ‘Edison invent`o la lampadina’ (Edison invented the light bulb) is typed as input query. Disambiguation results are displayed in extended view by default. Overlaying letters over the image are detailed in Section 4. of the fact that unsupervised systems tend to have a strong bias towards the MFS (Calvo and Gelbukh, 2015; Postma et al., 2016; Pasini et al., 2020), we accounted for potential skew towards MFS by including the parameter f reqws , i.e. the normalized value resulting from the number of occurrences for a given word sense in SemCor, divided by the total number of occurrences for all the senses of the same word. concurrently, we devised a strategy to mimic the MFS ranking function by associating a confidence score with each of the lexical resources from which BabelNet derives its lexicalizations (e.g. Wikidata, OmegaWiki or Wikipedia, among others). To this end, after conducting an empirical study to assess the quality of random translation s"
2020.acl-demos.6,P10-1154,1,0.785573,"Missing"
2020.acl-demos.6,L16-1268,0,0.0153609,"nguage, for which the Stanford CoreNLP pipeline has full coverage, in order to perform the lemmatization for German, French, and Spanish, we use instead TreeTagger (Schmid, 1995). 39 Figure 2: User interface of SyntagRank when the Italian language is selected and the sentence ‘Edison invent`o la lampadina’ (Edison invented the light bulb) is typed as input query. Disambiguation results are displayed in extended view by default. Overlaying letters over the image are detailed in Section 4. of the fact that unsupervised systems tend to have a strong bias towards the MFS (Calvo and Gelbukh, 2015; Postma et al., 2016; Pasini et al., 2020), we accounted for potential skew towards MFS by including the parameter f reqws , i.e. the normalized value resulting from the number of occurrences for a given word sense in SemCor, divided by the total number of occurrences for all the senses of the same word. concurrently, we devised a strategy to mimic the MFS ranking function by associating a confidence score with each of the lexical resources from which BabelNet derives its lexicalizations (e.g. Wikidata, OmegaWiki or Wikipedia, among others). To this end, after conducting an empirical study to assess the quality o"
2020.acl-demos.6,E17-1010,1,0.931492,"ledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/). 1 Introduction In Natural Language Processing, Word Sense Disambiguation (WSD) is an open problem concerning lexical ambiguity. It is aimed at determining which sense – among a finite inventory of many – is evoked by a given word in context (Navigli, 2009). This challenge has been tackled by exploiting huge amounts of hand-annotated data in a supervised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing structured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the 1 https://babelnet.org/ 37 Procee"
2020.acl-demos.6,D17-1120,1,0.934524,"ledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/). 1 Introduction In Natural Language Processing, Word Sense Disambiguation (WSD) is an open problem concerning lexical ambiguity. It is aimed at determining which sense – among a finite inventory of many – is evoked by a given word in context (Navigli, 2009). This challenge has been tackled by exploiting huge amounts of hand-annotated data in a supervised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing structured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the 1 https://babelnet.org/ 37 Procee"
2020.acl-demos.6,W04-0811,0,0.732485,"he architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic information. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and http://api.syntagnet.org, respectively. Evaluation Acknowledgments In order to assess its performance, we tested SyntagRank on the five English all-words WSD evaluation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of SyntagRank in a multilingual setting, we used the German, Spanish, French and Italian annotations available in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets10 , which is accordant with the BabelNet API 4.0.1 graph and The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union’s Horizon 2020 research and innovation programme. T"
2020.acl-main.255,E09-1005,0,0.106852,"knowledge in supervised sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approaches (Maru et al., 2019), the performances still lag behind (Huang et al., 2019; Vial et al., 2019). Building on Extended WSD Integrating Sense Embeddings (EWISE) (Kumar et al., 2019), a neural WSD system incorporating prior knowledge through synset embeddings, we present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a hybrid knowledge-based and supervised approach to WSD that integrates explicit relational information from the WordNet LKB. Our approach offers the following contributions: 1. We in"
2020.acl-main.255,R19-1015,1,0.943222,"chieve good results (Pasini, 2020). The best approaches currently rely on neural networks. The model presented by Raganato et al. (2017) formulates the task as a token classification problem, with an LSTM with attention classifier producing a probability distribution over both words and senses. Subsequent work has shown that better results can be obtained by only having scores for senses or synsets (Vial et al., 2019). Shallower, simpler networks can achieve even better performances (Uslu et al., 2018). Contextualized vectors can be exploited in token tagging architectures (Vial et al., 2019; Bevilacqua and Navigli, 2019; Hadiwinoto et al., 2019). However, purely supervised systems are dependent on the data they are trained on, therefore when some sense is underrepresented in the training corpus it is not easy for them to predict it. LKBs in Supervised WSD More closely related to the core of our contribution, LKB information, such as natural language definitions of word meaning, can be exploited in neural token tagging architectures. For example, in GlossBERT (Huang et al., 2019) a pretrained BERT encoder is fed both the context sentence and the gloss, and is trained to predict whether the gloss correctly des"
2020.acl-main.255,P19-1568,0,0.672332,"nowledge bases (Peters et al., 2019; Logan et al., 2019). In Word Sense Disambiguation (WSD), i.e., the task of associating a word in context with the most appropriate meaning from a finite set of possible choices (Navigli, 2009), the gap between supervision and knowledge (Navigli, 2018) has been overcome by several efforts directed at learning effective vector representations (Loureiro and Jorge, 2019; Scarlini et al., 2020) in the same space as contextualized embeddings, and exploring the usage of definitional knowledge in supervised sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approac"
2020.acl-main.255,N19-1423,0,0.0792886,"Missing"
2020.acl-main.255,P19-1598,0,0.0628546,"Missing"
2020.acl-main.255,S01-1001,0,0.811043,"(Miller et al., 1994) for 20 epochs, with a batch size of 4000 tokens. We do not employ sentences as context. Rather, we split documents in chunks of at most 100 tokens. The hidden size of the 2-layer feedforward is 512, with a dropout value of 0.2. The optimizer is Adam (Kingma and Ba, 2015), which we employ with a learning rate of 10−4 . Following Bevilacqua and Navigli (2019), we select as development set (to select the best epoch) the SemEval-2015 dataset (Moro and Navigli, 2015). As customary, we report the results on the concatenation (ALL) of all the evaluation datasets from Senseval2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and the aforementioned SemEval-2015. In addition, we report performances on ALL with all instances from the development set removed (No15), and on the subset of No15 whose gold synsets do not appear in SemCor (No15− ). 4.1.3 Results We report in Table 1 the results of the experiments on the addition of structured logits to the baseline architecture. As can be seen, the use of hypernyms brings the biggest gain to performances, with the strongest improvement against the baseline repo"
2020.acl-main.255,P19-1569,0,0.35714,"ks for Natural Language Processing (NLP) tasks, be it through pretraining on self-supervised tasks such as language modeling (Peters et al., 2018; Devlin et al., 2019), or through the incorporation of information from knowledge bases (Peters et al., 2019; Logan et al., 2019). In Word Sense Disambiguation (WSD), i.e., the task of associating a word in context with the most appropriate meaning from a finite set of possible choices (Navigli, 2009), the gap between supervision and knowledge (Navigli, 2018) has been overcome by several efforts directed at learning effective vector representations (Loureiro and Jorge, 2019; Scarlini et al., 2020) in the same space as contextualized embeddings, and exploring the usage of definitional knowledge in supervised sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into"
2020.acl-main.255,D18-1472,0,0.0565417,"Missing"
2020.acl-main.255,P18-1230,0,0.244394,"information from knowledge bases (Peters et al., 2019; Logan et al., 2019). In Word Sense Disambiguation (WSD), i.e., the task of associating a word in context with the most appropriate meaning from a finite set of possible choices (Navigli, 2009), the gap between supervision and knowledge (Navigli, 2018) has been overcome by several efforts directed at learning effective vector representations (Loureiro and Jorge, 2019; Scarlini et al., 2020) in the same space as contextualized embeddings, and exploring the usage of definitional knowledge in supervised sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualiz"
2020.acl-main.255,D19-1533,0,0.276431,"020). The best approaches currently rely on neural networks. The model presented by Raganato et al. (2017) formulates the task as a token classification problem, with an LSTM with attention classifier producing a probability distribution over both words and senses. Subsequent work has shown that better results can be obtained by only having scores for senses or synsets (Vial et al., 2019). Shallower, simpler networks can achieve even better performances (Uslu et al., 2018). Contextualized vectors can be exploited in token tagging architectures (Vial et al., 2019; Bevilacqua and Navigli, 2019; Hadiwinoto et al., 2019). However, purely supervised systems are dependent on the data they are trained on, therefore when some sense is underrepresented in the training corpus it is not easy for them to predict it. LKBs in Supervised WSD More closely related to the core of our contribution, LKB information, such as natural language definitions of word meaning, can be exploited in neural token tagging architectures. For example, in GlossBERT (Huang et al., 2019) a pretrained BERT encoder is fed both the context sentence and the gloss, and is trained to predict whether the gloss correctly describes the use of the targ"
2020.acl-main.255,D19-1359,1,0.793133,"ng et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approaches (Maru et al., 2019), the performances still lag behind (Huang et al., 2019; Vial et al., 2019). Building on Extended WSD Integrating Sense Embeddings (EWISE) (Kumar et al., 2019), a neural WSD system incorporating prior knowledge through synset embeddings, we present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a hybrid knowledge-based and supervised approach to WSD that integrates explicit relational information from the WordNet LKB. Our approach offers the following contributions: 1. We introduce the novel structured logits mechanism, which enables the exploitation of concept relatedness"
2020.acl-main.255,P18-1031,0,0.0183238,"sely, attested synsets can be further refined and predicted more accurately. If weights are frozen, the architecture will have to accommodate to the pretrained synset representations, meaning that, especially if there is no learned bias, it will be easier to predict unseen classes. No fine-tuning may, however, result in diminished performance, as the pre-trained synset representations are not tailored to WSD. An additional possibility to achieve better transfer between the information in the embeddings and the WSD system is to use a freeze-then-thaw scheme, similar to the chain-thaw method of Howard and Ruder (2018). The approach entails training an O-freeze model, restoring the best checkpoint, and then doing further training with O “thawed”, i.e., with trainable weights. 4 Experiments We assess the performance of EWISER in allwords English WSD, against both a simple but competitive baseline, i.e., the simple feedforward network taking BERT hidden states as input described in Section 3.2, and state-of-art approaches. We first experiment separately on the integration of explicit relational information through structured logits (Section 4.1), and the integration of synset embeddings through the initializa"
2020.acl-main.255,D19-1355,0,0.348298,"rs et al., 2019; Logan et al., 2019). In Word Sense Disambiguation (WSD), i.e., the task of associating a word in context with the most appropriate meaning from a finite set of possible choices (Navigli, 2009), the gap between supervision and knowledge (Navigli, 2018) has been overcome by several efforts directed at learning effective vector representations (Loureiro and Jorge, 2019; Scarlini et al., 2020) in the same space as contextualized embeddings, and exploring the usage of definitional knowledge in supervised sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approaches (Maru et al., 201"
2020.acl-main.255,S15-2049,1,0.874978,"ith the above-mentioned baseline. 4.1.2 Data & Hyperparameters We train the baseline and the configurations under comparison on SemCor (Miller et al., 1994) for 20 epochs, with a batch size of 4000 tokens. We do not employ sentences as context. Rather, we split documents in chunks of at most 100 tokens. The hidden size of the 2-layer feedforward is 512, with a dropout value of 0.2. The optimizer is Adam (Kingma and Ba, 2015), which we employ with a learning rate of 10−4 . Following Bevilacqua and Navigli (2019), we select as development set (to select the best epoch) the SemEval-2015 dataset (Moro and Navigli, 2015). As customary, we report the results on the concatenation (ALL) of all the evaluation datasets from Senseval2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and the aforementioned SemEval-2015. In addition, we report performances on ALL with all instances from the development set removed (No15), and on the subset of No15 whose gold synsets do not appear in SemCor (No15− ). 4.1.3 Results We report in Table 1 the results of the experiments on the addition of structured logits to the baseline architectur"
2020.acl-main.255,Q14-1019,1,0.899856,"sequence learning neural architectures (Luo et al., 2018; Kumar et al., 2019; Huang et al., 2019). However, the Lexical Knowledge Bases (LKBs) from which such information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approaches (Maru et al., 2019), the performances still lag behind (Huang et al., 2019; Vial et al., 2019). Building on Extended WSD Integrating Sense Embeddings (EWISE) (Kumar et al., 2019), a neural WSD system incorporating prior knowledge through synset embeddings, we present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a hybrid knowledge-based and supervised approach to WSD that integrates explicit relational information from the WordNet LKB. Our approach offers the following contributions: 1. We introduce the novel st"
2020.acl-main.255,S13-2040,1,0.890321,", we split documents in chunks of at most 100 tokens. The hidden size of the 2-layer feedforward is 512, with a dropout value of 0.2. The optimizer is Adam (Kingma and Ba, 2015), which we employ with a learning rate of 10−4 . Following Bevilacqua and Navigli (2019), we select as development set (to select the best epoch) the SemEval-2015 dataset (Moro and Navigli, 2015). As customary, we report the results on the concatenation (ALL) of all the evaluation datasets from Senseval2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and the aforementioned SemEval-2015. In addition, we report performances on ALL with all instances from the development set removed (No15), and on the subset of No15 whose gold synsets do not appear in SemCor (No15− ). 4.1.3 Results We report in Table 1 the results of the experiments on the addition of structured logits to the baseline architecture. As can be seen, the use of hypernyms brings the biggest gain to performances, with the strongest improvement against the baseline reported with simple hypernymy and fine-tuning of A: 1.7 points on ALL and 1.6 on No15. The closures, i.e., hyper* a"
2020.acl-main.255,N18-1202,0,0.115165,"Missing"
2020.acl-main.255,D19-1005,0,0.105434,"Missing"
2020.acl-main.255,D16-1174,0,0.052548,"Missing"
2020.acl-main.255,S07-1016,0,0.433674,"t employ sentences as context. Rather, we split documents in chunks of at most 100 tokens. The hidden size of the 2-layer feedforward is 512, with a dropout value of 0.2. The optimizer is Adam (Kingma and Ba, 2015), which we employ with a learning rate of 10−4 . Following Bevilacqua and Navigli (2019), we select as development set (to select the best epoch) the SemEval-2015 dataset (Moro and Navigli, 2015). As customary, we report the results on the concatenation (ALL) of all the evaluation datasets from Senseval2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and the aforementioned SemEval-2015. In addition, we report performances on ALL with all instances from the development set removed (No15), and on the subset of No15 whose gold synsets do not appear in SemCor (No15− ). 4.1.3 Results We report in Table 1 the results of the experiments on the addition of structured logits to the baseline architecture. As can be seen, the use of hypernyms brings the biggest gain to performances, with the strongest improvement against the baseline reported with simple hypernymy and fine-tuning of A: 1.7 points on ALL and 1.6"
2020.acl-main.255,D17-1120,1,0.876173,"ational Linguistics Our approach is simple and extensible, does not require fine tuning of contextualized embeddings, and has a very modest parameter budget apart from synset embeddings. EWISER achieves a new state of the art in all-words English WSD. Moreover, we obtain state-of-the-art performances on the crosslingual all-words WSD evaluation, without using non-English training data. 2 Related Work Supervised WSD Supervised systems have to rely on expensive hand-labeled data to achieve good results (Pasini, 2020). The best approaches currently rely on neural networks. The model presented by Raganato et al. (2017) formulates the task as a token classification problem, with an LSTM with attention classifier producing a probability distribution over both words and senses. Subsequent work has shown that better results can be obtained by only having scores for senses or synsets (Vial et al., 2019). Shallower, simpler networks can achieve even better performances (Uslu et al., 2018). Contextualized vectors can be exploited in token tagging architectures (Vial et al., 2019; Bevilacqua and Navigli, 2019; Hadiwinoto et al., 2019). However, purely supervised systems are dependent on the data they are trained on"
2020.acl-main.255,2020.acl-demos.6,1,0.912416,"75.6 73.6 75.2 77.4 77.9 77.4 77.9 78.4 67.3 68.1 68.1 72.5 69.5 71.0 71.0 70.3 71.0 69.4 75.1 71.1 76.1 78.7 76.0 76.4 77.4 76.2 78.9 74.5 77.0 76.2 80.4 78.3 77.8 78.7 76.3 79.3 74.0 80.4 79.6 79.9 80.7 79.4 81.7 60.2 65.9 66.4 65.1 65.9 66.3 78.0 79.5 79.0 80.9 80.0 81.2 82.1 85.5 85.5 86.1 86.7 85.8 X X X X X X X X X X X X X X X X Vial et al. (2019) Vial et al. (2019) - ENS EWISERhyper EWISERhyper+hypo 77.1 79.0? 80.1 79.8 78.4* 79.8 79.3 75.2 75.1 79.7 80.8 80.2 77.8 79.0 78.5 73.4 75.2 73.8 78.7 80.7 80.6 82.6 81.8 82.3 81.4 82.9 82.7 68.7 69.4 68.5 83.7 83.6 82.9 85.5 87.3 87.6 - X - - Scozzafava et al. (2020) Scarlini et al. (2020) - KB 71.7 - 71.0* - - 71.6 - 72.0 - 59.3 - 72.2 74.8 75.8 - 75.9 - - - Table 3: Evaluation of the joint use of structured logits and O-thaw* on English all-words WSD. F1 is reported. The column blocks report (i) the training corpora and system compared; (ii) overall F1; (iii) single dataset F1; (iv) POS-specific F1. †: Incorporates gloss information through synset embeddings. *: Computed from reported scores. ?: highest F1 that is statistically different from the best one (χ2 with p=0.1). F1 score, with the exception of O-thaw*, where the training run was underfitting."
2020.acl-main.255,W04-0811,0,0.715831,"th a batch size of 4000 tokens. We do not employ sentences as context. Rather, we split documents in chunks of at most 100 tokens. The hidden size of the 2-layer feedforward is 512, with a dropout value of 0.2. The optimizer is Adam (Kingma and Ba, 2015), which we employ with a learning rate of 10−4 . Following Bevilacqua and Navigli (2019), we select as development set (to select the best epoch) the SemEval-2015 dataset (Moro and Navigli, 2015). As customary, we report the results on the concatenation (ALL) of all the evaluation datasets from Senseval2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), and the aforementioned SemEval-2015. In addition, we report performances on ALL with all instances from the development set removed (No15), and on the subset of No15 whose gold synsets do not appear in SemCor (No15− ). 4.1.3 Results We report in Table 1 the results of the experiments on the addition of structured logits to the baseline architecture. As can be seen, the use of hypernyms brings the biggest gain to performances, with the strongest improvement against the baseline reported with simple hypernymy and fine-tu"
2020.acl-main.255,L18-1168,0,0.0117093,"raining data. 2 Related Work Supervised WSD Supervised systems have to rely on expensive hand-labeled data to achieve good results (Pasini, 2020). The best approaches currently rely on neural networks. The model presented by Raganato et al. (2017) formulates the task as a token classification problem, with an LSTM with attention classifier producing a probability distribution over both words and senses. Subsequent work has shown that better results can be obtained by only having scores for senses or synsets (Vial et al., 2019). Shallower, simpler networks can achieve even better performances (Uslu et al., 2018). Contextualized vectors can be exploited in token tagging architectures (Vial et al., 2019; Bevilacqua and Navigli, 2019; Hadiwinoto et al., 2019). However, purely supervised systems are dependent on the data they are trained on, therefore when some sense is underrepresented in the training corpus it is not easy for them to predict it. LKBs in Supervised WSD More closely related to the core of our contribution, LKB information, such as natural language definitions of word meaning, can be exploited in neural token tagging architectures. For example, in GlossBERT (Huang et al., 2019) a pretrain"
2020.acl-main.255,2019.gwc-1.14,0,0.226488,"ch information is retrieved, such as WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), also provide a great wealth of relational knowledge in structured form (i.e., hypernymy, meronymy, similarity, etc.), which is often neglected due to the non-trivial integration of data of this kind into neural architectures. Even though such information can, instead, be exploited by knowledge-based WSD algorithms (Agirre and Soroa, 2009; Moro et al., 2014), rivaling supervised pre-contextualized embedding approaches (Maru et al., 2019), the performances still lag behind (Huang et al., 2019; Vial et al., 2019). Building on Extended WSD Integrating Sense Embeddings (EWISE) (Kumar et al., 2019), a neural WSD system incorporating prior knowledge through synset embeddings, we present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a hybrid knowledge-based and supervised approach to WSD that integrates explicit relational information from the WordNet LKB. Our approach offers the following contributions: 1. We introduce the novel structured logits mechanism, which enables the exploitation of concept relatedness as determined by LKB edges. In our method, pre-softmax scores are a weighte"
2020.acl-main.425,N19-1423,0,0.0192729,"d to answer the question “is i pertinent to g?”. Possible answers are yes (i.e., i is an illustration of g), no (i.e., i is either not pertinent or in contradiction with g) and Model Since manual validation is time consuming, we are interested in developing a methodology for the automatic verification of synset-image associations. In the recent past there has been a great research effort to develop models for vision-language pretraining. Many such models (e.g., VLP (Zhou et al., 2020), VisualBERT (Li et al., 2019), ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019)) are built upon BERT (Devlin et al., 2019), a popular system for contextualized embeddings. BERT-based models achieve state-of-the-art scores on many language-vision tasks, hence they represent a promising resource for our task. 4682 The system that we use to perform classification is the fine-tuned VLP model. Despite the fact that LXMERT (Tan and Bansal, 2019) achieves a slightly higher score on yes/no questions on the VQA 2.0 dataset (Goyal et al., 2017), our preference goes for the VLP system since it is pre-trained on a wider and more general dataset. More specifically, the VLP model is pre-trained on Conceptual Captions (CC) (Sha"
2020.acl-main.425,N19-1200,0,0.019587,"in not being publicly released. The Open Images dataset (Kuznetsova et al., 2018) contains 9M images annotated with 19,794 classes taken from JFT. While Open Images does contain NC labels, the classes are not linked to an LKB, thus limiting their usefulness. The Tencent ML-Images dataset (Wu et al., 2019) was created starting from a subset of ImageNet and Open Images and includes images annotated with 11,166 categories, which are then linked to WordNet synsets. The dataset differs from our work since any NC label has been explicitly discarded. Our work is in some sense similar to MultiSense (Gella et al., 2019) and VerSe (Gella et al., 2016), two datasets including images annotated with verbal senses. However, MultiSense is not directly linked to an LKB and neither of these two datasets deals with nominal synsets. Finally, we note that datasets including images annotated with objectlevel categories (Lin et al., 2014; Plummer et al., 2015) or videos (Loui et al., 2007; Doll´ar et al., 2009; Moneglia et al., 2014; Heilbron et al., 2015; Abu-El-Haija et al., 2016) are outside the scope of this work, since we are only interested in the main NC concepts depicted within images. 3 Gold Dataset BabelPic is"
2020.acl-main.425,N16-1022,0,0.123075,"aries, such as events (e.g., FATALITY, COMPETITION), emotions (e.g., S ADNESS) and psychological features (e.g., S HARPNESS), have enjoyed less attention. For lack of a better term, we will henceforth refer to them as non-concrete (NC) concepts. On one hand, the inclusion of NC concepts would be an important step towards wide-coverage image semantic understanding. On the other hand, it also goes in the same direction as recent multimodal language-vision approaches, e.g., monoand cross-lingual Visual Sense Disambiguation (Barnard and Johnson, 2005; Loeff et al., 2006; Saenko and Darrell, 2008; Gella et al., 2016, 2019). Taking into account NC concepts could also be of crucial importance for fascinating languagefocused applications, such as Multimodal Machine Translation. Last but not least, NC concepts would represent a significative benchmark for real-world multimodal applications. In fact, traditional computer vision approaches rely on the detection of objects within the image, but many NC concepts are not well described by a bag of objects. Consider, for instance, Figure 1. The two images illustrate different NC concepts (i.e., HIGH JUMP and POLE VAULT) which are different configurations of the sa"
2020.acl-main.425,P06-2071,0,0.156258,"Missing"
2020.acl-main.425,D19-1514,0,0.0184575,"i.e., definition) for s. Annotators are asked to answer the question “is i pertinent to g?”. Possible answers are yes (i.e., i is an illustration of g), no (i.e., i is either not pertinent or in contradiction with g) and Model Since manual validation is time consuming, we are interested in developing a methodology for the automatic verification of synset-image associations. In the recent past there has been a great research effort to develop models for vision-language pretraining. Many such models (e.g., VLP (Zhou et al., 2020), VisualBERT (Li et al., 2019), ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019)) are built upon BERT (Devlin et al., 2019), a popular system for contextualized embeddings. BERT-based models achieve state-of-the-art scores on many language-vision tasks, hence they represent a promising resource for our task. 4682 The system that we use to perform classification is the fine-tuned VLP model. Despite the fact that LXMERT (Tan and Bansal, 2019) achieves a slightly higher score on yes/no questions on the VQA 2.0 dataset (Goyal et al., 2017), our preference goes for the VLP system since it is pre-trained on a wider and more general dataset. More specifically, the VLP model is p"
2020.acl-main.425,moneglia-etal-2014-imagact,0,0.0274405,"11,166 categories, which are then linked to WordNet synsets. The dataset differs from our work since any NC label has been explicitly discarded. Our work is in some sense similar to MultiSense (Gella et al., 2019) and VerSe (Gella et al., 2016), two datasets including images annotated with verbal senses. However, MultiSense is not directly linked to an LKB and neither of these two datasets deals with nominal synsets. Finally, we note that datasets including images annotated with objectlevel categories (Lin et al., 2014; Plummer et al., 2015) or videos (Loui et al., 2007; Doll´ar et al., 2009; Moneglia et al., 2014; Heilbron et al., 2015; Abu-El-Haija et al., 2016) are outside the scope of this work, since we are only interested in the main NC concepts depicted within images. 3 Gold Dataset BabelPic is built by exploiting the link between WordNet (Miller, 1995) and Wikipedia within BabelNet2 (Navigli and Ponzetto, 2012). Our approach is organised in a three-step process. First, we select a set of NC synsets from WordNet, on the basis of both their paradigmatic nature and relations in the knowledge base. Second, we gather all the corresponding images in BabelNet, which are themselves mostly taken from Wi"
2020.acl-main.425,P18-1238,0,0.0290274,"19), a popular system for contextualized embeddings. BERT-based models achieve state-of-the-art scores on many language-vision tasks, hence they represent a promising resource for our task. 4682 The system that we use to perform classification is the fine-tuned VLP model. Despite the fact that LXMERT (Tan and Bansal, 2019) achieves a slightly higher score on yes/no questions on the VQA 2.0 dataset (Goyal et al., 2017), our preference goes for the VLP system since it is pre-trained on a wider and more general dataset. More specifically, the VLP model is pre-trained on Conceptual Captions (CC) (Sharma et al., 2018), a dataset including more than 3M image-caption pairs, using two unsupervised vision-language tasks: bidirectional and sequence-to-sequence masked language prediction. The input images are preprocessed using Faster R-CNN (Ren et al., 2015) pre-trained on Visual Genome (Krishna et al., 2017; Anderson et al., 2018), hence obtaining 100 object regions per image. The model input consists of both class-aware region embeddings and word embeddings, the former obtained by combining the corresponding region features with the probability of each object label and region geometric information. Furthermor"
2020.coling-main.120,2020.acl-main.255,1,0.735303,"word representations with respect to all the predicates in a sentence in a single forward pass (Section 3.2); – a predicate-argument encoder which specializes the representation of each argument to a single predicate, for each predicate in the input sentence (Section 3.3). 1398 3.1 Contextualized Word Representation The prior knowledge encoded in pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) and XLM-RoBERTa (Conneau et al., 2020) is currently showing significant benefits in an ever-increasing array of NLP tasks. Taking inspiration from recent work (Bevilacqua and Navigli, 2020), our model computes word-level representations by combining the different knowledge encoded at different hidden layers of a pretrained language. More formally, let L : V |t |→ Rn×h be a language model with an input vocabulary V that, given a sequence t = ht1 , t2 , . . . , t|t |i of tokens ti ∈ V , returns a sequence o = ho1 , o2 , . . . , o|t |i of dense vector representations oi ∈ Rh . Also, let L be structured in K inner layers l1 , l2 , . . . , lK such that L(t) = lK (· · · (l1 (t))), and that, for the k-th layer lk , the corresponding output states ok = lk (ok−1 ) are accessible. Then, g"
2020.coling-main.120,2020.emnlp-main.195,1,0.785189,"Missing"
2020.coling-main.120,D19-1094,0,0.589127,"ural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in SRL by explicitly taking advantage of different properties in syntactic dependency trees to various extents (Wang et al., 2019; Zhang et al., 2019b; He et al., 2019). However, if we step back to observe th"
2020.coling-main.120,Q19-1022,0,0.484025,"ural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in SRL by explicitly taking advantage of different properties in syntactic dependency trees to various extents (Wang et al., 2019; Zhang et al., 2019b; He et al., 2019). However, if we step back to observe th"
2020.coling-main.120,C18-1233,0,0.466381,"span-based CoNLL-2012 English benchmark. End-to-end approaches. Regardless of the formalism of choice, SRL is traditionally divided into a set of simpler subtasks: predicate identification, predicate sense disambiguation, argument identification and argument classification. Early work used different sets of template features and statistical/neural models to tackle each predicate and argument subtask separately (Zhao et al., 2009), but the recent success of the multi-task learning paradigm (Caruana, 1997) prompted the development of end-to-end models that jointly address some of the subtasks (Cai et al., 2018; Li et al., 2019; He et al., 2019). Since the CoNLL2009 shared task provides pre-identified predicates, systems – end-to-end approaches included – usually 1 CoNLL-2009 originally included 7 languages (Catalan, Chinese, Czech, English, German, Japanese, Spanish), however we do not include Japanese in our studies as it is no longer available through LDC due to licensing problems. 1397 process the same sentence np times, where np is the number of predicates in the sentence; at the cost of longer training times, this approach lets a system contextualize a sentence with respect to a specific pre-i"
2020.coling-main.120,W05-0620,0,0.565532,"Missing"
2020.coling-main.120,D19-1544,0,0.314303,"Missing"
2020.coling-main.120,2020.coling-main.291,1,0.829155,"Missing"
2020.coling-main.120,2020.emnlp-demos.11,1,0.896335,"Missing"
2020.coling-main.120,2020.acl-main.747,0,0.129122,"Missing"
2020.coling-main.120,D19-1058,1,0.568151,"Missing"
2020.coling-main.120,P18-2077,0,0.0232761,"ostic SRL. Marcheggiani et al. (2017) opened the door to the initial wave of syntaxagnostic models for SRL by efficiently employing a BiLSTM-based encoder to capture longer predicateargument relations within an input sequence, thereby outperforming syntax-aware systems in the CoNLL-2009 English, Czech and Spanish evaluation sets for the first time. Cai et al. (2018) proposed the first full end-to-end syntax-agnostic SRL model to jointly learn to disambiguate predicate senses and recognize their corresponding semantic arguments, and further enhanced the model with an attentive biaffine scorer (Dozat and Manning, 2018) to better condition argument predictions on a given predicate in the input sentence. The combined contribution of these innovations realigned the performances of syntax-agnostic systems to the best syntax-aware systems. Most recently, Li et al. (2019) showed that the use of contextualized word embeddings such as ELMo (Peters et al., 2018) leads to further progress, thus lending support to the hunch that high-quality contextual information is key to enabling high-performing SRL systems. We stress that there is a subtle catch in the definition of syntax-agnostic: the foregoing approaches do not"
2020.coling-main.120,P00-1065,0,0.145367,"ve robustness across languages. Our approach outperforms the state of the art in all the languages of the CoNLL-2009 benchmark dataset, especially whenever a scarce amount of training data is available. Our objective is not to reject approaches that rely on syntax, rather to set a strong and consistent language-independent baseline for future innovations in Semantic Role Labeling. We release our model code and checkpoints at https://github.com/SapienzaNLP/multi-srl. 1 Introduction Semantic Role Labeling (SRL) – the task of automatically addressing “Who did What to Whom, How, When and Where?” (Gildea and Jurafsky, 2000; M`arquez et al., 2008) – is a long standing open problem in Natural Language Processing (NLP), and a central task required to complete the puzzle of Natural Lan-guage Understanding (Navigli, 2018). Its roots date back to several decades ago, to when Fillmore (1968) first theorized the existence of deep semantic relations between a predicate and other sentential constituents. Over the years, different linguistic formalisms and their corresponding predicate-argument structure inventories expanded Fillmore’s seminal intuition (Dowty, 1991; Levin, 1993), yet the need to rely on manually designed"
2020.coling-main.120,W09-1201,0,0.392872,"Missing"
2020.coling-main.120,D15-1076,0,0.163232,"of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in SRL by explicitly taking advantage of differ"
2020.coling-main.120,P17-1044,0,0.044621,"e years, different linguistic formalisms and their corresponding predicate-argument structure inventories expanded Fillmore’s seminal intuition (Dowty, 1991; Levin, 1993), yet the need to rely on manually designed complex feature templates severely limited early SRL models (Zhao et al., 2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predic"
2020.coling-main.120,P18-2058,0,0.0369217,"lexical- or sentence-level syntactic feature and is therefore truly syntax-agnostic. Syntax-aware SRL. Syntactic features have recently gained traction in SRL research, mostly due to the diversity of the available representations, the quality of the information they encode, and the wide range of techniques that can be used to take advantage of them. Notable work includes the use of graph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused innovations do not scale immediately across languages. In any case, reporting fragment"
2020.coling-main.120,P18-1192,0,0.0137457,"lexical- or sentence-level syntactic feature and is therefore truly syntax-agnostic. Syntax-aware SRL. Syntactic features have recently gained traction in SRL research, mostly due to the diversity of the available representations, the quality of the information they encode, and the wide range of techniques that can be used to take advantage of them. Notable work includes the use of graph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused innovations do not scale immediately across languages. In any case, reporting fragment"
2020.coling-main.120,D19-1538,0,0.494232,"s soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in SRL by explicitly taking advantage of different properties in syntactic dependency trees to various extents (Wang et al., 2019; Zhang et al., 2019b; He et al., 2019). However, if we step back to observe the larger picture, a significant gap among languages strikes the eye. Indeed, among the state-of-the-art multilingual SRL systems that reported their results on all the languages of the CoNLL-2009 benchmark dataset (Hajic et al., 2009), the currently best-performing system This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1396 Proceedings of the 28th International Conference on Computational Linguistics, pages 1396–1410 Barcelona, Spain (Online), December 8-"
2020.coling-main.120,N19-1075,0,0.0143265,"therefore truly syntax-agnostic. Syntax-aware SRL. Syntactic features have recently gained traction in SRL research, mostly due to the diversity of the available representations, the quality of the information they encode, and the wide range of techniques that can be used to take advantage of them. Notable work includes the use of graph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused innovations do not scale immediately across languages. In any case, reporting fragmented results strongly limits full-fledged comparisons and,"
2020.coling-main.120,2020.acl-main.703,0,0.0740379,"Missing"
2020.coling-main.120,D19-1099,0,0.605404,"raph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused innovations do not scale immediately across languages. In any case, reporting fragmented results strongly limits full-fledged comparisons and, therefore, hinders progress in multilingual SRL. 3 Model Description Building on top of recent successes in deep learning, our model learns to tackle predicate sense disambiguation and argument identification/classification jointly. In particular: • our model features a word encoder which, in contrast to current work in SRL, expl"
2020.coling-main.120,D17-1159,0,0.23842,"a result, their input is still language-dependent. In contrast, our approach does away with any lexical- or sentence-level syntactic feature and is therefore truly syntax-agnostic. Syntax-aware SRL. Syntactic features have recently gained traction in SRL research, mostly due to the diversity of the available representations, the quality of the information they encode, and the wide range of techniques that can be used to take advantage of them. Notable work includes the use of graph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused in"
2020.coling-main.120,K17-1041,0,0.454481,"ntial constituents. Over the years, different linguistic formalisms and their corresponding predicate-argument structure inventories expanded Fillmore’s seminal intuition (Dowty, 1991; Levin, 1993), yet the need to rely on manually designed complex feature templates severely limited early SRL models (Zhao et al., 2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant por"
2020.coling-main.120,N18-2078,0,0.283578,"d to rely on manually designed complex feature templates severely limited early SRL models (Zhao et al., 2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic ro"
2020.coling-main.120,J08-2001,0,0.127054,"Missing"
2020.coling-main.120,D18-1191,0,0.0241287,"Missing"
2020.coling-main.120,N18-1202,0,0.723885,"time. Cai et al. (2018) proposed the first full end-to-end syntax-agnostic SRL model to jointly learn to disambiguate predicate senses and recognize their corresponding semantic arguments, and further enhanced the model with an attentive biaffine scorer (Dozat and Manning, 2018) to better condition argument predictions on a given predicate in the input sentence. The combined contribution of these innovations realigned the performances of syntax-agnostic systems to the best syntax-aware systems. Most recently, Li et al. (2019) showed that the use of contextualized word embeddings such as ELMo (Peters et al., 2018) leads to further progress, thus lending support to the hunch that high-quality contextual information is key to enabling high-performing SRL systems. We stress that there is a subtle catch in the definition of syntax-agnostic: the foregoing approaches do not make use of sentence-level syntax, but do still consider lexical-level syntactic features, such as part-of-speech tags. As a result, their input is still language-dependent. In contrast, our approach does away with any lexical- or sentence-level syntactic feature and is therefore truly syntax-agnostic. Syntax-aware SRL. Syntactic features"
2020.coling-main.120,W12-4501,0,0.244774,"ax-focused SRL. 2 Related Work Dependency or Span? Currently, SRL is cast as either a span-based or a dependency-based labeling task. Given a predicate in a sentence, the main difference between the two settings is that, in the former, semantic role labels are assigned to the entire span of an argument, whereas, in the latter, semantic role labels are assigned only to the semantic head of the argument. Both span- and dependency-based SRL have continued to be developed and supported in parallel over the years with the organization of the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 (Pradhan et al., 2012) tasks for spanbased SRL, and the CoNLL-2008 (Surdeanu et al., 2008) and CoNLL-2009 tasks for dependency-based SRL. The debate over which representation is best is still open and subject to active investigation, with ongoing efforts aimed at merging the two into a unified formalism (Li et al., 2019). In this work, we focus mainly on dependency-based SRL as CoNLL-2009 includes the widest and most varied set of languages, but we also report results on the span-based CoNLL-2012 English benchmark. End-to-end approaches. Regardless of the formalism of choice, SRL is traditionally divided into a set"
2020.coling-main.120,2020.emnlp-main.285,1,0.86134,"Missing"
2020.coling-main.120,D18-1548,0,0.0300933,"features have recently gained traction in SRL research, mostly due to the diversity of the available representations, the quality of the information they encode, and the wide range of techniques that can be used to take advantage of them. Notable work includes the use of graph convolutional networks to capture short-distance relations between neighbors in the syntactic dependency graph (Marcheggiani and Titov, 2017a), deriving argument pruning rules from syntactic dependency trees (He et al., 2018b), clustering dependency relations (Kasai et al., 2019), and syntax-based attention mechanisms (Strubell et al., 2018; Zhang et al., 2019b). However, most of the work that reported performance improvements in the CoNLL-2009 benchmark dataset only did so in a subset of its in- and out-of-domain evaluations of the 6 available languages, with the noteworthy exception of Lyu et al. (2019). This may fuel the suspicion that either there is a lack of real interest in syntax-aware multilingual SRL, or syntaxfocused innovations do not scale immediately across languages. In any case, reporting fragmented results strongly limits full-fledged comparisons and, therefore, hinders progress in multilingual SRL. 3 Model Desc"
2020.coling-main.120,W08-2121,0,0.196973,"Missing"
2020.coling-main.120,taule-etal-2008-ancora,0,0.628075,"Missing"
2020.coling-main.120,P19-1529,0,0.0113763,"L, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in SRL by explicitly taking advantage of different properties in syntactic dependency trees to various extents (Wang et al., 2019; Zhang et al., 2019b; He et al., 2019). However, if we step back to observe the larger picture, a significant gap among languages strikes the eye. Indeed, among the state-of-the-art multilingual SRL systems that reported their results on all the languages of the CoNLL-2009 benchmark dataset (Hajic et al., 2009), the currently best-performing system This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1396 Proceedings of the 28th International Conference on Computational Linguistics, pages 1396–1410"
2020.coling-main.120,N19-1066,0,0.297822,"2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in"
2020.coling-main.120,D19-1057,0,0.061583,"2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang et al., 2019a), and Question Answering (He et al., 2015). As researchers constantly explored new approaches to improve SRL, the exploitation of syntactic features soon emerged as a natural choice. Cai and Lapata (2019b) suggested that syntax ought to help semantic role labelers since i) a significant portion of the predicate-argument relations in a semantic dependency graph mirrors the edges that appear in a syntactic dependency graph, and ii) there is often a deterministic mapping from syntactic to semantic roles. Following this line of thought, numerous papers from major venues reported improvements in"
2020.coling-main.120,W09-1209,0,0.228738,"Natural Language Processing (NLP), and a central task required to complete the puzzle of Natural Lan-guage Understanding (Navigli, 2018). Its roots date back to several decades ago, to when Fillmore (1968) first theorized the existence of deep semantic relations between a predicate and other sentential constituents. Over the years, different linguistic formalisms and their corresponding predicate-argument structure inventories expanded Fillmore’s seminal intuition (Dowty, 1991; Levin, 1993), yet the need to rely on manually designed complex feature templates severely limited early SRL models (Zhao et al., 2009). Fortunately, the recent great success of neural networks in NLP has drawn attention back to SRL and has led to considerable performance gains. This has been particularly the case for recurrent neural networks, thanks to their ability to better capture relations over sequences (Marcheggiani et al., 2017; He et al., 2017). The positive results obtained in SRL were rapidly extended to other fields where they proved to be beneficial to several downstream tasks, from Machine Translation (Marcheggiani et al., 2018) to Information Extraction (Christensen et al., 2011), Opinion Role Labeling (Zhang"
2020.coling-main.291,W13-3520,0,0.0375939,"LM (Conneau and Lample, 2019). Contextualized word embeddings are able to capture the many facets of a polysemous word in a context (Pilehvar and Camacho-Collados, 2019), but their implicitly encoded meanings are still disconnected from human-curated knowledge bases, even if more recent efforts showed promising results in imparting structured semantic knowledge to contextualized representations (Peters et al., 2019; Levine et al., 2019). Knowledge-enhanced representations. Alternative approaches implement multilinguality by making use of multilingual encyclopedic resources, such as Wikipedia (Al-Rfou et al., 2013), or multilingual knowledge graphs such as Open Multilingual WordNet (Bond and Foster, 2013), ConceptNet (Speer et al., 2017), or BabelNet (Navigli and Ponzetto, 2012a). A prominent example of knowledge-enhanced word embeddings is Conceptnet Numberbatch (Speer et al., 2017), which retrofits word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) word representations to the ConceptNet graph, achieving state-of-the-art results in Semantic Word Similarity (Speer and Lowry-Duda, 2017). A further step towards semantic representations involves modelling individual word senses as vectors"
2020.coling-main.291,P18-1073,0,0.0282177,"butional spaces for two languages onto a common space; Ammar et al. (2016) extended previous work to over fifty languages; Smith et al. (2017) reduced the need for bilingual supervision by compiling a pseudo-dictionary from the identical strings that appear in two languages; Jawanpuria et al. (2019) proposed a geometric approach to embedding alignment that leverages language-specific transformations; Singhal et al. (2019) learned multilingual word embeddings from image-text data. While these methods still require annotated cross-lingual data or parallel vocabularies, Conneau et al. (2017) and Artetxe et al. (2018) found success by employing unsupervised methods and adversarial training. Contextualized word embeddings. The above-mentioned approaches produce word-level representations that are independent of the specific context a word appears in, and such “static” representations often show a strong bias towards the most frequent sense of a word. Instead, context-aware word representation techniques, such as context2vec (Melamud et al., 2016) or ELMo (Peters et al., 2018), dynamically create a representation for a word in a sentential or documental context. Contextualized embeddings witnessed a dramatic"
2020.coling-main.291,2020.acl-main.255,1,0.630869,"on models the two concepts independently (Table 2, right), and it is consequently able to correctly capture similarity in more difficult settings. 6 Word Sense Disambiguation Word Sense Disambiguation (WSD) – the task of assigning the correct meaning to a target word in a context – is considered to be a fundamental step towards natural language understanding (Navigli, 2018). As with many other tasks, WSD has benefited greatly from the recent advances in other fields, such as language modelling (Scarlini et al., 2020b), game theory (Tripodi and Navigli, 2019), structured knowledge integration (Bevilacqua and Navigli, 2020), definition modelling (Bevilacqua et al., 2020) and label propagation (Barba et al., 2020; Pasini and Navigli, 2020), inter alia. Our experiments show that Conception can be used to create state-of-the-art sense embeddings, demonstrating empirically that our approach provides high-quality knowledge that is still not captured by recent language models. Experimental setup. We start from state-of-the-art, precomputed sense embeddings and adopt a simple strategy to enrich such representations with Conception in order to evaluate its effectiveness in WSD. First, we create an embedding ec for a con"
2020.coling-main.291,2020.emnlp-main.585,1,0.580181,"ght), and it is consequently able to correctly capture similarity in more difficult settings. 6 Word Sense Disambiguation Word Sense Disambiguation (WSD) – the task of assigning the correct meaning to a target word in a context – is considered to be a fundamental step towards natural language understanding (Navigli, 2018). As with many other tasks, WSD has benefited greatly from the recent advances in other fields, such as language modelling (Scarlini et al., 2020b), game theory (Tripodi and Navigli, 2019), structured knowledge integration (Bevilacqua and Navigli, 2020), definition modelling (Bevilacqua et al., 2020) and label propagation (Barba et al., 2020; Pasini and Navigli, 2020), inter alia. Our experiments show that Conception can be used to create state-of-the-art sense embeddings, demonstrating empirically that our approach provides high-quality knowledge that is still not captured by recent language models. Experimental setup. We start from state-of-the-art, precomputed sense embeddings and adopt a simple strategy to enrich such representations with Conception in order to evaluate its effectiveness in WSD. First, we create an embedding ec for a concept cP by averaging the precomputed embeddings"
2020.coling-main.291,2020.emnlp-main.195,1,0.607974,"ostic and human-readable concept vector representations. Evaluated across multiple multilingual and cross-lingual Semantic Word Similarity datasets, Conception shows state-of-the-art results not only compared to concept representations such as NASARI, but also to multilingual word embeddings such as Conceptnet Numberbatch and cross-lingual language models such as XLM. Additionally, our concept representations are particularly robust on resource-poor languages, like Farsi, along the lines of recent work in Semantic Parsing and Semantic Role Labeling aimed at bridging the gap between languages (Blloshmi et al., 2020; Conia and Navigli, 2020). Finally, Conception can be seamlessly applied to a downstream task: in Word Sense Disambiguation, it improves over state-of-the-art supervised and knowledge-based sense embeddings, showing that Conception encodes information that is still not captured by BERT-based contextualized representations. Furthermore, our approach produces much more than concept representations: since each concept is described by the relationships it has with other concepts, Conception can be seen as a weighted directed graph where each node is a concept whose vector representation is also i"
2020.coling-main.291,Q17-1010,0,0.0409592,"laces multilinguality at its core while retaining explicit relationships between concepts. Our approach results in highcoverage representations that outperform the state of the art in multilingual and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on lowresource languages. Conception – its software and the complete set of representations – is available at https://github.com/SapienzaNLP/conception. 1 Introduction Word vector representations, in particular dense representations or word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017), play a key role in a wide range of tasks, including Text Similarity (Kenter and de Rijke, 2015; Nguyen et al., 2019), Word Sense Disambiguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactical"
2020.coling-main.291,P13-1133,0,0.0222737,"facets of a polysemous word in a context (Pilehvar and Camacho-Collados, 2019), but their implicitly encoded meanings are still disconnected from human-curated knowledge bases, even if more recent efforts showed promising results in imparting structured semantic knowledge to contextualized representations (Peters et al., 2019; Levine et al., 2019). Knowledge-enhanced representations. Alternative approaches implement multilinguality by making use of multilingual encyclopedic resources, such as Wikipedia (Al-Rfou et al., 2013), or multilingual knowledge graphs such as Open Multilingual WordNet (Bond and Foster, 2013), ConceptNet (Speer et al., 2017), or BabelNet (Navigli and Ponzetto, 2012a). A prominent example of knowledge-enhanced word embeddings is Conceptnet Numberbatch (Speer et al., 2017), which retrofits word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) word representations to the ConceptNet graph, achieving state-of-the-art results in Semantic Word Similarity (Speer and Lowry-Duda, 2017). A further step towards semantic representations involves modelling individual word senses as vectors which are explicitly linked to a knowledge resource. Early approaches to sense embeddings a"
2020.coling-main.291,K18-1026,0,0.0237324,"ords lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by each dimension are often unclear. More recent studies have shed some light on their interpretability (Rothe and Sch¨utze, 2016; Senel et al., 2018; Wallace et al., 2019) or included interpretability directly in the learning process (Park et al., 2017; Koc¸ et al., 2018), but the opaqueness of dense vectors is still a key reason why research has not completely given up on sparse representations (Faruqui et al., 2015; Derby et al., 2018). Moreover, most current embedding techniques rely on large corpora which are often available in few languages, such as English or Chinese, strongly limiting their robustness on low-resource languages (Speer and Lowry-Duda, 2017). In an attempt to solve this issue, researchers turned to multilingual word representations by making use of parallel vocabularies (Mikolov et al., 2013b; Ammar et al., 2016; Smith et al., 2017), exploiting multilingual knowledge graphs (Speer et al., 2017), and exploring unsupervised methods to align monolingual embeddings in a single shared distributional space (Con"
2020.coling-main.291,P14-2050,0,0.0422344,"iguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactically similar words lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by each dimension are often unclear. More recent studies have shed some light on their interpretability (Rothe and Sch¨utze, 2016; Senel et al., 2018; Wallace et al., 2019) or included interpretability directly in the learning process (Park et al., 2017; Koc¸ et al., 2018), but the opaqueness of dense vectors is still a key reason why research has not completely given up on sparse representations (Faruqui et al., 2015; Derby et al., 2018). Moreover, most current embedding techniques rely on large corpora which are often available in few languages, such as English or"
2020.coling-main.291,D15-1200,0,0.0247102,", December 8-13, 2020 be told apart since they are conflated into a single representation. As a result, contextualized word representations have garnered attention (Melamud et al., 2016), enjoying great success in the form of pretrained language models like BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019). At the same time, modelling techniques for individual word senses, concepts and named entities have also gained traction (Camacho-Collados et al., 2016; Scarlini et al., 2020a), though their integration into downstream NLP applications is still subject of ongoing investigations (Li and Jurafsky, 2015; Pilehvar et al., 2017). The requirement of massive amounts of training data and the lack of interpretability hinder most of the above-mentioned approaches. To address these limits, we introduce Conception, a novel knowledgebased technique for modelling concepts and named entities through concepts and named entities. Our approach places multilinguality at its core by leveraging the mutually-reinforcing information coming from different languages, enabling seamless and robust cross-lingual scaling, while also providing explicit and easily interpretable semantic dimensions. In contrast to most"
2020.coling-main.291,D14-1162,0,0.0829487,"tions of concepts which places multilinguality at its core while retaining explicit relationships between concepts. Our approach results in highcoverage representations that outperform the state of the art in multilingual and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on lowresource languages. Conception – its software and the complete set of representations – is available at https://github.com/SapienzaNLP/conception. 1 Introduction Word vector representations, in particular dense representations or word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017), play a key role in a wide range of tasks, including Text Similarity (Kenter and de Rijke, 2015; Nguyen et al., 2019), Word Sense Disambiguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two s"
2020.coling-main.291,N18-1202,0,0.0140707,"image-text data. While these methods still require annotated cross-lingual data or parallel vocabularies, Conneau et al. (2017) and Artetxe et al. (2018) found success by employing unsupervised methods and adversarial training. Contextualized word embeddings. The above-mentioned approaches produce word-level representations that are independent of the specific context a word appears in, and such “static” representations often show a strong bias towards the most frequent sense of a word. Instead, context-aware word representation techniques, such as context2vec (Melamud et al., 2016) or ELMo (Peters et al., 2018), dynamically create a representation for a word in a sentential or documental context. Contextualized embeddings witnessed a dramatic rise in popularity thanks to the advent and wide availability of language models pretrained on massive amounts of text, such as BERT (Devlin et al., 2019), immediately 3269 followed by multilingual language models, such as m-BERT and XLM (Conneau and Lample, 2019). Contextualized word embeddings are able to capture the many facets of a polysemous word in a context (Pilehvar and Camacho-Collados, 2019), but their implicitly encoded meanings are still disconnecte"
2020.coling-main.291,D19-1005,0,0.151906,"dvent and wide availability of language models pretrained on massive amounts of text, such as BERT (Devlin et al., 2019), immediately 3269 followed by multilingual language models, such as m-BERT and XLM (Conneau and Lample, 2019). Contextualized word embeddings are able to capture the many facets of a polysemous word in a context (Pilehvar and Camacho-Collados, 2019), but their implicitly encoded meanings are still disconnected from human-curated knowledge bases, even if more recent efforts showed promising results in imparting structured semantic knowledge to contextualized representations (Peters et al., 2019; Levine et al., 2019). Knowledge-enhanced representations. Alternative approaches implement multilinguality by making use of multilingual encyclopedic resources, such as Wikipedia (Al-Rfou et al., 2013), or multilingual knowledge graphs such as Open Multilingual WordNet (Bond and Foster, 2013), ConceptNet (Speer et al., 2017), or BabelNet (Navigli and Ponzetto, 2012a). A prominent example of knowledge-enhanced word embeddings is Conceptnet Numberbatch (Speer et al., 2017), which retrofits word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) word representations to the ConceptN"
2020.coling-main.291,D17-1120,1,0.845229,"l and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on lowresource languages. Conception – its software and the complete set of representations – is available at https://github.com/SapienzaNLP/conception. 1 Introduction Word vector representations, in particular dense representations or word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017), play a key role in a wide range of tasks, including Text Similarity (Kenter and de Rijke, 2015; Nguyen et al., 2019), Word Sense Disambiguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactically similar words lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by"
2020.coling-main.291,E17-1010,1,0.866976,"l and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on lowresource languages. Conception – its software and the complete set of representations – is available at https://github.com/SapienzaNLP/conception. 1 Introduction Word vector representations, in particular dense representations or word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017), play a key role in a wide range of tasks, including Text Similarity (Kenter and de Rijke, 2015; Nguyen et al., 2019), Word Sense Disambiguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactically similar words lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by"
2020.coling-main.291,P16-2083,0,0.0471752,"Missing"
2020.coling-main.291,W04-0811,0,0.238178,"Missing"
2020.coling-main.291,S17-2008,0,0.107061,"cent studies have shed some light on their interpretability (Rothe and Sch¨utze, 2016; Senel et al., 2018; Wallace et al., 2019) or included interpretability directly in the learning process (Park et al., 2017; Koc¸ et al., 2018), but the opaqueness of dense vectors is still a key reason why research has not completely given up on sparse representations (Faruqui et al., 2015; Derby et al., 2018). Moreover, most current embedding techniques rely on large corpora which are often available in few languages, such as English or Chinese, strongly limiting their robustness on low-resource languages (Speer and Lowry-Duda, 2017). In an attempt to solve this issue, researchers turned to multilingual word representations by making use of parallel vocabularies (Mikolov et al., 2013b; Ammar et al., 2016; Smith et al., 2017), exploiting multilingual knowledge graphs (Speer et al., 2017), and exploring unsupervised methods to align monolingual embeddings in a single shared distributional space (Conneau et al., 2017) or to directly learn multilingual embeddings (Chen and Cardie, 2018). Nevertheless, a well-known pitfall of both monolingual and multilingual word representations is the so-called meaning conflation deficiency"
2020.coling-main.291,D19-1009,1,0.850702,"ation of borsaIT of CNNB (Table 2, left). Conversely, Conception models the two concepts independently (Table 2, right), and it is consequently able to correctly capture similarity in more difficult settings. 6 Word Sense Disambiguation Word Sense Disambiguation (WSD) – the task of assigning the correct meaning to a target word in a context – is considered to be a fundamental step towards natural language understanding (Navigli, 2018). As with many other tasks, WSD has benefited greatly from the recent advances in other fields, such as language modelling (Scarlini et al., 2020b), game theory (Tripodi and Navigli, 2019), structured knowledge integration (Bevilacqua and Navigli, 2020), definition modelling (Bevilacqua et al., 2020) and label propagation (Barba et al., 2020; Pasini and Navigli, 2020), inter alia. Our experiments show that Conception can be used to create state-of-the-art sense embeddings, demonstrating empirically that our approach provides high-quality knowledge that is still not captured by recent language models. Experimental setup. We start from state-of-the-art, precomputed sense embeddings and adopt a simple strategy to enrich such representations with Conception in order to evaluate its"
2020.coling-main.291,D19-3002,0,0.0121206,"slation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactically similar words lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by each dimension are often unclear. More recent studies have shed some light on their interpretability (Rothe and Sch¨utze, 2016; Senel et al., 2018; Wallace et al., 2019) or included interpretability directly in the learning process (Park et al., 2017; Koc¸ et al., 2018), but the opaqueness of dense vectors is still a key reason why research has not completely given up on sparse representations (Faruqui et al., 2015; Derby et al., 2018). Moreover, most current embedding techniques rely on large corpora which are often available in few languages, such as English or Chinese, strongly limiting their robustness on low-resource languages (Speer and Lowry-Duda, 2017). In an attempt to solve this issue, researchers turned to multilingual word representations by makin"
2020.coling-main.291,P15-1025,0,0.0149361,"eption – its software and the complete set of representations – is available at https://github.com/SapienzaNLP/conception. 1 Introduction Word vector representations, in particular dense representations or word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017), play a key role in a wide range of tasks, including Text Similarity (Kenter and de Rijke, 2015; Nguyen et al., 2019), Word Sense Disambiguation (Iacobacci et al., 2016; Raganato et al., 2017a), Semantic Role Labeling (He et al., 2017; Marcheggiani et al., 2017; Conia et al., 2020), Question Answering (Zhou et al., 2015) and Machine Translation (Mikolov et al., 2013b; Bahdanau et al., 2015). This is especially the case when they are used as the underlying input representation. Word embedding techniques map each word to a relatively low n-dimensional space where two semantically or syntactically similar words lie close together. Due to their latent nature, however, most embeddings are commonly considered to be uninterpretable (Levy and Goldberg, 2014) as the properties captured by each dimension are often unclear. More recent studies have shed some light on their interpretability (Rothe and Sch¨utze, 2016; Sen"
2020.emnlp-demos.11,P98-1013,0,0.726389,"can easily query linguistic information and automatically an77 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 77–84 c November 16-20, 2020. 2020 Association for Computational Linguistics notate sentences on-the-go without having to write a single line of code. VerbAtlas, whenever two predicate senses can bear the same semantic role, the semantics of this role is coherent across the two predicate senses by definition, resulting in readable labels for non-expert users. VerbAtlas also clusters predicate senses into so-called frames (C OOK, D RINK, H IT, etc.) inspired by FrameNet (Baker et al., 1998), with the idea that senses sharing similar semantic behavior lie in the same frame. For non-expert users, this organization has the added advantage of explicitly linking predicate senses that are otherwise unrelated, like make.01 and create.01 in PropBank which, instead, are part of the same frame M OUNT-A SSEMBLE P RODUCE in VerbAtlas and, therefore, also share the same semantic roles. In a bid to make SRL more accessible, the InVeRo platform adopts the intelligible verb senses and semantic roles of VerbAtlas. Notably, InVeRo also takes advantage of PropBank to get the best of both worlds, a"
2020.emnlp-demos.11,J02-3001,0,0.485656,"linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http://nlp.uniroma1.it/invero. 1 Introduction Since its introduction (Gildea and Jurafsky, 2002), Semantic Role Labeling (SRL) has been recognized as a key task to enable Natural Language Understanding in that it aims at explicitly answering the &quot;Who did What to Whom, When and Where?&quot; question by identifying and labeling the predicateargument structure of a sentence, namely, the actors that take part in the scenario outlined by a predicate. In fact, SRL has already proven to be useful in a wide range of downstream tasks, including Question Answering (Shen and Lapata, 2007; He et al., 2015), Information Extraction (Christensen et al., 2011), Situation Recognition (Yatskar et al., 2016), M"
2020.emnlp-demos.11,P18-2058,0,0.176436,"per-Schuler, 2005), or FrameNet (Baker et al., 1998). The linguistic intricacies of such resources may, however, dishearten and turn away new practitioners. Regardless of which linguistic resource is used in the task, to further complicate the situation SRL has been usually divided into four subtasks – predicate identification, predicate sense disambiguation, argument identification and argument classification – but, to the best of our knowledge, recent state-of-the-art systems do not address all these four subtasks simultaneously without relying on external systems (Swayamdipta et al., 2017; He et al., 2018; Strubell et al., 2018; He et al., 2019). Therefore, obtaining predicate sense and semantic role annotations necessitates the tedious orchestration of multiple automatic systems, which in its turn further complicates the use of SRL in practice and in semantics-first approaches to NLP more generally. In this paper, we present InVeRo (Intelligibile Verbs and Roles), an online platform designed to tackle the aforementioned issues and make Semantic Role Labeling accessible to a broad audience. InVeRo brings together resources and tools to perform human-readable SRL, and it accomplishes this by us"
2020.emnlp-demos.11,2020.acl-main.255,1,0.792447,"Missing"
2020.emnlp-demos.11,P17-1044,0,0.0465517,"On the other hand, VerbAtlas (Di Fabio et al., 2019), a recently proposed predicate-argument structure inventory, in contrast to the enumerative approach of PropBank and the thousands of framespecific roles of FrameNet, adopts a small set of explicit and intelligible semantic roles (AGENT, P ROD UCT, R ESULT , D ESTINATION , . . . , T HEME ) inspired by VerbNet (Kipper-Schuler, 2005). As a result, in Model Design. The InVeRo all-in-one system for SRL is based on the ideas put forward by He et al. (2018) in that, unlike other works that used word78 level BIO tagging schemes to label arguments (He et al., 2017; Strubell et al., 2018; Tan et al., 2018), it directly models span-level features. In particular, we follow He et al. (2018) by letting the neural model learn span-level representations from the word-level representations of the span start and span end words, while also adding a span-length specific trainable embedding. More formally, the span representation sij from word i to word j is obtained as follows: and made available as prepackaged downloads, e.g. SENNA2 , or as online demos, e.g., AllenNLP’s SRL demo3 . However, recent BERT-based online systems, such as AllenNLP’s SRL demo, do not p"
2020.emnlp-demos.11,2020.emnlp-main.195,1,0.521471,"an-readable linguistic information about VerbAtlas, but also annotate entire sentences on-the-go without the need to install any software. InVeRo is a growing platform: in the future, we plan to enhance our Model API by adding, alongside the already available state-of-the-art spanbased model, the state-of-the-art dependency-based model of Conia and Navigli (2020a), so that users can easily switch between the two approaches and choose the one that best suits their needs. Thanks to BabelNet and recent advances in cross-lingual techniques for tasks where semantics is crucial (Barba et al., 2020; Blloshmi et al., 2020; Conia and Navigli, 2020b; Pasini, 2020; Scarlini et al., 2020), we also plan to provide support for multiple languages to enable SRL integration into multilingual and cross-lingual settings. We believe that the InVeRo platform can make SRL more accessible to the research community, and we look forward to the development of semantics-first approaches in an ever wider range of NLP applications. Resource interface. Figure 1 shows the Web interface when a user inserts the name of a VerbAtlas frame in the search bar. Notice that, since the interface makes use of the Resource API, a user can also"
2020.emnlp-demos.11,D15-1076,0,0.160772,"Missing"
2020.emnlp-demos.11,D19-1538,0,0.116368,"t al., 1998). The linguistic intricacies of such resources may, however, dishearten and turn away new practitioners. Regardless of which linguistic resource is used in the task, to further complicate the situation SRL has been usually divided into four subtasks – predicate identification, predicate sense disambiguation, argument identification and argument classification – but, to the best of our knowledge, recent state-of-the-art systems do not address all these four subtasks simultaneously without relying on external systems (Swayamdipta et al., 2017; He et al., 2018; Strubell et al., 2018; He et al., 2019). Therefore, obtaining predicate sense and semantic role annotations necessitates the tedious orchestration of multiple automatic systems, which in its turn further complicates the use of SRL in practice and in semantics-first approaches to NLP more generally. In this paper, we present InVeRo (Intelligibile Verbs and Roles), an online platform designed to tackle the aforementioned issues and make Semantic Role Labeling accessible to a broad audience. InVeRo brings together resources and tools to perform human-readable SRL, and it accomplishes this by using the intelligible verb senses and sema"
2020.emnlp-demos.11,2020.coling-main.120,1,0.896335,"ovide a raw text sentence to obtain its corresponding predicate and argument labels. Moreover, the InVeRo platform includes an online Web interface which repackages the APIs in a user-friendly environment. Thanks to this interface, users can easily obtain human-readable linguistic information about VerbAtlas, but also annotate entire sentences on-the-go without the need to install any software. InVeRo is a growing platform: in the future, we plan to enhance our Model API by adding, alongside the already available state-of-the-art spanbased model, the state-of-the-art dependency-based model of Conia and Navigli (2020a), so that users can easily switch between the two approaches and choose the one that best suits their needs. Thanks to BabelNet and recent advances in cross-lingual techniques for tasks where semantics is crucial (Barba et al., 2020; Blloshmi et al., 2020; Conia and Navigli, 2020b; Pasini, 2020; Scarlini et al., 2020), we also plan to provide support for multiple languages to enable SRL integration into multilingual and cross-lingual settings. We believe that the InVeRo platform can make SRL more accessible to the research community, and we look forward to the development of semantics-first"
2020.emnlp-demos.11,2020.coling-main.291,1,0.487904,"ovide a raw text sentence to obtain its corresponding predicate and argument labels. Moreover, the InVeRo platform includes an online Web interface which repackages the APIs in a user-friendly environment. Thanks to this interface, users can easily obtain human-readable linguistic information about VerbAtlas, but also annotate entire sentences on-the-go without the need to install any software. InVeRo is a growing platform: in the future, we plan to enhance our Model API by adding, alongside the already available state-of-the-art spanbased model, the state-of-the-art dependency-based model of Conia and Navigli (2020a), so that users can easily switch between the two approaches and choose the one that best suits their needs. Thanks to BabelNet and recent advances in cross-lingual techniques for tasks where semantics is crucial (Barba et al., 2020; Blloshmi et al., 2020; Conia and Navigli, 2020b; Pasini, 2020; Scarlini et al., 2020), we also plan to provide support for multiple languages to enable SRL integration into multilingual and cross-lingual settings. We believe that the InVeRo platform can make SRL more accessible to the research community, and we look forward to the development of semantics-first"
2020.emnlp-demos.11,N18-2078,0,0.0997889,"ling (SRL) has been recognized as a key task to enable Natural Language Understanding in that it aims at explicitly answering the &quot;Who did What to Whom, When and Where?&quot; question by identifying and labeling the predicateargument structure of a sentence, namely, the actors that take part in the scenario outlined by a predicate. In fact, SRL has already proven to be useful in a wide range of downstream tasks, including Question Answering (Shen and Lapata, 2007; He et al., 2015), Information Extraction (Christensen et al., 2011), Situation Recognition (Yatskar et al., 2016), Machine Translation (Marcheggiani et al., 2018), and Opinion Role Labeling (Zhang et al., 2019). Unfortunately, the integration of SRL knowledge into downstream applications has often been hampered and slowed down by the intrinsic complexity of the task itself (Navigli, 2018). Indeed, SRL is strongly intertwined with elaborate linguistic theories, as identifying and labeling predicateargument relations requires well-defined predicate sense and semantic role inventories such as the popular PropBank (Palmer et al., 2005), VerbNet • a Resource API to obtain linguistic information about the verb senses and semantic roles in VerbAtlas. • a Mode"
2020.emnlp-demos.11,D19-1058,1,0.562188,"Missing"
2020.emnlp-demos.11,J05-1004,0,0.863789,"nformation Extraction (Christensen et al., 2011), Situation Recognition (Yatskar et al., 2016), Machine Translation (Marcheggiani et al., 2018), and Opinion Role Labeling (Zhang et al., 2019). Unfortunately, the integration of SRL knowledge into downstream applications has often been hampered and slowed down by the intrinsic complexity of the task itself (Navigli, 2018). Indeed, SRL is strongly intertwined with elaborate linguistic theories, as identifying and labeling predicateargument relations requires well-defined predicate sense and semantic role inventories such as the popular PropBank (Palmer et al., 2005), VerbNet • a Resource API to obtain linguistic information about the verb senses and semantic roles in VerbAtlas. • a Model API to effortlessly annotate sentences using a state-of-the-art end-to-end pretrained model for span-based SRL. • a Web interface where users can easily query linguistic information and automatically an77 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 77–84 c November 16-20, 2020. 2020 Association for Computational Linguistics notate sentences on-the-go without having to write a single line of code. VerbAtlas, whenever two predicate senses can bear the sam"
2020.emnlp-demos.11,N19-1066,0,0.204336,"Missing"
2020.emnlp-demos.11,W12-4501,0,0.317838,"Missing"
2020.emnlp-demos.11,2020.emnlp-main.285,1,0.785909,"notate entire sentences on-the-go without the need to install any software. InVeRo is a growing platform: in the future, we plan to enhance our Model API by adding, alongside the already available state-of-the-art spanbased model, the state-of-the-art dependency-based model of Conia and Navigli (2020a), so that users can easily switch between the two approaches and choose the one that best suits their needs. Thanks to BabelNet and recent advances in cross-lingual techniques for tasks where semantics is crucial (Barba et al., 2020; Blloshmi et al., 2020; Conia and Navigli, 2020b; Pasini, 2020; Scarlini et al., 2020), we also plan to provide support for multiple languages to enable SRL integration into multilingual and cross-lingual settings. We believe that the InVeRo platform can make SRL more accessible to the research community, and we look forward to the development of semantics-first approaches in an ever wider range of NLP applications. Resource interface. Figure 1 shows the Web interface when a user inserts the name of a VerbAtlas frame in the search bar. Notice that, since the interface makes use of the Resource API, a user can also search for other resource-specific information such as individua"
2020.emnlp-demos.11,D07-1002,0,0.0598086,"eb interface and RESTful APIs available at http://nlp.uniroma1.it/invero. 1 Introduction Since its introduction (Gildea and Jurafsky, 2002), Semantic Role Labeling (SRL) has been recognized as a key task to enable Natural Language Understanding in that it aims at explicitly answering the &quot;Who did What to Whom, When and Where?&quot; question by identifying and labeling the predicateargument structure of a sentence, namely, the actors that take part in the scenario outlined by a predicate. In fact, SRL has already proven to be useful in a wide range of downstream tasks, including Question Answering (Shen and Lapata, 2007; He et al., 2015), Information Extraction (Christensen et al., 2011), Situation Recognition (Yatskar et al., 2016), Machine Translation (Marcheggiani et al., 2018), and Opinion Role Labeling (Zhang et al., 2019). Unfortunately, the integration of SRL knowledge into downstream applications has often been hampered and slowed down by the intrinsic complexity of the task itself (Navigli, 2018). Indeed, SRL is strongly intertwined with elaborate linguistic theories, as identifying and labeling predicateargument relations requires well-defined predicate sense and semantic role inventories such as t"
2020.emnlp-demos.11,D18-1548,0,0.0331317,".01 and make.02) and which do not (e.g., A RG 1 is a product in make.01 but a result in make.02). On the other hand, VerbAtlas (Di Fabio et al., 2019), a recently proposed predicate-argument structure inventory, in contrast to the enumerative approach of PropBank and the thousands of framespecific roles of FrameNet, adopts a small set of explicit and intelligible semantic roles (AGENT, P ROD UCT, R ESULT , D ESTINATION , . . . , T HEME ) inspired by VerbNet (Kipper-Schuler, 2005). As a result, in Model Design. The InVeRo all-in-one system for SRL is based on the ideas put forward by He et al. (2018) in that, unlike other works that used word78 level BIO tagging schemes to label arguments (He et al., 2017; Strubell et al., 2018; Tan et al., 2018), it directly models span-level features. In particular, we follow He et al. (2018) by letting the neural model learn span-level representations from the word-level representations of the span start and span end words, while also adding a span-length specific trainable embedding. More formally, the span representation sij from word i to word j is obtained as follows: and made available as prepackaged downloads, e.g. SENNA2 , or as online demos, e."
2020.emnlp-main.195,P19-1309,0,0.0608678,"This choice is motivated by the existence of reliable machine translation systems for the languages of our interest. Moreover, we validate the silver translations through a back-translation step (Sennrich et al., 2016). That is, firstly, we translate the sentences from English to the target language and, secondly, using the same neural translation model, we translate the target language translations back to English. Then, to filter out less accurate translations we apply a 1-N N strategy based on the cosine similarity between translations and source sentence semantic embeddings, similarly to Artetxe and Schwenk (2019a). If the nearest neighbour of a translation corresponds to its source English sentence, we consider it a good translation, otherwise we discard it. We employ semantic similarity since we have a two-step automatic translation, due to which lexical differences are introduced into translations compared to the original sentence. Typical machine translation metrics, e.g., BLEU, METEOR, rely on lexical similarity, which could lead good translations being discarded. In fact, we do not need the translation to be word-to-word aligned, but rather to preserve the meaning of the sentence, thus consideri"
2020.emnlp-main.195,Q19-1038,0,0.109597,"This choice is motivated by the existence of reliable machine translation systems for the languages of our interest. Moreover, we validate the silver translations through a back-translation step (Sennrich et al., 2016). That is, firstly, we translate the sentences from English to the target language and, secondly, using the same neural translation model, we translate the target language translations back to English. Then, to filter out less accurate translations we apply a 1-N N strategy based on the cosine similarity between translations and source sentence semantic embeddings, similarly to Artetxe and Schwenk (2019a). If the nearest neighbour of a translation corresponds to its source English sentence, we consider it a good translation, otherwise we discard it. We employ semantic similarity since we have a two-step automatic translation, due to which lexical differences are introduced into translations compared to the original sentence. Typical machine translation metrics, e.g., BLEU, METEOR, rely on lexical similarity, which could lead good translations being discarded. In fact, we do not need the translation to be word-to-word aligned, but rather to preserve the meaning of the sentence, thus consideri"
2020.emnlp-main.195,W13-2322,0,0.804395,"omatic AMR annotations across languages and develop a crosslingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xlamr. 1 Introduction Abstract Meaning Representation (AMR) is a popular formalism for natural language (Banarescu et al., 2013). It represents sentences as rooted, directed and acyclic graphs in which nodes are concepts and edges are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detect"
2020.emnlp-main.195,2020.lrec-1.86,0,0.130055,", in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of natural language understanding. A peculiar feature of the AMR formalism is that it aims at abstracting away from word forms. AMR graphs are unanchored, i.e., the linkage between tokens in a s"
2020.emnlp-main.195,P13-2131,0,0.745174,"on. This is largely attributable to the lack of training data and evaluation benchmarks in languages other than English. Damonte and Cohen (2018) propose, to the best of our knowledge, the only cross-lingual AMR parser to date and, moreover, their proposed cross-lingual AMR evaluation benchmark has been released only very recently (Damonte and Cohen, 2020). The authors adapt a transition-based English AMR parser (Damonte et al., 2017) for cross-lingual AMR parsing, which is trained on silver annotated data. However, the performances it has achieved are not satisfying in terms of Smatch score (Cai and Knight, 2013), mostly as a result of concept identification errors, which in turn are directly related to the usage of noisy word-to-node alignments projected from English. Throughout the literature English AMR parsers commonly rely on AMR alignments which are automatically created using heuristics (Flanigan et al., 2014), or on pretrained aligners (Pourdamghani et al., 2014; Liu et al., 2018), treated as latent variables of the model (Lyu and Titov, 2018) or implicitly modelled through sourcecopy mechanisms (Zhang et al., 2019). These alignments, however, take advantage of the fact that AMR nodes and Engl"
2020.emnlp-main.195,2020.coling-main.291,1,0.74605,"resentations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of natural language understanding. A peculiar feature of the AMR formalism is that it aims at abstracting away from word forms. AMR graphs are unanchored, i.e., the linkage between tokens in a sentence and nodes in the corresponding graph is not explicitly annotated. Hence, the feature of being agnostic about how to derive meanings from strings makes AMR particularly suitable for representing semantics cross-ling"
2020.emnlp-main.195,N18-1104,0,0.136147,"to this, the available resources and modelling techniques focus mostly on English, while leaving cross-lingual AMR understudied. Some preliminary studies showed the limits of AMR as an interlingua, categorizing them as due to distinctions in the underlying ontologies or structural divergences among languages (Xue et al., 2014; Hajiˇc et al., 2014). More recent studies, instead, have provided evidence that AMR or a simplified version of it can be used as a formalism for cross-lingual semantic representation, showing that it is possible to overcome some of the structural linguistic divergences (Damonte and Cohen, 2018; Zhu et al., 2019). The underlying idea of this paper is that AMR can be used to represent semantic information in 2487 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2487–2500, c November 16–20, 2020. 2020 Association for Computational Linguistics different languages since there exist key linguistic features that are shared across languages, such as predicates, roles and conjunctions (Von Fintel and Matthewson, 2008). However, developing an AMR parser for multiple languages is hard because the existing annotated training resources that are suffi"
2020.emnlp-main.195,E17-1051,0,0.5746,"r to express only essential semantic features of a sentence, such as predicate roles and linguistic relations. Cross-lingual AMR parsing, instead, has received relatively less attention. This is largely attributable to the lack of training data and evaluation benchmarks in languages other than English. Damonte and Cohen (2018) propose, to the best of our knowledge, the only cross-lingual AMR parser to date and, moreover, their proposed cross-lingual AMR evaluation benchmark has been released only very recently (Damonte and Cohen, 2020). The authors adapt a transition-based English AMR parser (Damonte et al., 2017) for cross-lingual AMR parsing, which is trained on silver annotated data. However, the performances it has achieved are not satisfying in terms of Smatch score (Cai and Knight, 2013), mostly as a result of concept identification errors, which in turn are directly related to the usage of noisy word-to-node alignments projected from English. Throughout the literature English AMR parsers commonly rely on AMR alignments which are automatically created using heuristics (Flanigan et al., 2014), or on pretrained aligners (Pourdamghani et al., 2014; Liu et al., 2018), treated as latent variables of t"
2020.emnlp-main.195,N19-2003,0,0.037147,"c graphs in which nodes are concepts and edges are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of natural language understanding. A peculiar feature of the AMR formalism is that it aims at abstract"
2020.emnlp-main.195,N19-1423,0,0.0620542,"Missing"
2020.emnlp-main.195,D19-1058,1,0.492532,"Missing"
2020.emnlp-main.195,P90-1017,0,0.720997,"nyms of the corresponding concepts in the gold graph, e.g., say-01 → state-01, stop-01 → halt-01, best friend → best mate, demand-01 → urge-01, etc. We notice that the predicted concepts (to the left of the arrow) are less specific than the gold concepts, yet somehow preserve the meaning. These examples show that the parser captures a close meaning even when failing to predict the exact concept. Translation divergences We investigate how deals with the cases where there exist translation divergences, i.e., cases in which source and target language have different syntactic ordering properties (Dorr, 1990), as classified by Dorr (1994) using the following 7 categories: i) thematic, ii) promotional, iii) demotional, iv) structural, v) conflational, vi) categorial, vii) lexical.11 A thematic divergence happens when the argument-predicate structure is different across lanXL - AMR 11 9 github.com/mdtux89/amr-evaluation 10 github.com/mdtux89/amr-eager-multilingual In absence of a larger available resource for language divergences, here we make use of some of the pre-classified examples from Dorr (1990, 1994). 2494 guages, e.g., I like travelling where I is the subject, in Italian becomes Mi piace vi"
2020.emnlp-main.195,J94-4004,0,0.272833,"hes. In summary, translating the gold standard training data, i.e., G OLD A MR-S ILVER T RNS, leads XLAMR to achieve higher performances than when trained on parallel sentences associated with silver AMR graphs, i.e., PAR S ENTS-S ILVER A MR . 5 Qualitative Analysis We manually check the predictions of XL-AMR in order to establish the nature of the mistakes based on the Smatch score between the gold and predicted AMR graphs and determine their severity. Then, we observe how XL-AMR handles the translation divergences, i.e., linguistic distinctions that make transfer across languages difficult (Dorr, 1994). Smatch errors The parser has difficulties with some compounded words in German, e.g., Uranproduktionsf¨ahigkeit (uranium production capability), Kernkraftstoffkreislauf (nuclear fuel cycle), for which it fails to break their meaning down to the correct subgraph, e.g., (c / cycle-02 :ARG1 (f /fuel :mod (n / nucleus))), thus predicting a generic node, i.e., (t / thing). This issue can be alleviated using a better preprocessing to split the compounds. Several cases with low Smatch score are due to inconsistent translations of test set sentences into the target language, even though, we recall,"
2020.emnlp-main.195,P14-1134,0,0.0812951,"ed as a seq2seq problem, and relation identification, based on a biaffine attention classifier (Dozat and Manning, 2017). We use a seq2seq model to dispose of the need for an AMR alignment module. Lyu and Titov (2018) argue that alignments are important for injecting a useful inductive bias for AMR parsing and maintain that alignment-based parsers might be better than seq2seq for AMR parsing, owing to the relatively small amount of data available for AMR. However, aligning words to AMR nodes in cross-lingual parsing is challenging. The widely used AMR aligners are usually based on heuristics (Flanigan et al., 2014), or on the fact that AMR and English are highly cognate (Pourdamghani et al., 2014). Hence, these approaches would not be valid for cross-lingual alignment and, moreover, projecting the alignments across languages through English has shown to be noisy and to affect the parsing performance (Damonte and Cohen, 2018). Concept identification At training time we obtain the list of nodes by first converting the graph into a tree, duplicating the nodes occurring in multiple relations, and then using a pre-order traversal over the tree. To account for reentrancies we assign a unique index to each nod"
2020.emnlp-main.195,W14-5808,0,0.372718,"Missing"
2020.emnlp-main.195,D18-1086,0,0.136454,"opular formalism for natural language (Banarescu et al., 2013). It represents sentences as rooted, directed and acyclic graphs in which nodes are concepts and edges are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual"
2020.emnlp-main.195,P82-1020,0,0.787052,"Missing"
2020.emnlp-main.195,N18-1041,0,0.0444998,"represents sentences as rooted, directed and acyclic graphs in which nodes are concepts and edges are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of natural language understanding. A peculiar fe"
2020.emnlp-main.195,kingsbury-palmer-2002-treebank,0,0.577313,"oduce better quality training corpora. This leads to significant improvements and provides evidence that better quality data – and models – allow for using AMR as an interlingua. (A) Parallel Sentences Iran Figure 1: Cross-Lingual AMR Parsing: (A) Sentences written in different languages sharing the same meaning; (B) concepts representing the words in the sentences; (C) the final AMR graph. 3.1 The Task Cross-lingual AMR parsing is defined as the task of transducing a sentence in any language to the AMR graph of its English translation whose nodes are either English words, PropBank framesets (Kingsbury and Palmer, 2002) or special AMR keywords. Breaking down this definition, given an English sentence and its translation TL in a language L, their meaning representation is ideally formalized by the same AMR, G = (V, E), where V is a list of concept nodes and E is the set of semantic relations between them. Figure 1-A shows an example of a sentence in English, with its translations into Chinese, German, Italian and Spanish which have the same meaning and therefore the same abstract representation (Figure 1-C). Following state-of-the-art models for English AMR parsing (Zhang et al., 2019), we tackle cross-lingua"
2020.emnlp-main.195,Q16-1023,0,0.0210196,"tification, which we briefly 2489 overview here and later detail in Section 3.2. For concept identification, given the sequence TL = (t1 , t2 , . . . , tj ), ti being a word in language L (i ∈ {1, . . . , j}, L ∈ {EN, DE, ES, IT, ZH}), we train a neural network to generate the list of nodes V = (v1 , v2 , . . . , vn ), vi ∈ English words ∪ PropBank framesets ∪ AMR keywords. In Figure 1-B we show the list of concepts that represent the words in the sentences of Figure 1-A. The relation identification procedure, instead, is inspired by the arc-factored approaches employed in dependency parsing (Kiperwasser and Goldberg, 2016), i.e., searching for the maximum-scoring connected subgraph over the identified concepts in the previous step. Thus, given the list of predicted nodes V = (v1 , v2 , . . . , vn ) and a learned score for each candidate edge, we search for the highest-scoring spanning tree and then merge the duplicate nodes based on unique node indices (see Section 3.2) to restore the final AMR graph. Figure 1-C shows the AMR representing the shared semantics of the sentences in Figure 1-A. 3.2 XL - AMR Model XL - AMR is composed of two modules which are learned jointly, i.e., concept identification, modeled as"
2020.emnlp-main.195,2005.mtsummit-papers.11,0,0.0436156,"t al., 2019). Different techniques include annotation projection, machine translation and language-independent feature-based models. Extensive works in this direction exist, applied to different NLP tasks, i.e., WSD (Barba et al., 2020), SRL (Pad´o and Lapata, 2009; Kozhevnikov and Titov, 2013), Dependency Parsing (Tiedemann, 2015), concept representation (Conia and Navigli, 2020), etc. In cross-lingual AMR parsing, annotation projection is employed by Damonte and Cohen (2018), who produce cross-lingual silver AMR annotations by exploiting parallel sentences selected from the Europarl corpus (Koehn, 2005): English sentences are parsed using an English parser (Damonte et al., 2017, AMR E AGER) and the resulting graphs are associated with the corresponding parallel sentences. However, the data on which AMR E AGER was trained is very different from those used to produce the silver annotations, thus affecting the quality and reliability of the AMR graphs produced. Here we test two different techniques: we conduct experiments with annotation projection using Europarl for comparison, and, in addition, we use translation techniques to produce better quality training corpora. This leads to significant"
2020.emnlp-main.195,P17-1014,0,0.105598,"Missing"
2020.emnlp-main.195,P13-1117,0,0.02691,":w Cross-Lingual AMR Die Stadt Tel Aviv ist weniger als 650 Meilen vom iranischen Territorium entfernt. 2 3 DE :op Transfer learning The idea behind this method is to leverage annotations available in one language, commonly English, to enable learning models that generalize to languages where labelled resources are scarce (Ruder et al., 2019). Different techniques include annotation projection, machine translation and language-independent feature-based models. Extensive works in this direction exist, applied to different NLP tasks, i.e., WSD (Barba et al., 2020), SRL (Pad´o and Lapata, 2009; Kozhevnikov and Titov, 2013), Dependency Parsing (Tiedemann, 2015), concept representation (Conia and Navigli, 2020), etc. In cross-lingual AMR parsing, annotation projection is employed by Damonte and Cohen (2018), who produce cross-lingual silver AMR annotations by exploiting parallel sentences selected from the Europarl corpus (Koehn, 2005): English sentences are parsed using an English parser (Damonte et al., 2017, AMR E AGER) and the resulting graphs are associated with the corresponding parallel sentences. However, the data on which AMR E AGER was trained is very different from those used to produce the silver anno"
2020.emnlp-main.195,C18-1101,0,0.0771911,"ral language (Banarescu et al., 2013). It represents sentences as rooted, directed and acyclic graphs in which nodes are concepts and edges are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of nat"
2020.emnlp-main.195,D18-1264,0,0.111075,"Missing"
2020.emnlp-main.195,D15-1166,0,0.0103545,"d, takes the hidden states of the previous layer as input. The decoder also consists of L recurrent neural network (unidirectional) layers with LSTM cells. The decoder embedding layer concatenates word embeddings, node index embeddings and characterlevel embeddings. The layer l of the decoder call−1 l culates dlt = decoderl (dl−1 is t , dt−1 ), where dt the concept hidden state of the previous layer at timestep t while dlt−1 that of previous timestep. dl0 is initialized with the concatenation of the en→ − ← − coder’s last hidden states hl = [ hl ; hl ]. We follow the input feeding approach of Luong et al. (2015), which concatenates the output of the decoder’s embedding layer and an attentional vector computed at the previous timestep. We first compute the source attention distribution at using additive attention (Bahdanau et al., 2015) as follows: L et,i = v > tanh(Wh hL i + Ws dt + bs ) at = softmax(et ) X ct = at,i hi i where v, Wh , Ws and bs are model parameters, and ct is the source context vector. Then, we compute the attentional vector, d˜t = tanh(Wc [ct ; dL t ] + bc ), where Wc and bc are model parameters. Zhang et al. (2019) used the attentional vector to allow the decoder to copy nodes pre"
2020.emnlp-main.195,P18-1037,0,0.190677,"d score for each candidate edge, we search for the highest-scoring spanning tree and then merge the duplicate nodes based on unique node indices (see Section 3.2) to restore the final AMR graph. Figure 1-C shows the AMR representing the shared semantics of the sentences in Figure 1-A. 3.2 XL - AMR Model XL - AMR is composed of two modules which are learned jointly, i.e., concept identification, modeled as a seq2seq problem, and relation identification, based on a biaffine attention classifier (Dozat and Manning, 2017). We use a seq2seq model to dispose of the need for an AMR alignment module. Lyu and Titov (2018) argue that alignments are important for injecting a useful inductive bias for AMR parsing and maintain that alignment-based parsers might be better than seq2seq for AMR parsing, owing to the relatively small amount of data available for AMR. However, aligning words to AMR nodes in cross-lingual parsing is challenging. The widely used AMR aligners are usually based on heuristics (Flanigan et al., 2014), or on the fact that AMR and English are highly cognate (Pourdamghani et al., 2014). Hence, these approaches would not be valid for cross-lingual alignment and, moreover, projecting the alignmen"
2020.emnlp-main.195,S19-2012,0,0.0348768,"Missing"
2020.emnlp-main.195,E17-1035,0,0.112985,"Missing"
2020.emnlp-main.195,P14-5010,0,0.00359211,"(Navigli and Ponzetto, 2010), DBpedia Spotlight API (Daiber et al., 2013) for wikifiAMR 2491 Dataset Lang Gold PAR S ENTS S ILVER A MR G OLD A MR S ILVER T RNS Train Insts Dev Insts Source EN 36521 1368 AMR DE EN ES IT 20000 20000 20000 20000 2000 2000 2000 2000 Europarl Europarl Europarl Europarl DE ES IT ZH 34415 34552 34521 32154 1319 1325 1322 1276 AMR 2.0 2.0 2.0 AMR 2.0 AMR 2.0 AMR Table 1: Dataset quality standard, instances per language, and the source corpus of the sentences. cation in all languages but Chinese, for which we use Babelfy (Moro et al., 2014) instead, Stanford CoreNLP (Manning et al., 2014) for English preprocessing pipeline, the Stanza Toolkit (Qi et al., 2020) for Chinese, German and Spanish sentences, and Tint3 (Aprosio and Moretti, 2016) for Italian. The preprocessing steps consist of: i) lemmatization, ii) PoS tagging, iii) NER, iv) re-categorization of entities and senses, v) removal of wiki links and polarity attributes. The postprocessing steps consist of restoring i) anonymized subgraphs, ii) wikification, iii) senses, iv) polarity attributes. We give full details on pre- and postprocessing in Appendix A. 4 choose Europarl as parallel corpus.4 We predict the silver AMR"
2020.emnlp-main.195,de-marneffe-etal-2014-universal,0,0.114702,"Missing"
2020.emnlp-main.195,Q14-1019,1,0.723247,"gual resources such as Wikipedia, BabelNet 4.0 (Navigli and Ponzetto, 2010), DBpedia Spotlight API (Daiber et al., 2013) for wikifiAMR 2491 Dataset Lang Gold PAR S ENTS S ILVER A MR G OLD A MR S ILVER T RNS Train Insts Dev Insts Source EN 36521 1368 AMR DE EN ES IT 20000 20000 20000 20000 2000 2000 2000 2000 Europarl Europarl Europarl Europarl DE ES IT ZH 34415 34552 34521 32154 1319 1325 1322 1276 AMR 2.0 2.0 2.0 AMR 2.0 AMR 2.0 AMR Table 1: Dataset quality standard, instances per language, and the source corpus of the sentences. cation in all languages but Chinese, for which we use Babelfy (Moro et al., 2014) instead, Stanford CoreNLP (Manning et al., 2014) for English preprocessing pipeline, the Stanza Toolkit (Qi et al., 2020) for Chinese, German and Spanish sentences, and Tint3 (Aprosio and Moretti, 2016) for Italian. The preprocessing steps consist of: i) lemmatization, ii) PoS tagging, iii) NER, iv) re-categorization of entities and senses, v) removal of wiki links and polarity attributes. The postprocessing steps consist of restoring i) anonymized subgraphs, ii) wikification, iii) senses, iv) polarity attributes. We give full details on pre- and postprocessing in Appendix A. 4 choose Europar"
2020.emnlp-main.195,P10-1023,1,0.75136,", METEOR, rely on lexical similarity, which could lead good translations being discarded. In fact, we do not need the translation to be word-to-word aligned, but rather to preserve the meaning of the sentence, thus considering valid also the cases when certain words are translated into synonyms or related words. We refer to this method as G OLD A MR-S ILVER T RNS. 3.4 Pre- and Postprocessing parsers in the literature rely on several preand postprocessing rules. We extend these rules for the cross-lingual AMR parsing task based on several multilingual resources such as Wikipedia, BabelNet 4.0 (Navigli and Ponzetto, 2010), DBpedia Spotlight API (Daiber et al., 2013) for wikifiAMR 2491 Dataset Lang Gold PAR S ENTS S ILVER A MR G OLD A MR S ILVER T RNS Train Insts Dev Insts Source EN 36521 1368 AMR DE EN ES IT 20000 20000 20000 20000 2000 2000 2000 2000 Europarl Europarl Europarl Europarl DE ES IT ZH 34415 34552 34521 32154 1319 1325 1322 1276 AMR 2.0 2.0 2.0 AMR 2.0 AMR 2.0 AMR Table 1: Dataset quality standard, instances per language, and the source corpus of the sentences. cation in all languages but Chinese, for which we use Babelfy (Moro et al., 2014) instead, Stanford CoreNLP (Manning et al., 2014) for Eng"
2020.emnlp-main.195,oepen-lonning-2006-discriminant,0,0.0748953,"sis of the ability of XL-AMR to transfer semantic structures across languages and of AMR to represent the meaning of sentences cross-lingually. 2 Related Work Our work lies between two areas, namely, semantic parsing and cross-lingual transfer learning. Semantic parsing Semantic parsing is a key task required to complete the puzzle of Natural Language Understanding (Navigli, 2018), and one which is receiving growing attention in the scientific community. Besides AMR, various different formalisms have been proposed over the years to encode semantic structures: Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), Universal Decompositional Semantics (White et al., 2016, UDS), inter alia. While some frameworks, such as UCCA and UDS, have been exploited in a cross-linguistic setting (Lyu et al., 2019; Zhang et al., 2018), cross-lingual AMR has mainly been studied within the scope of annotation analysis works (Xue et al., 2014; Hajiˇc et al., 2014). These works point out the limitations of AMR as an interlingua, and consider them partly due to the distinctions in t"
2020.emnlp-main.195,petrov-etal-2012-universal,0,0.105565,"Missing"
2020.emnlp-main.195,D14-1048,0,0.0655012,"ion classifier (Dozat and Manning, 2017). We use a seq2seq model to dispose of the need for an AMR alignment module. Lyu and Titov (2018) argue that alignments are important for injecting a useful inductive bias for AMR parsing and maintain that alignment-based parsers might be better than seq2seq for AMR parsing, owing to the relatively small amount of data available for AMR. However, aligning words to AMR nodes in cross-lingual parsing is challenging. The widely used AMR aligners are usually based on heuristics (Flanigan et al., 2014), or on the fact that AMR and English are highly cognate (Pourdamghani et al., 2014). Hence, these approaches would not be valid for cross-lingual alignment and, moreover, projecting the alignments across languages through English has shown to be noisy and to affect the parsing performance (Damonte and Cohen, 2018). Concept identification At training time we obtain the list of nodes by first converting the graph into a tree, duplicating the nodes occurring in multiple relations, and then using a pre-order traversal over the tree. To account for reentrancies we assign a unique index to each node during traversal, similarly to Zhang et al. (2019). Following the attention-based"
2020.emnlp-main.195,2020.acl-demos.14,0,0.020329,"wikifiAMR 2491 Dataset Lang Gold PAR S ENTS S ILVER A MR G OLD A MR S ILVER T RNS Train Insts Dev Insts Source EN 36521 1368 AMR DE EN ES IT 20000 20000 20000 20000 2000 2000 2000 2000 Europarl Europarl Europarl Europarl DE ES IT ZH 34415 34552 34521 32154 1319 1325 1322 1276 AMR 2.0 2.0 2.0 AMR 2.0 AMR 2.0 AMR Table 1: Dataset quality standard, instances per language, and the source corpus of the sentences. cation in all languages but Chinese, for which we use Babelfy (Moro et al., 2014) instead, Stanford CoreNLP (Manning et al., 2014) for English preprocessing pipeline, the Stanza Toolkit (Qi et al., 2020) for Chinese, German and Spanish sentences, and Tint3 (Aprosio and Moretti, 2016) for Italian. The preprocessing steps consist of: i) lemmatization, ii) PoS tagging, iii) NER, iv) re-categorization of entities and senses, v) removal of wiki links and polarity attributes. The postprocessing steps consist of restoring i) anonymized subgraphs, ii) wikification, iii) senses, iv) polarity attributes. We give full details on pre- and postprocessing in Appendix A. 4 choose Europarl as parallel corpus.4 We predict the silver AMR using the model of Zhang et al. (2019). For the second approach, instead,"
2020.emnlp-main.195,W17-2315,0,0.0734116,"Missing"
2020.emnlp-main.195,P16-1009,0,0.0230558,"ranslations In addition to pivoting through parallel sentences, we investigate whether considering human-annotated AMR graphs could bring more benefits than system produced AMR graphs. To this end, we make use of the existing gold standard datasets for AMR parsing, i.e., English sentence-AMR graph pairs, and use machine translation systems to translate the training sentences into the target language. This choice is motivated by the existence of reliable machine translation systems for the languages of our interest. Moreover, we validate the silver translations through a back-translation step (Sennrich et al., 2016). That is, firstly, we translate the sentences from English to the target language and, secondly, using the same neural translation model, we translate the target language translations back to English. Then, to filter out less accurate translations we apply a 1-N N strategy based on the cosine similarity between translations and source sentence semantic embeddings, similarly to Artetxe and Schwenk (2019a). If the nearest neighbour of a translation corresponds to its source English sentence, we consider it a good translation, otherwise we discard it. We employ semantic similarity since we have"
2020.emnlp-main.195,Q19-1002,0,0.349991,"es are semantic relations among them. AMR unifies, in a single structure, a rich set of information coming from different tasks, such as Named Entity Recognition (NER), Semantic Role Labeling (SRL), Word Sense Disambiguation (WSD) and coreference resolution. Such representations are actively integrated in several Natural Language Processing (NLP) applications, inter alia, information extraction (Rao et al., 2017), text summarization (Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa et al., 2018), spoken language understanding (Damonte et al., 2019), machine translation (Song et al., 2019b) and human-robot interaction (Bonial et al., 2020). It is therefore desirable to extend AMR semantic representations across languages along the lines of cross-lingual representations for grammatical annotation (de Marneffe et al., 2014), concepts (Conia and Navigli, 2020) and semantic roles (Akbik et al., 2015; Di Fabio et al., 2019). Furthermore, it could be especially useful to integrate cross-lingual semantic structures in multilingual applications of natural language understanding. A peculiar feature of the AMR formalism is that it aims at abstracting away from word forms. AMR graphs are"
2020.emnlp-main.195,W15-1824,0,0.0231941,"er als 650 Meilen vom iranischen Territorium entfernt. 2 3 DE :op Transfer learning The idea behind this method is to leverage annotations available in one language, commonly English, to enable learning models that generalize to languages where labelled resources are scarce (Ruder et al., 2019). Different techniques include annotation projection, machine translation and language-independent feature-based models. Extensive works in this direction exist, applied to different NLP tasks, i.e., WSD (Barba et al., 2020), SRL (Pad´o and Lapata, 2009; Kozhevnikov and Titov, 2013), Dependency Parsing (Tiedemann, 2015), concept representation (Conia and Navigli, 2020), etc. In cross-lingual AMR parsing, annotation projection is employed by Damonte and Cohen (2018), who produce cross-lingual silver AMR annotations by exploiting parallel sentences selected from the Europarl corpus (Koehn, 2005): English sentences are parsed using an English parser (Damonte et al., 2017, AMR E AGER) and the resulting graphs are associated with the corresponding parallel sentences. However, the data on which AMR E AGER was trained is very different from those used to produce the silver annotations, thus affecting the quality an"
2020.emnlp-main.195,2020.eamt-1.61,0,0.0365514,"cessing steps consist of restoring i) anonymized subgraphs, ii) wikification, iii) senses, iv) polarity attributes. We give full details on pre- and postprocessing in Appendix A. 4 choose Europarl as parallel corpus.4 We predict the silver AMR using the model of Zhang et al. (2019). For the second approach, instead, i.e., G OLD A MR-S ILVER T RNS, we choose AMR 2.0 as gold dataset and translate the sentences into Chinese, German, Italian and Spanish. For German, Italian and Spanish, for both translating and back-translating the sentences we use the machine translation models made available by Tiedemann and Thottingal (2020, OPUS-MT).5 For Chinese, instead, since OPUS-MT does not provide translation models, we employ the released MASS6 (Song et al., 2019a) supervised neural translation models. Then, to filter out less accurate translations, we compute the cosine similarity between dense semantic representations of the original English sentence and its back-translated counterpart. To embed the sentences we use LASER (Artetxe and Schwenk, 2019b), a state-ofthe-art model for sentence embeddings. Details on the number of instances per language and for each silver data approach are shown in Table 1. Training configur"
2020.emnlp-main.195,D16-1177,0,0.0873558,"Missing"
2020.emnlp-main.195,xue-etal-2014-interlingua,0,0.197402,"Missing"
2020.emnlp-main.195,P19-1009,0,0.135881,"Missing"
2020.emnlp-main.195,D18-1194,0,0.0412974,"Missing"
2020.emnlp-main.195,W19-3320,0,0.276953,"sources and modelling techniques focus mostly on English, while leaving cross-lingual AMR understudied. Some preliminary studies showed the limits of AMR as an interlingua, categorizing them as due to distinctions in the underlying ontologies or structural divergences among languages (Xue et al., 2014; Hajiˇc et al., 2014). More recent studies, instead, have provided evidence that AMR or a simplified version of it can be used as a formalism for cross-lingual semantic representation, showing that it is possible to overcome some of the structural linguistic divergences (Damonte and Cohen, 2018; Zhu et al., 2019). The underlying idea of this paper is that AMR can be used to represent semantic information in 2487 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2487–2500, c November 16–20, 2020. 2020 Association for Computational Linguistics different languages since there exist key linguistic features that are shared across languages, such as predicates, roles and conjunctions (Von Fintel and Matthewson, 2008). However, developing an AMR parser for multiple languages is hard because the existing annotated training resources that are sufficiently large are a"
2020.emnlp-main.285,W18-2505,0,0.0667604,"Missing"
2020.emnlp-main.285,2020.emnlp-main.585,1,0.851263,"guity in texts (Moro et al., 2014; Agirre et al., 2014; Tripodi and Navigli, 2019; Scozzafava et al., 2020). These approaches do not rely on semantically-tagged training data and are hence able to scale over all the languages supported by their underlying knowledge base. Nevertheless, they lag behind their supervised counterparts on English in terms of performance. Supervised approaches, by framing WSD as a classification task, have acquitted themselves as the state of the art in English (Hadiwinoto et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Bevilacqua et al., 2020), outperforming their knowledge-based competitors by several points. Recently, Pilehvar and Camacho-Collados (2019) provided a new declination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations"
2020.emnlp-main.285,2020.acl-main.255,1,0.811465,"edge such as lexical knowledge bases (LKB). The task of associating a word in context with the most suitable meaning from a predefined sense inventory is better known as Word Sense Disambiguation (Navigli, 2009, WSD), and is usually tackled by two kinds of approach: knowledge-based and supervised ones. On the one hand, knowledgebased approaches (Scozzafava et al., 2020; Conia and Navigli, 2020) are able to scale across languages since they do not need sense-annotated corpora and rely only on the information within their underlying LKB. On the other hand, supervised models (Huang et al., 2019; Bevilacqua and Navigli, 2020) have proved to achieve state-of-the-art results on the English benchmarks by taking advantage of manually-annotated data for the task and machine learning algorithms. However, supervised approaches are mostly focused on English (Navigli, 2018; Pasini, 2020) and have only recently been applied to lower-resourced languages thanks to automatically-produced datasets (Scarlini et al., 2019; Barba et al., 2020; Pasini and Navigli, 2020). Another effective approach in this direction has been presented by Scarlini et al. (2020), who introduced SensEmBERT, a knowledge-based approach to building sense"
2020.emnlp-main.285,2020.acl-main.95,0,0.258911,"12), and employ algorithms on graphs to address the word ambiguity in texts (Moro et al., 2014; Agirre et al., 2014; Tripodi and Navigli, 2019; Scozzafava et al., 2020). These approaches do not rely on semantically-tagged training data and are hence able to scale over all the languages supported by their underlying knowledge base. Nevertheless, they lag behind their supervised counterparts on English in terms of performance. Supervised approaches, by framing WSD as a classification task, have acquitted themselves as the state of the art in English (Hadiwinoto et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Bevilacqua et al., 2020), outperforming their knowledge-based competitors by several points. Recently, Pilehvar and Camacho-Collados (2019) provided a new declination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have"
2020.emnlp-main.285,2020.emnlp-main.195,1,0.796572,"nderlying pre-trained language models. We further tested our embeddings in the WiC task where they lead a baseline neural model to outperform its closest competitors that rely on larger architectures or dedicated pre-training routines. Our embeddings computed with BERT large and mBERT and the automatically-extracted contexts are available at http://sensembert.org/ares. As future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual Semantic Role Labeling (Di Fabio et al., 2019; Conia et al., 2020) and cross-lingual Semantic Parsing (Blloshmi et al., 2020). Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union’s Horizon 2020 research and innovation programme. WiC Results In Table 8 we report the results of the systems under comparison on the WiC test set. BERTARES attains 2.6 points more than its base model, i.e., BERTLARGE , while exploiting ARES embeddings 17 Conclusion This work was supported in part by the MIUR under grant “Dipartimenti di eccellenza 2018-2022” of the Department of Computer Science of the Sapienza University of Rome. See Appendix A.3 for train"
2020.emnlp-main.285,2020.emnlp-demos.11,1,0.785909,"rks, leveraging BERT large and mBERT, respectively, as underlying pre-trained language models. We further tested our embeddings in the WiC task where they lead a baseline neural model to outperform its closest competitors that rely on larger architectures or dedicated pre-training routines. Our embeddings computed with BERT large and mBERT and the automatically-extracted contexts are available at http://sensembert.org/ares. As future work, we plan to exploit the information brought by our embeddings to other downstream tasks, such as multilingual Semantic Role Labeling (Di Fabio et al., 2019; Conia et al., 2020) and cross-lingual Semantic Parsing (Blloshmi et al., 2020). Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union’s Horizon 2020 research and innovation programme. WiC Results In Table 8 we report the results of the systems under comparison on the WiC test set. BERTARES attains 2.6 points more than its base model, i.e., BERTLARGE , while exploiting ARES embeddings 17 Conclusion This work was supported in part by the MIUR under grant “Dipartimenti di eccellenza 2018-2022” of the Department of Computer Science of"
2020.emnlp-main.285,2020.coling-main.291,1,0.797181,"across the board. Nevertheless, these latent representations do not provide any explicit information regarding the meaning expressed by the word in context, hence making it difficult to link texts to structured sources of knowledge such as lexical knowledge bases (LKB). The task of associating a word in context with the most suitable meaning from a predefined sense inventory is better known as Word Sense Disambiguation (Navigli, 2009, WSD), and is usually tackled by two kinds of approach: knowledge-based and supervised ones. On the one hand, knowledgebased approaches (Scozzafava et al., 2020; Conia and Navigli, 2020) are able to scale across languages since they do not need sense-annotated corpora and rely only on the information within their underlying LKB. On the other hand, supervised models (Huang et al., 2019; Bevilacqua and Navigli, 2020) have proved to achieve state-of-the-art results on the English benchmarks by taking advantage of manually-annotated data for the task and machine learning algorithms. However, supervised approaches are mostly focused on English (Navigli, 2018; Pasini, 2020) and have only recently been applied to lower-resourced languages thanks to automatically-produced datasets (S"
2020.emnlp-main.285,N19-1423,0,0.620921,"quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares. 1 Introduction Contextualized word embeddings have proved to be highly beneficial to the majority of Natural Language Processing tasks (Wang et al., 2019). Indeed, language models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), etc., enable architectures built on top of them to attain performances that were previously out of reach (Wang et al., 2019). The main reason behind this great success is the fact that contextualized embeddings of words encode the semantics defined by their input context (Reif et al., 2019). Indeed, when tested in the Word-in-Context (WiC) task (Pilehvar and Camacho-Collados, 2019), i.e., a binary classification problem where a model has to classify whether a target word is used with the same meaning in two different sentences, contextua"
2020.emnlp-main.285,D19-1058,1,0.804585,"Missing"
2020.emnlp-main.285,S01-1001,0,0.652712,"synsets (77,195 out of 117,659), providing at least one annotated example for 56,022 synsets that are not covered by SemCor. The total number of distinct tagged sentences is more than 10M for a total of 13M annotations. On average, most synsets have around 150 annotated examples, as shown in Figure 1. 6 WSD Experimental Setup We now report the setup of the evaluation we conducted on the English and multilingual WSD tasks. Evaluation Datasets We carried out the evaluation on the English all-words WSD framework by Raganato et al. (2017),6 comprising five standard test sets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) along with ALL, i.e., the concatenation of all the test sets. As concerns the multilingual evaluation, we considered the latest versions of the two multilingual all-words WSD datasets of 3532 6 http://nlp.uniroma1.it/wsdeval/ SemEval-13 (Navigli et al., 2013) and SemEval15 (Moro and Navigli, 2015), containing test sets for French, German, Italian and Spanish.7 We report all results in terms of the F1 score, i.e., the harmonic mean of the precision an"
2020.emnlp-main.285,D19-1533,0,0.215035,"1990) and BabelNet (Navigli and Ponzetto, 2012), and employ algorithms on graphs to address the word ambiguity in texts (Moro et al., 2014; Agirre et al., 2014; Tripodi and Navigli, 2019; Scozzafava et al., 2020). These approaches do not rely on semantically-tagged training data and are hence able to scale over all the languages supported by their underlying knowledge base. Nevertheless, they lag behind their supervised counterparts on English in terms of performance. Supervised approaches, by framing WSD as a classification task, have acquitted themselves as the state of the art in English (Hadiwinoto et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Bevilacqua et al., 2020), outperforming their knowledge-based competitors by several points. Recently, Pilehvar and Camacho-Collados (2019) provided a new declination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al"
2020.emnlp-main.285,D19-1355,0,0.381074,"red sources of knowledge such as lexical knowledge bases (LKB). The task of associating a word in context with the most suitable meaning from a predefined sense inventory is better known as Word Sense Disambiguation (Navigli, 2009, WSD), and is usually tackled by two kinds of approach: knowledge-based and supervised ones. On the one hand, knowledgebased approaches (Scozzafava et al., 2020; Conia and Navigli, 2020) are able to scale across languages since they do not need sense-annotated corpora and rely only on the information within their underlying LKB. On the other hand, supervised models (Huang et al., 2019; Bevilacqua and Navigli, 2020) have proved to achieve state-of-the-art results on the English benchmarks by taking advantage of manually-annotated data for the task and machine learning algorithms. However, supervised approaches are mostly focused on English (Navigli, 2018; Pasini, 2020) and have only recently been applied to lower-resourced languages thanks to automatically-produced datasets (Scarlini et al., 2019; Barba et al., 2020; Pasini and Navigli, 2020). Another effective approach in this direction has been presented by Scarlini et al. (2020), who introduced SensEmBERT, a knowledge-ba"
2020.emnlp-main.285,P19-1568,0,0.151076,"ent languages. Comparison systems We compared ARES with both knowledge-based and supervised approaches on English. As knowledge-based systems, we considered UKB with SyntagNet’s relations (Scozzafava et al., 2020, UKB+Syn ), and SensEmBERT (Scarlini et al., 2020), along with its supervised version, i.e., SensEmBERTsup . SensEmBERT and SensEmBERTsup cover only nominal senses, so we used the Most Frequent Sense (MFS) backoff strategy, i.e., predicting the most frequent sense of a lemma in WordNet, for tagging instances with other POS tags. Among supervised systems, we tested against EWISEConvE (Kumar et al., 2019), KnowBERT (Peters et al., 2019), the vocabulary compression model by Vial et al. (2019, BERThyp ),11 GlossBERT (Huang et al., 2019) and the approach proposed by Hadiwinoto et al. (2019, BERTGLU+LW ). Moreover, we compared against Loureiro and Jorge (2019, LMMS) and Peters et al. (2018)’s method using BERT (BERT k-NN). We also report the performance of these two latter approaches by using mBERT instead of BERT large, i.e., LMMSmBERT and mBERT k-NN. All supervised systems under comparison use SemCor only as training corpus. We performed additional comparisons by using Peters et al. (2018)’s met"
2020.emnlp-main.285,2021.ccl-1.108,0,0.119747,"Missing"
2020.emnlp-main.285,P19-1569,0,0.409297,". This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations that can be applied directly to disambiguation. Some of the first approaches of this kind were proposed by Melamud et al. (2016) and Peters et al. (2018), who exploited the semantically-tagged sentences of SemCor (Miller et al., 1993) and neural language models to create embeddings for the senses in WordNet. Similarly, Loureiro and Jorge (2019, LMMS) computed sense embeddings using BERT (Devlin et al., 2019) and the relations in a lexical knowledge base in order to also provide vectors for those meanings that do not appear in SemCor. The most recent effort in this direction is SensEmBERT (Scarlini et al., 2020), which drops the need for sense-annotated corpora by exploiting the BabelNet mapping between WordNet senses and Wikipedia pages so as to collect contextual information for the senses in WordNet. Since it does not rely on manually-annotated data SensEmBERT can scale over different languages, being limited, however, to nominal"
2020.emnlp-main.285,D19-1359,1,0.849558,"Regarding the context extraction step (see Section 4.1), we set the number of clusters k for a lexeme l as the number of its senses in WordNet. We varied the number of words n to give as input to UKB between 5 and 25 with a 5 step and selected the value n = 5 by manually assessing the quality of a sample of the clusters’ disambiguation output. As for the number of sentences t and ξ, we ranged them between 50 and 300 with a 50 step9 and selected the values that maximized the performance in terms of F1 of ARES on SemEval-07,10 i.e., t = 150 and ξ = 50. As regards the window size w, we followed Maru et al. (2019) and set w = 3. Concerning BERT representations, we used the BERT large-cased model for English. To scale across languages, instead, we made use of BERT base-multilingual-cased (mBERT) so as to build unified representations that are shared across languages, i.e., ARESm . For our multilingual representations, we focused on synset embeddings rather than sense ones. In fact, senses are languagespecific as they are tied to one of the lemmas of the synset. Hence, we built ARESm synset embeddings by averaging the representations of their English senses. We note that, while the pre-trained model diff"
2020.emnlp-main.285,K16-1006,0,0.0510959,"(2019) provided a new declination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations that can be applied directly to disambiguation. Some of the first approaches of this kind were proposed by Melamud et al. (2016) and Peters et al. (2018), who exploited the semantically-tagged sentences of SemCor (Miller et al., 1993) and neural language models to create embeddings for the senses in WordNet. Similarly, Loureiro and Jorge (2019, LMMS) computed sense embeddings using BERT (Devlin et al., 2019) and the relations in a lexical knowledge base in order to also provide vectors for those meanings that do not appear in SemCor. The most recent effort in this direction is SensEmBERT (Scarlini et al., 2020), which drops the need for sense-annotated corpora by exploiting the BabelNet mapping between WordNet senses a"
2020.emnlp-main.285,H93-1061,0,0.5208,"target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations that can be applied directly to disambiguation. Some of the first approaches of this kind were proposed by Melamud et al. (2016) and Peters et al. (2018), who exploited the semantically-tagged sentences of SemCor (Miller et al., 1993) and neural language models to create embeddings for the senses in WordNet. Similarly, Loureiro and Jorge (2019, LMMS) computed sense embeddings using BERT (Devlin et al., 2019) and the relations in a lexical knowledge base in order to also provide vectors for those meanings that do not appear in SemCor. The most recent effort in this direction is SensEmBERT (Scarlini et al., 2020), which drops the need for sense-annotated corpora by exploiting the BabelNet mapping between WordNet senses and Wikipedia pages so as to collect contextual information for the senses in WordNet. Since it does not re"
2020.emnlp-main.285,S15-2049,1,0.897722,"istinct tagged sentences is more than 10M for a total of 13M annotations. On average, most synsets have around 150 annotated examples, as shown in Figure 1. 6 WSD Experimental Setup We now report the setup of the evaluation we conducted on the English and multilingual WSD tasks. Evaluation Datasets We carried out the evaluation on the English all-words WSD framework by Raganato et al. (2017),6 comprising five standard test sets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) along with ALL, i.e., the concatenation of all the test sets. As concerns the multilingual evaluation, we considered the latest versions of the two multilingual all-words WSD datasets of 3532 6 http://nlp.uniroma1.it/wsdeval/ SemEval-13 (Navigli et al., 2013) and SemEval15 (Moro and Navigli, 2015), containing test sets for French, German, Italian and Spanish.7 We report all results in terms of the F1 score, i.e., the harmonic mean of the precision and recall.8 ARES Configuration We used Wikipedia as input corpus since it is the largest general-domain resource currently available. Regarding th"
2020.emnlp-main.285,S13-2040,1,0.880894,"ed by SemCor. The total number of distinct tagged sentences is more than 10M for a total of 13M annotations. On average, most synsets have around 150 annotated examples, as shown in Figure 1. 6 WSD Experimental Setup We now report the setup of the evaluation we conducted on the English and multilingual WSD tasks. Evaluation Datasets We carried out the evaluation on the English all-words WSD framework by Raganato et al. (2017),6 comprising five standard test sets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) along with ALL, i.e., the concatenation of all the test sets. As concerns the multilingual evaluation, we considered the latest versions of the two multilingual all-words WSD datasets of 3532 6 http://nlp.uniroma1.it/wsdeval/ SemEval-13 (Navigli et al., 2013) and SemEval15 (Moro and Navigli, 2015), containing test sets for French, German, Italian and Spanish.7 We report all results in terms of the F1 score, i.e., the harmonic mean of the precision and recall.8 ARES Configuration We used Wikipedia as input corpus since it is the largest general-domain resou"
2020.emnlp-main.285,2020.acl-main.369,1,0.791754,"in SyntagNet to extract a meaningful set of contexts where s is likely to appear (Section 4.1); 2. Synset embedding, which creates the embedding of the synset s by encoding the contextual information of the sentences gathered in the previous step (Section 4.2); 3. Sense embedding, which combines the senseannotated contexts in SemCor, the definitional information of the glosses and our synset embeddings to create the final sense representation (Section 4.3). 4.1 Context Extraction In this Section we describe our approach for automatically retrieving contexts for WordNet’s synsets. First, as in Pasini et al. (2020), we utilize BERT and UKB to find contexts that are similar to each other and link them to a meaning in WordNet. Then, we enrich the set of contexts retrieved for a given synset s by exploiting the semantic collocations available in SyntagNet. Similarity-Based Extraction Given a synset s and one of its lexicalizations l, we collect the occurrences of l in the input corpus C and compute their contextualized representations by means of BERT.3 We then cluster the contextualized vectors of l’s occurrences by using the k-means algorithm. We note that the sentences comprised within the same cluster"
2020.emnlp-main.285,N18-1202,0,0.360852,"lination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations that can be applied directly to disambiguation. Some of the first approaches of this kind were proposed by Melamud et al. (2016) and Peters et al. (2018), who exploited the semantically-tagged sentences of SemCor (Miller et al., 1993) and neural language models to create embeddings for the senses in WordNet. Similarly, Loureiro and Jorge (2019, LMMS) computed sense embeddings using BERT (Devlin et al., 2019) and the relations in a lexical knowledge base in order to also provide vectors for those meanings that do not appear in SemCor. The most recent effort in this direction is SensEmBERT (Scarlini et al., 2020), which drops the need for sense-annotated corpora by exploiting the BabelNet mapping between WordNet senses and Wikipedia pages so as"
2020.emnlp-main.285,D19-1005,0,0.367824,"English (Hadiwinoto et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Bevilacqua et al., 2020), outperforming their knowledge-based competitors by several points. Recently, Pilehvar and Camacho-Collados (2019) provided a new declination of WSD, formulating it as a binary classification problem where, given a target word and two contexts, a model has to predict whether the target word is used with the same meaning. This setting has the advantage of not drawing on sense inventories and provides an effective testbed for context-based word embeddings (Peters et al., 2019; Levine et al., 2020). Contextualized sense representations have recently been employed to compute sense representations that can be applied directly to disambiguation. Some of the first approaches of this kind were proposed by Melamud et al. (2016) and Peters et al. (2018), who exploited the semantically-tagged sentences of SemCor (Miller et al., 1993) and neural language models to create embeddings for the senses in WordNet. Similarly, Loureiro and Jorge (2019, LMMS) computed sense embeddings using BERT (Devlin et al., 2019) and the relations in a lexical knowledge base in order to also pro"
2020.emnlp-main.285,N19-1128,0,0.102723,"Missing"
2020.emnlp-main.285,P19-1493,0,0.0490908,"Missing"
2020.emnlp-main.285,S07-1016,0,0.160234,"r 56,022 synsets that are not covered by SemCor. The total number of distinct tagged sentences is more than 10M for a total of 13M annotations. On average, most synsets have around 150 annotated examples, as shown in Figure 1. 6 WSD Experimental Setup We now report the setup of the evaluation we conducted on the English and multilingual WSD tasks. Evaluation Datasets We carried out the evaluation on the English all-words WSD framework by Raganato et al. (2017),6 comprising five standard test sets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) along with ALL, i.e., the concatenation of all the test sets. As concerns the multilingual evaluation, we considered the latest versions of the two multilingual all-words WSD datasets of 3532 6 http://nlp.uniroma1.it/wsdeval/ SemEval-13 (Navigli et al., 2013) and SemEval15 (Moro and Navigli, 2015), containing test sets for French, German, Italian and Spanish.7 We report all results in terms of the F1 score, i.e., the harmonic mean of the precision and recall.8 ARES Configuration We used Wikipedia as input corpus since it"
2020.emnlp-main.285,P19-1069,1,0.939971,") are able to scale across languages since they do not need sense-annotated corpora and rely only on the information within their underlying LKB. On the other hand, supervised models (Huang et al., 2019; Bevilacqua and Navigli, 2020) have proved to achieve state-of-the-art results on the English benchmarks by taking advantage of manually-annotated data for the task and machine learning algorithms. However, supervised approaches are mostly focused on English (Navigli, 2018; Pasini, 2020) and have only recently been applied to lower-resourced languages thanks to automatically-produced datasets (Scarlini et al., 2019; Barba et al., 2020; Pasini and Navigli, 2020). Another effective approach in this direction has been presented by Scarlini et al. (2020), who introduced SensEmBERT, a knowledge-based approach to building sense embeddings without relying on sense-annotated data. Since it is not tied to semantic annotations, SensEmBERT scales over different languages. However, it is limited to nominal concepts only and provides different 3528 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3528–3539, c November 16–20, 2020. 2020 Association for Computational Lingui"
2020.emnlp-main.285,2020.acl-demos.6,1,0.904399,"s as the best approaches across the board. Nevertheless, these latent representations do not provide any explicit information regarding the meaning expressed by the word in context, hence making it difficult to link texts to structured sources of knowledge such as lexical knowledge bases (LKB). The task of associating a word in context with the most suitable meaning from a predefined sense inventory is better known as Word Sense Disambiguation (Navigli, 2009, WSD), and is usually tackled by two kinds of approach: knowledge-based and supervised ones. On the one hand, knowledgebased approaches (Scozzafava et al., 2020; Conia and Navigli, 2020) are able to scale across languages since they do not need sense-annotated corpora and rely only on the information within their underlying LKB. On the other hand, supervised models (Huang et al., 2019; Bevilacqua and Navigli, 2020) have proved to achieve state-of-the-art results on the English benchmarks by taking advantage of manually-annotated data for the task and machine learning algorithms. However, supervised approaches are mostly focused on English (Navigli, 2018; Pasini, 2020) and have only recently been applied to lower-resourced languages thanks to automati"
2020.emnlp-main.285,W04-0811,0,0.234306,"ding at least one annotated example for 56,022 synsets that are not covered by SemCor. The total number of distinct tagged sentences is more than 10M for a total of 13M annotations. On average, most synsets have around 150 annotated examples, as shown in Figure 1. 6 WSD Experimental Setup We now report the setup of the evaluation we conducted on the English and multilingual WSD tasks. Evaluation Datasets We carried out the evaluation on the English all-words WSD framework by Raganato et al. (2017),6 comprising five standard test sets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) along with ALL, i.e., the concatenation of all the test sets. As concerns the multilingual evaluation, we considered the latest versions of the two multilingual all-words WSD datasets of 3532 6 http://nlp.uniroma1.it/wsdeval/ SemEval-13 (Navigli et al., 2013) and SemEval15 (Moro and Navigli, 2015), containing test sets for French, German, Italian and Spanish.7 We report all results in terms of the F1 score, i.e., the harmonic mean of the precision and recall.8 ARES Configuration We used"
2020.emnlp-main.285,K15-1037,0,0.0494955,"Missing"
2020.emnlp-main.285,D19-1009,1,0.836537,"erform their competitors and achieve the state of the art on all the languages available in the all-words multilingual WSD tasks, i.e., French, German, Italian and Spanish. 2 Related Work Word Sense Disambiguation (WSD) is a core task in lexical semantics and has mainly been tackled by two kinds of approach: knowledge-based and supervised ones. Knowledge-based methods build upon lexical knowledge bases, such as WordNet (Miller et al., 1990) and BabelNet (Navigli and Ponzetto, 2012), and employ algorithms on graphs to address the word ambiguity in texts (Moro et al., 2014; Agirre et al., 2014; Tripodi and Navigli, 2019; Scozzafava et al., 2020). These approaches do not rely on semantically-tagged training data and are hence able to scale over all the languages supported by their underlying knowledge base. Nevertheless, they lag behind their supervised counterparts on English in terms of performance. Supervised approaches, by framing WSD as a classification task, have acquitted themselves as the state of the art in English (Hadiwinoto et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Bevilacqua et al., 2020), outperforming their knowledge-based competitors by seve"
2020.emnlp-main.285,2019.gwc-1.14,0,0.0705987,"ed approaches on English. As knowledge-based systems, we considered UKB with SyntagNet’s relations (Scozzafava et al., 2020, UKB+Syn ), and SensEmBERT (Scarlini et al., 2020), along with its supervised version, i.e., SensEmBERTsup . SensEmBERT and SensEmBERTsup cover only nominal senses, so we used the Most Frequent Sense (MFS) backoff strategy, i.e., predicting the most frequent sense of a lemma in WordNet, for tagging instances with other POS tags. Among supervised systems, we tested against EWISEConvE (Kumar et al., 2019), KnowBERT (Peters et al., 2019), the vocabulary compression model by Vial et al. (2019, BERThyp ),11 GlossBERT (Huang et al., 2019) and the approach proposed by Hadiwinoto et al. (2019, BERTGLU+LW ). Moreover, we compared against Loureiro and Jorge (2019, LMMS) and Peters et al. (2018)’s method using BERT (BERT k-NN). We also report the performance of these two latter approaches by using mBERT instead of BERT large, i.e., LMMSmBERT and mBERT k-NN. All supervised systems under comparison use SemCor only as training corpus. We performed additional comparisons by using Peters et al. (2018)’s method with BERT on SemCor+OMSTI (Taghipour and Ng, 2015, SemCor+OMSTIBERT ), a semi-autom"
2020.emnlp-main.285,E17-1010,1,0.857817,"el differs between the two representations, the sentences used to create the embeddings are the same as the ones used for English. Following Loureiro and Jorge (2019), we took as BERT representation the sum of the last four hidden layers. WSD Setup To test ARES on the WSD task, we employed the 1-NN algorithm. To this end, we computed the BERT representation of each word w in the test sentences and compared it with the embeddings corresponding to the senses of w in WordNet. Since ARES vectors are made of the con7 https://github.com/SapienzaNLP/ mwsd-datasets 8 We used the scoring script in the Raganato et al. (2017)’s framework to compute all performances. 9 All hyperparameters search spaces were manually chosen. 10 We chose SemEval-07 as it is the standard development set used in the literature (Raganato et al., 2017). catenation of two BERT representations (Section 4.3), we repeated the embedding of w in order to match the shape of ARES vectors. Thus, we took as prediction the sense that maximizes the similarity with w’s representation. For languages other than English, we considered as candidate synsets for a lemma those associated with it in BabelNet 4.0, i.e., a multilingual knowledge base providing"
2020.emnlp-main.585,P18-2043,0,0.479951,"d the recently-proposed Word-inContext (Pilehvar and Camacho-Collados, 2019, 1 To ensure better readability, here we will use the term “gloss” as a synonym of the traditional dictionary “definition”. 7207 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7207–7221, c November 16–20, 2020. 2020 Association for Computational Linguistics WiC). This, in turn, results in a more solid assessment of the generation quality, a notorious problem in Natural Language Generation (NLG) evaluation (Gatt and Krahmer, 2018). In contrast to previous approaches in DM (Gadetsky et al., 2018), we dispense with the requirement of having the definiendum represented by a single vector, and we condition gloss generation on a context of which the definiendum is an arbitrary span. This allows for the generation of contextual definitions for items that are rarely covered by sense inventories, such as free word combinations (e.g. clumsy apology or nutty complexion). Finally, the generative formulation makes it possible to train on several lexicographic resources at once, resulting in a versatile model that performs well across inventories, datasets, and tasks. The main contributions of ou"
2020.emnlp-main.585,W02-0109,0,0.56516,"Missing"
2020.emnlp-main.585,S13-2040,1,0.917132,"en-UNI ppl↓ BL↑ R-L↑ MT↑ BS↑ HEI CHAS model WSD Evaluation We now move to the assessment of Generationary in a traditional WSD setting. Even though our approach goes beyond fixed sense inventories, here we want to show that this degree of freedom does not come at the expense of performance when presented with the task of choosing a sense from a finite predefined list. We test on the five datasets collected in the evaluation framework of Raganato et al. (2017), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), SemEval-2015 (Moro and Navigli, 2015), which are all annotated with WordNet 3.0 senses (or converted to its inventory). We denote with ALL and ALL− the concatenation of all evaluation datasets, including or excluding, respectively, SemEval-2007, which is our development set for this experiment. Moreover, we test on the subset of ALL− containing instances whose lemmas are not covered in SemCor (0-shot). 5.2.1 Systems To choose a possible sense from WordNet and perform WSD, we evaluate the techniques presented in Section 3.2, i.e. probability scoring (Prob.), simple similarity scoring (Sim.),"
2020.emnlp-main.585,S07-1006,1,0.786303,"Missing"
2020.emnlp-main.585,S07-1016,0,0.0864705,"ari-CHAS * Ishiwatari-UNI* Gen-CHAS Gen-UNI ppl↓ BL↑ R-L↑ MT↑ BS↑ HEI CHAS model WSD Evaluation We now move to the assessment of Generationary in a traditional WSD setting. Even though our approach goes beyond fixed sense inventories, here we want to show that this degree of freedom does not come at the expense of performance when presented with the task of choosing a sense from a finite predefined list. We test on the five datasets collected in the evaluation framework of Raganato et al. (2017), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), SemEval-2015 (Moro and Navigli, 2015), which are all annotated with WordNet 3.0 senses (or converted to its inventory). We denote with ALL and ALL− the concatenation of all evaluation datasets, including or excluding, respectively, SemEval-2007, which is our development set for this experiment. Moreover, we test on the subset of ALL− containing instances whose lemmas are not covered in SemCor (0-shot). 5.2.1 Systems To choose a possible sense from WordNet and perform WSD, we evaluate the techniques presented in Section 3.2, i.e. probability scoring (Prob."
2020.emnlp-main.585,J91-4003,0,0.1853,"Missing"
2020.emnlp-main.585,P19-1069,1,0.870811,"Missing"
2020.emnlp-main.585,2020.emnlp-main.285,1,0.904259,"Missing"
2020.emnlp-main.585,2020.acl-demos.6,1,0.841955,"Missing"
2020.emnlp-main.585,W04-0811,0,0.723946,"72.0 47.0 90.7 CHAU Random Chang Ishiwatari-CHAS * Ishiwatari-UNI* Gen-CHAS Gen-UNI ppl↓ BL↑ R-L↑ MT↑ BS↑ HEI CHAS model WSD Evaluation We now move to the assessment of Generationary in a traditional WSD setting. Even though our approach goes beyond fixed sense inventories, here we want to show that this degree of freedom does not come at the expense of performance when presented with the task of choosing a sense from a finite predefined list. We test on the five datasets collected in the evaluation framework of Raganato et al. (2017), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013), SemEval-2015 (Moro and Navigli, 2015), which are all annotated with WordNet 3.0 senses (or converted to its inventory). We denote with ALL and ALL− the concatenation of all evaluation datasets, including or excluding, respectively, SemEval-2007, which is our development set for this experiment. Moreover, we test on the subset of ALL− containing instances whose lemmas are not covered in SemCor (0-shot). 5.2.1 Systems To choose a possible sense from WordNet and perform WSD, we evaluate the techniques presented in Section"
2020.emnlp-main.585,D19-1009,1,0.651631,"Missing"
2020.lrec-1.366,W04-3221,0,0.0647178,"argument structure, and is centered on the lexical aspects of meaning. CPA inherits some of its intents from the Generative Lexicon (Pustejovsky, 1991), the theory of preference semantics (Wilks, 1975), and others. Generally speaking, words are not taken in isolation and the meaning they are attributed is ascertained on a contextual basis through prototypical sentence patterns. However, these theories and methods for building semantic resources remain linked to the lexical basis and are suitable for the manual effort of lexicographers. On the other hand, automatic approaches such as those of Almuhareb and Poesio (2004), Baroni et al. (2010), Navigli and Velardi (2010) and Boella and Di Caro (2013) used surface text patterns to automatically extract concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extractin"
2020.lrec-1.366,P98-1013,0,0.613113,"not only to improve historical NLP tasks such as Word Sense Disambiguation and Machine Translation, but also to enable numerous applications (Hovy et al., 2013) like intelligent personal assistants, Question Answering, Information Retrieval, etc. Lexical-semantic knowledge has typically been encoded on a large scale using word senses as meaning units, starting from WordNet (Miller, 1995) and then carrying on with VerbNet (Schuler, 2005), PropBank (Kingsbury and Palmer, 2002) and, more recently, VerbAtlas (Di Fabio et al., 2019). Large repositories of frames have been introduced with FrameNet (Baker et al., 1998) and its counterparts in other languages. However, frames are focused on situation-based representations with semantic roles and commonly involved objects, without defining the embodied ontological semantics of single concepts. Word senses, on the other hand, suffer from a lack of explicit common-sense semantic information. Indeed, the well-known fine granularity of word senses in WordNet (Palmer et al., 2007) is due to the lack of a meaning encoding system capable of managing the representation of concepts in a flexible way. To address this issue, Pustejovsky introduced an innovative model (P"
2020.lrec-1.366,P13-2095,1,0.878568,"Missing"
2020.lrec-1.366,P13-1133,0,0.0842602,"Missing"
2020.lrec-1.366,Q15-1038,1,0.833963,"als and Vehicles. dog and the slot-value pair (part, tail), we retrieved those sentences having both the concepts dog and tail within a window w of words and associated them with the slot part. In order to increase the precision of the extraction, we used a limited window w = 5. The corpus extracted for our 50-concept semagram base contains 1,040,312 sentences. The next step consisted of extracting the textual patterns, i.e., for each sentence, the text contained between the concept term and the disambiguated semagram value, on a slot by slot basis. We developed an OIE system, based on (Delli Bovi et al., 2015), to extract phrase excerpts that unveil the relation between a concept and a semagram value. The proposed OIE system takes as input a sentence and two lemmatized arguments (the concept and the semagram value) and returns a phrase excerpt. First, it generates all possible lexical variants of the lemmatized words (the word itself, its plural form, its past simple form, its -ing form, and so forth) by using the Unimorph English Corpus6 and a set of handcrafted rules. Then, it processes the sentences through two steps: a parsing step and a merging step. In the first step, a Dependency Parser7 is"
2020.lrec-1.366,D19-1058,1,0.89177,"Missing"
2020.lrec-1.366,D11-1142,0,0.01211,"concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wikipedia and the Web (Banko et al., 2007; Wu and Weld, 2010; Carlson et al., 2010; Fader et al., 2011; Del Corro and Gemulla, 2013). Common-sense Knowledge Common-sense knowledge (CSK) may be described as a set of shared and general facts or views of a set of concepts. CSK displays some similarity with a semagram-type of knowledge in that it describes the kind of general information that humans use to describe, differentiate and reason about the conceptualizations they have in mind. ConceptNet (Speer and Havasi, 2012; Speer et al., 2016) is one of the largest resources of this kind, collecting and automatically integrating data starting from the original MIT Open Mind Common Sense project1 ."
2020.lrec-1.366,P12-1092,0,0.045141,"whose dimensions are qualitative features. For example, colors may be represented with three dimensions: hue, saturation, and brightness. While this model allows for direct similarity computation among instances, the knowledge it encodes does not define concepts explicitly, and dimensions usually represent perceptual mechanisms only. Other methods include topic models such as Latent Semantic Analysis (Dumais, 2004), Latent Dirichlet Allocation (Blei et al., 2003), and, more recently, embeddings of words (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016) and word senses (Huang et al., 2012; Iacobacci et al., 2015; Scarlini et al., 2020). However, the relations holding between vector representations are not typed, nor are they organized systematically. Semagrams To address the issues with existing approaches to concept representation, Moerdijk et al. (2008) proposed the concept of semantic gram, or semagram, and manually constructed a semantic resource in Dutch – the ANW (Algemeen Nederlands Woorden boek - General Dutch Dictionary) Dictionary – containing approximately 70,000 headwords, organized in 20 domains (animal, plants, etc.) with a total of 200 slots. However, most of th"
2020.lrec-1.366,P15-1010,1,0.925506,"qualitative features. For example, colors may be represented with three dimensions: hue, saturation, and brightness. While this model allows for direct similarity computation among instances, the knowledge it encodes does not define concepts explicitly, and dimensions usually represent perceptual mechanisms only. Other methods include topic models such as Latent Semantic Analysis (Dumais, 2004), Latent Dirichlet Allocation (Blei et al., 2003), and, more recently, embeddings of words (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016) and word senses (Huang et al., 2012; Iacobacci et al., 2015; Scarlini et al., 2020). However, the relations holding between vector representations are not typed, nor are they organized systematically. Semagrams To address the issues with existing approaches to concept representation, Moerdijk et al. (2008) proposed the concept of semantic gram, or semagram, and manually constructed a semantic resource in Dutch – the ANW (Algemeen Nederlands Woorden boek - General Dutch Dictionary) Dictionary – containing approximately 70,000 headwords, organized in 20 domains (animal, plants, etc.) with a total of 200 slots. However, most of the headwords lack any sem"
2020.lrec-1.366,Q14-1035,1,0.895976,"Missing"
2020.lrec-1.366,kingsbury-palmer-2002-treebank,0,0.204365,". Introduction The representation of knowledge is one of the great dreams of Artificial Intelligence. Acquiring and encoding knowledge is essential not only to improve historical NLP tasks such as Word Sense Disambiguation and Machine Translation, but also to enable numerous applications (Hovy et al., 2013) like intelligent personal assistants, Question Answering, Information Retrieval, etc. Lexical-semantic knowledge has typically been encoded on a large scale using word senses as meaning units, starting from WordNet (Miller, 1995) and then carrying on with VerbNet (Schuler, 2005), PropBank (Kingsbury and Palmer, 2002) and, more recently, VerbAtlas (Di Fabio et al., 2019). Large repositories of frames have been introduced with FrameNet (Baker et al., 1998) and its counterparts in other languages. However, frames are focused on situation-based representations with semantic roles and commonly involved objects, without defining the embodied ontological semantics of single concepts. Word senses, on the other hand, suffer from a lack of explicit common-sense semantic information. Indeed, the well-known fine granularity of word senses in WordNet (Palmer et al., 2007) is due to the lack of a meaning encoding syste"
2020.lrec-1.366,D19-1359,1,0.889688,"Missing"
2020.lrec-1.366,Q17-1017,0,0.0123909,"scertained on a contextual basis through prototypical sentence patterns. However, these theories and methods for building semantic resources remain linked to the lexical basis and are suitable for the manual effort of lexicographers. On the other hand, automatic approaches such as those of Almuhareb and Poesio (2004), Baroni et al. (2010), Navigli and Velardi (2010) and Boella and Di Caro (2013) used surface text patterns to automatically extract concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wikipedia and the Web (Banko et al., 2007; Wu and Weld, 2010; Carlson et al., 2010; Fader et al., 2011; Del Corro and Gemulla, 2013). Common-sense Knowledge Common-sense knowledge (CSK) may be described as a set of shared and general facts or views of a"
2020.lrec-1.366,W08-1903,0,0.43228,"d senses, on the other hand, suffer from a lack of explicit common-sense semantic information. Indeed, the well-known fine granularity of word senses in WordNet (Palmer et al., 2007) is due to the lack of a meaning encoding system capable of managing the representation of concepts in a flexible way. To address this issue, Pustejovsky introduced an innovative model (Pustejovsky, 1991) based on qualia roles to enable the creation of new meanings via semantic slot filling. However, the approach was limited by the number and type of these slots. An interesting and novel extension was presented by Moerdijk et al. (2008) with the ANW dictionary and the introduction of the concept of semagram. A semagram is a conceptual structure that describes a lexical entity on the basis of a wide range of characteristics, defined with a rich slot-filler structure. The semagrams provided in the ANW dictionary are, however, limited in coverage, often expressed with a fragmented set of semantic slots and written in Dutch. The aim of this paper is threefold: (i) to propose a novel model of semantic representation of concepts; (ii) to create a new semantic resource through manual annotation and semi-automatic techniques; and (i"
2020.lrec-1.366,Q14-1019,1,0.741112,"synset identifier associated with each filler of the semagram base, we built the distributional semantic profile for each slot retrieving the most frequent WordNet supersense4 for each of them. We built different semantic profiles for each of the ten categories in order to make them more precise. Since adjectives are not semantically well categorised in WordNet (they often have 2 A filtering step based on word2vec embeddings (Mikolov et al., 2013b) has been applied to remove those fillers having a similarity lower than 0.2 with the embedding of the lexicalized concept. 3 We utilized Babelfy (Moro et al., 2014) as Word Sense Disambiguation tool - http://babelfy.org. 4 https://wordnet.princeton.edu/man/ lexnames.5WN.html. 2994 Slot S1: accessory S2: S3: S4: S5: S6: Description All those objects that may have to do with X. The constraint is that there must be a physical contact and that the use of such object is strictly necessary for X. All actions that X can actively or consciously do. All the psychological features of X, including they attitude to they nature. All the body parts which are involved in interacting with X. All the features that refer to the color or texture of X. All the entries with"
2020.lrec-1.366,P10-1023,1,0.680952,"e giving lexicalized meaning to similarity values. 2991 2. Related Work Models of explicit lexical-semantic knowledge representation may be classified into five main categories, which we overview in this Section. Computational Lexicons Word senses are the basis of computational lexicons such as WordNet (Miller, 1995) and its counterparts in other languages (Bond and Foster, 2013). They usually provide human-readable concept definitions and contextualize meanings mainly in terms of the paradigmatic relations (hypernymy, meronymy) that hold between them. While larger resources such as BabelNet (Navigli and Ponzetto, 2010) also integrate other relations, these are typically unlabeled and are not systematized. A key unsolved issue with wordnets is the fine granularity of their inventories. Frames Frames (Fillmore, 1977) encode meanings through simple slot-filler structures. In other words, they represent knowledge as a set of attributes and values with the aim of defining situations or events. However, although semantic roles provide prototypical lexical units, these latter are not the primary focus of the frame model. Therefore, frames are not a viable option for encoding the prototypical meaning of concepts. C"
2020.lrec-1.366,P10-1134,1,0.541567,"aspects of meaning. CPA inherits some of its intents from the Generative Lexicon (Pustejovsky, 1991), the theory of preference semantics (Wilks, 1975), and others. Generally speaking, words are not taken in isolation and the meaning they are attributed is ascertained on a contextual basis through prototypical sentence patterns. However, these theories and methods for building semantic resources remain linked to the lexical basis and are suitable for the manual effort of lexicographers. On the other hand, automatic approaches such as those of Almuhareb and Poesio (2004), Baroni et al. (2010), Navigli and Velardi (2010) and Boella and Di Caro (2013) used surface text patterns to automatically extract concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wi"
2020.lrec-1.366,D14-1162,0,0.0796207,"Missing"
2020.lrec-1.366,J91-4003,0,0.591913,") and its counterparts in other languages. However, frames are focused on situation-based representations with semantic roles and commonly involved objects, without defining the embodied ontological semantics of single concepts. Word senses, on the other hand, suffer from a lack of explicit common-sense semantic information. Indeed, the well-known fine granularity of word senses in WordNet (Palmer et al., 2007) is due to the lack of a meaning encoding system capable of managing the representation of concepts in a flexible way. To address this issue, Pustejovsky introduced an innovative model (Pustejovsky, 1991) based on qualia roles to enable the creation of new meanings via semantic slot filling. However, the approach was limited by the number and type of these slots. An interesting and novel extension was presented by Moerdijk et al. (2008) with the ANW dictionary and the introduction of the concept of semagram. A semagram is a conceptual structure that describes a lexical entity on the basis of a wide range of characteristics, defined with a rich slot-filler structure. The semagrams provided in the ANW dictionary are, however, limited in coverage, often expressed with a fragmented set of semantic"
2020.lrec-1.366,P13-1056,0,0.448452,"a given concept via a slot-filler structure. In Table 1 we show an excerpt of semagram for the concepts of dog (right) and piano (left). As can be seen, the two semagrams share several slot types (but, obviously, not their values), while some slots are used only for one of the two concepts due to their belonging to different categories. The first contribution of this paper is a careful, systematic analysis, unification and extension of semagram slots from different resources. This analysis was initially performed on a development sample of 20 concepts over the 10 different categories used by Silberer et al. (2013) and then refined using another set of 30 concepts (3 per category). We chose these categories for their variety across conceptual types. The full sample is shown in Table 2 (development concepts are shown in bold in each category row). The 5 concepts in each category were chosen from among popular terms according to a diversification criterion specific to each category that would maximize the intra-category variability. For example, for the category Animals, we chose five concepts with different kinds of movement, for the category Clothes we selected those that are worn on different body part"
2020.lrec-1.366,speer-havasi-2012-representing,0,0.0407406,"raction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wikipedia and the Web (Banko et al., 2007; Wu and Weld, 2010; Carlson et al., 2010; Fader et al., 2011; Del Corro and Gemulla, 2013). Common-sense Knowledge Common-sense knowledge (CSK) may be described as a set of shared and general facts or views of a set of concepts. CSK displays some similarity with a semagram-type of knowledge in that it describes the kind of general information that humans use to describe, differentiate and reason about the conceptualizations they have in mind. ConceptNet (Speer and Havasi, 2012; Speer et al., 2016) is one of the largest resources of this kind, collecting and automatically integrating data starting from the original MIT Open Mind Common Sense project1 . However, terms in ConceptNet are not disambiguated, which leads to the confusion of lexical-semantic relations involving concepts denoted by ambiguous words (e.g. mouse as a device vs. a rodent). NELL (Carlson et al., 2010) matches entity pairs from seeds to extract relational phrases from a Web corpus, although without linking patterns to a slotfiller knowledge model, being mostly oriented to named entities rather th"
2020.lrec-1.366,W13-0215,0,0.153748,"nts from the Generative Lexicon (Pustejovsky, 1991), the theory of preference semantics (Wilks, 1975), and others. Generally speaking, words are not taken in isolation and the meaning they are attributed is ascertained on a contextual basis through prototypical sentence patterns. However, these theories and methods for building semantic resources remain linked to the lexical basis and are suitable for the manual effort of lexicographers. On the other hand, automatic approaches such as those of Almuhareb and Poesio (2004), Baroni et al. (2010), Navigli and Velardi (2010) and Boella and Di Caro (2013) used surface text patterns to automatically extract concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wikipedia and the Web (Banko et"
2020.lrec-1.366,P10-1013,0,0.0160193,"ce text patterns to automatically extract concept descriptions. However, these methods do not have a knowledge model as they extract semantics from statistically significant word properties. Mishra et al. (2017) extract domain-targeted knowledge, identifying clusters of similar-meaning predicates. However, the approach does not organize knowledge through semantic relations, and is not suitable for general knowledge building. Finally, Open Information Extraction (OIE) achieved notable results in extracting relational phrases from large corpora such as Wikipedia and the Web (Banko et al., 2007; Wu and Weld, 2010; Carlson et al., 2010; Fader et al., 2011; Del Corro and Gemulla, 2013). Common-sense Knowledge Common-sense knowledge (CSK) may be described as a set of shared and general facts or views of a set of concepts. CSK displays some similarity with a semagram-type of knowledge in that it describes the kind of general information that humans use to describe, differentiate and reason about the conceptualizations they have in mind. ConceptNet (Speer and Havasi, 2012; Speer et al., 2016) is one of the largest resources of this kind, collecting and automatically integrating data starting from the origi"
2020.lrec-1.723,R19-1015,1,0.797243,"ile remaining competitive with manuallycurated resources on English. Furthermore, we exploited automatic approaches for inducing the distribution of word senses (Pasini and Navigli, 2018) to build 5 additional datasets for English, each peculiar to a different semantic domain. These domain-specific datasets proved to lead a supervised model to a significant gain in performance when it comes to in-domain WSD evaluation. 2. Related Work Tackling the Word Sense Disambiguation problem from a supervised point of view has been the focus of several recent works (Luo et al., 2018; Kumar et al., 2019; Bevilacqua and Navigli, 2019). While on the one hand these models proved to attain state-of-the-art results on the English WSD task, on the other hand they still depended heavily on sense-annotated training corpora, and hence were limited to a restricted set of words, senses and languages. In fact, 5905 the line of research focused on new methodologies for automatically creating high-quality sense-annotated datasets is long-standing (Taghipour and Ng, 2015; Raganato et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020). These latter five appr"
2020.lrec-1.723,E17-2036,1,0.892829,"Missing"
2020.lrec-1.723,P17-2094,1,0.881193,"a supervised point of view has been the focus of several recent works (Luo et al., 2018; Kumar et al., 2019; Bevilacqua and Navigli, 2019). While on the one hand these models proved to attain state-of-the-art results on the English WSD task, on the other hand they still depended heavily on sense-annotated training corpora, and hence were limited to a restricted set of words, senses and languages. In fact, 5905 the line of research focused on new methodologies for automatically creating high-quality sense-annotated datasets is long-standing (Taghipour and Ng, 2015; Raganato et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020). These latter five approaches enabled supervised systems to step outside the boundaries of standard English WSD tasks and allowed them to scale on languages where manually-curated resources are not available. Train-O-Matic (Pasini and Navigli, 2017) is a languageindependent method for the creation of large-scale senseannotated corpora. By exploiting the information enclosed within a semantic network it is able to provide high-quality annotations of raw sentences, leading a supervised model to comp"
2020.lrec-1.723,D19-1058,1,0.89268,"Missing"
2020.lrec-1.723,S01-1001,0,0.131878,"sted its performance on 5 standard WSD benchmarks for English and for another four languages, i.e., Italian, Spanish, French and German. Reference WSD models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the ALL dataset, i.e., the concatenation of all the aforementioned test sets. For evaluating the corpora in the other four languages, instead, we tested the reference WSD model (BiLSTM) on the multilingual datasets of SemEval-2013 task 12 (Navigli et al., 2013) (Italian, Spanish, French and German) and SemEval-2015 task 13 (Moro and Navigli, 2015) (Italian and Spanish). Domain-Specific Evaluation To evaluate"
2020.lrec-1.723,H92-1045,0,0.800832,"1) v1 v2 rw + rw 2i i=1 w∈I vi where rw is the rank of the word w in the vector vi and I is the set of intersecting dimensions of v1 and v2 . Then, we assign to (l, C) the sense that maxil mizes the WO similarity with BC . Formally, being vs1 . . . vsn the NASARI vectors of the senses of l and 4 1 https://www.wikipedia.org/ 2 https://babelnet.org/ 3 http://nlp.uniroma1.it/nasari/ O NE S E C We now move on to describe O NE S E C, which, by relying on the previously introduced resources, automatically produces sense-annotated data in multiple languages. It extends the “One Sense per Discourse” (Gale et al., 1992) assumption to “One Sense per Wikipedia Category”, i.e., all the occurrences of a word in the same category share a common meaning, and leverages the information in a Wikipedia category to automatically tag the occurrences of the words therein. Given a target lexeme l, O NE S E C produces annotated examples for each of its senses by applying the following three steps. We use the notation lemma#pos. We preferred the Weighted Overlap over the most common Cosine Similarity as it has been proved to be more effective in capturing the distance between sparse vectors (Pilehvar et al., 2013). 5 5906 C"
2020.lrec-1.723,S10-1095,0,0.0741991,"Missing"
2020.lrec-1.723,N06-2015,0,0.334102,"Missing"
2020.lrec-1.723,P18-1031,0,0.0287267,"robability of a sentence being sampled. At the end of this procedure every occurrence of l in the selected sentences is tagged with the sense si . 5. Creating the Corpora We apply O NE S E C’s procedure to all the nominal words in the vocabulary of five major European languages, i.e., English, Italian, Spanish, French and German. More in detail, as regards English, we consider the whole set of nouns of WordNet. As for the other languages, we take into account the set of nouns in the WordNet part of BabelNet, i.e., 6 The perplexity is computed by means of the Neural Language Model presented in Howard and Ruder (2018). we take all the synsets in BabelNet that contain a WordNet sense and collect all the nouns in the target language therein. Moreover, we slightly modify O NE S E C to create domainspecific datasets for all English nominal lexemes by exploiting DaD (Pasini et al., 2018), i.e., a knowledge-based method for inducing in-domain sense distributions. DaD computes the distribution over BabelNet synsets by first approximating the probability of a BabelDomain (CamachoCollados and Navigli, 2017) being a topic covered in the corpus of raw sentences, and then leverages the connections in BabelNet to propa"
2020.lrec-1.723,P19-1568,0,0.0136503,"ingual WSD tasks, while remaining competitive with manuallycurated resources on English. Furthermore, we exploited automatic approaches for inducing the distribution of word senses (Pasini and Navigli, 2018) to build 5 additional datasets for English, each peculiar to a different semantic domain. These domain-specific datasets proved to lead a supervised model to a significant gain in performance when it comes to in-domain WSD evaluation. 2. Related Work Tackling the Word Sense Disambiguation problem from a supervised point of view has been the focus of several recent works (Luo et al., 2018; Kumar et al., 2019; Bevilacqua and Navigli, 2019). While on the one hand these models proved to attain state-of-the-art results on the English WSD task, on the other hand they still depended heavily on sense-annotated training corpora, and hence were limited to a restricted set of words, senses and languages. In fact, 5905 the line of research focused on new methodologies for automatically creating high-quality sense-annotated datasets is long-standing (Taghipour and Ng, 2015; Raganato et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli"
2020.lrec-1.723,D18-1170,0,0.0832139,"ults in all multilingual WSD tasks, while remaining competitive with manuallycurated resources on English. Furthermore, we exploited automatic approaches for inducing the distribution of word senses (Pasini and Navigli, 2018) to build 5 additional datasets for English, each peculiar to a different semantic domain. These domain-specific datasets proved to lead a supervised model to a significant gain in performance when it comes to in-domain WSD evaluation. 2. Related Work Tackling the Word Sense Disambiguation problem from a supervised point of view has been the focus of several recent works (Luo et al., 2018; Kumar et al., 2019; Bevilacqua and Navigli, 2019). While on the one hand these models proved to attain state-of-the-art results on the English WSD task, on the other hand they still depended heavily on sense-annotated training corpora, and hence were limited to a restricted set of words, senses and languages. In fact, 5905 the line of research focused on new methodologies for automatically creating high-quality sense-annotated datasets is long-standing (Taghipour and Ng, 2015; Raganato et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019"
2020.lrec-1.723,D19-1359,1,0.864185,"Missing"
2020.lrec-1.723,H93-1061,0,0.556815,"eval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL O NE S E C 73.2 68.2 63.5 66.5 70.8 69.0 TOM 70.5 67.4 59.8 65.5 68.6 67.3† OMSTI 74.1 67.2 62.3 62.8 63.1 66.4† SemCor 76.8 73.8 67.3 65.5 66.1 70.4 MFS 72.1 72.0 65.4 63.0 66.3 67.6 Table 4: Results of IMS trained on different corpora on the English WSD tasks. † marks statistical significance between O NE S E C and its competitors. tal, our corpora all together cover 40,043 distinct meanings with more than 15M annotations. As can be seen in Table 1, when compared to other manually- and semi-automatically annotated corpora, i.e., SemCor (Miller et al., 1993) and SemCor+OMSTI (Taghipour and Ng, 2015), O NE S E C covers more than double their lemmas and senses with one order of magnitude more annotations. Compared to SemCor+OMSTI, in fact, O NE S E C covers almost three times more nouns and two times more senses. Furthermore, OMSTI covers roughly the same number of nouns as SemCor with a slightly higher number of meanings. Train-O-Matic, instead, is the training corpus with the largest coverage in terms of both lemmas and senses. One reason why O NE S E C falls behind Train-O-Matic in terms of coverage is that it filters out all the categories that"
2020.lrec-1.723,S15-2049,1,0.840466,"models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the ALL dataset, i.e., the concatenation of all the aforementioned test sets. For evaluating the corpora in the other four languages, instead, we tested the reference WSD model (BiLSTM) on the multilingual datasets of SemEval-2013 task 12 (Navigli et al., 2013) (Italian, Spanish, French and German) and SemEval-2015 task 13 (Moro and Navigli, 2015) (Italian and Spanish). Domain-Specific Evaluation To evaluate the domainspecific corpora produced by O NE S E C we generated a specific training corpus (O NE S E Cdom ) for each of the following five domains: B"
2020.lrec-1.723,S13-2040,1,0.938801,"Missing"
2020.lrec-1.723,D17-1008,1,0.941528,"words comprised in WordNet (Miller et al., 1990), i.e., the largest and most-used electronic dictionary for English. Moreover, as we shift our focus towards lower-resourced languages, the need for semantically-annotated data becomes increasingly urgent. Indeed, SemCor, with some exceptions (Bentivogli and Pianta, 2005; Bond et al., 2012), lacks an adequate multilingual counterpart in most world languages and, hence, WSD models are limited when it comes to scaling over languages other than English. In this scenario, several automatic approaches for producing multilingual sense-annotated data (Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020) have tried to mitigate the aforementioned shortcomings. In fact, being able to create silver annotated data on a large scale is crucial to freeing WSD models from dependence on exclusively those words and languages that are comprised within manually-curated resources. In this paper we follow Scarlini et al. (2019) and apply their approach to all the nominal lexemes of 5 major European languages, i.e., English, Italian, Spanish, French and German, so as to ensure full coverage in terms of both words and senses. We release t"
2020.lrec-1.723,L18-1268,1,0.755039,"t (Miller et al., 1990), i.e., the largest and most-used electronic dictionary for English. Moreover, as we shift our focus towards lower-resourced languages, the need for semantically-annotated data becomes increasingly urgent. Indeed, SemCor, with some exceptions (Bentivogli and Pianta, 2005; Bond et al., 2012), lacks an adequate multilingual counterpart in most world languages and, hence, WSD models are limited when it comes to scaling over languages other than English. In this scenario, several automatic approaches for producing multilingual sense-annotated data (Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020) have tried to mitigate the aforementioned shortcomings. In fact, being able to create silver annotated data on a large scale is crucial to freeing WSD models from dependence on exclusively those words and languages that are comprised within manually-curated resources. In this paper we follow Scarlini et al. (2019) and apply their approach to all the nominal lexemes of 5 major European languages, i.e., English, Italian, Spanish, French and German, so as to ensure full coverage in terms of both words and senses. We release to the community train"
2020.lrec-1.723,passonneau-etal-2012-masc,0,0.0735144,"Missing"
2020.lrec-1.723,P13-1132,1,0.896404,"the Wikipedia categories C1 . . . Cn such that l appears at least t times across the sentences of their pages. Then, for each category C of the lemma l, we create a Bag of Words repl , i.e., a sparse vector in which dimensions resentation BC are words scored by their frequency in the sentences of C where l appears. Sense Assignment. We assign a sense to each lexemecategory pair (l, C) by leveraging the NASARI lexical vectors (see Section 3.). That is, as first step we compute the l similarity between BC and each NASARI vector vs associated with a sense s of l by means of the Weighted Overlap (Pilehvar et al., 2013)[WO] measure5 . The WO of two vectors v1 and v2 is computed as follows: −1 !  |I| X X 1 1  W O(v1 , v2 ) = ln(|I|+1) v1 v2 rw + rw 2i i=1 w∈I vi where rw is the rank of the word w in the vector vi and I is the set of intersecting dimensions of v1 and v2 . Then, we assign to (l, C) the sense that maxil mizes the WO similarity with BC . Formally, being vs1 . . . vsn the NASARI vectors of the senses of l and 4 1 https://www.wikipedia.org/ 2 https://babelnet.org/ 3 http://nlp.uniroma1.it/nasari/ O NE S E C We now move on to describe O NE S E C, which, by relying on the previously introduced re"
2020.lrec-1.723,S07-1016,0,0.0683966,"four languages, i.e., Italian, Spanish, French and German. Reference WSD models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the ALL dataset, i.e., the concatenation of all the aforementioned test sets. For evaluating the corpora in the other four languages, instead, we tested the reference WSD model (BiLSTM) on the multilingual datasets of SemEval-2013 task 12 (Navigli et al., 2013) (Italian, Spanish, French and German) and SemEval-2015 task 13 (Moro and Navigli, 2015) (Italian and Spanish). Domain-Specific Evaluation To evaluate the domainspecific corpora produced by O NE S E C we generated a specific"
2020.lrec-1.723,E17-1010,1,0.873858,"re, instead, peculiar to a specific domain. 7. Experimental Setup For assessing the quality of O NE S E C annotations we trained a supervised WSD model on our automaticallygenerated data and tested its performance on 5 standard WSD benchmarks for English and for another four languages, i.e., Italian, Spanish, French and German. Reference WSD models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the ALL dataset, i.e., the concatenation of all the aforementioned test sets. For evaluating the corpora in the other four languages, instead, we tested the reference WSD model (BiLSTM) on the multilingual datasets"
2020.lrec-1.723,D17-1120,1,0.922582,"Missing"
2020.lrec-1.723,P19-1069,1,0.890805,"0), i.e., the largest and most-used electronic dictionary for English. Moreover, as we shift our focus towards lower-resourced languages, the need for semantically-annotated data becomes increasingly urgent. Indeed, SemCor, with some exceptions (Bentivogli and Pianta, 2005; Bond et al., 2012), lacks an adequate multilingual counterpart in most world languages and, hence, WSD models are limited when it comes to scaling over languages other than English. In this scenario, several automatic approaches for producing multilingual sense-annotated data (Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020) have tried to mitigate the aforementioned shortcomings. In fact, being able to create silver annotated data on a large scale is crucial to freeing WSD models from dependence on exclusively those words and languages that are comprised within manually-curated resources. In this paper we follow Scarlini et al. (2019) and apply their approach to all the nominal lexemes of 5 major European languages, i.e., English, Italian, Spanish, French and German, so as to ensure full coverage in terms of both words and senses. We release to the community training corpora in 5 differ"
2020.lrec-1.723,W04-0811,0,0.0374678,"benchmarks for English and for another four languages, i.e., Italian, Spanish, French and German. Reference WSD models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the ALL dataset, i.e., the concatenation of all the aforementioned test sets. For evaluating the corpora in the other four languages, instead, we tested the reference WSD model (BiLSTM) on the multilingual datasets of SemEval-2013 task 12 (Navigli et al., 2013) (Italian, Spanish, French and German) and SemEval-2015 task 13 (Moro and Navigli, 2015) (Italian and Spanish). Domain-Specific Evaluation To evaluate the domainspecific corpora produced by"
2020.lrec-1.723,K15-1037,0,0.403639,"Tackling the Word Sense Disambiguation problem from a supervised point of view has been the focus of several recent works (Luo et al., 2018; Kumar et al., 2019; Bevilacqua and Navigli, 2019). While on the one hand these models proved to attain state-of-the-art results on the English WSD task, on the other hand they still depended heavily on sense-annotated training corpora, and hence were limited to a restricted set of words, senses and languages. In fact, 5905 the line of research focused on new methodologies for automatically creating high-quality sense-annotated datasets is long-standing (Taghipour and Ng, 2015; Raganato et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017; Pasini et al., 2018; Scarlini et al., 2019; Pasini and Navigli, 2020). These latter five approaches enabled supervised systems to step outside the boundaries of standard English WSD tasks and allowed them to scale on languages where manually-curated resources are not available. Train-O-Matic (Pasini and Navigli, 2017) is a languageindependent method for the creation of large-scale senseannotated corpora. By exploiting the information enclosed within a semantic network it is able to provide high-quality annotations of r"
2020.lrec-1.723,P10-4014,0,0.244743,"e created for 5 distinct semantic domains on English. As can be seen, O NE S E C can find as many examples in each domain as in the general-domain corpus (Table 2), meaning that it can also cover senses that are, instead, peculiar to a specific domain. 7. Experimental Setup For assessing the quality of O NE S E C annotations we trained a supervised WSD model on our automaticallygenerated data and tested its performance on 5 standard WSD benchmarks for English and for another four languages, i.e., Italian, Spanish, French and German. Reference WSD models As for English, we used It Makes Sense (Zhong and Ng, 2010, IMS), a support vector machine-system which builds a single model for each target word in the training set. For the other languages, instead, we employed the BiLSTM-based model proposed by Raganato et al. (2017b). Test Data We tested on all the nominal instances comprised in the 5 standard English datasets included in the framework of Raganato et al. (2017a), namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013) and SemEval-15 (Moro and Navigli, 2015). Furthermore, we report the results on the"
2021.eacl-main.286,2020.emnlp-main.585,1,0.949479,"SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) This work 76.3 – 77.1 78.0 77.7 78.0 79.4 80.8 80.4 75.6 – 76.4 75.4 75.2 77.1 77.4 79.0 77.8 68.1 – 70.3 71.9 72.5 71.0 74.5 75.2 76.2 75.1 78.7 76.2 77.0 76.1 77.3 79.7 80.7 81.8 77.0 – 77.2 77.6 80.4 83.2 81.7 81.8 83.3 78.0 80.4 78.7 79.9 – 80.6 81.4 82.9 82.9 64.0 – 65.6 64.8 – 68.3 68.5 69.4 70.3 80.7 – 81.1 79.2 – 80.5 83.0 82.9 83.4 84.5 – 84.7 86.4 – 83.5 87.9 87.6 85.5 75.4 – 76.4 76.7 77.0 77.9 79.0 80.1 80.2 Table 1: WSD results in F1 scores on Senseval-2 (SE2), Senseval-3 (SE3"
2021.eacl-main.286,2020.acl-main.255,1,0.710501,"e extended seamlessly to exploit structured knowledge from semantic networks to achieve stateof-the-art results in English all-words WSD. 1 Introduction Word Sense Disambiguation (WSD) is traditionally framed as the task of associating a word in context with its correct meaning from a finite set of possible choices (Navigli, 2009). Following this definition, recently proposed neural models were trained to maximize the probability of the most appropriate meaning while minimizing the probability of the other possible choices (Huang et al., 2019; Vial et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020). Although this training objective proved to be extremely effective and even led to Bevilacqua and Navigli (2020) reaching the estimated upper bound of interannotator agreement for WSD performance on the unified evaluation framework of Raganato et al. (2017b), adhering to it underplays a fundamental aspect of how human annotators disambiguate text. Indeed, past studies have observed that it is not uncommon for a word to have multiple appropriate meanings in a given context, meanings that can be used interchangeably under some circumstances because their boundaries are not clear cut (Tuggy, 199"
2021.eacl-main.286,2020.acl-main.95,0,0.749485,"biguate text, but it can also be extended seamlessly to exploit structured knowledge from semantic networks to achieve stateof-the-art results in English all-words WSD. 1 Introduction Word Sense Disambiguation (WSD) is traditionally framed as the task of associating a word in context with its correct meaning from a finite set of possible choices (Navigli, 2009). Following this definition, recently proposed neural models were trained to maximize the probability of the most appropriate meaning while minimizing the probability of the other possible choices (Huang et al., 2019; Vial et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020). Although this training objective proved to be extremely effective and even led to Bevilacqua and Navigli (2020) reaching the estimated upper bound of interannotator agreement for WSD performance on the unified evaluation framework of Raganato et al. (2017b), adhering to it underplays a fundamental aspect of how human annotators disambiguate text. Indeed, past studies have observed that it is not uncommon for a word to have multiple appropriate meanings in a given context, meanings that can be used interchangeably under some circumstances because their boundarie"
2021.eacl-main.286,2020.coling-main.291,1,0.938977,"the work of: 3271 SE2 SE3 SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) This work 76.3 – 77.1 78.0 77.7 78.0 79.4 80.8 80.4 75.6 – 76.4 75.4 75.2 77.1 77.4 79.0 77.8 68.1 – 70.3 71.9 72.5 71.0 74.5 75.2 76.2 75.1 78.7 76.2 77.0 76.1 77.3 79.7 80.7 81.8 77.0 – 77.2 77.6 80.4 83.2 81.7 81.8 83.3 78.0 80.4 78.7 79.9 – 80.6 81.4 82.9 82.9 64.0 – 65.6 64.8 – 68.3 68.5 69.4 70.3 80.7 – 81.1 79.2 – 80.5 83.0 82.9 83.4 84.5 – 84.7 86.4 – 83.5 87.9 87.6 85.5 75.4 – 76.4 76.7 77.0 77.9 79.0 80.1 80.2 Table 1: WSD results in F1 scores on Senseval"
2021.eacl-main.286,N19-1423,0,0.0210257,"evaluation includes five gold standard datasets, namely, Senseval-2, Senseval-3, SemEval-2007, SemEval2013, and SemEval-2015. Following standard practice we use the smallest gold standard as our development set, SemEval-2007, and the remaining ones as test sets. We distinguish between two settings: closed and open. In the former setting, we include systems that only use SemCor (Miller et al., 1994) as the training corpus, while in the latter we also include those systems that use WordNet glosses and examples and/or Wikipedia. Hyperparameters. We use the pretrained version of BERT-large-cased (Devlin et al., 2019) available on HuggingFace’s Transformers library (Wolf et al., 2020) to build our contextualized embeddings (Section 2). BERT is left frozen, that is, its parameters are not updated during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We avoid hyperparameter tuning and opt for values that are close to the ones reported in the literature so as to have a fairer comparison. ow = Wo hw + bo b−i w where is the hidden state of the i-th layer of BERT from the topmost one, BatchNorm(·) is the Comparison systems. In order to have a compreh"
2021.eacl-main.286,D09-1046,0,0.68085,"Missing"
2021.eacl-main.286,J13-3003,0,0.0683009,"Missing"
2021.eacl-main.286,D19-1533,0,0.474662,"during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We avoid hyperparameter tuning and opt for values that are close to the ones reported in the literature so as to have a fairer comparison. ow = Wo hw + bo b−i w where is the hidden state of the i-th layer of BERT from the topmost one, BatchNorm(·) is the Comparison systems. In order to have a comprehensive comparison with the current state of the art in WSD, we include the work of: 3271 SE2 SE3 SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua"
2021.eacl-main.286,P19-1568,0,0.290096,"negligible cost in terms of training time and number of parameters, while at the same time attaining state-of-the-art results in English all-words WSD. 2 Method Single-label vs multi-label. WSD is the task of selecting the best-fitting sense s among the possible senses Sw of a target word w in a given context c = hw1 , w2 , . . . , wn i, where Sw is a subset of a predefined sense inventory S. Abstracting away from the intricacies of any particular supervised model for WSD, the output of a WSD system provides a probability yi for each sense si ∈ Sw . Recently proposed machine learning models – Kumar et al., 2019; Barba et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020, inter alia – are trained to maximize the probability of the single most appropriate sense sˆ by minimizing the cross-entropy loss LCE : LCE (w, sˆ) = − log(ysˆ) (1) We observe that this loss function is only suitable for single-label classification problems. In the case of WSD, this is equivalent to assuming that there is just a single appropriate sense sˆ ∈ Sw for the target word w in the given context c, that is, sˆ is clearly dissimilar from any other sense in Sw . Indeed, minimizing the cross-entropy loss in"
2021.eacl-main.286,P19-1569,0,0.3145,"h the current state of the art in WSD, we include the work of: 3271 SE2 SE3 SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) This work 76.3 – 77.1 78.0 77.7 78.0 79.4 80.8 80.4 75.6 – 76.4 75.4 75.2 77.1 77.4 79.0 77.8 68.1 – 70.3 71.9 72.5 71.0 74.5 75.2 76.2 75.1 78.7 76.2 77.0 76.1 77.3 79.7 80.7 81.8 77.0 – 77.2 77.6 80.4 83.2 81.7 81.8 83.3 78.0 80.4 78.7 79.9 – 80.6 81.4 82.9 82.9 64.0 – 65.6 64.8 – 68.3 68.5 69.4 70.3 80.7 – 81.1 79.2 – 80.5 83.0 82.9 83.4 84.5 – 84.7 86.4 – 83.5 87.9 87.6 85.5 75.4 – 76.4 76.7 77.0 77.9 79.0 80.1 8"
2021.eacl-main.286,H94-1046,0,0.796736,", we choose only the meaning with highest probability for our multi-label models. Datasets. We evaluate the models on the Unified Evaluation Framework for English all-words WSD proposed by Raganato et al. (2017b). This evaluation includes five gold standard datasets, namely, Senseval-2, Senseval-3, SemEval-2007, SemEval2013, and SemEval-2015. Following standard practice we use the smallest gold standard as our development set, SemEval-2007, and the remaining ones as test sets. We distinguish between two settings: closed and open. In the former setting, we include systems that only use SemCor (Miller et al., 1994) as the training corpus, while in the latter we also include those systems that use WordNet glosses and examples and/or Wikipedia. Hyperparameters. We use the pretrained version of BERT-large-cased (Devlin et al., 2019) available on HuggingFace’s Transformers library (Wolf et al., 2020) to build our contextualized embeddings (Section 2). BERT is left frozen, that is, its parameters are not updated during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We avoid hyperparameter tuning and opt for values that are close to the ones repor"
2021.eacl-main.286,S15-2049,1,0.940651,"Missing"
2021.eacl-main.286,D19-1355,0,0.086292,"bs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) This work 76.3 – 77.1 78.0 77.7 78.0 79.4 80.8 80.4 75.6 – 76.4 75.4 75.2 77.1 77.4 79.0 77.8 68.1 – 70.3 71.9 72.5 71.0 74.5 75.2 76.2 75.1 78.7 76.2 77.0 76.1 77.3 79.7 80.7 81.8 77.0 – 77.2 77.6 80.4 83.2 81.7 81.8 83.3 78.0 80.4 78.7 79.9 – 80.6 81.4 82.9 82.9 64.0 – 65.6 64.8 – 68.3 68.5 69.4 70.3 80.7 – 81.1 79.2 – 80.5 83.0 82.9 83.4 84.5 – 84.7 86.4 – 83.5 87.9 87.6 85.5 75.4 – 76.4 76.7 77.0 77.9 79.0 80.1 80.2 Table 1: WSD results in F1 scores on Senseval-2 (SE2), Senseval-3 (SE3), SemEval-2007 (SE0"
2021.eacl-main.286,S13-2040,1,0.923971,"Missing"
2021.eacl-main.286,2020.emnlp-main.285,1,0.891053,"art in WSD, we include the work of: 3271 SE2 SE3 SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) This work 76.3 – 77.1 78.0 77.7 78.0 79.4 80.8 80.4 75.6 – 76.4 75.4 75.2 77.1 77.4 79.0 77.8 68.1 – 70.3 71.9 72.5 71.0 74.5 75.2 76.2 75.1 78.7 76.2 77.0 76.1 77.3 79.7 80.7 81.8 77.0 – 77.2 77.6 80.4 83.2 81.7 81.8 83.3 78.0 80.4 78.7 79.9 – 80.6 81.4 82.9 82.9 64.0 – 65.6 64.8 – 68.3 68.5 69.4 70.3 80.7 – 81.1 79.2 – 80.5 83.0 82.9 83.4 84.5 – 84.7 86.4 – 83.5 87.9 87.6 85.5 75.4 – 76.4 76.7 77.0 77.9 79.0 80.1 80.2 Table 1: WSD resul"
2021.eacl-main.286,D19-1005,0,0.151301,"el is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We avoid hyperparameter tuning and opt for values that are close to the ones reported in the literature so as to have a fairer comparison. ow = Wo hw + bo b−i w where is the hidden state of the i-th layer of BERT from the topmost one, BatchNorm(·) is the Comparison systems. In order to have a comprehensive comparison with the current state of the art in WSD, we include the work of: 3271 SE2 SE3 SE07 SE13 SE15 Nouns Verbs Adj Adv ALL SemCor only Raganato et al. (2017a) BERTLarge Hadiwinoto et al. (2019) Peters et al. (2019) Vial et al. (2019) Vial et al. (2019) - Ensemble This work 72.0 76.3 75.5 – – 77.5 78.4 69.1 73.2 73.6 – – 77.4 77.8 64.8 66.2 68.1 – – 69.5 72.2 66.9 71.7 71.1 – – 76.0 76.7 71.5 74.1 76.2 – – 78.3 78.2 71.5 – – – – 79.6 80.1 57.5 – – – – 65.9 67.0 75.0 – – – – 79.5 80.5 83.8 – – – – 85.5 86.2 69.9 73.5 73.7 75.1 75.6 76.7 77.6 SemCor + definitions / examples Concatenation of ALL datasets Loureiro and Jorge (2019) Scarlini et al. (2020a) Conia and Navigli (2020) Bevilacqua et al. (2020) Huang et al. (2019) Scarlini et al. (2020b) Blevins and Zettlemoyer (2020) Bevilacqua and Navigli (2020) T"
2021.eacl-main.286,S07-1016,0,0.202155,"Missing"
2021.eacl-main.286,D17-1120,1,0.91883,"rrect meaning from a finite set of possible choices (Navigli, 2009). Following this definition, recently proposed neural models were trained to maximize the probability of the most appropriate meaning while minimizing the probability of the other possible choices (Huang et al., 2019; Vial et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020). Although this training objective proved to be extremely effective and even led to Bevilacqua and Navigli (2020) reaching the estimated upper bound of interannotator agreement for WSD performance on the unified evaluation framework of Raganato et al. (2017b), adhering to it underplays a fundamental aspect of how human annotators disambiguate text. Indeed, past studies have observed that it is not uncommon for a word to have multiple appropriate meanings in a given context, meanings that can be used interchangeably under some circumstances because their boundaries are not clear cut (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk and McCarthy, 2009). This is especially evident if the underlying sense inventory is fine-grained, as the complexity, and therefore performance, of WSD is tightly coupled to sense granularity (Lacerra et al., 2020). The"
2021.eacl-main.286,2020.acl-demos.6,1,0.834557,"Missing"
2021.eacl-main.286,W04-0811,0,0.29569,"Missing"
2021.eacl-main.286,2019.gwc-1.14,0,0.521806,"is way, not only would the model learn that canines, foxes and arctic foxes are closely related, but it would also learn that canines and arctic foxes may have the ability to jump, and this could act as a data augmentation strategy especially for those senses that do not appear in the training set. There is a growing interest in injecting relational information from knowledge bases into neural networks but, so far, recent attempts have required purposely-designed strategies or layers. Among others, Kumar et al. (2019) aid their model with a gloss encoder that uses the WordNet graph structure; Vial et al. (2019) adopt a preprocessing strategy aimed at clustering related senses to decrease the number of output classes; Bevilacqua and Navigli (2020) introduce a logit aggregation layer that takes into account the neighboring meanings in the WordNet graph. In contrast, our multi-labeling approach to WSD can be seamlessly extended to integrate relational knowledge from semantic networks such as WordNet without any increase in architectural complexity, training time, and number of trainable param3270 eters. We simply relax the definition of the set of possible senses Sw for a word w to include all the sens"
2021.eacl-main.286,E17-1010,1,0.906598,"rrect meaning from a finite set of possible choices (Navigli, 2009). Following this definition, recently proposed neural models were trained to maximize the probability of the most appropriate meaning while minimizing the probability of the other possible choices (Huang et al., 2019; Vial et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020). Although this training objective proved to be extremely effective and even led to Bevilacqua and Navigli (2020) reaching the estimated upper bound of interannotator agreement for WSD performance on the unified evaluation framework of Raganato et al. (2017b), adhering to it underplays a fundamental aspect of how human annotators disambiguate text. Indeed, past studies have observed that it is not uncommon for a word to have multiple appropriate meanings in a given context, meanings that can be used interchangeably under some circumstances because their boundaries are not clear cut (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk and McCarthy, 2009). This is especially evident if the underlying sense inventory is fine-grained, as the complexity, and therefore performance, of WSD is tightly coupled to sense granularity (Lacerra et al., 2020). The"
2021.emnlp-demo.16,2020.emnlp-main.195,1,0.858536,"Missing"
2021.emnlp-demo.16,2020.lrec-1.86,0,0.103871,"neration). The Web interface machines, make both AMR parsing and generation has been developed to be easily used by the very rewarding problems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for such applications are http://nlp.uniroma1.it/spring and, in addition, obtained automatically through an AMR parser, we release extra model checkpoints to be used 1 the benefits of AMR integration are h"
2021.emnlp-demo.16,2020.dmr-1.7,0,0.101157,"neration). The Web interface machines, make both AMR parsing and generation has been developed to be easily used by the very rewarding problems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for such applications are http://nlp.uniroma1.it/spring and, in addition, obtained automatically through an AMR parser, we release extra model checkpoints to be used 1 the benefits of AMR integration are h"
2021.emnlp-demo.16,W13-2322,0,0.127769,"Missing"
2021.emnlp-demo.16,2021.emnlp-main.112,1,0.653471,"ting modambition to be comprehensive, AMR graphs are els make use of cumbersome, data-specific techcomplex objects that require a parser – an auto- niques and components which not only limit the matic algorithm that transduces a natural language out-of-distribution generalizability, but also make utterance into an AMR graph – to subsume multi- it difficult to integrate such models in the pipeline ple traditional Natural Language Processing tasks: of downstream applications. In our recent paper, Word Sense Disambiguation (Bevilacqua et al., SPRING (Bevilacqua et al., 2021a), we proposed 2021b; Barba et al., 2021), Semantic Role Labeling a solution through a simple, end-to-end approach (Màrquez et al., 2008; Conia et al., 2021; Blloshmi with no heavy inbuilt data processing assumptions. et al., 2021), Named Entity Recognition (Yadav Our model achieved unprecedented performance in and Bethard, 2018), Entity Linking (Ling et al., AMR parsing and generation, both in- and out-of2015; Tedeschi et al., 2021), and Coreference Res- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to"
2021.emnlp-demo.16,2020.acl-main.119,0,0.018583,"tation (Banarescu et al., In recent years, AMR parsing and generation 2013, AMR) is a popular formalism for repre- models have become more reliable than they used senting the semantics of natural language in a to be, thanks to both the availability of pretrained readable and hierarchical way. AMR pairs En- language models (Devlin et al., 2019; Lewis et al., glish sentences with graph-based logical formu- 2020) and the continuous improvements in the las which are easily accessible by both humans AMR-specific model architectures (Zhou et al., and machines, while abstracting away from many 2020; Cai and Lam, 2020; Fernandez Astudillo syntactic variations. Because of the formalism’s et al., 2020). However, most of the existing modambition to be comprehensive, AMR graphs are els make use of cumbersome, data-specific techcomplex objects that require a parser – an auto- niques and components which not only limit the matic algorithm that transduces a natural language out-of-distribution generalizability, but also make utterance into an AMR graph – to subsume multi- it difficult to integrate such models in the pipeline ple traditional Natural Language Processing tasks: of downstream applications. In our rec"
2021.emnlp-demo.16,P13-2131,0,0.0311361,"s not provide extra functionalities, RESTful APIs, or any interaction with the users. Similarly, Damonte et al. (2017) and Damonte and Cohen (2019) do not provide other functionalities in their demos beside parsing (Damonte et al., 2017, AMREager)6 and generation (Damonte and Cohen, 2019, AMRGen)7 . However, SPRING outperforms the aforementioned systems by more than 20 points in both, Smatch for AMR parsing and BLEU for AMR generation. In addition, through SPRING Online Services we provide a highly-interactive Web interface, RESTful APIs, and the feedback mechanism. Results. We report Smatch (Cai and Knight, 2013) and BLEU (Papineni et al., 2002) scores for AMR parsing and generation, respectively. In Table 1 we summarize the performances of recent systems in the literature on the AMR 3.0 parsing and generation tasks. In parsing, SPRING achieves the highest results across the board. In fact, we note that Zhou et al. (2021) was published after Bevilacqua et al. (2021a), yet SPRING remains the best-performing parser in the literature to date. In generation, instead, SPRING attains considerably higher results than Zhang et al. (2020) and T5 Fine-Tune (Ribeiro et al., 2021) models. In fact, while the latte"
2021.emnlp-demo.16,2021.naacl-main.31,1,0.694413,"hat require a parser – an auto- niques and components which not only limit the matic algorithm that transduces a natural language out-of-distribution generalizability, but also make utterance into an AMR graph – to subsume multi- it difficult to integrate such models in the pipeline ple traditional Natural Language Processing tasks: of downstream applications. In our recent paper, Word Sense Disambiguation (Bevilacqua et al., SPRING (Bevilacqua et al., 2021a), we proposed 2021b; Barba et al., 2021), Semantic Role Labeling a solution through a simple, end-to-end approach (Màrquez et al., 2008; Conia et al., 2021; Blloshmi with no heavy inbuilt data processing assumptions. et al., 2021), Named Entity Recognition (Yadav Our model achieved unprecedented performance in and Bethard, 2018), Entity Linking (Ling et al., AMR parsing and generation, both in- and out-of2015; Tedeschi et al., 2021), and Coreference Res- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to AMR applica134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demo"
2021.emnlp-demo.16,N19-1366,0,0.0252162,"on the development set of AMR 3.0, i.e, SPRINGbi trained on Bio+AMR 3.0 for both parsing and generation jointly. This model allows for the achievement of all the goals we set at the beginning of this Section: performance, generalizability and efficiency. Furthermore, we release the additional model checkpoints to be used with the original SPRING Python code, available at https: //github.com/SapienzaNLP/spring. 5 Related Work With a view to demonstrating the progress made in AMR, over the years different Web services for state-of-the-art AMR systems (Konstas et al., 2017; Damonte et al., 2017; Damonte and Cohen, 2019) have been developed. Similarly to SPRING, Konstas et al. (2017), proposed an encoder-decoder system to perform both parsing and generation by relying on data augmentation techniques. This system is associated with a demo5 to only parse into or generate from AMR and does not provide extra functionalities, RESTful APIs, or any interaction with the users. Similarly, Damonte et al. (2017) and Damonte and Cohen (2019) do not provide other functionalities in their demos beside parsing (Damonte et al., 2017, AMREager)6 and generation (Damonte and Cohen, 2019, AMRGen)7 . However, SPRING outperforms t"
2021.emnlp-demo.16,N19-1423,0,0.00778836,", we release extra model checkpoints to be used 1 the benefits of AMR integration are highly correwith the original SPRING Python code. lated with the performance of the underlying parser 1 Introduction across various data distributions and domains. Abstract Meaning Representation (Banarescu et al., In recent years, AMR parsing and generation 2013, AMR) is a popular formalism for repre- models have become more reliable than they used senting the semantics of natural language in a to be, thanks to both the availability of pretrained readable and hierarchical way. AMR pairs En- language models (Devlin et al., 2019; Lewis et al., glish sentences with graph-based logical formu- 2020) and the continuous improvements in the las which are easily accessible by both humans AMR-specific model architectures (Zhou et al., and machines, while abstracting away from many 2020; Cai and Lam, 2020; Fernandez Astudillo syntactic variations. Because of the formalism’s et al., 2020). However, most of the existing modambition to be comprehensive, AMR graphs are els make use of cumbersome, data-specific techcomplex objects that require a parser – an auto- niques and components which not only limit the matic algorithm that"
2021.emnlp-demo.16,2020.acl-main.703,0,0.0151424,"decoding and response time of the SPRING Online Services. The SPRING model is based on the Transformer architecture (Vaswani et al., 2017), a sequence-tosequence neural network that, briefly, i) uses attention instead of recurrence to encode sequences, ii) is made up of an encoder module that embeds σ, and a decoder that, based on both the encoder output and τ , produces the final distribution output. Key to the high performances of SPRING is the fact that its parameters are not randomly initialized, but, instead, are adopted from those of a large pretrained encoder-decoder model, i.e., BART (Lewis et al., 2020). Owing to this, SPRING can exploit the extensive knowledge BART encompasses, gained through optimization on large amounts of raw text with an unsupervised denoising objective. Here we describe the functionalities of the Web interface (Section 3.1) and those of the RESTful APIs (Section 3.2). We further provide the architectural details and libraries used in Appendix A. 2.3 3.1 Linearization As we have mentioned, the bare SPRING model can translate from and into linearized AMR graphs. PENMAN, i.e., the format that is used to distribute the AMR meaning bank, is an example of a linearization. In"
2021.emnlp-demo.16,2020.findings-emnlp.89,0,0.0628017,"Missing"
2021.emnlp-demo.16,C18-1101,0,0.0139323,"ion system, SPRING (Symmetric PaRsapplications as an interface between human and Ing aNd Generation). The Web interface machines, make both AMR parsing and generation has been developed to be easily used by the very rewarding problems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for such applications are http://nlp.uniroma1.it/spring and, in addition, obtained automatically through an AMR p"
2021.emnlp-demo.16,2020.acl-demos.35,0,0.0393675,"s- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to AMR applica134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 134–142 August 1–6, 2021. ©2021 Association for Computational Linguistics tion research, we present SPRING Online Services which include: • a Web interface to easily produce and visualize an AMR graph for a given sentence and, vice versa, a sentence for a given AMR graph in PENMAN (Goodman, 2020) notation. • RESTful APIs to programmatically request AMR parsing and generation services. • a bidirectional SPRING model also trained on Bio-AMR, resulting in much stronger performances for biomedical applications. • a feedback mechanism which allows users to submit modifications to the system’s outputs – aided by the visualization – which we collect to enable future enhancements of AMR systems using active learning (Settles, 2009). 2 SPRING In this Section we revisit the details of SPRING as in Bevilacqua et al. (2021a) along with the alterations we employ for this demonstration. 2.1 Task Fo"
2021.emnlp-demo.16,D18-1086,0,0.0247523,"t AMR parsing and generation system, SPRING (Symmetric PaRsapplications as an interface between human and Ing aNd Generation). The Web interface machines, make both AMR parsing and generation has been developed to be easily used by the very rewarding problems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for such applications are http://nlp.uniroma1.it/spring and, in addition, obtained automatica"
2021.emnlp-demo.16,2020.coling-main.331,0,0.0207265,"eam applications. In our recent paper, Word Sense Disambiguation (Bevilacqua et al., SPRING (Bevilacqua et al., 2021a), we proposed 2021b; Barba et al., 2021), Semantic Role Labeling a solution through a simple, end-to-end approach (Màrquez et al., 2008; Conia et al., 2021; Blloshmi with no heavy inbuilt data processing assumptions. et al., 2021), Named Entity Recognition (Yadav Our model achieved unprecedented performance in and Bethard, 2018), Entity Linking (Ling et al., AMR parsing and generation, both in- and out-of2015; Tedeschi et al., 2021), and Coreference Res- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to AMR applica134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 134–142 August 1–6, 2021. ©2021 Association for Computational Linguistics tion research, we present SPRING Online Services which include: • a Web interface to easily produce and visualize an AMR graph for a given sentence and, vice versa, a sentence for a given AMR graph in PENMAN (Goodman, 2020) notation. • RESTful APIs to progra"
2021.emnlp-demo.16,Q15-1023,0,0.0687674,"Missing"
2021.emnlp-demo.16,2021.emnlp-main.714,0,0.0333611,"Missing"
2021.emnlp-demo.16,J08-2001,0,0.0190751,"techcomplex objects that require a parser – an auto- niques and components which not only limit the matic algorithm that transduces a natural language out-of-distribution generalizability, but also make utterance into an AMR graph – to subsume multi- it difficult to integrate such models in the pipeline ple traditional Natural Language Processing tasks: of downstream applications. In our recent paper, Word Sense Disambiguation (Bevilacqua et al., SPRING (Bevilacqua et al., 2021a), we proposed 2021b; Barba et al., 2021), Semantic Role Labeling a solution through a simple, end-to-end approach (Màrquez et al., 2008; Conia et al., 2021; Blloshmi with no heavy inbuilt data processing assumptions. et al., 2021), Named Entity Recognition (Yadav Our model achieved unprecedented performance in and Bethard, 2018), Entity Linking (Ling et al., AMR parsing and generation, both in- and out-of2015; Tedeschi et al., 2021), and Coreference Res- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to AMR applica134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro"
2021.emnlp-demo.16,S17-2090,0,0.0174916,"o, we examine different variants of SPRING to ensure: i) high performance, ii) high generalizability across domains, and iii) efficient and light SPRING Online Services. Datasets. To deal with i) and ii), we perform experiments with the AMR 3.0 (LDC2020T023 ) benchmark – currently the largest AMR-annotated corpus which includes and corrects both of its previous inferior-sized versions, i.e., AMR 2.0 and AMR 1.0. In addition to this, motivated by AMRbased approaches in biomedical applications (Rao et al., 2017; Bonial et al., 2020b), we jointly train and evaluate SPRING in the Bio-AMR4 corpus (May and Priyadarshi, 2017) as well. Systems. While Bevilacqua et al. (2021a) train one specular model for each of the AMR tasks (henceforth SPRINGuni , denoting unidirectional), to satisfy the point iii) above, we train a version of SPRING that handles both AMR parsing and generation with the same model (henceforth SPRINGbi , denoting bidirectional). This allows us to load into memory only one model to perform both tasks, thus decreasing the potential overload of the server where the demo resides, as well as enabling lower memory footprint for users employing SPRING with our Python code. To train SPRING variants, we em"
2021.emnlp-demo.16,P10-1023,1,0.601518,"graph in PENMAN format. This box is editable to enable user feedback (see Section 3.1.3). When generating from an AMR graph, the Result box shows the generated sentence which can also be modified by the user and submitted to the feedback system. lighted corner) and the concept they represent. The variable makes it easy to locate the node in the PENMAN box on the left Panel. Futhermore, both predicate and wiki nodes are associated with an onhover/onclick tooltip box that further defines them. The tooltip associated with the wiki node contains information taken from the corresponding BabelNet2 (Navigli and Ponzetto, 2010; Navigli et al., 2021) concept, displaying a short entity description and image (when applicable), also redirecting the user to the corresponding BabelNet page when clicking on it. This choice is motivated by the fact that BabelNet concepts function as a hub of information beyond that of Wikipedia, which paves the way for future integration of other resources in AMR. The tooltip of the predicate node, instead, provides details on the predicate definition and arguments taken from the PropBank framesets (Palmer et al., 2005). In addition, we display an example sentence containing the predicate"
2021.emnlp-demo.16,J05-1004,0,0.0168766,"ode contains information taken from the corresponding BabelNet2 (Navigli and Ponzetto, 2010; Navigli et al., 2021) concept, displaying a short entity description and image (when applicable), also redirecting the user to the corresponding BabelNet page when clicking on it. This choice is motivated by the fact that BabelNet concepts function as a hub of information beyond that of Wikipedia, which paves the way for future integration of other resources in AMR. The tooltip of the predicate node, instead, provides details on the predicate definition and arguments taken from the PropBank framesets (Palmer et al., 2005). In addition, we display an example sentence containing the predicate in the specified sense. The user is redirected to the PropBank predicate page when clicking the tooltip. We mean the extra information shown by the tooltip component to be useful for the user to identify potential parsing mistakes in the output of the system, and ideally to use the provided feedback mechanism to suggest corrections. C. AMR view panel. This is a key component of the Results View, which visualizes an AMR as a hierarchical graph with labeled nodes and labeled edges. We devise a custom node and edge layout mean"
2021.emnlp-demo.16,P02-1040,0,0.109951,"ies, RESTful APIs, or any interaction with the users. Similarly, Damonte et al. (2017) and Damonte and Cohen (2019) do not provide other functionalities in their demos beside parsing (Damonte et al., 2017, AMREager)6 and generation (Damonte and Cohen, 2019, AMRGen)7 . However, SPRING outperforms the aforementioned systems by more than 20 points in both, Smatch for AMR parsing and BLEU for AMR generation. In addition, through SPRING Online Services we provide a highly-interactive Web interface, RESTful APIs, and the feedback mechanism. Results. We report Smatch (Cai and Knight, 2013) and BLEU (Papineni et al., 2002) scores for AMR parsing and generation, respectively. In Table 1 we summarize the performances of recent systems in the literature on the AMR 3.0 parsing and generation tasks. In parsing, SPRING achieves the highest results across the board. In fact, we note that Zhou et al. (2021) was published after Bevilacqua et al. (2021a), yet SPRING remains the best-performing parser in the literature to date. In generation, instead, SPRING attains considerably higher results than Zhang et al. (2020) and T5 Fine-Tune (Ribeiro et al., 2021) models. In fact, while the latter has a comparable architecture t"
2021.emnlp-demo.16,2021.naacl-main.30,1,0.815956,"Missing"
2021.emnlp-demo.16,C18-1182,0,0.0368931,"Missing"
2021.emnlp-demo.16,2020.emnlp-main.169,0,0.0784986,"Missing"
2021.emnlp-demo.16,2021.naacl-main.443,0,0.205404,"ry only one model to perform both tasks, thus decreasing the potential overload of the server where the demo resides, as well as enabling lower memory footprint for users employing SPRING with our Python code. To train SPRING variants, we employ the same hyperparameters as in Bevilacqua et al. (2021a). In addition, we summarize the state-of-the-art systems on AMR 3.0. The RESTful APIs we provide can be used effectively to query the SPRING services programmatically. Our APIs are simple and, differently 138 3 4 catalog.ldc.upenn.edu/LDC2020T02 amr.isi.edu/download.html Parsing Lyu et al. (2020) Zhou et al. (2021) 75.8 81.2 SPRING (Bevilacqua et al., 2021a) 83.0 Generation Zhang et al. (2020) T5 Fine-Tune (Ribeiro et al., 2021) STRUCTADAPT-RGCN (Ribeiro et al., 2021) 34.3 41.6 48.0 SPRING (Bevilacqua et al., 2021a) 44.9 Table 1: Comparison with literature on AMR 3.0. Generation Parsing AMR 3.0 Bio-AMR Train dataset Dev Test Dev Test SPRINGuni SPRINGbi AMR 3.0 AMR 3.0 83.9 83.6 82.6 82.3 60.6 60.5 60.6 59.2 SPRINGuni SPRINGbi Bio+AMR 3.0 Bio+AMR 3.0 83.9 84.1 82.5 82.7 80.0 79.5 80.1 80.2 SPRINGuni SPRINGbi AMR 3.0 AMR 3.0 45.0 43.9 44.9 44.5 22.9 21.1 19.4 17.1 SPRINGuni SPRINGbi Bio+AMR 3.0 Bio+AMR 3."
2021.emnlp-demo.16,2020.acl-main.397,0,0.0690569,"Missing"
2021.emnlp-demo.16,W17-2315,0,0.123865,"roblems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for such applications are http://nlp.uniroma1.it/spring and, in addition, obtained automatically through an AMR parser, we release extra model checkpoints to be used 1 the benefits of AMR integration are highly correwith the original SPRING Python code. lated with the performance of the underlying parser 1 Introduction across various dat"
2021.emnlp-demo.16,2021.emnlp-main.351,0,0.14295,"des, as well as enabling lower memory footprint for users employing SPRING with our Python code. To train SPRING variants, we employ the same hyperparameters as in Bevilacqua et al. (2021a). In addition, we summarize the state-of-the-art systems on AMR 3.0. The RESTful APIs we provide can be used effectively to query the SPRING services programmatically. Our APIs are simple and, differently 138 3 4 catalog.ldc.upenn.edu/LDC2020T02 amr.isi.edu/download.html Parsing Lyu et al. (2020) Zhou et al. (2021) 75.8 81.2 SPRING (Bevilacqua et al., 2021a) 83.0 Generation Zhang et al. (2020) T5 Fine-Tune (Ribeiro et al., 2021) STRUCTADAPT-RGCN (Ribeiro et al., 2021) 34.3 41.6 48.0 SPRING (Bevilacqua et al., 2021a) 44.9 Table 1: Comparison with literature on AMR 3.0. Generation Parsing AMR 3.0 Bio-AMR Train dataset Dev Test Dev Test SPRINGuni SPRINGbi AMR 3.0 AMR 3.0 83.9 83.6 82.6 82.3 60.6 60.5 60.6 59.2 SPRINGuni SPRINGbi Bio+AMR 3.0 Bio+AMR 3.0 83.9 84.1 82.5 82.7 80.0 79.5 80.1 80.2 SPRINGuni SPRINGbi AMR 3.0 AMR 3.0 45.0 43.9 44.9 44.5 22.9 21.1 19.4 17.1 SPRINGuni SPRINGbi Bio+AMR 3.0 Bio+AMR 3.0 45.3 44.3 45.7 45.0 39.5 38.5 43.5 42.0 Table 2: SPRING variants in AMR 3.0 and Bio-AMR. 1, since here, as we reca"
2021.emnlp-demo.16,Q19-1002,0,0.0202507,"ful APIs included in AMR graphs, as well as their obvious for our state-of-the-art AMR parsing and generation system, SPRING (Symmetric PaRsapplications as an interface between human and Ing aNd Generation). The Web interface machines, make both AMR parsing and generation has been developed to be easily used by the very rewarding problems to solve. As a matter Natural Language Processing community, as of fact, AMR has been successfully applied to diwell as by the general public. It provides, verse downstream applications, such as Machine among other things, a highly interactive viTranslation (Song et al., 2019), Text Summarizasualization platform and a feedback mechation (Hardy and Vlachos, 2018; Liao et al., 2018), nism to obtain user suggestions for further imHuman-Robot Interaction (Bonial et al., 2020a), Inprovements of the system’s output. Moreover, our RESTful APIs enable easy integration of formation Extraction (Rao et al., 2017) and, more SPRING in downstream applications where recently, Question Answering (Lim et al., 2020; BoAMR structures are needed. Finally, we make nial et al., 2020b; Kapanipathi et al., 2021). HowSPRING Online Services freely available at ever, since AMR graphs for suc"
2021.emnlp-demo.16,2021.findings-emnlp.220,1,0.652831,"peline ple traditional Natural Language Processing tasks: of downstream applications. In our recent paper, Word Sense Disambiguation (Bevilacqua et al., SPRING (Bevilacqua et al., 2021a), we proposed 2021b; Barba et al., 2021), Semantic Role Labeling a solution through a simple, end-to-end approach (Màrquez et al., 2008; Conia et al., 2021; Blloshmi with no heavy inbuilt data processing assumptions. et al., 2021), Named Entity Recognition (Yadav Our model achieved unprecedented performance in and Bethard, 2018), Entity Linking (Ling et al., AMR parsing and generation, both in- and out-of2015; Tedeschi et al., 2021), and Coreference Res- distribution. olution (Kobayashi and Ng, 2020). Owing to this To make SPRING accessible to the community, 1 https://github.com/SapienzaNLP/spring thereby lowering the entry point to AMR applica134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 134–142 August 1–6, 2021. ©2021 Association for Computational Linguistics tion research, we present SPRING Online Services which include: • a Web interface to easily produce and visualize an AMR graph for a given sentence and, vice versa, a sentence for a given A"
2021.emnlp-demo.34,S07-1002,0,0.0961264,"train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, systems that only 300 require raw text in input. AMuSE-WSD (XLMRlarge) offers a very significant improvement over SyntagRank (Scozzafava et al., 2020), the previous best en"
2021.emnlp-demo.34,2021.emnlp-main.112,1,0.798395,"Missing"
2021.emnlp-demo.34,2020.acl-main.255,1,0.785659,"scape.com cecconi@babelscape.com navigli@diag.uniroma1.it Abstract (Shimura et al., 2019) and question answering (Ramakrishnan et al., 2003). Over the past few years, Word Sense Disambiguation (WSD) has received renewed interWSD approaches usually fall into two categories: est: recently proposed systems have shown the knowledge-based (Moro et al., 2014; Agirre et al., remarkable effectiveness of deep learning tech2014; Chaplot and Salakhutdinov, 2018), which niques in this task, especially when aided by leverage computational lexicons, and supervised modern pretrained language models. Unfortu(Bevilacqua and Navigli, 2020; Blevins and Zettlenately, such systems are still not available as moyer, 2020; Conia and Navigli, 2021; Barba et al., ready-to-use end-to-end packages, making it 2021; ElSheikh et al., 2021), which train machine difficult for researchers to take advantage of learning models on sense-annotated data. While their performance. The only alternative for a user interested in applying WSD to downearly work mainly belongs to the former category stream tasks is to use currently available end(Navigli, 2009), recent studies have shown the suto-end WSD systems, which, however, still periority in performa"
2021.emnlp-demo.34,2020.acl-main.95,0,0.0459801,"Missing"
2021.emnlp-demo.34,2021.emnlp-main.79,1,0.808748,"few years, WSD has received growing attention have been several attempts at providing ready-toand has been proven to be useful in an increas- use WSD systems that can be easily integrated into ing range of applications, such as machine trans- other systems (Navigli and Ponzetto, 2012b; Moro lation (Liu et al., 2018; Pu et al., 2018; Raganato et al., 2014; Agirre et al., 2014; Scozzafava et al., et al., 2019), information extraction (Zhong and 2020). Nevertheless, current ready-to-use WSD Ng, 2012; Delli Bovi et al., 2015), information re- systems are either English-only or based on aptrieval (Blloshmi et al., 2021), text categorization proaches that now lag behind state-of-the-art mod298 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 298–307 August 1–6, 2021. ©2021 Association for Computational Linguistics els in terms of performance. In this paper, we fill the gap and present AMuSEWSD, an easy-to-use, off-the-shelf WSD package that provides sense annotations in multiple languages through a state-of-the-art neural-based model. The main features of AMuSE-WSD can be summarized as follows: • We propose the first ready-to-use WSD package"
2021.emnlp-demo.34,S01-1001,0,0.377086,"25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, systems that only 300 require raw text in input. AMuSE-WSD (XLMRlarge) offers a very significant"
2021.emnlp-demo.34,2021.emnlp-main.715,1,0.769931,"has received renewed interWSD approaches usually fall into two categories: est: recently proposed systems have shown the knowledge-based (Moro et al., 2014; Agirre et al., remarkable effectiveness of deep learning tech2014; Chaplot and Salakhutdinov, 2018), which niques in this task, especially when aided by leverage computational lexicons, and supervised modern pretrained language models. Unfortu(Bevilacqua and Navigli, 2020; Blevins and Zettlenately, such systems are still not available as moyer, 2020; Conia and Navigli, 2021; Barba et al., ready-to-use end-to-end packages, making it 2021; ElSheikh et al., 2021), which train machine difficult for researchers to take advantage of learning models on sense-annotated data. While their performance. The only alternative for a user interested in applying WSD to downearly work mainly belongs to the former category stream tasks is to use currently available end(Navigli, 2009), recent studies have shown the suto-end WSD systems, which, however, still periority in performance of the latter category, esperely on graph-based heuristics or non-neural cially thanks to complex neural networks (Bevilacmachine learning algorithms. In this paper, we qua et al., 2021)."
2021.emnlp-demo.34,D09-1046,0,0.0498747,"SemEval-2015 (SE15), and the concatenation of all the datasets (ALL). We also include results on multilingual WSD in SemEval-2013 (DE, ES, FR, IT), SemEval-2015 (IT, ES), and XL-WSD (average over 17 languages, English excluded). We distinguish between WSD Modules, that is, research systems that need to be inserted into a pipeline, and End-to-End WSD Systems. Best results among end-to-end systems in bold. which the model can learn to assign multiple valid senses to each target word. Indeed, the “boundaries” between different senses of a polysemous word are not always clear cut or well defined (Erk and McCarthy, 2009), often leading to cases in which, given a word in context, more than one meaning is deemed appropriate by human annotators. Framing WSD as a multi-label classification problem allows the model to take advantage of such cases. In particular, this means that the model is trained to predict whether a sense s ∈ Sw is appropriate for a word w in a given context, independently of the other senses in Sw . 2.3 Evaluation (Pasini et al., 2021), a new multilingual dataset which comprises 17 languages. Experimental setup. We evaluate how the performance of AMuSE-WSD varies when using four different pret"
2021.emnlp-demo.34,2020.coling-main.291,1,0.844792,"Missing"
2021.emnlp-demo.34,N18-1121,0,0.0508916,"Missing"
2021.emnlp-demo.34,2021.eacl-main.286,1,0.884856,"ing (Ramakrishnan et al., 2003). Over the past few years, Word Sense Disambiguation (WSD) has received renewed interWSD approaches usually fall into two categories: est: recently proposed systems have shown the knowledge-based (Moro et al., 2014; Agirre et al., remarkable effectiveness of deep learning tech2014; Chaplot and Salakhutdinov, 2018), which niques in this task, especially when aided by leverage computational lexicons, and supervised modern pretrained language models. Unfortu(Bevilacqua and Navigli, 2020; Blevins and Zettlenately, such systems are still not available as moyer, 2020; Conia and Navigli, 2021; Barba et al., ready-to-use end-to-end packages, making it 2021; ElSheikh et al., 2021), which train machine difficult for researchers to take advantage of learning models on sense-annotated data. While their performance. The only alternative for a user interested in applying WSD to downearly work mainly belongs to the former category stream tasks is to use currently available end(Navigli, 2009), recent studies have shown the suto-end WSD systems, which, however, still periority in performance of the latter category, esperely on graph-based heuristics or non-neural cially thanks to complex ne"
2021.emnlp-demo.34,2020.acl-main.747,0,0.0273751,"tage of such cases. In particular, this means that the model is trained to predict whether a sense s ∈ Sw is appropriate for a word w in a given context, independently of the other senses in Sw . 2.3 Evaluation (Pasini et al., 2021), a new multilingual dataset which comprises 17 languages. Experimental setup. We evaluate how the performance of AMuSE-WSD varies when using four different pretrained language models to represent input sentences: a high-performing Englishonly version based on BERT-large-cased (Devlin et al., 2019), a high-performing multilingual version based on XLM-RoBERTa-large (Conneau et al., 2020), a multilingual version that relies on the smaller XLM-RoBERTa-base to balance quality and inference time, and a multilingual version based on Multilingual-MiniLM (Wang et al., 2020) that minimizes inference time while still providing good results. In any case, the weights of the underlying language model are left frozen, that is, they are not updated during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard"
2021.emnlp-demo.34,Q15-1038,1,0.725462,"anings depending on the context. Over the past In order to make WSD more accessible, there few years, WSD has received growing attention have been several attempts at providing ready-toand has been proven to be useful in an increas- use WSD systems that can be easily integrated into ing range of applications, such as machine trans- other systems (Navigli and Ponzetto, 2012b; Moro lation (Liu et al., 2018; Pu et al., 2018; Raganato et al., 2014; Agirre et al., 2014; Scozzafava et al., et al., 2019), information extraction (Zhong and 2020). Nevertheless, current ready-to-use WSD Ng, 2012; Delli Bovi et al., 2015), information re- systems are either English-only or based on aptrieval (Blloshmi et al., 2021), text categorization proaches that now lag behind state-of-the-art mod298 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 298–307 August 1–6, 2021. ©2021 Association for Computational Linguistics els in terms of performance. In this paper, we fill the gap and present AMuSEWSD, an easy-to-use, off-the-shelf WSD package that provides sense annotations in multiple languages through a state-of-the-art neural-based model. The main featu"
2021.emnlp-demo.34,W04-0807,0,0.19885,"015) with a learning rate of 10−4 . We train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, systems that only 300 require raw text in input. AMuSE-WSD (XLMRlarge) offers a very significant improvement over SyntagRank (Scozzaf"
2021.emnlp-demo.34,H94-1046,0,0.738098,"multilingual version based on XLM-RoBERTa-large (Conneau et al., 2020), a multilingual version that relies on the smaller XLM-RoBERTa-base to balance quality and inference time, and a multilingual version based on Multilingual-MiniLM (Wang et al., 2020) that minimizes inference time while still providing good results. In any case, the weights of the underlying language model are left frozen, that is, they are not updated during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-20"
2021.emnlp-demo.34,S15-2049,1,0.748881,"the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, systems that only 300 require raw text in input. AMuSE-WSD (XLMRlarge) offers a very significant improvement over SyntagRank (Scozzafava et al., 2020), the previous best end-to-end system, both in English WSD (+7.6 in F1 score on the concatenation of a"
2021.emnlp-demo.34,Q14-1019,1,0.854982,"sambiguation Riccardo Orlando Babelscape, Italy Simone Conia Sapienza University of Rome orlando@babelscape.com conia@di.uniroma1.it Fabrizio Brignone Babelscape, Italy Francesco Cecconi Babelscape, Italy Roberto Navigli Sapienza University of Rome brignone@babelscape.com cecconi@babelscape.com navigli@diag.uniroma1.it Abstract (Shimura et al., 2019) and question answering (Ramakrishnan et al., 2003). Over the past few years, Word Sense Disambiguation (WSD) has received renewed interWSD approaches usually fall into two categories: est: recently proposed systems have shown the knowledge-based (Moro et al., 2014; Agirre et al., remarkable effectiveness of deep learning tech2014; Chaplot and Salakhutdinov, 2018), which niques in this task, especially when aided by leverage computational lexicons, and supervised modern pretrained language models. Unfortu(Bevilacqua and Navigli, 2020; Blevins and Zettlenately, such systems are still not available as moyer, 2020; Conia and Navigli, 2021; Barba et al., ready-to-use end-to-end packages, making it 2021; ElSheikh et al., 2021), which train machine difficult for researchers to take advantage of learning models on sense-annotated data. While their performance."
2021.emnlp-demo.34,S13-2040,1,0.791218,"union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, systems that only 300 require raw text in input. AMuSE-WSD (XLMRlarge) offers a very significant improvement over SyntagRank (Scozzafava et al., 2020), the previous best end-to-end system, both in English WSD"
2021.emnlp-demo.34,P12-3012,1,0.642339,"Missing"
2021.emnlp-demo.34,D17-2018,0,0.0176991,"nization, lemmatization and PoS tags to the core WSD model. We provide more technical details about the features of our preprocessing pipeline in Section 3. 2.2 Model Architecture The core of AMuSE-WSD is its WSD model. Since the main objective of our system is to provide the best possible automatic annotations for WSD, our package features a reimplementation of the stateof-the-art WSD model proposed recently by Conia and Navigli (2021). Differently from other ready-to-use WSD packages which are based on graph-based heuristics (Moro et al., 2014; Scozzafava et al., 2020) or non-neural models (Papandrea et al., 2017), this neural architecture is built on top of a Transformer encoder (Vaswani et al., 2017). More specifically, given a word w in context, the WSD model i) builds a contextualized representation ew ∈ RdL of the word w as the average of the hidden states of the last four layers of a pretrained Transformer encoder L, ii) applies a non-linear transformation to obtain a sense-specific hidden representation hw ∈ Rdh , and finally iii) computes the output score distribution ow ∈ R|S| over all the possible senses of a sense inventory S. More formally:  X  4 1 −k ew = BatchNorm lw 4 i=1 2.1 Preproces"
2021.emnlp-demo.34,2020.acl-demos.14,0,0.0270952,"tandard benchmarks for WSD (Section 2.3). a good preprocessing pipeline is a necessary condition for a high-quality WSD system. Indeed, the most popular sense inventories for WSD, e.g. WordNet (Miller, 1992) and BabelNet (Navigli and Ponzetto, 2012a; Navigli et al., 2021), define the possible meanings of a word with respect to its lemma and PoS tag. Therefore, an accurate preprocessing pipeline is fundamental in order to generate the correct candidate set of possible meanings. AMuSE-WSD’s preprocessing pipeline takes advantage of two popular toolkits, spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), to provide high-quality document splitting, tokenization, lemmatization and PoS tags to the core WSD model. We provide more technical details about the features of our preprocessing pipeline in Section 3. 2.2 Model Architecture The core of AMuSE-WSD is its WSD model. Since the main objective of our system is to provide the best possible automatic annotations for WSD, our package features a reimplementation of the stateof-the-art WSD model proposed recently by Conia and Navigli (2021). Differently from other ready-to-use WSD packages which are based on graph-based heuristics (Moro et al., 201"
2021.emnlp-demo.34,E17-1010,1,0.933948,"are left frozen, that is, they are not updated during training. Each model is trained for 25 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 10−4 . We train each model configuration on the union of SemCor (Miller et al., 1994) and the WordNet Gloss Corpus. Following standard practice, we perform model selection choosing the checkpoint with highest F1 score on SemEval-2007, the smallest evaluation dataset. Datasets. We compare the performance of our model against currently available end-to-end WSD systems on the unified evaluation framework for English all-words WSD proposed by Raganato et al. (2017). This evaluation includes five gold datasets, namely, Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Agirre and Soroa, 2007), SemEval-2013 (Navigli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). We also evaluate our model in multilingual WSD using the French, German, Italian and Spanish datasets provided as part of SemEval-2013, and Results. Table 1 shows how AMuSE-WSD perthe Italian and Spanish datasets of SemEval-2015. forms in comparison to currently available endFinally, we evaluate AMuSE-WSD on XL-WSD to-end WSD systems, that is, syste"
2021.emnlp-demo.34,W19-5354,0,0.0582257,"Missing"
2021.emnlp-demo.34,W03-1201,0,0.319852,"Missing"
2021.emnlp-demo.34,2020.emnlp-main.285,1,0.862518,"Missing"
2021.emnlp-demo.34,2020.acl-demos.6,1,0.904393,"o provide high-quality document splitting, tokenization, lemmatization and PoS tags to the core WSD model. We provide more technical details about the features of our preprocessing pipeline in Section 3. 2.2 Model Architecture The core of AMuSE-WSD is its WSD model. Since the main objective of our system is to provide the best possible automatic annotations for WSD, our package features a reimplementation of the stateof-the-art WSD model proposed recently by Conia and Navigli (2021). Differently from other ready-to-use WSD packages which are based on graph-based heuristics (Moro et al., 2014; Scozzafava et al., 2020) or non-neural models (Papandrea et al., 2017), this neural architecture is built on top of a Transformer encoder (Vaswani et al., 2017). More specifically, given a word w in context, the WSD model i) builds a contextualized representation ew ∈ RdL of the word w as the average of the hidden states of the last four layers of a pretrained Transformer encoder L, ii) applies a non-linear transformation to obtain a sense-specific hidden representation hw ∈ Rdh , and finally iii) computes the output score distribution ow ∈ R|S| over all the possible senses of a sense inventory S. More formally:  X"
2021.emnlp-demo.34,P19-1105,0,0.0610974,"Missing"
2021.emnlp-demo.34,Q18-1044,0,0.022419,"explicit semantic information in other areas of research, but who are not experts in ing of text (Navigli, 2018): indeed, a word can WSD. be polysemous, that is, it can refer to different meanings depending on the context. Over the past In order to make WSD more accessible, there few years, WSD has received growing attention have been several attempts at providing ready-toand has been proven to be useful in an increas- use WSD systems that can be easily integrated into ing range of applications, such as machine trans- other systems (Navigli and Ponzetto, 2012b; Moro lation (Liu et al., 2018; Pu et al., 2018; Raganato et al., 2014; Agirre et al., 2014; Scozzafava et al., et al., 2019), information extraction (Zhong and 2020). Nevertheless, current ready-to-use WSD Ng, 2012; Delli Bovi et al., 2015), information re- systems are either English-only or based on aptrieval (Blloshmi et al., 2021), text categorization proaches that now lag behind state-of-the-art mod298 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 298–307 August 1–6, 2021. ©2021 Association for Computational Linguistics els in terms of performance. In this paper, w"
2021.emnlp-demo.34,P12-1029,0,0.073597,"Missing"
2021.emnlp-demo.36,W13-2322,0,0.0385248,"t system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that our system – with its easy-to-use RESTful API and Web interface – will become a valuable tool for the research community, encouraging the integration of sentencelevel semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniroma1.it/invero. 1 Introduction (Christensen et al., 2010), Question Answering (He et al., 2015), Machine Translation (Marcheggiani et al., 2018) and Semantic Parsing (Banarescu et al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et"
2021.emnlp-demo.36,C18-1233,0,0.0353805,"Missing"
2021.emnlp-demo.36,D19-1094,0,0.0407121,"Missing"
2021.emnlp-demo.36,Q19-1022,0,0.0292645,"Missing"
2021.emnlp-demo.36,D19-1544,0,0.0415984,"Missing"
2021.emnlp-demo.36,W10-0907,0,0.0440054,"tools for cross-lingual Semantic Role Labeling. In this paper, we fill this gap and present InVeRo-XL, an off-the-shelf state-of-the-art system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that our system – with its easy-to-use RESTful API and Web interface – will become a valuable tool for the research community, encouraging the integration of sentencelevel semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniroma1.it/invero. 1 Introduction (Christensen et al., 2010), Question Answering (He et al., 2015), Machine Translation (Marcheggiani et al., 2018) and Semantic Parsing (Banarescu et al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways t"
2021.emnlp-demo.36,2021.naacl-main.31,1,0.917411,"uch as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et al., 2005), AllenNLP’s SRL demo2 , which provides a reimplementation of a BERT-based model (Shi and Lin, 2019), and InVeRo (Conia et al., 2020), which offers annotations a"
2021.emnlp-demo.36,2020.emnlp-demos.11,1,0.811674,"heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et al., 2005), AllenNLP’s SRL demo2 , which provides a reimplementation of a BERT-based model (Shi and Lin, 2019), and InVeRo (Conia et al., 2020), which offers annotations according to two different linguistic inventories, PropBank (Palmer et al., 2005) and Informally, Semantic Role Labeling (SRL) is often defined as the task of automatically answering the question “Who did What, to Whom, Where, When, and How?” (Màrquez et al., 2008). More precisely, SRL aims at recovering the predicateargument structures within a sentence, providing an explicit overlay that uncovers the underlying semantics of text. For this reason, SRL is thought to be key in enabling Natural Language Understanding (Navigli, 2018). Today SRL is still an open problem,"
2021.emnlp-demo.36,2020.coling-main.120,1,0.69125,"gnition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et"
2021.emnlp-demo.36,2020.acl-main.747,0,0.113556,"to cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniroma1.it/invero. 1 Introduction (Christensen et al., 2010), Question Answering (He et al., 2015), Machine Translation (Marcheggiani et al., 2018) and Semantic Parsing (Banarescu et al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguisti"
2021.emnlp-demo.36,N19-1423,0,0.0134979,"egration of sentencelevel semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniroma1.it/invero. 1 Introduction (Christensen et al., 2010), Question Answering (He et al., 2015), Machine Translation (Marcheggiani et al., 2018) and Semantic Parsing (Banarescu et al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is"
2021.emnlp-demo.36,D19-1058,1,0.89992,"Missing"
2021.emnlp-demo.36,W09-1201,0,0.0881049,"Missing"
2021.emnlp-demo.36,D19-1538,0,0.107219,"and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-b"
2021.emnlp-demo.36,N18-2078,0,0.0125479,"esent InVeRo-XL, an off-the-shelf state-of-the-art system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that our system – with its easy-to-use RESTful API and Web interface – will become a valuable tool for the research community, encouraging the integration of sentencelevel semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniroma1.it/invero. 1 Introduction (Christensen et al., 2010), Question Answering (He et al., 2015), Machine Translation (Marcheggiani et al., 2018) and Semantic Parsing (Banarescu et al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage"
2021.emnlp-demo.36,K17-1041,0,0.051435,"Missing"
2021.emnlp-demo.36,J08-2001,0,0.03208,"egration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et al., 2005), AllenNLP’s SRL demo2 , which provides a reimplementation of a BERT-based model (Shi and Lin, 2019), and InVeRo (Conia et al., 2020), which offers annotations according to two different linguistic inventories, PropBank (Palmer et al., 2005) and Informally, Semantic Role Labeling (SRL) is often defined as the task of automatically answering the question “Who did What, to Whom, Where, When, and How?” (Màrquez et al., 2008). More precisely, SRL aims at recovering the predicateargument structures within a sentence, providing an explicit overlay that uncovers the underlying semantics of text. For this reason, SRL is thought to be key in enabling Natural Language Understanding (Navigli, 2018). Today SRL is still an open problem, with several research papers being published each year at top-tier conferences, revealing novel insights and proposing better approaches. Over the years, thanks to this active development, SRL has been successfully exploited in a wide array of downstream tasks that span across different are"
2021.emnlp-demo.36,E17-2068,0,0.0177309,"cessing module based on spaCy and Stanza; 3 InVeRo-XL can be downloaded upon request at http:// InVeRo-XL is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International. nlp.uniroma1.it/resources. Preprocessing The previous version of InVeRo-XL preprocessed an English sentence using a very limited and simple set of rules. In order to correctly support more languages, InVeRo-XL now relies on both spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020) to deal transparently with document splitting and tokenization. An automatic language detector based on fastText4 (Joulin et al., 2017) is used to dynamically choose between the two preprocessing tools, depending on the language detected: spaCy is faster for high-resource languages, e.g. English, but also less reliable on lower-resource languages, e.g. Catalan, for which our system falls back to Stanza. 2.2 As previously mentioned, InVeRo-XL is the successor of InVeRo. Although its main new feature is the ability to provide predicate sense and semantic role annotations in over 40 languages with 7 different inventories, InVeRo-XL has been overhauled to also improve several other important aspects. In particular: • SRL model: t"
2021.emnlp-demo.36,W05-0625,0,0.0783222,"li, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et al., 2005), AllenNLP’s SRL demo2 , which provides a reimplementation of a BERT-based model (Shi and Lin, 2019), and InVeRo (Conia et al., 2020), which offers annotations according to two different linguistic inventories, PropBank (Palmer et al., 2005) and Informally, Semantic Role Labeling (SRL) is often defined as the task of automatically answering the question “Who did What, to Whom, Where, When, and How?” (Màrquez et al., 2008). More precisely, SRL aims at recovering the predicateargument structures within a sentence, providing an explicit overlay that uncovers the underlying semantics of text. For"
2021.emnlp-demo.36,D19-1099,0,0.034308,"Missing"
2021.emnlp-demo.36,J05-1004,0,0.426121,"ies, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable examples include SENNA1 (Collobert et al., 2011), which uses an ensemble of feature-based classifiers (Koomen et al., 2005), AllenNLP’s SRL demo2 , which provides a reimplementation of a BERT-based model (Shi and Lin, 2019), and InVeRo (Conia et al., 2020), which offers annotations according to two different linguistic inventories, PropBank (Palmer et al., 2005) and Informally, Semantic Role Labeling (SRL) is often defined as the task of automatically answering the question “Who did What, to Whom, Where, When, and How?” (Màrquez et al., 2008). More precisely, SRL aims at recovering the predicateargument structures within a sentence, providing an explicit overlay that uncovers the underlying semantics of text. For this reason, SRL is thought to be key in enabling Natural Language Understanding (Navigli, 2018). Today SRL is still an open problem, with several research papers being published each year at top-tier conferences, revealing novel insights an"
2021.emnlp-demo.36,N18-1202,0,0.0299119,"al., 2013), to Computer Vision with Visual Semantic Role Labeling (Gupta and Malik, 2015) and Situation Recognition (Yatskar et al., 2016). Recently, the growing interest in cross-lingual NLP, supported by the increasingly wide availability of pretrained multilingual language models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), has sparked renewed interest in multilingual and cross-lingual SRL. In just a few years, researchers have found ways to design fully-neural end-to-end systems for SRL (Cai et al., 2018), to take advantage of contextual word representations (Peters et al., 2018; Li et al., 2019), to achieve high performance on multiple languages (He et al., 2019a; Conia and Navigli, 2020), to generate sense and role labels with sequence-tosequence models (Blloshmi et al., 2021) and to perform SRL jointly across heterogeneous inventories (Conia et al., 2021). Since SRL is a task that involves complex linguistic theories, inventories and techniques, there have been efforts to develop easy-to-use tools that offer automatic predicate sense and semantic role annotations to users interested in the integration of sentence-level semantics into downstream tasks. Some notable"
2021.emnlp-demo.36,W12-4501,0,0.0258845,"span-based SRL by treating spans as sequences of BIO tags. In order to correctly decode valid spans at inference time, InVeRo-XL makes use of a Viterbi decoder. Other improvements include training the model with the RAdam optimizer (Liu et al., 2020), ensuring that each training batch features a balanced number of instances for each language in the training set, and searching randomly for better hyperparameter values. 2.3 Evaluation Datasets. We report the performance of InVeRoXL on two gold standard benchmarks for SRL: CoNLL-2009 (Hajiˇc et al., 2009) for dependencybased SRL and CoNLL-2012 (Pradhan et al., 2012) for span-based SRL. To the best of our knowledge, CoNLL-2009 is the largest benchmark for multilingual SRL as it comprises six languages, namely, Catalan, Chinese, Czech, English, German and Spanish.5 The main challenge of this benchmark is that each language was annotated with a different predicate-argument structure inventory, e.g. the English PropBank (Palmer et al., 2005) for English, AnCora (Taulé et al., 2008) for Spanish/Catalan and PDT-Vallex (Hajic et al., 2003) for Czech. While CoNLL-2009 is an ideal test bed for evaluating the multilingual capabilities of an SRL system, dependency-"
2021.emnlp-demo.36,2020.acl-demos.14,0,0.0201796,"cessing: while its predecessor used a very limited set of rules to preprocess English text, InVeRo-XL features a multilingual preprocessing module based on spaCy and Stanza; 3 InVeRo-XL can be downloaded upon request at http:// InVeRo-XL is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International. nlp.uniroma1.it/resources. Preprocessing The previous version of InVeRo-XL preprocessed an English sentence using a very limited and simple set of rules. In order to correctly support more languages, InVeRo-XL now relies on both spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020) to deal transparently with document splitting and tokenization. An automatic language detector based on fastText4 (Joulin et al., 2017) is used to dynamically choose between the two preprocessing tools, depending on the language detected: spaCy is faster for high-resource languages, e.g. English, but also less reliable on lower-resource languages, e.g. Catalan, for which our system falls back to Stanza. 2.2 As previously mentioned, InVeRo-XL is the successor of InVeRo. Although its main new feature is the ability to provide predicate sense and semantic role annotations in over 40 languages wi"
2021.emnlp-demo.36,taule-etal-2008-ancora,0,0.0637977,"2.3 Evaluation Datasets. We report the performance of InVeRoXL on two gold standard benchmarks for SRL: CoNLL-2009 (Hajiˇc et al., 2009) for dependencybased SRL and CoNLL-2012 (Pradhan et al., 2012) for span-based SRL. To the best of our knowledge, CoNLL-2009 is the largest benchmark for multilingual SRL as it comprises six languages, namely, Catalan, Chinese, Czech, English, German and Spanish.5 The main challenge of this benchmark is that each language was annotated with a different predicate-argument structure inventory, e.g. the English PropBank (Palmer et al., 2005) for English, AnCora (Taulé et al., 2008) for Spanish/Catalan and PDT-Vallex (Hajic et al., 2003) for Czech. While CoNLL-2009 is an ideal test bed for evaluating the multilingual capabilities of an SRL system, dependency-based annotations may look unfamiliar to end users who are not used to the Inventory-specific decoders. Finally, the universal encodings are given to a set of classifiers in order to obtain the desired output labels. More specifically, for each inventory, we need three types of output: i) whether a word wi is a predicate wp ; ii) the most appropriate sense s for a predicate wp ; iii) which semantic role r, possibly t"
2021.emnlp-demo.36,2021.findings-emnlp.197,1,0.690129,"s on particular case studies, attract new researchers to the field, and inspire others to exploit SRL in downstream tasks or even real-world scenarios. 5 Conclusion and Future Work tem for cross-lingual SRL through a RESTful API that relieves them from the need to reimplement complex neural models and/or to build an efficient preprocessing/postprocessing pipeline. Although InVeRo-XL is a major step forward compared to its predecessor, we intend to further improve our system by adopting future and more advanced SRL model architectures and by including new training datasets, such as UniteD-SRL (Tripodi et al., 2021). We strongly believe that InVeRo-XL will facilitate the integration of SRL into downstream cross-lingual tasks, hopefully aiding further advancements in cross-lingual Natural Language Understanding. Over the years, the research community has greatly advanced the field of SRL, proposing ever more Acknowledgments complex approaches to tackle the task more effecThe authors gratefully acknowledge tively. However, despite the growing interest in the support of the ERC Consolidacross-lingual NLP, there have been very few efforts tor Grant MOUSSE No. 726487 to develop automatic tools to perform SRL"
2021.emnlp-main.112,2021.naacl-main.371,1,0.735875,"ational Linguistics In this work, we focus on this shortcoming and propose CONtinuous SEnse Comprehension (C ON S E C), a novel approach to WSD that exploits a feedback loop strategy to condition the disambiguation process also on the senses of cooccurring words. In particular, given an input text, we define an ordering of the words contained therein and disambiguate each word conditioning not only on its context and possible meanings, but also on the senses assigned to those words already classified. As the underlying neural architecture, inspired by the recent re-framing of WSD presented by Barba et al. (2021a) and its nimble adaptability to our setting, we leverage a Transformer model trained with a text extraction objective: given as input a text with a target word, its possible sense definitions and the list of already disambiguated words along with their chosen glosses, the model has to learn to extract the text span associated with the sense definition that best expresses the target word’s meaning. Backed by several experiments on the English all-words WSD task (Raganato et al. (2017a), we show the benefits of our formulation, which surpasses the prior state of the art by 1.3 F1 points, and p"
2021.emnlp-main.112,2020.acl-main.255,1,0.938509,"h a number of different neural formulations. Early neural approaches (Kågebäck and Salomonsson, 2016; Raganato et al., 2017b) focused on architectures where WSD was framed as token classification over WordNet senses. While already effective, these architectures displayed a number of shortcomings, especially with regard to modeling rare and unseen senses. To cope with these, many works started to complement the training data by exploiting different forms of lexical knowledge stored in WordNet, such as sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and semantic relations (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021), or with silver data produced via novel generative formulations (Barba et al., 2021b). Sense definitions, in particular, have been shown to significantly improve models’ scalability to senses that are underrepresented in the training corpus, and their usage has been thoroughly investigated. Huang et al. (2019) frame WSD as a binary classification problem where, given a word in context and one of its • We put forward C ON S E C, a novel approach possible definitions in the sense inventory, a model to WSD where the disambiguation process has to determine whether the me"
2021.emnlp-main.112,2021.eacl-main.286,1,0.728818,"formulations. Early neural approaches (Kågebäck and Salomonsson, 2016; Raganato et al., 2017b) focused on architectures where WSD was framed as token classification over WordNet senses. While already effective, these architectures displayed a number of shortcomings, especially with regard to modeling rare and unseen senses. To cope with these, many works started to complement the training data by exploiting different forms of lexical knowledge stored in WordNet, such as sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and semantic relations (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021), or with silver data produced via novel generative formulations (Barba et al., 2021b). Sense definitions, in particular, have been shown to significantly improve models’ scalability to senses that are underrepresented in the training corpus, and their usage has been thoroughly investigated. Huang et al. (2019) frame WSD as a binary classification problem where, given a word in context and one of its • We put forward C ON S E C, a novel approach possible definitions in the sense inventory, a model to WSD where the disambiguation process has to determine whether the meaning expressed is conditi"
2021.emnlp-main.112,C08-1021,0,0.0369351,"Missing"
2021.emnlp-main.112,N19-1423,0,0.0128464,"E C-specific parameters, we use max_deps = 9, α = 0.7 and β = 0.1 in all our experiments.8 When working with document-level datasets, rather than treating the sentence where w ˜i occurs as its context, we augment it so that cw˜i also includes its preceding and subsequent sentence. Comparison Systems We compare C ON S E C with two common baselines in the WSD literature, namely i) MFS-SemCor, where target words are disambiguated by simply emitting their most frequent sense in SemCor, and ii) BERT-base, which employs a linear classifier over WordNet senses on top of frozen BERT representations (Devlin et al., 2019; Blevins and Zettlemoyer, 2020). Furthermore, to contextualize C ON S E C performances in the current landscape of English WSD, we further consider a number of recent state-of-theart systems and evaluate our approach against: SVC (Vial et al., 2019), which leverages WordNet relations to compress the output vocabulary and compensate for the lack of annotated data; ARES (Scarlini et al., 2020), a nearest-neighbor approach based on sense embeddings; GlossBERT (Huang et al., 2019), BEM (Blevins and Zettlemoyer, 2020), EWISER (Bevilacqua and Navigli, 2020)9 , WMLC (Conia and Navigli, 2021) and ESC"
2021.emnlp-main.112,S01-1001,0,0.69947,"ction 4.1) and via an ablation study of its components (Section 4.2). We then proceed to investigate how C ON S E C fares in the cross-lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba et al. (2021a), namely: i) MFS, containing all the instances in ALL where the target word is tagged with its most frequent sense5 ; ii) LFS, containing all the instances in"
2021.emnlp-main.112,D09-1046,0,0.389339,"Missing"
2021.emnlp-main.112,D19-1533,0,0.128152,"Missing"
2021.emnlp-main.112,D19-1355,0,0.193175,"n 4.1. 1495 4 WSD Evaluation We now assess the effectiveness of C ON S E C examining first its applicability to English all-words WSD both in terms of performances (Section 4.1) and via an ablation study of its components (Section 4.2). We then proceed to investigate how C ON S E C fares in the cross-lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba"
2021.emnlp-main.112,W16-5307,0,0.026977,"follows: able meaning in a fixed sense inventory (Bevilacqua et al., 2021), which is usually a dictionary-like lexical resource where a word’s meanings (senses) are enumerated and defined via definitions (glosses) and usage examples. Nowadays dominated by supervised systems, with WordNet (Miller et al., 1990) and SemCor (Miller et al., 1993) acting, respectively, as the de facto standard sense inventory and training corpus for the English language, this task is generally approached as a multi-label classification problem with a number of different neural formulations. Early neural approaches (Kågebäck and Salomonsson, 2016; Raganato et al., 2017b) focused on architectures where WSD was framed as token classification over WordNet senses. While already effective, these architectures displayed a number of shortcomings, especially with regard to modeling rare and unseen senses. To cope with these, many works started to complement the training data by exploiting different forms of lexical knowledge stored in WordNet, such as sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and semantic relations (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021), or with silver data produced via novel gener"
2021.emnlp-main.112,P19-1568,0,0.0140008,"is task is generally approached as a multi-label classification problem with a number of different neural formulations. Early neural approaches (Kågebäck and Salomonsson, 2016; Raganato et al., 2017b) focused on architectures where WSD was framed as token classification over WordNet senses. While already effective, these architectures displayed a number of shortcomings, especially with regard to modeling rare and unseen senses. To cope with these, many works started to complement the training data by exploiting different forms of lexical knowledge stored in WordNet, such as sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and semantic relations (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021), or with silver data produced via novel generative formulations (Barba et al., 2021b). Sense definitions, in particular, have been shown to significantly improve models’ scalability to senses that are underrepresented in the training corpus, and their usage has been thoroughly investigated. Huang et al. (2019) frame WSD as a binary classification problem where, given a word in context and one of its • We put forward C ON S E C, a novel approach possible definitions in the sense inven"
2021.emnlp-main.112,2020.acl-main.703,0,0.0508874,"cess. Ablation As both ESCHER and C ON S E C leverage text extraction formulations, with similar underlying architectures, and yet C ON S E C significantly outperforms ESCHER, we ablate here the differences between the two systems, namely i) the usage of D E B ERTA, ii) having cw˜i also include the previous and subsequent sentence11 , and iii) the introduction of context definitions. We show iteratively how performances change, in terms of F1 score, as we move from ESCHER to C ON S E C in Table 3. As a first result, we note that, as expected, changing the underlying model of ESCHER from BART (Lewis et al., 2020) to D E B ERTA does not cause any significant difference in performances. Indeed, the two systems feature an almost identical number of parameters and attain similar scores on text extraction tasks such as SQuAD (Lewis et al., 2020; He et al., 2021), with D E B ERTA behaving slightly better. Once we include more surrounding context in cw˜i , performances rise to 81.0, suggesting that the additional context provides valuable information to the neural model.12 However, this system, which differs from C ON S E C only in what pertains context definitions, achieves 1 F1 point less than C ON S E C;"
2021.emnlp-main.112,2020.tacl-1.47,0,0.303641,"the start of the gloss that best represents w ˜i . As our reference architecture, we use a linear classification head on top of D E B ERTA2 (He et al., 2021), a recently proposed Transformer model that improves over RoBERTa (Liu et al., 2019). The main reason behind this choice is the D E B ERTA usage of relative positions, an encoding which, differently from its absolute counterparts commonly used in other Transformer architectures, models text positional information via a bi-dimensional matrix that stores the relative distance between each word pair. Leveraging this encoding and inspired by Liu et al. (2020b), we propose an elegant approach to inform the model that, ∀w ˜j ∈ {w ˜1 , . . . , w ˜i−1 }, the meaning of w ˜j is the one expressed by δj : we place definitions immediately after the words they refer to, while simultaneously leaving unchanged the word order of cw˜i . This is achieved by overriding the relative distances in the positional matrix so that w ˜j perceives next to it both δj and its subsequent words in cw˜i . Formally, we manipulate the relative positions as follows: Formally, given a sense inventory S, consider a target word w ˆi occurring in a context cwˆi and let Dwˆi = di,1"
2021.emnlp-main.112,2021.ccl-1.108,0,0.0350989,"Missing"
2021.emnlp-main.112,D19-1359,1,0.883204,"multilingual version of D E B ERTA is not available, we replace it with mBART (Liu et al., 2020c);13 however, as mBART does not support relative positions, to inject the knowledge that w ˜j means δj , we first prepend w ˜j to each δj ∈ ∆w˜i , and then concatenate them right after the candidate definitions. We compare ConSeC against three systems included in Pasini et al. (2021), namely EWISER, XLMR-Large14 and SyntagRank (Scozzafava et al., 2020); EWISER and XLMR-Large are supervised systems, while SyntagRank is an unsupervised knowledge-based approach that builds upon syntagmatic relations (Maru et al., 2019). For both EWISER and XLMR-Large, we only consider the zero-shot scenario the authors illustrate, as it is the 13 Details on the parameters used in Appendix B. We consider the XLMR-Large architecture as it achieved the best average results on all the languages of the framework. 1498 14 Test ALL ∆ No Context Definitions Adversarial Teacher Forcing 80.7 80.0 82.5 -1.3 -2.0 +0.5 Table 5: Behavioral tests on C ON S E C. The ∆ column reports the relative difference w.r.t. C ON S E C trained on SemCor and evaluated on ALL. only setting where EWISER is available and, besides, the one where XLMR-Large"
2021.emnlp-main.112,H93-1061,0,0.181772,"Furthermore, we also examine the scalability of our approach on the cross-lingual setting, evaluating C ON S E C on the recently proposed framework of Pasini et al. (2021), and report significant improvements over prior systems. The contributions of this work are therefore as follows: able meaning in a fixed sense inventory (Bevilacqua et al., 2021), which is usually a dictionary-like lexical resource where a word’s meanings (senses) are enumerated and defined via definitions (glosses) and usage examples. Nowadays dominated by supervised systems, with WordNet (Miller et al., 1990) and SemCor (Miller et al., 1993) acting, respectively, as the de facto standard sense inventory and training corpus for the English language, this task is generally approached as a multi-label classification problem with a number of different neural formulations. Early neural approaches (Kågebäck and Salomonsson, 2016; Raganato et al., 2017b) focused on architectures where WSD was framed as token classification over WordNet senses. While already effective, these architectures displayed a number of shortcomings, especially with regard to modeling rare and unseen senses. To cope with these, many works started to complement the"
2021.emnlp-main.112,S15-2049,1,0.887644,"lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba et al. (2021a), namely: i) MFS, containing all the instances in ALL where the target word is tagged with its most frequent sense5 ; ii) LFS, containing all the instances in ALL annotated with a least frequent sense of the target word that appeared at least once in the training corpus; iii) Unseen Senses,"
2021.emnlp-main.112,S13-2040,1,0.902683,"investigate how C ON S E C fares in the cross-lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba et al. (2021a), namely: i) MFS, containing all the instances in ALL where the target word is tagged with its most frequent sense5 ; ii) LFS, containing all the instances in ALL annotated with a least frequent sense of the target word that appeared at least o"
2021.emnlp-main.112,W04-0844,1,0.375704,"Missing"
2021.emnlp-main.112,S07-1016,0,0.0557234,"pplicability to English all-words WSD both in terms of performances (Section 4.1) and via an ablation study of its components (Section 4.2). We then proceed to investigate how C ON S E C fares in the cross-lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba et al. (2021a), namely: i) MFS, containing all the instances in ALL where the target word is tagge"
2021.emnlp-main.112,E17-1010,1,0.963399,"ly on its context but also on the explicit senses assigned to nearby words. We evaluate C ON S E C and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how C ON S E C fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/ SapienzaNLP/consec. 1 Introduction essentially frame this task as a multi-label classification problem over a large vocabulary of discrete senses (Raganato et al., 2017b; Hadiwinoto et al., 2019). However, although effective and straightforward, this formulation suffers from a number of pitfalls, most notably i) senses are only defined via their training set occurrences, with their actual linguistic meaning not explicitly embedded within the neural model, and ii) these architectures either behave poorly on rare and unseen senses, or cannot handle them at all. In order to address these issues, recent literature has proposed more sophisticated forms of supervision where definitions of senses, i.e. glosses (Kumar et al., 2019; Blevins and Zettlemoyer, 2020), an"
2021.emnlp-main.112,D17-1120,1,0.957812,"ly on its context but also on the explicit senses assigned to nearby words. We evaluate C ON S E C and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how C ON S E C fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/ SapienzaNLP/consec. 1 Introduction essentially frame this task as a multi-label classification problem over a large vocabulary of discrete senses (Raganato et al., 2017b; Hadiwinoto et al., 2019). However, although effective and straightforward, this formulation suffers from a number of pitfalls, most notably i) senses are only defined via their training set occurrences, with their actual linguistic meaning not explicitly embedded within the neural model, and ii) these architectures either behave poorly on rare and unseen senses, or cannot handle them at all. In order to address these issues, recent literature has proposed more sophisticated forms of supervision where definitions of senses, i.e. glosses (Kumar et al., 2019; Blevins and Zettlemoyer, 2020), an"
2021.emnlp-main.112,2020.emnlp-main.285,1,0.830992,"r, where target words are disambiguated by simply emitting their most frequent sense in SemCor, and ii) BERT-base, which employs a linear classifier over WordNet senses on top of frozen BERT representations (Devlin et al., 2019; Blevins and Zettlemoyer, 2020). Furthermore, to contextualize C ON S E C performances in the current landscape of English WSD, we further consider a number of recent state-of-theart systems and evaluate our approach against: SVC (Vial et al., 2019), which leverages WordNet relations to compress the output vocabulary and compensate for the lack of annotated data; ARES (Scarlini et al., 2020), a nearest-neighbor approach based on sense embeddings; GlossBERT (Huang et al., 2019), BEM (Blevins and Zettlemoyer, 2020), EWISER (Bevilacqua and Navigli, 2020)9 , WMLC (Conia and Navigli, 2021) and ESCHER (Barba et al., 2021a), all of which are supervised systems that exploit sense definitions or relational knowledge to better model the meaning of words. Finally, following the trend of augmenting training data with WNGE, we also consider how C ON S E C fares in this setting and evaluate it against SVC, EWISER, WMLC and ESCHER. 8 Further details on this choice in Appendix B. We note that Be"
2021.emnlp-main.112,2020.acl-demos.6,1,0.837537,"he same language as that of its datasets, we translate English glosses towards each language using the multilingual translation model released by Tang et al. (2020). Since a multilingual version of D E B ERTA is not available, we replace it with mBART (Liu et al., 2020c);13 however, as mBART does not support relative positions, to inject the knowledge that w ˜j means δj , we first prepend w ˜j to each δj ∈ ∆w˜i , and then concatenate them right after the candidate definitions. We compare ConSeC against three systems included in Pasini et al. (2021), namely EWISER, XLMR-Large14 and SyntagRank (Scozzafava et al., 2020); EWISER and XLMR-Large are supervised systems, while SyntagRank is an unsupervised knowledge-based approach that builds upon syntagmatic relations (Maru et al., 2019). For both EWISER and XLMR-Large, we only consider the zero-shot scenario the authors illustrate, as it is the 13 Details on the parameters used in Appendix B. We consider the XLMR-Large architecture as it achieved the best average results on all the languages of the framework. 1498 14 Test ALL ∆ No Context Definitions Adversarial Teacher Forcing 80.7 80.0 82.5 -1.3 -2.0 +0.5 Table 5: Behavioral tests on C ON S E C. The ∆ column"
2021.emnlp-main.112,W04-0811,0,0.069356,"components (Section 4.2). We then proceed to investigate how C ON S E C fares in the cross-lingual setting (Section 4.3). 4.1 English WSD Data We evaluate C ON S E C on English all-words WSD through the framework presented by Raganato et al. (2017a), using SemCor (Miller et al., 1993) as the training corpus. Following established practices in the WSD literature (Raganato et al., 2017b; Huang et al., 2019; Blevins and Zettlemoyer, 2020), we perform model selection on SemEval-2007 (Pradhan et al., 2007, SE07), while carrying out testing on Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3), SemEval-2013 (Navigli et al., 2013, SE13) and SemEval-2015 (Moro and Navigli, 2015, SE15). As in previous works, we report the F1 score WSD systems achieve on each of these evaluation datasets and on their concatenation (ALL). In order to have a better picture of models’ performances and generalization power, we also consider the five synthetic datasets introduced by Barba et al. (2021a), namely: i) MFS, containing all the instances in ALL where the target word is tagged with its most frequent sense5 ; ii) LFS, containing all the instances in ALL annotated with a least frequent sense o"
2021.emnlp-main.112,2019.gwc-1.14,0,0.0140317,"ceding and subsequent sentence. Comparison Systems We compare C ON S E C with two common baselines in the WSD literature, namely i) MFS-SemCor, where target words are disambiguated by simply emitting their most frequent sense in SemCor, and ii) BERT-base, which employs a linear classifier over WordNet senses on top of frozen BERT representations (Devlin et al., 2019; Blevins and Zettlemoyer, 2020). Furthermore, to contextualize C ON S E C performances in the current landscape of English WSD, we further consider a number of recent state-of-theart systems and evaluate our approach against: SVC (Vial et al., 2019), which leverages WordNet relations to compress the output vocabulary and compensate for the lack of annotated data; ARES (Scarlini et al., 2020), a nearest-neighbor approach based on sense embeddings; GlossBERT (Huang et al., 2019), BEM (Blevins and Zettlemoyer, 2020), EWISER (Bevilacqua and Navigli, 2020)9 , WMLC (Conia and Navigli, 2021) and ESCHER (Barba et al., 2021a), all of which are supervised systems that exploit sense definitions or relational knowledge to better model the meaning of words. Finally, following the trend of augmenting training data with WNGE, we also consider how C ON"
2021.emnlp-main.715,2021.emnlp-main.112,1,0.865244,"Missing"
2021.emnlp-main.715,2020.acl-main.255,1,0.942351,"NLP/ neural-pagerank-wsd. 1 Introduction Zettlemoyer, 2020). These methods, thanks, inter alia, to the game-changing effect of pretrained language models, have widened the margin over so-called knowledge-based approaches (Agirre and Soroa, 2009; Moro et al., 2014; Scozzafava et al., 2020), which usually disambiguate using only global graph information – a source of information that, however, is not easy to integrate explictly into supervised WSD. Although there have been a few successful approaches integrating graphs into a standard neural classification architecture (Conia and Navigli, 2021; Bevilacqua and Navigli, 2020), such methods only exploit the local relational structure, leaving the global structure unused. Thus, while the model is able to directly utilize the explicit knowledge that a cairn is a terrier, it is not able to also utilize the fact that, following the hyponymy relation, a cairn is also a dog. In this paper we propose a method for integrating global graph information into neural supervised WSD through a Personalized PageRank (Page et al., 1999, PPR) approximation, blurring the distinction between knowledge-based and supervised methods. We achieve this by generalizing the logit aggregation"
2021.emnlp-main.715,2020.acl-main.95,0,0.680111,"a token, and I(·) denotes the set of all WordNet synsets. At test time, the predicted synset sˆi is the one with the highest probability, searching only among the set of synsets that are consistent with the current token (I(ti )): sˆi = argmax P (s0 |ti ) (2) s0 ∈I(ti ) This formulation is very straightforward, but also wasteful, as scores are computed for “impossible” synsets, i.e. those 6∈ I(ti ). 2.2 EWISER Recently, there has been a trend towards the inclusion of knowledge from the WordNet sense inventory in WSD. While many successful approaches have exploited glosses (Huang et al., 2019; Blevins and Zettlemoyer, 2020), the use of relational information, i.e. the graph structure of WordNet, has mostly been exploited by so-called knowledgebased algorithms, i.e., those that make use of no corpus supervision at all. However, one recent approach (Bevilacqua and Navigli, 2020, EWISER) has made the use of graph relations part of its core method: it employs trainable edge weights to aggregate scores of related synsets together, thus taking advantage of the scores over the whole vocabulary: where Ein (s) is the set of all the synsets s0 ∈ V (i.e., the set of nodes), such that there is an edge from s0 to s, and w(s0"
2021.emnlp-main.715,2021.eacl-main.286,1,0.776676,"tps://github.com/SapienzaNLP/ neural-pagerank-wsd. 1 Introduction Zettlemoyer, 2020). These methods, thanks, inter alia, to the game-changing effect of pretrained language models, have widened the margin over so-called knowledge-based approaches (Agirre and Soroa, 2009; Moro et al., 2014; Scozzafava et al., 2020), which usually disambiguate using only global graph information – a source of information that, however, is not easy to integrate explictly into supervised WSD. Although there have been a few successful approaches integrating graphs into a standard neural classification architecture (Conia and Navigli, 2021; Bevilacqua and Navigli, 2020), such methods only exploit the local relational structure, leaving the global structure unused. Thus, while the model is able to directly utilize the explicit knowledge that a cairn is a terrier, it is not able to also utilize the fact that, following the hyponymy relation, a cairn is also a dog. In this paper we propose a method for integrating global graph information into neural supervised WSD through a Personalized PageRank (Page et al., 1999, PPR) approximation, blurring the distinction between knowledge-based and supervised methods. We achieve this by gene"
2021.emnlp-main.715,S01-1001,0,0.787908,"Missing"
2021.emnlp-main.715,D19-1533,0,0.0123352,"ted by su- In this section we explain how our method, building pervised methods (Yap et al., 2020; Blevins and on top of previous approaches to WSD. 9092 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9092–9098 c November 7–11, 2021. 2021 Association for Computational Linguistics 2.1 Neural WSD The most popular formulation for WSD employs token-level classifiers, which encode each token ti into a vector ei using a sequence classifier, e.g. an LSTM (Raganato et al., 2017b) or, more recently, pre-trained Transformer-based contextualized embeddings (Hadiwinoto et al., 2019). The vector is then fed to one or more feedforward (FFN) layers, producing a softmax-normalized probability distribution over all the output classes, i.e., all the synsets (groups of senses sharing the same meaning) in WordNet: exp(FFN(ei ))s s0 ∈I(·) exp(FFN(ei ))s0 ) P (s|ti ) = P (1) = softmax(FFN(ei ))s where I is an inventory function that returns the set of possible synsets for a token, and I(·) denotes the set of all WordNet synsets. At test time, the predicted synset sˆi is the one with the highest probability, searching only among the set of synsets that are consistent with the curre"
2021.emnlp-main.715,D19-1355,0,0.0810625,"ossible synsets for a token, and I(·) denotes the set of all WordNet synsets. At test time, the predicted synset sˆi is the one with the highest probability, searching only among the set of synsets that are consistent with the current token (I(ti )): sˆi = argmax P (s0 |ti ) (2) s0 ∈I(ti ) This formulation is very straightforward, but also wasteful, as scores are computed for “impossible” synsets, i.e. those 6∈ I(ti ). 2.2 EWISER Recently, there has been a trend towards the inclusion of knowledge from the WordNet sense inventory in WSD. While many successful approaches have exploited glosses (Huang et al., 2019; Blevins and Zettlemoyer, 2020), the use of relational information, i.e. the graph structure of WordNet, has mostly been exploited by so-called knowledgebased algorithms, i.e., those that make use of no corpus supervision at all. However, one recent approach (Bevilacqua and Navigli, 2020, EWISER) has made the use of graph relations part of its core method: it employs trainable edge weights to aggregate scores of related synsets together, thus taking advantage of the scores over the whole vocabulary: where Ein (s) is the set of all the synsets s0 ∈ V (i.e., the set of nodes), such that there i"
2021.emnlp-main.715,P19-1569,0,0.015747,"opicsensitive PageRank (Haveliwala, 2002) instead of PPR, and apply it to the task of node classification in citation graphs. Differently from them, however, we exploit the graph structure of the output, not that of the input. 3 Experiments Setting We evaluate our proposed addition to the WSD task by employing it on top of the simple feedforward classifier baseline (taking as input frozen BERT large embeddings) used by Conia and Navigli (2021). We train the model with vanilla categorical cross-entropy. Following Bevilacqua and Navigli (2020) we use SensEmBERT (Scarlini et al., 2020) and LMMS (Loureiro and Jorge, 2019) embeddings to initialize the output embeddings (i.e., the last transformation matrix of the FFN) for, respectively, nominal and all other synsets. The evaluation measure is the F1 score. We compare against EWISER, but also report results for other recent high-performing methods from the literature, both sequence-tagging (Huang et al., 2019; Yap et al., 2020) and token-tagging (Blevins and Zettlemoyer, 2020; Conia and Navigli, 2021).1 Data As usual in the WSD literature, we train our model on SemCor (Miller et al., 1994), i.e. the largest available manually semantically annotated corpus; follo"
2021.emnlp-main.715,D19-1359,1,0.827537,"djectives (A), adverbs (R). Models grouped in the same row block are mutually comparable: 1) models trained on just SemCor, 2) models trained on SemCor and WNTG. ∗ : computed from the reported results. ? : highest F1 that is statistically different from the best one (McNemar’s test with p = 0.05). from WordNet, i.e., hyponymy, hypernyms, similarity, derivationally related, and verb group. The weight As0 ,s from synset s0 to s is initialized as 1/|Ein (s) |where Ein (s) is the set of all s0 s.t. the edge hs0 , si is in the graph. Additionally, we experiment with including edges from SyntagNet (Maru et al., 2019), a resource that includes edges representing semantic collocations.4 Collocational information is orthogonal to that contained in WordNet, providing paths between regions of the WordNet graph that would otherwise be distant or disconnected. Finally, to check whether it is feasible to include global information by precomputing the PPR instead of approximating it in the network forward pass, we experiment with a baseline approach where we directly initialize ATs with a PPR distribution, built using WordNet as the starting graph and a one-hot vector z (with zs = 1) as personalization. To keep A"
2021.emnlp-main.715,H94-1046,0,0.500642,"Missing"
2021.emnlp-main.715,S15-2049,1,0.926344,"Missing"
2021.emnlp-main.715,Q14-1019,1,0.922764,"Missing"
2021.emnlp-main.715,S13-2040,1,0.912199,"Missing"
2021.emnlp-main.715,S07-1016,0,0.0744607,"Missing"
2021.emnlp-main.715,2021.acl-long.406,0,0.0430691,"Missing"
2021.emnlp-main.715,2020.findings-emnlp.4,0,0.172554,"ng assumption that word meaning can be discretized in a finite number of classes, thus casting polysemy resolution as a multi-class classification problem, where the classes, i.e., the senses, are specific to a word. Senses are registered in a dictionarylike resource called the sense inventory. In English WSD the sense inventory is virtually always WordNet (Miller et al., 1990), which, for example, lists 2 Method separately the fish and musical instrument senses of the word “bass”. The WSD task is currently dominated by su- In this section we explain how our method, building pervised methods (Yap et al., 2020; Blevins and on top of previous approaches to WSD. 9092 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9092–9098 c November 7–11, 2021. 2021 Association for Computational Linguistics 2.1 Neural WSD The most popular formulation for WSD employs token-level classifiers, which encode each token ti into a vector ei using a sequence classifier, e.g. an LSTM (Raganato et al., 2017b) or, more recently, pre-trained Transformer-based contextualized embeddings (Hadiwinoto et al., 2019). The vector is then fed to one or more feedforward (FFN) layers, produci"
2021.emnlp-main.715,E17-1010,1,0.894196,"parately the fish and musical instrument senses of the word “bass”. The WSD task is currently dominated by su- In this section we explain how our method, building pervised methods (Yap et al., 2020; Blevins and on top of previous approaches to WSD. 9092 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9092–9098 c November 7–11, 2021. 2021 Association for Computational Linguistics 2.1 Neural WSD The most popular formulation for WSD employs token-level classifiers, which encode each token ti into a vector ei using a sequence classifier, e.g. an LSTM (Raganato et al., 2017b) or, more recently, pre-trained Transformer-based contextualized embeddings (Hadiwinoto et al., 2019). The vector is then fed to one or more feedforward (FFN) layers, producing a softmax-normalized probability distribution over all the output classes, i.e., all the synsets (groups of senses sharing the same meaning) in WordNet: exp(FFN(ei ))s s0 ∈I(·) exp(FFN(ei ))s0 ) P (s|ti ) = P (1) = softmax(FFN(ei ))s where I is an inventory function that returns the set of possible synsets for a token, and I(·) denotes the set of all WordNet synsets. At test time, the predicted synset sˆi is the one w"
2021.emnlp-main.715,D17-1120,1,0.834618,"parately the fish and musical instrument senses of the word “bass”. The WSD task is currently dominated by su- In this section we explain how our method, building pervised methods (Yap et al., 2020; Blevins and on top of previous approaches to WSD. 9092 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9092–9098 c November 7–11, 2021. 2021 Association for Computational Linguistics 2.1 Neural WSD The most popular formulation for WSD employs token-level classifiers, which encode each token ti into a vector ei using a sequence classifier, e.g. an LSTM (Raganato et al., 2017b) or, more recently, pre-trained Transformer-based contextualized embeddings (Hadiwinoto et al., 2019). The vector is then fed to one or more feedforward (FFN) layers, producing a softmax-normalized probability distribution over all the output classes, i.e., all the synsets (groups of senses sharing the same meaning) in WordNet: exp(FFN(ei ))s s0 ∈I(·) exp(FFN(ei ))s0 ) P (s|ti ) = P (1) = softmax(FFN(ei ))s where I is an inventory function that returns the set of possible synsets for a token, and I(·) denotes the set of all WordNet synsets. At test time, the predicted synset sˆi is the one w"
2021.emnlp-main.715,2020.acl-demos.6,1,0.911489,"Missing"
2021.emnlp-main.715,W04-0811,0,0.18122,"Missing"
2021.emnlp-main.79,D19-1352,0,0.021052,"ng et al., 2013); the second, instead, focuses on learning an estimator for the relevance of a document with respect to a query (Guo et al., 2016). More recently, with the advent of transformer-based language models such as BERT (Devlin et al., 2019), contextualized representations rapidly got incorporated into retrieval models (MacAvaney et al., 2019) — which previously had relied on static embeddings only — mainly by pairing contextualized models with a binary classifier to compute a score per query-document (MacAvaney et al., 2019; Nogueira and Cho, 2019) or query-sentence pair (Akkalyoncu Yilmaz et al., 2019; Dai and Callan, 2019). Nevertheless, most of the supervised works focused on the English retrieval task, where enough labeled data are available to train a neural model. Instead, while datasets in languages other than English do exist in several tracks of TREC (Braschler et al., 2000; Oard and Gey, 2002) or CLEF (Braschler, 2003), they are rather small and not suitable for training deep neural networks. In this setting, multilingual pretrained language models came out as an effective solution, and showed themselves able to successfully leverage annotations in one language (typically English)"
2021.emnlp-main.79,2021.naacl-main.371,1,0.891007,"glish, where the lack of estimates the relevance of a document to a given training data hinders the use of machine learning 1030 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1030–1041 c November 7–11, 2021. 2021 Association for Computational Linguistics in the multilingual setting. Recently, Word Sense Disambiguation (WSD) has received greatly increased attention (Bevilacqua et al., 2021), reporting large improvements not only in English (Lacerra et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Conia and Navigli, 2021; Barba et al., 2021a), but also across other languages (Scarlini et al., 2020; Procopio et al., 2021). We argue that word senses, thanks to their glosses, i.e., sentences defining word meanings, can provide valuable information to enrich the input query and to aid retrieving relevant documents that are semantically related. Moreover, multilingual sense vocabularies (where concepts are lexicalized with synonymous words in different languages) may provide a bridge across languages, leading neural models to perform better in a zeroshot setting. Based on these hypotheses, this paper makes the following contributions"
2021.emnlp-main.79,2021.emnlp-main.112,1,0.773172,"glish, where the lack of estimates the relevance of a document to a given training data hinders the use of machine learning 1030 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1030–1041 c November 7–11, 2021. 2021 Association for Computational Linguistics in the multilingual setting. Recently, Word Sense Disambiguation (WSD) has received greatly increased attention (Bevilacqua et al., 2021), reporting large improvements not only in English (Lacerra et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Conia and Navigli, 2021; Barba et al., 2021a), but also across other languages (Scarlini et al., 2020; Procopio et al., 2021). We argue that word senses, thanks to their glosses, i.e., sentences defining word meanings, can provide valuable information to enrich the input query and to aid retrieving relevant documents that are semantically related. Moreover, multilingual sense vocabularies (where concepts are lexicalized with synonymous words in different languages) may provide a bridge across languages, leading neural models to perform better in a zeroshot setting. Based on these hypotheses, this paper makes the following contributions"
2021.emnlp-main.79,2020.acl-main.255,1,0.914252,"em is the ranking module, which languages other than English, where the lack of estimates the relevance of a document to a given training data hinders the use of machine learning 1030 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1030–1041 c November 7–11, 2021. 2021 Association for Computational Linguistics in the multilingual setting. Recently, Word Sense Disambiguation (WSD) has received greatly increased attention (Bevilacqua et al., 2021), reporting large improvements not only in English (Lacerra et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Conia and Navigli, 2021; Barba et al., 2021a), but also across other languages (Scarlini et al., 2020; Procopio et al., 2021). We argue that word senses, thanks to their glosses, i.e., sentences defining word meanings, can provide valuable information to enrich the input query and to aid retrieving relevant documents that are semantically related. Moreover, multilingual sense vocabularies (where concepts are lexicalized with synonymous words in different languages) may provide a bridge across languages, leading neural models to perform better in a zeroshot setting. Based on these hypotheses,"
2021.emnlp-main.79,P08-1018,0,0.0583604,"i et al., 2020). However, by relying on large pretrained language models, these approaches assume that queries are expressive enough to model their underlying semantics, which is not always the case. Our findings show that word definitions are indeed This is a long-standing issue in IR, and one which beneficial to the task, allowing SIR to better con- has stimulated extensive research for years. Differtextualize queries and thus match more relevant ent approaches to query expansion such as Markov documents in respect of all its baselines. chains (Metzler and Croft, 2007), term classification (Cao et al., 2008), and static word embeddings 2 Related Work (Diaz et al., 2016; Zamani and Croft, 2016) have Information Retrieval approaches have long relied been applied effectively to improve query repreon simple statistical metrics based on term fre- sentation. More recently, researchers have tried to quency, such as TF-IDF and BM25 (Robertson tackle the problem from the opposite perspective et al., 1996), to represent texts and to match doc- by expanding documents (Nogueira et al., 2019; uments against a given query. These methods are Nogueira and Lin, 2019; Raffel et al., 2020) or still used as strong b"
2021.emnlp-main.79,2021.eacl-main.286,1,0.651424,"h languages other than English, where the lack of estimates the relevance of a document to a given training data hinders the use of machine learning 1030 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1030–1041 c November 7–11, 2021. 2021 Association for Computational Linguistics in the multilingual setting. Recently, Word Sense Disambiguation (WSD) has received greatly increased attention (Bevilacqua et al., 2021), reporting large improvements not only in English (Lacerra et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Conia and Navigli, 2021; Barba et al., 2021a), but also across other languages (Scarlini et al., 2020; Procopio et al., 2021). We argue that word senses, thanks to their glosses, i.e., sentences defining word meanings, can provide valuable information to enrich the input query and to aid retrieving relevant documents that are semantically related. Moreover, multilingual sense vocabularies (where concepts are lexicalized with synonymous words in different languages) may provide a bridge across languages, leading neural models to perform better in a zeroshot setting. Based on these hypotheses, this paper makes the fol"
2021.emnlp-main.79,2020.acl-main.747,0,0.0654369,"th an underlying formal modeling such as the Vector Space Model, probabilistic models and, more recently, neural models (Guo et al., 2020). Lately, IR systems have begun taking advantage of these latter models, whose aim is learning continuous representations capable of grasping the semantics of the text, as opposed to the traditional lexical approaches comprising the bag-of-words representation. In this new line of research, following the success of neural models in several Natural Language Processing (NLP) tasks, researchers employed contextualized word representations (Devlin et al., 2019; Conneau et al., 2020) in IR to capture semantic aspects of texts (for query and documents) which prove beneficial to ranking approaches (MacAvaney et al., 2019, 2020b). Moreover, thanks to the unsupervised training strategy of contextualized language models, i.e., Masked Language Modeling, it is feasible to train multilingual models which are able to encode sentences across languages within the same semantic space. Nonetheless, there are challenges peculiar to IR that may hinder the effectiveness of contextualized embeddings. For example, queries are typically 1 Introduction composed of just a few keywords, which"
2021.emnlp-main.79,N19-1423,0,0.447183,"tion that complies with an underlying formal modeling such as the Vector Space Model, probabilistic models and, more recently, neural models (Guo et al., 2020). Lately, IR systems have begun taking advantage of these latter models, whose aim is learning continuous representations capable of grasping the semantics of the text, as opposed to the traditional lexical approaches comprising the bag-of-words representation. In this new line of research, following the success of neural models in several Natural Language Processing (NLP) tasks, researchers employed contextualized word representations (Devlin et al., 2019; Conneau et al., 2020) in IR to capture semantic aspects of texts (for query and documents) which prove beneficial to ranking approaches (MacAvaney et al., 2019, 2020b). Moreover, thanks to the unsupervised training strategy of contextualized language models, i.e., Masked Language Modeling, it is feasible to train multilingual models which are able to encode sentences across languages within the same semantic space. Nonetheless, there are challenges peculiar to IR that may hinder the effectiveness of contextualized embeddings. For example, queries are typically 1 Introduction composed of just"
2021.emnlp-main.79,P16-1035,0,0.0295005,"age models, these approaches assume that queries are expressive enough to model their underlying semantics, which is not always the case. Our findings show that word definitions are indeed This is a long-standing issue in IR, and one which beneficial to the task, allowing SIR to better con- has stimulated extensive research for years. Differtextualize queries and thus match more relevant ent approaches to query expansion such as Markov documents in respect of all its baselines. chains (Metzler and Croft, 2007), term classification (Cao et al., 2008), and static word embeddings 2 Related Work (Diaz et al., 2016; Zamani and Croft, 2016) have Information Retrieval approaches have long relied been applied effectively to improve query repreon simple statistical metrics based on term fre- sentation. More recently, researchers have tried to quency, such as TF-IDF and BM25 (Robertson tackle the problem from the opposite perspective et al., 1996), to represent texts and to match doc- by expanding documents (Nogueira et al., 2019; uments against a given query. These methods are Nogueira and Lin, 2019; Raffel et al., 2020) or still used as strong baselines nowadays (Lin, 2019), single passages (MacAvaney et a"
2021.emnlp-main.79,2020.emnlp-main.285,1,0.899174,"ocument to a given training data hinders the use of machine learning 1030 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1030–1041 c November 7–11, 2021. 2021 Association for Computational Linguistics in the multilingual setting. Recently, Word Sense Disambiguation (WSD) has received greatly increased attention (Bevilacqua et al., 2021), reporting large improvements not only in English (Lacerra et al., 2020; Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Conia and Navigli, 2021; Barba et al., 2021a), but also across other languages (Scarlini et al., 2020; Procopio et al., 2021). We argue that word senses, thanks to their glosses, i.e., sentences defining word meanings, can provide valuable information to enrich the input query and to aid retrieving relevant documents that are semantically related. Moreover, multilingual sense vocabularies (where concepts are lexicalized with synonymous words in different languages) may provide a bridge across languages, leading neural models to perform better in a zeroshot setting. Based on these hypotheses, this paper makes the following contributions: 1. we introduce, for the first time, a neural approach t"
2021.emnlp-main.79,P10-1023,1,0.533645,"a topic composed of a title and a description. More formally, let Qtitle = [t1 , . . . , tn ] be the sequence of n terms of the topic title,1 Qdesc = [d1 , . . . , dl ] the sequence of l words describing the topic, and C a collection of documents. The retrieval task we focus on consists of learning a scoring function Sθ (Q, D) ∀D ∈ C, to rank documents in the collection according to their relevance to the query Q, where Q = Qtitle ||Qdesc = [q1 , . . . , qn+l ], i.e., the concatenation of Qtitle and Qdesc and θ denotes model parameters. 3.2 Resources In our approach we make use of BabelNet2 (Navigli and Ponzetto, 2010; Navigli et al., 2021) as vocabulary of senses. BabelNet is a multilingual knowledge base, which organizes word meanings — namely senses — into synsets, i.e., sets of synonyms that express a common concept in different languages (up to 500). Each synset within BabelNet is associated with different glosses in multiple languages3 that describe its meaning. 4 SIR 4.1 Motivation While previous works have focused on expanding the query with related terms in a two-pass reranking procedure, we argue that providing sense definitions related to the input query would be more effective for injecting sem"
2021.emnlp-main.79,2020.findings-emnlp.249,0,0.0310649,"ain a neural model. Instead, while datasets in languages other than English do exist in several tracks of TREC (Braschler et al., 2000; Oard and Gey, 2002) or CLEF (Braschler, 2003), they are rather small and not suitable for training deep neural networks. In this setting, multilingual pretrained language models came out as an effective solution, and showed themselves able to successfully leverage annotations in one language (typically English) and perform retrieval in other languages, e.g., Arabic, Mandarin, and Spanish (MacAvaney et al., 2020b) or Chinese, Arabic, French, Hindi and Bengali (Shi et al., 2020). However, by relying on large pretrained language models, these approaches assume that queries are expressive enough to model their underlying semantics, which is not always the case. Our findings show that word definitions are indeed This is a long-standing issue in IR, and one which beneficial to the task, allowing SIR to better con- has stimulated extensive research for years. Differtextualize queries and thus match more relevant ent approaches to query expansion such as Markov documents in respect of all its baselines. chains (Metzler and Croft, 2007), term classification (Cao et al., 200"
2021.emnlp-main.79,P12-1029,0,0.313813,"ines it separately with a relevant (D+) and a non relevant (D-) document and is trained to optimize pairwise cross-entropy loss. Word Sense Disambiguation (WSD) is specifically tailored to resolve this issue, and several attempts were made in the past to include word senses within IR pipelines. These early attempts, unfortunately, did not produce encouraging results (Krovetz and Croft, 1992; Voorhees, 1993; Sanderson, 1994). Indeed, Sanderson (2000) emphasised that the effectiveness of WSD integration was diminished by the inaccuracies in disambiguation. A little over a decade later, instead, Zhong and Ng (2012) presented a successful application of WSD in IR by incorporating word senses and synonym relations into a language modeling approach. In addition, further developments over the years led to the remarkable performance attained by modern WSD models, which now perform close to the inter-annotator agreement upper bound (Blevins and Zettlemoyer, 2020; Bevilacqua and Navigli, 2020; Barba et al., 2021a,b). This makes us optimistic that these models are finally suitable to be used within downstream tasks. Differently from previous works, in this paper, we explore this possibility and focus on enrichi"
2021.emnlp-main.844,D18-1523,0,0.0151053,"Although it is not explicitly required by either of the two tasks, a good substitution system is expected to capture the semantics of its input and implicitly perform a soft disambiguation. For example, denoting bright as target word in the context sentence &quot;She is a bright student&quot;, we expect a good substitution system to provide a set of substitutes closer to {intelligent, clever, smart} than to {luminous, clear, light}. Thanks to this implicit disambiguation capability, lexical substitution has shown its usefulness in several scenarios, such as word sense induction (Ba¸skaya et al., 2013; Amrami and Goldberg, 2018; Arefyev et al., 2019), data augmentation (Jia et al., 2019; Arefyev et al., 2020), word sense disambiguation (Hou et al., 2020) and semantic role labeling (Bingel et al., 2018). However, despite having been employed in numerous downstream tasks, the lexical substitution task still presents unresolved issues that need to be addressed. First, the shortage of large-scale corpora annotated with the expected substitutes hinders the use of supervised techniques, including powerful Transformer-based language models, thus leaving the task in a possibly sub-optimal setting. Second, the evaluation met"
2021.emnlp-main.844,R19-1008,0,0.0824669,"tly required by either of the two tasks, a good substitution system is expected to capture the semantics of its input and implicitly perform a soft disambiguation. For example, denoting bright as target word in the context sentence &quot;She is a bright student&quot;, we expect a good substitution system to provide a set of substitutes closer to {intelligent, clever, smart} than to {luminous, clear, light}. Thanks to this implicit disambiguation capability, lexical substitution has shown its usefulness in several scenarios, such as word sense induction (Ba¸skaya et al., 2013; Amrami and Goldberg, 2018; Arefyev et al., 2019), data augmentation (Jia et al., 2019; Arefyev et al., 2020), word sense disambiguation (Hou et al., 2020) and semantic role labeling (Bingel et al., 2018). However, despite having been employed in numerous downstream tasks, the lexical substitution task still presents unresolved issues that need to be addressed. First, the shortage of large-scale corpora annotated with the expected substitutes hinders the use of supervised techniques, including powerful Transformer-based language models, thus leaving the task in a possibly sub-optimal setting. Second, the evaluation metrics provided for the t"
2021.emnlp-main.844,2020.coling-main.107,0,0.354106,"system is expected to capture the semantics of its input and implicitly perform a soft disambiguation. For example, denoting bright as target word in the context sentence &quot;She is a bright student&quot;, we expect a good substitution system to provide a set of substitutes closer to {intelligent, clever, smart} than to {luminous, clear, light}. Thanks to this implicit disambiguation capability, lexical substitution has shown its usefulness in several scenarios, such as word sense induction (Ba¸skaya et al., 2013; Amrami and Goldberg, 2018; Arefyev et al., 2019), data augmentation (Jia et al., 2019; Arefyev et al., 2020), word sense disambiguation (Hou et al., 2020) and semantic role labeling (Bingel et al., 2018). However, despite having been employed in numerous downstream tasks, the lexical substitution task still presents unresolved issues that need to be addressed. First, the shortage of large-scale corpora annotated with the expected substitutes hinders the use of supervised techniques, including powerful Transformer-based language models, thus leaving the task in a possibly sub-optimal setting. Second, the evaluation metrics provided for the task are bound to the test vocabulary, hence they fail to cap"
2021.emnlp-main.844,S13-2050,0,0.0692056,"Missing"
2021.emnlp-main.844,biemann-2012-turk,0,0.0290793,"t. 2010 sentences with a single target word per Earlier methods made use of external knowledge sentence, including around 200 distinct targets. bases such as WordNet (Miller, 1995) to extract Each instance is associated with several substitutes possible substitutes and construct delexicalized fea- that were chosen by five native English speaker tures (Szarvas et al., 2013), or they employed word annotators. The small coverage of LST led to embeddings to represent both the target and the the creation of the Turk bootstrap Word Sense substitutes in their context and rank them through Inventory (Biemann, 2012, TWSI), a first attempt ad-hoc metrics (Melamud et al., 2015, 2016). How- to collect a large-scale dataset. The author deever, the recent spread of pre-trained language mod- ployed Amazon Mechanical Turk to annotate 25K els has deeply reshaped approaches to lexical sub- sentences from Wikipedia, which, however, only stitution, standardizing the use of contextualized cover noun targets. To overcome this shortcoming, word representations to provide a context-aware Kremer et al. (2014) proposed Concept In Context distribution over the output vocabulary. The first (CoInCo), a dataset of 2474 sent"
2021.emnlp-main.844,C18-1021,0,0.0259368,"uation. For example, denoting bright as target word in the context sentence &quot;She is a bright student&quot;, we expect a good substitution system to provide a set of substitutes closer to {intelligent, clever, smart} than to {luminous, clear, light}. Thanks to this implicit disambiguation capability, lexical substitution has shown its usefulness in several scenarios, such as word sense induction (Ba¸skaya et al., 2013; Amrami and Goldberg, 2018; Arefyev et al., 2019), data augmentation (Jia et al., 2019; Arefyev et al., 2020), word sense disambiguation (Hou et al., 2020) and semantic role labeling (Bingel et al., 2018). However, despite having been employed in numerous downstream tasks, the lexical substitution task still presents unresolved issues that need to be addressed. First, the shortage of large-scale corpora annotated with the expected substitutes hinders the use of supervised techniques, including powerful Transformer-based language models, thus leaving the task in a possibly sub-optimal setting. Second, the evaluation metrics provided for the task are bound to the test vocabulary, hence they fail to capture the quality of substitutes outside the vocabulary; moreover, the vocabulary is usually sma"
2021.emnlp-main.844,2020.emnlp-main.195,1,0.735776,"nity. Indeed, pre-trained 10811 models such as BART (Lewis et al., 2020) suit a wide range of NLP applications. Thanks to the flexibility of seq2seq learning, these models can be easily adapted to different tasks, including sequence and token classification or sequence generation, inter alia. Interestingly, generative models have also been employed in tasks that are not usually formulated as sequence-to-sequence learning; for example, there have been effective applications of seq2seq architectures to definition modeling (Bevilacqua et al., 2020), cross-lingual Abstract Meaning Representation (Blloshmi et al., 2020), end-to-end Semantic Role Labeling (Blloshmi et al., 2021) and Semantic Parsing (Procopio et al., 2021; Bevilacqua et al., 2021a). Inspired by these successful applications of generative approaches we here propose applying, for the first time, a generative seq2seq model to the lexical substitution task. Differently from previous approaches in the field, we finetune a pre-trained model to produce substitutes starting from a word in its context. Moreover, our method can be used to generate silver data for the lexical substitution task, reducing the lack of annotated data. 3 G ENE S IS The task"
2021.emnlp-main.844,P19-1569,0,0.0193621,"74k and so on. The final dataset, that includes all the SemCor sentences for which at least one substitute has been generated, is identified by G EN S EM C OR. We highlight that, when training on G EN S EM C OR datasets, we use only silver data, without concatenating gold corpora. The properties of the gold and generated datasets are summarised in Table 1. Evaluation Metrics We evaluate the performance of our model using the metrics originally proposed for the task (McCarthy and Navigli, 5 These layers better capture the semantics than the last layer only, as shown in Devlin et al. (2019) and Loureiro and Jorge (2019), inter alia. 10814 Dataset CT T CT D LST G EN S EM C OR37k G EN S EM C OR74k G EN S EM C OR148k G EN S EM C OR targets contexts avg substitutes 4,301 297 205 37,172 3,095 2,003 6.6 6.8 4.0 9,423 12,726 17,714 22,435 37,000 67,231 137,497 220,237 4.2 3.4 3.4 4.2 Table 1: The number of targets, context sentences and average substitutes per instance in the gold (first block) and in the generated (second block) datasets. 2009), i.e., best and out-of-ten (oot), together with their mode variations. The best metric allows a system to produce as many substitutes as are considered useful, by dividing"
2021.emnlp-main.844,S07-1009,1,0.672456,"et and substitute. This combined representation is then used to obtain a ranking of substitutes from the XLNet vocabulary that is further refined with postprocessing. Despite the improvement in performances that large language models brought to the task, these methods work in a potentially sub-optimal setting, since they are used as feature extractors and are not finetuned, due to the paucity of large-scale annotated data (Garí Soler et al., 2019). Lexical Substitution Resources The first Lexical Substitution Approaches Since its pre- dataset released was the Lexical Substitution sentation by McCarthy and Navigli (2007), a vari- Task (LST), proposed as test set for the task ety of different approaches have been explored to by McCarthy and Navigli (2007). It contains produce the substitutes that better fit the context. 2010 sentences with a single target word per Earlier methods made use of external knowledge sentence, including around 200 distinct targets. bases such as WordNet (Miller, 1995) to extract Each instance is associated with several substitutes possible substitutes and construct delexicalized fea- that were chosen by five native English speaker tures (Szarvas et al., 2013), or they employed word a"
2021.emnlp-main.844,K16-1006,0,0.0487345,"Missing"
2021.emnlp-main.844,W15-1501,0,0.133485,"y small and often biased by the particular linguistic style and background of the annotators who developed the datasets.1 In this paper, we focus on substitutes prediction and address the above problems by proposing G EN E S IS, a generative approach to lexical substitution. We find that not only is this novel approach effecThe lexical substitution task (McCarthy and Navigli, 2009) requires a system to provide adequate replacements for a target word in a given context. Through the years, two lexical substitution variants have been proposed, i.e., candidates ranking and substitutes prediction (Melamud et al., 2015). While the former aims at ranking a list of prede1 fined candidate substitutes for a word in a given The benchmarks for lexical substitution were released context, the latter is more challenging, requiring more than ten years ago. 10810 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10810–10823 c November 7–11, 2021. 2021 Association for Computational Linguistics tive when tested on the lexical substitution task, but also that it can be applied to generate substitutes from raw text, enabling the effortless construction of large-scale silver data."
2021.emnlp-main.844,H93-1061,0,0.787986,"lexical substitution task. To this end, first, we finetune G ENE S IS on a gold dataset for the lexical substitution task; then, we give as input to the finetuned model the corpus C, generating as output a list of replacements for each input instance. The input instances, associated with the generated substitutes, constitute the silver corpus. To ensure the quality of the generated substitutes, we apply a similarity threshold λ on the ranking step of G ENE S IS (cf. Section 3), keeping only the substitutes whose similarity to the target is higher than λ. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense inventory4 . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense distribution is also beneficial for lexical substitution. Indeed, having a frequency of target words that 4 We use the version released at http://nlp. uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0. Model We use BART (Lewis et al., 2020) as seq2seq model, trained through the RAdam optimiser (lr 10−5 ); we train it for a ma"
2021.emnlp-main.844,2021.naacl-main.30,1,0.715297,"ations. Thanks to the flexibility of seq2seq learning, these models can be easily adapted to different tasks, including sequence and token classification or sequence generation, inter alia. Interestingly, generative models have also been employed in tasks that are not usually formulated as sequence-to-sequence learning; for example, there have been effective applications of seq2seq architectures to definition modeling (Bevilacqua et al., 2020), cross-lingual Abstract Meaning Representation (Blloshmi et al., 2020), end-to-end Semantic Role Labeling (Blloshmi et al., 2021) and Semantic Parsing (Procopio et al., 2021; Bevilacqua et al., 2021a). Inspired by these successful applications of generative approaches we here propose applying, for the first time, a generative seq2seq model to the lexical substitution task. Differently from previous approaches in the field, we finetune a pre-trained model to produce substitutes starting from a word in its context. Moreover, our method can be used to generate silver data for the lexical substitution task, reducing the lack of annotated data. 3 G ENE S IS The task of substitutes prediction requires finding replacements for a target word in a context that ideally do"
2021.emnlp-main.844,N13-1133,0,0.0307614,"ution sentation by McCarthy and Navigli (2007), a vari- Task (LST), proposed as test set for the task ety of different approaches have been explored to by McCarthy and Navigli (2007). It contains produce the substitutes that better fit the context. 2010 sentences with a single target word per Earlier methods made use of external knowledge sentence, including around 200 distinct targets. bases such as WordNet (Miller, 1995) to extract Each instance is associated with several substitutes possible substitutes and construct delexicalized fea- that were chosen by five native English speaker tures (Szarvas et al., 2013), or they employed word annotators. The small coverage of LST led to embeddings to represent both the target and the the creation of the Turk bootstrap Word Sense substitutes in their context and rank them through Inventory (Biemann, 2012, TWSI), a first attempt ad-hoc metrics (Melamud et al., 2015, 2016). How- to collect a large-scale dataset. The author deever, the recent spread of pre-trained language mod- ployed Amazon Mechanical Turk to annotate 25K els has deeply reshaped approaches to lexical sub- sentences from Wikipedia, which, however, only stitution, standardizing the use of context"
2021.emnlp-main.844,P19-1328,0,0.33555,"contextualized cover noun targets. To overcome this shortcoming, word representations to provide a context-aware Kremer et al. (2014) proposed Concept In Context distribution over the output vocabulary. The first (CoInCo), a dataset of 2474 sentences covering work in this direction was that of Garí Soler et al. 3874 distinct targets with diverse part-of-speech (2019), where ELMo (Peters et al., 2018) embed- tags. Each sentence has one or more targets, for a dings are used to rank substitutes according to their total of 15k instances annotated through Amazon cosine similarity to the target. In Zhou et al. (2019), Mechanical Turk. instead, the input context is represented through a BERT model (Devlin et al., 2019). The authors Generative Approaches Generative pre-trained partially mask the target word in its context, in or- language models such as GPT (Radford et al., der to obtain a representation that includes a faded 2018) have shown to be highly effective in Nattarget information; this representation is then used ural Language Generation, catching the attention to obtain a probability distribution over the BERT of the research community. Indeed, pre-trained 10811 models such as BART (Lewis et al.,"
2021.emnlp-main.844,N18-1202,0,0.0316043,"spread of pre-trained language mod- ployed Amazon Mechanical Turk to annotate 25K els has deeply reshaped approaches to lexical sub- sentences from Wikipedia, which, however, only stitution, standardizing the use of contextualized cover noun targets. To overcome this shortcoming, word representations to provide a context-aware Kremer et al. (2014) proposed Concept In Context distribution over the output vocabulary. The first (CoInCo), a dataset of 2474 sentences covering work in this direction was that of Garí Soler et al. 3874 distinct targets with diverse part-of-speech (2019), where ELMo (Peters et al., 2018) embed- tags. Each sentence has one or more targets, for a dings are used to rank substitutes according to their total of 15k instances annotated through Amazon cosine similarity to the target. In Zhou et al. (2019), Mechanical Turk. instead, the input context is represented through a BERT model (Devlin et al., 2019). The authors Generative Approaches Generative pre-trained partially mask the target word in its context, in or- language models such as GPT (Radford et al., der to obtain a representation that includes a faded 2018) have shown to be highly effective in Nattarget information; this"
2021.findings-emnlp.197,W13-2322,0,0.13953,"Missing"
2021.findings-emnlp.197,2020.emnlp-main.195,1,0.77576,"et al., 2021), annotation projection techniques, in the specific case of SRL, retain parallel annotations only when they satisfy specific constraints (e.g., same predicate-argument structure). These limitations can hinder the evaluation of cross-lingual transfer learning techniques on this task, since the benchmarks created contain only examples for which it is already known, by means of the DST assumption behind annotation projection techniques, that the same features are present in the source and the target languages, thereby omitting cases of translation divergences altogether (Dorr, 1994; Blloshmi et al., 2020) and evaluating only on a subset of cases for which the transfer from one sentence to another is direct. that can express the same meaning of predicates and arguments of the English gold annotations. 3 The U NITE D-SRL Dataset U NITE D-SRL consists of two parallel training sets in Chinese and English with 5, 503 and 3, 645 sentences, respectively. It also includes 2, 000 parallel sentences for Chinese, English, French and Spanish (1, 000 for the development and 1, 000 for the test set of each language). The overall statistics of the dataset including the number of sentences, the number of pred"
2021.findings-emnlp.197,W04-2412,0,0.109175,"Missing"
2021.findings-emnlp.197,W05-0620,0,0.121038,"r multilingual SRL, language-to-language comdard benchmarks for SRL, namely, CoNLL-2005, parisons are affected by the fact that each lanCoNLL-2008, CoNLL-2009 and CoNLL-2012. guage has its own dataset which differs from However, despite their widespread use, the the others in size, domains, sets of labels and CoNLL datasets suffer from a considerable level annotation guidelines. In this paper, we adof heterogeneity, which prevents systems from easdress this issue and propose U NITE D-SRL, ily scaling across task formulations and languages: a new benchmark for multilingual and crossCoNLL-2005 (Carreras and Màrquez, 2005) is delingual, span- and dependency-based SRL. U NITE D-SRL provides expert-curated paralvised for span-based SRL where systems are relel annotations using a common predicatequired to identify and classify argument spans, argument structure inventory, allowing direct whereas CoNLL-2009 (Hajiˇc et al., 2009) is framed comparisons across languages and encouragas a dependency-based task, where only the syntacing studies on cross-lingual transfer in SRL. tic head of an argument has to be tagged. Moreover, We release U NITE D-SRL v1.0 at https:// when multiple languages are covered, for examgithub."
2021.findings-emnlp.197,2021.naacl-main.31,1,0.508704,"Missing"
2021.findings-emnlp.197,2020.coling-main.120,1,0.69275,"SRL. In the following, we evaluate the performance of this system with two different pretrained language models, multilingual-BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), distinguishing between the results when their weights are left frozen or fine-tuned together with the rest of the system during training. We train each model configuration for 20 epochs using Adam (Kingma and Ba, 2015) with a learning rate that is initially warmed up to 10−5 for 1 epoch and then cooled down to 10−6 in 15 epochs. We leave the rest of the hyperparameter values as in the original paper of Conia and Navigli (2020). All the experimental details are reported in Appendix C. 4.2 Results on Sense Disambiguation In the following, we describe and discuss the results of CN-20 on predicate sense disambiguation, that is, the task of assigning the most appropriate sense to a predicate in context. We first focus on zeroshot cross-lingual predicate sense disambiguation and then show how even a small language-specific sample leads to significant improvements. 0-shot Cross-lingual Sense Disambiguation. Table 2 shows the results obtained by CN-20 on predicate sense disambiguation in different training settings. We obs"
2021.findings-emnlp.197,2020.acl-main.747,0,0.0542797,"linguistic resources, different domains and varying dataset sizes. To this end, we use U NITE D-SRL to train and evaluate a recently proposed state-of-the-art SRL system, showing how our dataset provides interesting insights into the cross-lingual transferability of predicate senses and their argument structures. language models, and iii) it has been shown to attain state-of-the-art results in both dependency- and span-based SRL. In the following, we evaluate the performance of this system with two different pretrained language models, multilingual-BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), distinguishing between the results when their weights are left frozen or fine-tuned together with the rest of the system during training. We train each model configuration for 20 epochs using Adam (Kingma and Ba, 2015) with a learning rate that is initially warmed up to 10−5 for 1 epoch and then cooled down to 10−6 in 15 epochs. We leave the rest of the hyperparameter values as in the original paper of Conia and Navigli (2020). All the experimental details are reported in Appendix C. 4.2 Results on Sense Disambiguation In the following, we describe and discuss the results of CN-20 on predica"
2021.findings-emnlp.197,2020.emnlp-main.321,0,0.0350731,"(like the financial domain of CoNLL-2009 in-domain English dataset). To ensure the heterogeneity of the textual material in U NITE D-SRL, we sampled documents belonging to the 10 most frequent domains of the UN corpus, following the domain distribution in the corpus. One of the main novelties of our dataset is the use of a new verbal resource: VerbAtlas4 (Di Fabio et al., 2019). This resource contains 13, 767 synsets from BabelNet5 (Navigli and Ponzetto, 2012; Navigli et al., 2021) manually clustered in around 400 On this line of research lies X-SRL, a recently introduced dataset proposed by Daza and Frank (2020). X-SRL is a multilingual parallel SRL corpus that is based on the English part of CoNLL2009 (Hajiˇc et al., 2009) for in-domain dependencybased SRL. The gold annotations of CoNLL-2009 in English have been translated using high-quality machine translation services into three target languages, namely, French, German and Spanish. Once a machine-translated parallel corpus has been created, mBERT (Devlin et al., 2019) is used to produce vector representations of text and to compute the similarity between source and target tokens. These embeddings are then used to align tokens and to transfer annot"
2021.findings-emnlp.197,de-marneffe-etal-2014-universal,0,0.0907508,"Missing"
2021.findings-emnlp.197,N19-1423,0,0.204729,"ets from BabelNet5 (Navigli and Ponzetto, 2012; Navigli et al., 2021) manually clustered in around 400 On this line of research lies X-SRL, a recently introduced dataset proposed by Daza and Frank (2020). X-SRL is a multilingual parallel SRL corpus that is based on the English part of CoNLL2009 (Hajiˇc et al., 2009) for in-domain dependencybased SRL. The gold annotations of CoNLL-2009 in English have been translated using high-quality machine translation services into three target languages, namely, French, German and Spanish. Once a machine-translated parallel corpus has been created, mBERT (Devlin et al., 2019) is used to produce vector representations of text and to compute the similarity between source and target tokens. These embeddings are then used to align tokens and to transfer annotations across different languages. The annotation of the training and development sets of this dataset is automatic while 3 https://conferences.unite.un.org/ the annotation of the test set is semi-automatic. In uncorpus 4 particular, annotators were asked to validate transhttp://verbatlas.org 5 lations and to mark in the target sentence tokens https://babelnet.org 2295 EN ES FR ZH The next triennial comprehensive"
2021.findings-emnlp.197,D19-1058,1,0.88266,"Missing"
2021.findings-emnlp.197,J94-4004,0,0.250745,"nnotation ∗ projection techniques to produce annotations for Work partially carried out while at the Sapienza University of Rome. other languages starting from English annotated 2293 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2293–2305 November 7–11, 2021. ©2021 Association for Computational Linguistics data. These approaches, however, tend to ignore the fact that ProbBank was conceived expressly for English predicates. Forcing the semantics of different languages to adapt to English, without considering possible translation divergences in parallel sentences (Dorr, 1994), can lead to two distinct problems: i) incorrect projections, if divergent sentences are retained in the dataset; ii) elimination from the dataset of all the sources of linguistic divergences, if divergent sentence pairs are discarded. Another issue of SRL annotation projection techniques regards the use of PropBank as verbal resource. This, in fact, limits the informativeness of the annotations, since it does not mark semantic roles with semanticallyconsistent labels, leading to ambiguous or unclear annotations. For example, in John is sleeping and John loves Mary, John would be tagged A RG"
2021.findings-emnlp.197,2021.eacl-main.181,0,0.0201,"3.65 3.59 Table 1: Overall statistics of the U NITE D-SRL dataset. Number of sentences (Sents), annotated predicates (Preds) and arguments (Args) and average number of predicates per sentence (P/S) in each split and for each language. yS and yT , where x and y are predicates and arguments, respectively. Another constraint of this model is that xT has to be a verb (verbal predicate). Even if, thanks to the progress made in machine translation, multilingual sentence embedding and multilingual language modeling the task of aligning spans of text in different languages has become quite effective (Dou and Neubig, 2021; Lacerra et al., 2021; Procopio et al., 2021), annotation projection techniques, in the specific case of SRL, retain parallel annotations only when they satisfy specific constraints (e.g., same predicate-argument structure). These limitations can hinder the evaluation of cross-lingual transfer learning techniques on this task, since the benchmarks created contain only examples for which it is already known, by means of the DST assumption behind annotation projection techniques, that the same features are present in the source and the target languages, thereby omitting cases of translation div"
2021.findings-emnlp.197,P18-1082,0,0.026367,"Missing"
2021.findings-emnlp.197,J02-3001,0,0.088226,"used, such as English Propositional Bank (Palmer et al., 2005, Semantic Role Labeling (SRL) – often considered PropBank), Chinese PropBank (Xue and Palmer, to be a fundamental step towards Natural Language Understanding (Navigli, 2018) – consists in recov- 2003) and AnCora (Taulé et al., 2008) for Spanish and Catalan, making it difficult to evaluate whether ering the latent predicate-argument structure of a a system is able to generalize across languages sentence by identifying the semantic relationship and, if so, to what extent. In fact, these multilinbetween a predicate and its arguments (Gildea and Jurafsky, 2002). SRL can be used to extract in- gual datasets are not aligned, they are considerably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution, strongly limiting cations in a wide range of Natural Language Pro- language-to-language comparisons. Some studies (Akbik et al., 2015; Daza and cessing (NLP) areas such as Machine Translation Frank, 2020) worked around this issue by elect(Marcheggiani et al., 2018), Question Answering (Shen and Lapata, 2007; He et al., 2015), V"
2021.findings-emnlp.197,W09-1201,0,0.529795,"Missing"
2021.findings-emnlp.197,D15-1076,0,0.0261291,"and Jurafsky, 2002). SRL can be used to extract in- gual datasets are not aligned, they are considerably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution, strongly limiting cations in a wide range of Natural Language Pro- language-to-language comparisons. Some studies (Akbik et al., 2015; Daza and cessing (NLP) areas such as Machine Translation Frank, 2020) worked around this issue by elect(Marcheggiani et al., 2018), Question Answering (Shen and Lapata, 2007; He et al., 2015), Visual Se- ing the English PropBank as a universal semantic mantic Role Labeling (Silberer and Pinkal, 2018), inventory and employing cross-lingual annotation ∗ projection techniques to produce annotations for Work partially carried out while at the Sapienza University of Rome. other languages starting from English annotated 2293 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2293–2305 November 7–11, 2021. ©2021 Association for Computational Linguistics data. These approaches, however, tend to ignore the fact that ProbBank was conceived expressly for English pre"
2021.findings-emnlp.197,N18-2078,0,0.138785,". In fact, these multilinbetween a predicate and its arguments (Gildea and Jurafsky, 2002). SRL can be used to extract in- gual datasets are not aligned, they are considerably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution, strongly limiting cations in a wide range of Natural Language Pro- language-to-language comparisons. Some studies (Akbik et al., 2015; Daza and cessing (NLP) areas such as Machine Translation Frank, 2020) worked around this issue by elect(Marcheggiani et al., 2018), Question Answering (Shen and Lapata, 2007; He et al., 2015), Visual Se- ing the English PropBank as a universal semantic mantic Role Labeling (Silberer and Pinkal, 2018), inventory and employing cross-lingual annotation ∗ projection techniques to produce annotations for Work partially carried out while at the Sapienza University of Rome. other languages starting from English annotated 2293 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2293–2305 November 7–11, 2021. ©2021 Association for Computational Linguistics data. These approaches, however, tend to ignore t"
2021.findings-emnlp.197,J05-1004,0,0.921535,"dicatequired to identify and classify argument spans, argument structure inventory, allowing direct whereas CoNLL-2009 (Hajiˇc et al., 2009) is framed comparisons across languages and encouragas a dependency-based task, where only the syntacing studies on cross-lingual transfer in SRL. tic head of an argument has to be tagged. Moreover, We release U NITE D-SRL v1.0 at https:// when multiple languages are covered, for examgithub.com/SapienzaNLP/united-srl. ple in CoNLL-2009 and CoNLL-2012 (Pradhan 1 Introduction et al., 2012), different inventories are used, such as English Propositional Bank (Palmer et al., 2005, Semantic Role Labeling (SRL) – often considered PropBank), Chinese PropBank (Xue and Palmer, to be a fundamental step towards Natural Language Understanding (Navigli, 2018) – consists in recov- 2003) and AnCora (Taulé et al., 2008) for Spanish and Catalan, making it difficult to evaluate whether ering the latent predicate-argument structure of a a system is able to generalize across languages sentence by identifying the semantic relationship and, if so, to what extent. In fact, these multilinbetween a predicate and its arguments (Gildea and Jurafsky, 2002). SRL can be used to extract in- gua"
2021.findings-emnlp.197,W12-4501,0,0.138027,"Generation (Fan et al., 2018). Multilingual and cross-lingual Semantic Role Given the popularity of this task, over the Labeling (SRL) have recently garnered increasing attention as multilingual text repreyears several competitions have been organized sentation techniques have become more effecwithin the Conference on Computational Language tive and widely available. While recent work Learning (CoNLL) to evaluate SRL systems (Carhas attained growing success, results on gold reras and Màrquez, 2004, 2005; Surdeanu et al., multilingual benchmarks are still not easily 2008; Hajiˇc et al., 2009; Pradhan et al., 2012). comparable across languages, making it diffiThese shared tasks led to the release of several cult to grasp where we stand. For example, datasets that nowadays represent the de facto stanin CoNLL-2009, the standard benchmark for multilingual SRL, language-to-language comdard benchmarks for SRL, namely, CoNLL-2005, parisons are affected by the fact that each lanCoNLL-2008, CoNLL-2009 and CoNLL-2012. guage has its own dataset which differs from However, despite their widespread use, the the others in size, domains, sets of labels and CoNLL datasets suffer from a considerable level annotation gu"
2021.findings-emnlp.197,2020.acl-demos.14,0,0.0868224,"Missing"
2021.findings-emnlp.197,N19-5004,0,0.051439,"Missing"
2021.findings-emnlp.197,D07-1002,0,0.0681438,"its arguments (Gildea and Jurafsky, 2002). SRL can be used to extract in- gual datasets are not aligned, they are considerably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution, strongly limiting cations in a wide range of Natural Language Pro- language-to-language comparisons. Some studies (Akbik et al., 2015; Daza and cessing (NLP) areas such as Machine Translation Frank, 2020) worked around this issue by elect(Marcheggiani et al., 2018), Question Answering (Shen and Lapata, 2007; He et al., 2015), Visual Se- ing the English PropBank as a universal semantic mantic Role Labeling (Silberer and Pinkal, 2018), inventory and employing cross-lingual annotation ∗ projection techniques to produce annotations for Work partially carried out while at the Sapienza University of Rome. other languages starting from English annotated 2293 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2293–2305 November 7–11, 2021. ©2021 Association for Computational Linguistics data. These approaches, however, tend to ignore the fact that ProbBank was conceived express"
2021.findings-emnlp.197,D18-1282,0,0.0127827,"ably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution, strongly limiting cations in a wide range of Natural Language Pro- language-to-language comparisons. Some studies (Akbik et al., 2015; Daza and cessing (NLP) areas such as Machine Translation Frank, 2020) worked around this issue by elect(Marcheggiani et al., 2018), Question Answering (Shen and Lapata, 2007; He et al., 2015), Visual Se- ing the English PropBank as a universal semantic mantic Role Labeling (Silberer and Pinkal, 2018), inventory and employing cross-lingual annotation ∗ projection techniques to produce annotations for Work partially carried out while at the Sapienza University of Rome. other languages starting from English annotated 2293 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2293–2305 November 7–11, 2021. ©2021 Association for Computational Linguistics data. These approaches, however, tend to ignore the fact that ProbBank was conceived expressly for English predicates. Forcing the semantics of different languages to adapt to English, without considering possible transl"
2021.findings-emnlp.197,taule-etal-2008-ancora,0,0.785146,"e syntacing studies on cross-lingual transfer in SRL. tic head of an argument has to be tagged. Moreover, We release U NITE D-SRL v1.0 at https:// when multiple languages are covered, for examgithub.com/SapienzaNLP/united-srl. ple in CoNLL-2009 and CoNLL-2012 (Pradhan 1 Introduction et al., 2012), different inventories are used, such as English Propositional Bank (Palmer et al., 2005, Semantic Role Labeling (SRL) – often considered PropBank), Chinese PropBank (Xue and Palmer, to be a fundamental step towards Natural Language Understanding (Navigli, 2018) – consists in recov- 2003) and AnCora (Taulé et al., 2008) for Spanish and Catalan, making it difficult to evaluate whether ering the latent predicate-argument structure of a a system is able to generalize across languages sentence by identifying the semantic relationship and, if so, to what extent. In fact, these multilinbetween a predicate and its arguments (Gildea and Jurafsky, 2002). SRL can be used to extract in- gual datasets are not aligned, they are considerably formation from text and to provide a shallow se- different in size and show significant dissimilarimantic representation of sentences, finding appli- ties in their domain distribution"
2021.findings-emnlp.197,W03-1707,0,0.186603,"Missing"
2021.findings-emnlp.204,L18-1544,0,0.358449,"ns; sort() Sorts by placement in input text; Start: E = sort(E); lin_triplets = """"; for e ∈ E do R(e) = relations with e as subject; R(e) = sort(R(e)); lin_triplets += &lt;triplet> + e; for r ∈ R(e) do o = E(e, r) object of relation r; lin_triplets += &lt;subj> + o + &lt;obj> + r; end end 3.2 REBEL dataset Autoregressive transformer models such as BART or T5, have been shown to perform well on different generative tasks such as translation or summarization, but they do require large amounts of data to be trained. On the other hand, end-to-end relation extraction datasets are scarce and often small. In Elsahar et al. (2018) the T-REx dataset was created by devising a pipeline that extracts entities and relations from DBpedia abstracts to overcome this lack of big RE datasets. While the result is a large dataset, the quality of the annotation presents some issues. First, the use of a somewhat old entity linking tool (Daiber et al., 2013) leads to entities being wrongly disambiguated. Since the relations are extracted by using those entities, this leads to missing or faulty relations. Moreover, most of the relations are extracted by assuming that, if the two entities are present in the text, the relation is theref"
2021.findings-emnlp.204,2020.emnlp-main.585,1,0.734227,"the extraction order for the triplets. Moreover, seq2seq approaches suffer from exposure bias, since at training time the prediction is always dependent on the gold-standard output. In Zhang et al. (2020) a tree-decoding approach mitigates these issues while still using an autoregressive seq2seq approach. In the meantime, seq2seq Transformer models, such as BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) have been used in NLU tasks such as Entity Linking (Cao et al., 2021), AMR parsing (Bevilacqua et al., 2021), Semantic Role Labeling (Blloshmi et al., 2021) or Word SenseDisambiguation (Bevilacqua et al., 2020) by reframing them as seq2seq tasks. Not only do they show strong performance, but they also showcase the flexibility of seq2seq models by not relying on predefined entity sets, but rather on the decoding mechanism, which can easily be extended to new or unseen entities. For our model, we employ an Encoder-Decoder framework that can alleviate some of the previous issues seq2seq for RE has faced. While exposure bias can still occur, the attention mechanism enables long-distance dependencies as well as attending (or not) to the previously decoded output. Additionally, we devise a novel triplet l"
2021.findings-emnlp.204,C16-1239,0,0.018782,"We refer to the resulting model as REBELpre−training . While REBELpre−training is in and of itself capable of extracting relations subsuming about 220 types, we show that it also functions as a base step for downstream RE and RC tasks, which are finetuned on top of it. 4.2 CONLL04 CONLL04 (Roth and Yih, 2004) is composed of sentences from news articles, annotated with four entity types (person, organization, location and other) and five relation types (kill, work for, organization based in, live in and located in). To compare with previous work, we use the test split 4 Experimental Setup from Gupta et al. (2016), and the same validation set as Eberts and Ulges (2020), although we do not In this section, we describe the setup to train and include the validation set at final training time. evaluate REBEL for four different widely used RE For CONLL04 we expand REBEL to include datasets and one RC dataset. Statistics for all the entity types. As described in Section 3.1, we intro4 xlm-roberta-large-xnli 5 duce a set of new tokens for each entity type. For https://github.com/Babelscape/ crocodile CONLL04 these are &lt;peop>, &lt;org>, &lt;loc>, 2374 &lt;other>. We fine-tune on top of REBEL for 30 epochs and test on t"
2021.findings-emnlp.204,P17-1085,0,0.0226495,"ut. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using 1 https://github.com/babelscape/rebel BILOU tags and then a biaffine classifier extracts their relations, sharing part of the encoders for both tasks. These range from LSTMs (Miwa and Bansal, 2016; Katiyar and Cardie, 2017) to CNNs (Adel and Schütze, 2017; Zheng et al., 2017) and, lately, Transformer-based architectures (Eberts and Ulges, 2020), that explicitly predict and encode entity spans instead of the BILOU approach used in NER. All recent sentence-level RE models are based on Transformer models, such as BERT (Eberts and Ulges, 2020; Wang et al., 2020) or ALBERT (Lan et al., 2020; Wang and Lu, 2020). To tackle document-level RE, Eberts and Ulges (2021) use a pipeline approach jointly trained on a multi-task setup that leverages coreference resolution to operate at an entity level, rather than mentions. Whi"
2021.findings-emnlp.204,2020.acl-main.703,0,0.0427823,"s, they still pose some issues. The triplets need to 2371 be linearized into a somewhat arbitrary sequential order, such as the alphabetical one. This issue is explored by Zeng et al. (2019), who use Reinforcement Learning to compute the extraction order for the triplets. Moreover, seq2seq approaches suffer from exposure bias, since at training time the prediction is always dependent on the gold-standard output. In Zhang et al. (2020) a tree-decoding approach mitigates these issues while still using an autoregressive seq2seq approach. In the meantime, seq2seq Transformer models, such as BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) have been used in NLU tasks such as Entity Linking (Cao et al., 2021), AMR parsing (Bevilacqua et al., 2021), Semantic Role Labeling (Blloshmi et al., 2021) or Word SenseDisambiguation (Bevilacqua et al., 2020) by reframing them as seq2seq tasks. Not only do they show strong performance, but they also showcase the flexibility of seq2seq models by not relying on predefined entity sets, but rather on the decoding mechanism, which can easily be extended to new or unseen entities. For our model, we employ an Encoder-Decoder framework that can alleviate some of the prev"
2021.findings-emnlp.204,P16-1105,0,0.0245397,"re the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using 1 https://github.com/babelscape/rebel BILOU tags and then a biaffine classifier extracts their relations, sharing part of the encoders for both tasks. These range from LSTMs (Miwa and Bansal, 2016; Katiyar and Cardie, 2017) to CNNs (Adel and Schütze, 2017; Zheng et al., 2017) and, lately, Transformer-based architectures (Eberts and Ulges, 2020), that explicitly predict and encode entity spans instead of the BILOU approach used in NER. All recent sentence-level RE models are based on Transformer models, such as BERT (Eberts and Ulges, 2020; Wang et al., 2020) or ALBERT (Lan et al., 2020; Wang and Lu, 2020). To tackle document-level RE, Eberts and Ulges (2021) use a pipeline approach jointly trained on a multi-task setup that leverages coreference resolution to operate at an entity level"
2021.findings-emnlp.204,D14-1200,0,0.0280616,"y Recognition, and then classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs to exploit sentencelevel semantics and classify the relations between two given entities (Zeng et al., 2014; Zhou et al., 2016). Current approaches to Relation Classification use Transformer models, with (Yamada et al., 2020) being the current state of the art by enhancing BERT (Devlin et al., 2019) with entity-aware components. Early end-to-end approaches using neural networks classified all word pairs present in the input text (Miwa and Sasaki, 2014; Pawar et al., 2017) using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using 1 https://git"
2021.findings-emnlp.204,E17-1077,0,0.017818,"classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs to exploit sentencelevel semantics and classify the relations between two given entities (Zeng et al., 2014; Zhou et al., 2016). Current approaches to Relation Classification use Transformer models, with (Yamada et al., 2020) being the current state of the art by enhancing BERT (Devlin et al., 2019) with entity-aware components. Early end-to-end approaches using neural networks classified all word pairs present in the input text (Miwa and Sasaki, 2014; Pawar et al., 2017) using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using 1 https://github.com/babelscape/re"
2021.findings-emnlp.204,W04-2401,0,0.237438,"ained in Section 3.2. To pre-train our model, we use a sentencelevel version of it, where only relations between entities present in each sentence are kept. We keep the 220 most frequent relations in the train split. We fine-tune REBEL (using BART-large as the base model) on the silver dataset for 6 epochs. We refer to the resulting model as REBELpre−training . While REBELpre−training is in and of itself capable of extracting relations subsuming about 220 types, we show that it also functions as a base step for downstream RE and RC tasks, which are finetuned on top of it. 4.2 CONLL04 CONLL04 (Roth and Yih, 2004) is composed of sentences from news articles, annotated with four entity types (person, organization, location and other) and five relation types (kill, work for, organization based in, live in and located in). To compare with previous work, we use the test split 4 Experimental Setup from Gupta et al. (2016), and the same validation set as Eberts and Ulges (2020), although we do not In this section, we describe the setup to train and include the validation set at final training time. evaluate REBEL for four different widely used RE For CONLL04 we expand REBEL to include datasets and one RC dat"
2021.findings-emnlp.204,2020.emnlp-main.301,0,0.240143,"though it is devised for Relation Extraction, the same approach can be generalized to Relation Classification, achieving competitive results. We make REBEL available1 both as a standalone model that can extract more than 200 different relation types, and as a pre-trained RE model that can be easily fine-tuned on new RE and RC datasets. We also provide the REBEL dataset and the pipeline to extract high-quality RE datasets from any Wikipedia dump. 2 Related work 2.1 Relation Extraction The term Relation Extraction is often used in the literature for different tasks and setups in the literature (Taillé et al., 2020). For clarity, we refer to Relation Extraction (RE) as the task of extracting triplets of relations between entities from raw text, with no given entity spans, usually also called endto-end Relation Extraction. We refer to classifying the relation between two entities in a given context as Relation Classification (RC). Early approaches tackled RE as a pipeline system, identifying the entities present in the text using Named Entity Recognition, and then classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs t"
2021.findings-emnlp.204,2020.emnlp-main.133,0,0.158737,"ns between two given entities (Zeng et al., 2014; Zhou et al., 2016). Current approaches to Relation Classification use Transformer models, with (Yamada et al., 2020) being the current state of the art by enhancing BERT (Devlin et al., 2019) with entity-aware components. Early end-to-end approaches using neural networks classified all word pairs present in the input text (Miwa and Sasaki, 2014; Pawar et al., 2017) using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using 1 https://github.com/babelscape/rebel BILOU tags and then a biaffine classifier extracts their relations, sharing part of the encoders for both tasks. These range from LSTMs (Miwa and Bansal, 2016; Katiyar and Cardie, 2017) to CNNs (Ad"
2021.findings-emnlp.204,2020.coling-main.138,0,0.062451,"Missing"
2021.findings-emnlp.204,2020.emnlp-main.523,0,0.204939,"ction. We refer to classifying the relation between two entities in a given context as Relation Classification (RC). Early approaches tackled RE as a pipeline system, identifying the entities present in the text using Named Entity Recognition, and then classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs to exploit sentencelevel semantics and classify the relations between two given entities (Zeng et al., 2014; Zhou et al., 2016). Current approaches to Relation Classification use Transformer models, with (Yamada et al., 2020) being the current state of the art by enhancing BERT (Devlin et al., 2019) with entity-aware components. Early end-to-end approaches using neural networks classified all word pairs present in the input text (Miwa and Sasaki, 2014; Pawar et al., 2017) using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder. Finally, there are pipeline systems t"
2021.findings-emnlp.204,D17-1004,0,0.015683,"e triplet decomposiTraditionally this task has been approached as tion into a text sequence. By pre-training an a two-step problem. First, the entities are ex- Encoder-Decoder Transformer (BART) using our tracted from text as in Named Entity Recogni- new dataset, REBEL achieves state-of-the-art pertion (NER). Second, Relation Classification (RC) formance on an array of RE baselines within a few checks whether there exists any pairwise relation epochs of fine-tuning. Its simplicity makes it highly between the extracted entities (Zeng et al., 2014; flexible to adapt to new domains or longer docuZhang et al., 2017). However, identifying which ments. As the same model weights are still utilized entities truly share a relation can become a bottle- after the pre-training phase, there is no need to train neck, requiring additional steps such as negative model-specific components from scratch, making sampling and expensive annotation procedures. training more efficient. 2370 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370–2381 November 7–11, 2021. ©2021 Association for Computational Linguistics Moreover, although it is devised for Relation Extraction, the same approach can b"
2021.findings-emnlp.204,P19-1074,0,0.0470609,"Missing"
2021.findings-emnlp.204,C14-1220,0,0.27297,"as the construction of Knowledge Bases. to our adoption of a simple triplet decomposiTraditionally this task has been approached as tion into a text sequence. By pre-training an a two-step problem. First, the entities are ex- Encoder-Decoder Transformer (BART) using our tracted from text as in Named Entity Recogni- new dataset, REBEL achieves state-of-the-art pertion (NER). Second, Relation Classification (RC) formance on an array of RE baselines within a few checks whether there exists any pairwise relation epochs of fine-tuning. Its simplicity makes it highly between the extracted entities (Zeng et al., 2014; flexible to adapt to new domains or longer docuZhang et al., 2017). However, identifying which ments. As the same model weights are still utilized entities truly share a relation can become a bottle- after the pre-training phase, there is no need to train neck, requiring additional steps such as negative model-specific components from scratch, making sampling and expensive annotation procedures. training more efficient. 2370 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370–2381 November 7–11, 2021. ©2021 Association for Computational Linguistics Moreover, alt"
2021.findings-emnlp.204,D19-1035,0,0.0118638,"quire all possible entity pairs to be inferred, which can become computationally expensive. Seq2seq approaches for RE (Zeng et al., 2018, 2020; Nayak and Ng, 2020) offer some off-theshelf solutions to these problems. Decoding mechanisms can output the same entities multiple times, as well as conditioning future decoding on previous predictions, implicitly dealing with incompatible ones. However, as Zhang et al. (2020) discuss, they still pose some issues. The triplets need to 2371 be linearized into a somewhat arbitrary sequential order, such as the alphabetical one. This issue is explored by Zeng et al. (2019), who use Reinforcement Learning to compute the extraction order for the triplets. Moreover, seq2seq approaches suffer from exposure bias, since at training time the prediction is always dependent on the gold-standard output. In Zhang et al. (2020) a tree-decoding approach mitigates these issues while still using an autoregressive seq2seq approach. In the meantime, seq2seq Transformer models, such as BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) have been used in NLU tasks such as Entity Linking (Cao et al., 2021), AMR parsing (Bevilacqua et al., 2021), Semantic Role Labeling (Blloshmi"
2021.findings-emnlp.204,P18-1047,0,0.06938,"es (if available for the dataset). 2.2 Seq2seq and Relation Extraction The pipeline and table filling methods described so far have proved to perform well on RE, but still face some challenges. They often assume at most one relation type between each entity pair, and multiclass approaches do not take other predictions into account. For instance, they could predict two “birth dates” for the same head entity, or predict relations that are incompatible together. Moreover, they require all possible entity pairs to be inferred, which can become computationally expensive. Seq2seq approaches for RE (Zeng et al., 2018, 2020; Nayak and Ng, 2020) offer some off-theshelf solutions to these problems. Decoding mechanisms can output the same entities multiple times, as well as conditioning future decoding on previous predictions, implicitly dealing with incompatible ones. However, as Zhang et al. (2020) discuss, they still pose some issues. The triplets need to 2371 be linearized into a somewhat arbitrary sequential order, such as the alphabetical one. This issue is explored by Zeng et al. (2019), who use Reinforcement Learning to compute the extraction order for the triplets. Moreover, seq2seq approaches suffer"
2021.findings-emnlp.204,P16-2034,0,0.0181322,"entities from raw text, with no given entity spans, usually also called endto-end Relation Extraction. We refer to classifying the relation between two entities in a given context as Relation Classification (RC). Early approaches tackled RE as a pipeline system, identifying the entities present in the text using Named Entity Recognition, and then classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs to exploit sentencelevel semantics and classify the relations between two given entities (Zeng et al., 2014; Zhou et al., 2016). Current approaches to Relation Classification use Transformer models, with (Yamada et al., 2020) being the current state of the art by enhancing BERT (Devlin et al., 2019) with entity-aware components. Early end-to-end approaches using neural networks classified all word pairs present in the input text (Miwa and Sasaki, 2014; Pawar et al., 2017) using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the"
2021.findings-emnlp.204,2020.findings-emnlp.23,0,0.0250067,"oaches do not take other predictions into account. For instance, they could predict two “birth dates” for the same head entity, or predict relations that are incompatible together. Moreover, they require all possible entity pairs to be inferred, which can become computationally expensive. Seq2seq approaches for RE (Zeng et al., 2018, 2020; Nayak and Ng, 2020) offer some off-theshelf solutions to these problems. Decoding mechanisms can output the same entities multiple times, as well as conditioning future decoding on previous predictions, implicitly dealing with incompatible ones. However, as Zhang et al. (2020) discuss, they still pose some issues. The triplets need to 2371 be linearized into a somewhat arbitrary sequential order, such as the alphabetical one. This issue is explored by Zeng et al. (2019), who use Reinforcement Learning to compute the extraction order for the triplets. Moreover, seq2seq approaches suffer from exposure bias, since at training time the prediction is always dependent on the gold-standard output. In Zhang et al. (2020) a tree-decoding approach mitigates these issues while still using an autoregressive seq2seq approach. In the meantime, seq2seq Transformer models, such as"
2021.findings-emnlp.215,W09-3302,0,0.0513045,".64M 1.87M 2.13M 2.62M 2.83M 1.91M 2.20M 20k 25k 20k 35k 29k 34k 41k 48k 759k 598k 706k 1.18M 21.41 17.69 17.29 24.31 1.55 1.44 1.37 1.70 12k 17k 17k 20k 23k 8k 12k 13k 6k 18k 23k 38k 3k 6k 3k 12k 0.54M 0.51M 0.61M 1.02M Table 2: Statistics on the produced data on a fixed number of articles. “Avg. length” is the average sentence length and “Avg. NEs” is the average number of named entities per sentence. DA stands for Domain Adaptation. well-known collection of newswire articles for English and German taken from the Reuters Corpus and the ECI Multilingual Text Corpus, respectively. • WikiGold (Balasuriya et al., 2009): a small set of English Wikipedia articles manually annotated with CoNLL named entity classes. • OntoNotes 5.0 (Pradhan et al., 2012): this includes texts from five different text genres: broadcast conversation, broadcast news, magazine, newswire, and web data. We use it as an additional test set for English. • BSNLP-2017 (Piskorski et al., 2017): this consists of articles in various Slavic languages and we use it to evaluate Russian and Polish performances. Two test sets are provided: one contains articles about a specific politician, the other one about the European Commission. 6 6.1 Result"
2021.findings-emnlp.215,N19-1423,0,0.0156258,"synset s in language L or, if it does not exist, a special token [NOLEMMA], shared among languages. We rank BabelNet synsets according to the number of glosses they contain and take the top 500k synsets to build our training and validation splits (450k and 50k respectively). As this system represents only one step of the entire WikiNEuRal pipeline, we measure its quality in vivo (see Section 7), as a test set generated with the same distribution would yield inconclusive results. To tag each BabelNet synset (and therefore each Wikipedia article) as either a concept or a named entity, we follow Devlin et al. (2019) and use a simple yet efficient Sequence Classification architecture, using Multilingual BERT as encoder and a linear layer to perform binary classification on top of the [CLS] token. The model is trained on the inputs generated by the function I applied to the aforementioned 500k synsets. Lastly, in order to classify any particular synset, we gather all its possible inputs (i.e., we take into account ˆ 3) all of its glosses in the considered languages L ˆ g ∈ GL (s)} and label I(s) = {I(s, L, g)|L ∈ L, them independently using the trained model. Then, we select, for synset s, the label with t"
2021.findings-emnlp.215,C96-1079,0,0.248039,"echniques that lead to considerable improvements. To the best of our knowledge, we are the first to exploit multilingual BERT’s power in a silver data creation process for NER: we use it to independently i) distinguish named entities from concepts, ii) validate annotations and, iii) discover annotations. Further, to address the sparsity problem, rather than relying on often-noisy redirections, we exploit the synonymy information provided by a multilingual lexical knowledge base, i.e., BabelNet1 (Navigli and Ponzetto, 2012; Navigli et al., 2021). Since the first shared task on NER organized by Grishman and Sundheim (1996), several tasks and human-annotated datasets have been proposed. The CoNLL-2002 and 2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) were created in four different languages (Spanish, Dutch, English, and German) from newswire articles and focused on 4 entity types: P ER (Person), O RG (Organization), L OC (Location), and M ISC (Miscellaneous, i.e., all other entity types). Several other NER shared tasks were organized in the years which followed, covering further languages such as Indic (Rajeev Sangal and Singh, 2008) and Balto-Slavic languages (Piskorski et al., 2017)"
2021.findings-emnlp.215,U06-1009,0,0.172236,"Missing"
2021.findings-emnlp.215,2020.acl-main.720,0,0.0139748,"3.5 Confirming and Augmenting Annotations We are now left with two potential weaknesses: first, even though aiming for high precision, our annotations might still include some mistakes; second, our tagged sentences might still contain unannotated entities due to unlinked or unmatched entity mentions. To address both issues, we first introduce our NER classifier, which we will use to perform tagging with our annotated data, then we apply it to confirm our annotations and augment the sentences with additional tags. 3.5.1 Our NER model Our NER model is a variant of the BERT-based neural model of Mueller et al. (2020): following recent literature, rather than representing a word with the first contextualized subword representation as provided by multilingual BERT, we take the mean of its subwords (Ács et al., 2021). The resulting vectors are passed through a multi-layer sentencelevel BiLSTM network, whose logits are then fed into a CRF model (Lafferty et al., 2001), trained to maximize the log-likelihood of the span-based gold label sequences. 3.5.2 Improving Precision and Recall To address the above-mentioned weaknesses, we train our NER model with the WikiNER dataset (cf. Section 2) and use it to confirm"
2021.findings-emnlp.215,P17-1178,0,0.405645,"fective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements of up to 6 span-based F1 -score points over previous state-of-the-art systems for data creation. 1 Introduction Various works have been put forward which address data paucity by aiming at automatically producing multilingual silver-standard training data for NER (Nothman et al., 2013; Al-Rfou et al., 2015; Tsai et al., 2016; Pan et al., 2017). Each of these leverages the link structure of Wikipedia to generate named entity annotations. However, this strategy has two drawbacks: only small portions of text in Wikipedia are linked, and mapping Wikipedia links to the corresponding NER classes is not trivial and introduces errors. Different methods have been investigated to cope with these problems, such as heuristics based on Wikipedia redirects, surface form token matching, and category-based rules. Although we also rely on Wikipedia text and its hypertext organization, we depart from previous works in our exploration of new language"
2021.findings-emnlp.215,W12-4501,0,0.135754,"12k 17k 17k 20k 23k 8k 12k 13k 6k 18k 23k 38k 3k 6k 3k 12k 0.54M 0.51M 0.61M 1.02M Table 2: Statistics on the produced data on a fixed number of articles. “Avg. length” is the average sentence length and “Avg. NEs” is the average number of named entities per sentence. DA stands for Domain Adaptation. well-known collection of newswire articles for English and German taken from the Reuters Corpus and the ECI Multilingual Text Corpus, respectively. • WikiGold (Balasuriya et al., 2009): a small set of English Wikipedia articles manually annotated with CoNLL named entity classes. • OntoNotes 5.0 (Pradhan et al., 2012): this includes texts from five different text genres: broadcast conversation, broadcast news, magazine, newswire, and web data. We use it as an additional test set for English. • BSNLP-2017 (Piskorski et al., 2017): this consists of articles in various Slavic languages and we use it to evaluate Russian and Polish performances. Two test sets are provided: one contains articles about a specific politician, the other one about the European Commission. 6 6.1 Results Multilingual Evaluation We assess the quality of the WikiNEuRal datasets extensively, comparing the performances obtained training t"
2021.findings-emnlp.215,P19-1015,0,0.0206126,"lder, 2003) for English and German, and the OntoNotes 5.0 dataset for English. All silver- and gold-standard datasets are tagged with the four standard entity types (P ER, O RG, L OC, M ISC), except for WikiANN which does not contain the M ISC label. 5.3 Test Data We use five different test sets in our experiments: • CoNLL-2002 NER Shared Task dataset (Tjong Kim Sang, 2002): a popular collection of newswire articles for Spanish and Dutch. • CoNLL-2003 NER Shared Task dataset (Tjong Kim Sang and De Meulder, 2003): a 10 The version used corresponds to the balanced train, dev, and test splits of Rahimi et al. (2019), which supports 176 of the 282 languages from the original WikiANN corpus, available at https://huggingface.co/datasets/wikiann. 2526 WikiNEuRal English Spanish Dutch German Russian Italian French Polish Portuguese English DA (CoNLL) Dutch DA (CoNLL) German DA (CoNLL) English DA (OntoNotes) Articles Sentences Tokens Avg. length Avg. NEs P ER O RG L OC M ISC OTHER 50k 50k 65k 50k 105k 50k 50k 105k 80k 116k 95k 107k 124k 123k 111k 127k 141k 106k 2.73M 2.33M 1.91M 2.19M 2.39M 2.99M 3.24M 2.29M 2.53M 23.53 24.46 17.91 17.66 19.49 26.85 25.47 16.21 23.99 1.67 1.61 1.43 1.42 1.47 1.90 1.80 1.65 1.8"
2021.findings-emnlp.215,2021.findings-emnlp.220,1,0.73478,"(Nadeau and Sekine, 2007). trained language models to produce highNER is widely used in many downstream tasks, quality annotations for multilingual NER; like question answering (Mollá et al., 2006), machine translation (Babych and Hartley, 2003), in2. We assess the quality of the corpora produced formation retrieval (Petkova and Croft, 2007), text with an extensive evaluation and a statistical summarization (Aone et al., 1998), text understandanalysis, showing consistent improvements of ing (Zhang et al., 2019; Cheng and Erk, 2019) and up to 6 span-based F1 -score points on comentity linking (Tedeschi et al., 2021), among others. mon benchmarks for NER against state-of-theWith recent advances in Natural Language Proart alternative data production methods; cessing, and in particular with the advent of pretrained language models such as BERT (Devlin 3. We present a novel approach for creating inet al., 2019), once a sufficient amount of training terpretable word embeddings; data is available for the task of interest, fine-tuning 4. Based on these embeddings, we introduce a is often employed to address the task successfully. domain adaptation algorithm which yields furUnfortunately, such training data are"
2021.findings-emnlp.215,W02-2024,0,0.45807,"a silver data creation process for NER: we use it to independently i) distinguish named entities from concepts, ii) validate annotations and, iii) discover annotations. Further, to address the sparsity problem, rather than relying on often-noisy redirections, we exploit the synonymy information provided by a multilingual lexical knowledge base, i.e., BabelNet1 (Navigli and Ponzetto, 2012; Navigli et al., 2021). Since the first shared task on NER organized by Grishman and Sundheim (1996), several tasks and human-annotated datasets have been proposed. The CoNLL-2002 and 2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) were created in four different languages (Spanish, Dutch, English, and German) from newswire articles and focused on 4 entity types: P ER (Person), O RG (Organization), L OC (Location), and M ISC (Miscellaneous, i.e., all other entity types). Several other NER shared tasks were organized in the years which followed, covering further languages such as Indic (Rajeev Sangal and Singh, 2008) and Balto-Slavic languages (Piskorski et al., 2017). Early NER systems were based on domainspecific features and rules, which require human engineering. Starting from (Co"
2021.findings-emnlp.215,W03-0419,0,0.377037,"Missing"
2021.findings-emnlp.215,P19-1139,0,0.0223852,"ined semantic types, such as Person, Location, upon external knowledge bases and preOrganization, etc. (Nadeau and Sekine, 2007). trained language models to produce highNER is widely used in many downstream tasks, quality annotations for multilingual NER; like question answering (Mollá et al., 2006), machine translation (Babych and Hartley, 2003), in2. We assess the quality of the corpora produced formation retrieval (Petkova and Croft, 2007), text with an extensive evaluation and a statistical summarization (Aone et al., 1998), text understandanalysis, showing consistent improvements of ing (Zhang et al., 2019; Cheng and Erk, 2019) and up to 6 span-based F1 -score points on comentity linking (Tedeschi et al., 2021), among others. mon benchmarks for NER against state-of-theWith recent advances in Natural Language Proart alternative data production methods; cessing, and in particular with the advent of pretrained language models such as BERT (Devlin 3. We present a novel approach for creating inet al., 2019), once a sufficient amount of training terpretable word embeddings; data is available for the task of interest, fine-tuning 4. Based on these embeddings, we introduce a is often employed to addres"
2021.findings-emnlp.220,2020.acl-main.703,0,0.0303344,"pecially thanks to the advances in contextualized word embedding and entity representation techniques (Ganea and Hofmann, 2017; Le and Titov, 2018, 2019; Yang et al., 2019). While initial work relied on static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) to represent mentions and entities, recent studies (Shahbazi et al., 2019; Broscheit, 2019; Botha et al., 2020; Cao et al., 2021) have shown the benefit of employing contextualized embeddings from pretrained language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Notably, researchers are tackling EL with a variety of very different approaches. For example, Botha et al. (2020) put Named Entity Recognition. Similarly to almost forward a dual-encoder architecture, composed of any other area in NLP, Named Entity Recognition two separate encoders for mentions and entities, systems have benefited greatly from the advent of that maximizes the similarity between a mention pretrained language models (Virtanen et al., 2019; embedding and its corresponding entity embedding, Mueller et al., 2020; Liang et al., 2020; Souza et al., whereas Cao et al. (2021) propos"
2021.findings-emnlp.220,D11-1072,0,0.402857,"budget must therefore decide between training on lower amounts of data at the cost of drastic drops in performance – the performance of GENRE drops by 8.8 points in F1 when trained only on the AIDA-YAGO-CoNLL training set – and long training times. In this paper, instead, we show that the clever use of NER for EL can significantly narrow the gap between systems trained on thousands as opposed to millions of instances, while retaining the benefits of shorter training times. Enriching Entity Linking. While the first successful approaches to EL often relied on non-neural graph-based techniques (Hoffart et al., 2011; Rao et al., 2013; Moro et al., 2014), there is a growing body of work that studies how to enrich neural models by taking advantage of relational knowledge from semantic networks such as Wikidata, YAGO (Suchanek et al., 2007), WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012; Navigli et al., 2021), inter alia. For example, Raiman and Raiman (2018) proposed DeepType which relies on Wikidata to integrate symbolic knowledge into the reasoning process of a neural network. In particular, they make use of a type system to constrain the behavior of an entity prediction model with resp"
2021.findings-emnlp.220,P11-1115,0,0.126209,"Missing"
2021.findings-emnlp.220,N18-1131,0,0.0285163,"tities and then used these classes to enhance a strong EL baseline with i) NER-enriched entity representations, ii) NER-enhanced candidate selection, iii) NER-based negative sampling, and iv) NER-constrained decoding. Our experiments show that the integration of NER information can aid an EL system trained on less than 20K instances in narrowing the gap with EL systems trained on millions of samples. Over the past few years, the field of NER has witnessed continuous growth, with many researchers studying more complex forms of NER, including nested and structured NER (Finkel and Manning, 2009; Ju et al., 2018; Straková et al., 2019; Qian et al., 2020). Although we focused on the benefits of traditional NER in EL, we trust that more complex forms of NER can lead to even greater improvements in EL. In conclusion, we believe that our work can encourage further developments on Entity Linking systems that require fewer and fewer training instances and still achieve strong results across indomain and out-of-domain evaluations. Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union’s Horizon 2020 research and innovation prog"
2021.findings-emnlp.220,K18-1050,0,0.017977,"oposed GENRE which, 2020). Nowadays, their performance makes such 2585 systems extremely compelling options in downstream tasks such as EL. Indeed, thanks to its coarse-grained classes, NER is an obvious way to cluster entities and, therefore, to reduce the intrinsic sparsity of the Entity Linking task. However, there is a surprisingly low number of studies on the effectiveness of enriching EL models with NER information. Most of the contributions in this direction use NER as a preprocessing step before EL, or learn directly to perform the tasks jointly (Luo et al., 2015; Nguyen et al., 2016; Kolitsas et al., 2018; Martins et al., 2019; Broscheit, 2019). In our work, we take the best of both worlds, and not only do we propose other ways to exploit NER for EL, but we also show that individual NER approaches can be combined to further improve a strong EL model. pute the cosine similarity score of each mentioncandidate pair (m, ci ) for each i ∈ {1, . . . , k} and selects the highest-scoring entity ε as follows: φ(m)T ψ(ci ) ε = argmax i∈{1,...,k} kφ(m)kkψ(ci )k (1) Following Botha et al. (2020), the mention encoder φ takes as input a sequence of tokens in which the start and the end of the mention m is i"
2021.findings-emnlp.220,W12-3016,0,0.0351017,"as Named Entity Disambiguation (NED), is the task of associating an ambiguous textual mention with a named entity in a knowledge base. Indeed, named entities may have several surface forms – their full names, partial names, aliases and abbreviations – making EL a very challenging task in Natural Language Processing (NLP). Over the years, EL systems have achieved impressive results in standard benchmarks, especially thanks to the advent of modern language models (Devlin et al., 2019), and have found innumerable applications in a wide range of downstream tasks, including Information Extraction (Lin et al., 2012; Guo et al., 2013; Rao et al., 2013), Question Answering (Yin et al., 2016; • We introduce new fine-grained classes for Dubey et al., 2018), knowledge base population (Ji NER and use them to automatically label each and Grishman, 2011) and recommender systems entity in Wikipedia; 2584 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2584–2596 November 7–11, 2021. ©2021 Association for Computational Linguistics • We show how such classes can easily be used to integrate type information into the entity representations of EL systems; • We propose a NER-enhanced candid"
2021.findings-emnlp.220,D15-1104,0,0.0289066,"za et al., whereas Cao et al. (2021) proposed GENRE which, 2020). Nowadays, their performance makes such 2585 systems extremely compelling options in downstream tasks such as EL. Indeed, thanks to its coarse-grained classes, NER is an obvious way to cluster entities and, therefore, to reduce the intrinsic sparsity of the Entity Linking task. However, there is a surprisingly low number of studies on the effectiveness of enriching EL models with NER information. Most of the contributions in this direction use NER as a preprocessing step before EL, or learn directly to perform the tasks jointly (Luo et al., 2015; Nguyen et al., 2016; Kolitsas et al., 2018; Martins et al., 2019; Broscheit, 2019). In our work, we take the best of both worlds, and not only do we propose other ways to exploit NER for EL, but we also show that individual NER approaches can be combined to further improve a strong EL model. pute the cosine similarity score of each mentioncandidate pair (m, ci ) for each i ∈ {1, . . . , k} and selects the highest-scoring entity ε as follows: φ(m)T ψ(ci ) ε = argmax i∈{1,...,k} kφ(m)kkψ(ci )k (1) Following Botha et al. (2020), the mention encoder φ takes as input a sequence of tokens in which"
2021.findings-emnlp.220,P19-2026,0,0.0192617,"0). Nowadays, their performance makes such 2585 systems extremely compelling options in downstream tasks such as EL. Indeed, thanks to its coarse-grained classes, NER is an obvious way to cluster entities and, therefore, to reduce the intrinsic sparsity of the Entity Linking task. However, there is a surprisingly low number of studies on the effectiveness of enriching EL models with NER information. Most of the contributions in this direction use NER as a preprocessing step before EL, or learn directly to perform the tasks jointly (Luo et al., 2015; Nguyen et al., 2016; Kolitsas et al., 2018; Martins et al., 2019; Broscheit, 2019). In our work, we take the best of both worlds, and not only do we propose other ways to exploit NER for EL, but we also show that individual NER approaches can be combined to further improve a strong EL model. pute the cosine similarity score of each mentioncandidate pair (m, ci ) for each i ∈ {1, . . . , k} and selects the highest-scoring entity ε as follows: φ(m)T ψ(ci ) ε = argmax i∈{1,...,k} kφ(m)kkψ(ci )k (1) Following Botha et al. (2020), the mention encoder φ takes as input a sequence of tokens in which the start and the end of the mention m is identified by special t"
2021.findings-emnlp.220,Q16-1016,0,0.0615502,"Missing"
2021.findings-emnlp.220,D14-1162,0,0.090478,"rmerbased architecture. However, while there is clear evidence that integrating relational knowledge into EL approaches is beneficial, the sparsity of such relations may make them an unappealing option for low-data scenarios. Entity Linking. Over the past few years, neural approaches have attained strong results in EL, especially thanks to the advances in contextualized word embedding and entity representation techniques (Ganea and Hofmann, 2017; Le and Titov, 2018, 2019; Yang et al., 2019). While initial work relied on static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) to represent mentions and entities, recent studies (Shahbazi et al., 2019; Broscheit, 2019; Botha et al., 2020; Cao et al., 2021) have shown the benefit of employing contextualized embeddings from pretrained language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Notably, researchers are tackling EL with a variety of very different approaches. For example, Botha et al. (2020) put Named Entity Recognition. Similarly to almost forward a dual-encoder architecture, composed of any other area in NLP, Named Entity Recognition two separate encod"
2021.findings-emnlp.220,W12-4501,0,0.237721,"Missing"
2021.findings-emnlp.220,2020.emnlp-main.517,0,0.0415479,"hance a strong EL baseline with i) NER-enriched entity representations, ii) NER-enhanced candidate selection, iii) NER-based negative sampling, and iv) NER-constrained decoding. Our experiments show that the integration of NER information can aid an EL system trained on less than 20K instances in narrowing the gap with EL systems trained on millions of samples. Over the past few years, the field of NER has witnessed continuous growth, with many researchers studying more complex forms of NER, including nested and structured NER (Finkel and Manning, 2009; Ju et al., 2018; Straková et al., 2019; Qian et al., 2020). Although we focused on the benefits of traditional NER in EL, we trust that more complex forms of NER can lead to even greater improvements in EL. In conclusion, we believe that our work can encourage further developments on Entity Linking systems that require fewer and fewer training instances and still achieve strong results across indomain and out-of-domain evaluations. Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union’s Horizon 2020 research and innovation programme. References Jan A. Botha, Zifei Shan,"
2021.findings-emnlp.220,P19-1527,0,0.0198081,"sed these classes to enhance a strong EL baseline with i) NER-enriched entity representations, ii) NER-enhanced candidate selection, iii) NER-based negative sampling, and iv) NER-constrained decoding. Our experiments show that the integration of NER information can aid an EL system trained on less than 20K instances in narrowing the gap with EL systems trained on millions of samples. Over the past few years, the field of NER has witnessed continuous growth, with many researchers studying more complex forms of NER, including nested and structured NER (Finkel and Manning, 2009; Ju et al., 2018; Straková et al., 2019; Qian et al., 2020). Although we focused on the benefits of traditional NER in EL, we trust that more complex forms of NER can lead to even greater improvements in EL. In conclusion, we believe that our work can encourage further developments on Entity Linking systems that require fewer and fewer training instances and still achieve strong results across indomain and out-of-domain evaluations. Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 under the European Union’s Horizon 2020 research and innovation programme. References Jan A"
2021.findings-emnlp.220,N18-1202,0,0.0100066,"s, neural approaches have attained strong results in EL, especially thanks to the advances in contextualized word embedding and entity representation techniques (Ganea and Hofmann, 2017; Le and Titov, 2018, 2019; Yang et al., 2019). While initial work relied on static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) to represent mentions and entities, recent studies (Shahbazi et al., 2019; Broscheit, 2019; Botha et al., 2020; Cao et al., 2021) have shown the benefit of employing contextualized embeddings from pretrained language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Notably, researchers are tackling EL with a variety of very different approaches. For example, Botha et al. (2020) put Named Entity Recognition. Similarly to almost forward a dual-encoder architecture, composed of any other area in NLP, Named Entity Recognition two separate encoders for mentions and entities, systems have benefited greatly from the advent of that maximizes the similarity between a mention pretrained language models (Virtanen et al., 2019; embedding and its corresponding entity embedding, Mueller et al., 2020; Liang et"
2021.findings-emnlp.220,2021.findings-emnlp.215,1,0.73478,"ovide further details about the hyperparameter values, training times and hardware infrastructure in Appendix A. 4.2 Datasets In the following, we describe the datasets we use to train, validate and test our contributions. We stress that we train each of our model configurations on only the AIDA-YAGO-CoNLL training split, i.e., on only 18K labeled instances as opposed to the millions on which current state-of-the-art systems are trained, showing the benefits of NER when a scarce amount of labeled instances are available. While there is a growing interest in multilingual datasets for both NER (Tedeschi et al., 2021) and EL (Botha et al., 2020), in this work we focus only on the English language. AIDA-YAGO-CoNLL (Hoffart et al., 2011) is one of the largest manually annotated EL datasets for English as it contains 388 articles with 27,817 linkable mentions corresponding to the named entities annotated for the original CoNLL-2003 entity recognition task (Tjong Kim Sang and De Meulder, 2003) This dataset comprises a number of newswire articles taken from the Reuters Corpus. In this Section, we describe our experimental setup MSNBC, AQUAINT and ACE2004 are smaller (Section 4.1), the datasets we use to train a"
2021.findings-emnlp.220,2021.naacl-main.200,0,0.0890576,"Missing"
2021.findings-emnlp.220,D19-1026,0,0.0185702,"ch uses the edges defined in Wikidata and YAGO to encode entity relations and entity types as input embeddings to a Transformerbased architecture. However, while there is clear evidence that integrating relational knowledge into EL approaches is beneficial, the sparsity of such relations may make them an unappealing option for low-data scenarios. Entity Linking. Over the past few years, neural approaches have attained strong results in EL, especially thanks to the advances in contextualized word embedding and entity representation techniques (Ganea and Hofmann, 2017; Le and Titov, 2018, 2019; Yang et al., 2019). While initial work relied on static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) to represent mentions and entities, recent studies (Shahbazi et al., 2019; Broscheit, 2019; Botha et al., 2020; Cao et al., 2021) have shown the benefit of employing contextualized embeddings from pretrained language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Notably, researchers are tackling EL with a variety of very different approaches. For example, Botha et al. (2020) put Named Entity Recognition. Simila"
2021.findings-emnlp.220,C16-1164,0,0.0288798,"us textual mention with a named entity in a knowledge base. Indeed, named entities may have several surface forms – their full names, partial names, aliases and abbreviations – making EL a very challenging task in Natural Language Processing (NLP). Over the years, EL systems have achieved impressive results in standard benchmarks, especially thanks to the advent of modern language models (Devlin et al., 2019), and have found innumerable applications in a wide range of downstream tasks, including Information Extraction (Lin et al., 2012; Guo et al., 2013; Rao et al., 2013), Question Answering (Yin et al., 2016; • We introduce new fine-grained classes for Dubey et al., 2018), knowledge base population (Ji NER and use them to automatically label each and Grishman, 2011) and recommender systems entity in Wikipedia; 2584 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2584–2596 November 7–11, 2021. ©2021 Association for Computational Linguistics • We show how such classes can easily be used to integrate type information into the entity representations of EL systems; • We propose a NER-enhanced candidate generation module which decreases the size of the candidate set while i"
2021.naacl-main.30,P13-1023,0,0.0336557,"ss-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, whil"
2021.naacl-main.30,W13-2322,0,0.0649986,"ormances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, ho"
2021.naacl-main.30,2020.emnlp-main.195,1,0.954511,"y language, the AMR graph corresponding to its English translation. Using an adapted version of the transition-based parser originally proposed by Damonte et al. (2017) and training it on silver data generated through annotation projection, they examined whether AMR graphs could be recovered starting from non-English sentences. Even though their models fell short when compared to MT alternatives,3 their work showed promising results and suggested that, despite translation divergences, AMR could act effectively as an interlingua. Annotation projection has been focal in subsequent work as well. Blloshmi et al. (2020) propose an aligner-free cross-lingual parser, thus disposing of the need for word alignments in the annotation projection pipeline; their parser manages to outperform MT alternatives when both annotation projection and these baselines have access to comparable amounts of data. Conversely, Sheth et al. (2021) leverage powerful contextualized word embeddings to improve the foreign-text-to-English-AMR alignments, surpassing all previous approaches and, most importantly, the yet-unbeaten MT baselines that have access to larger amounts of data. We stand out from previous research and show that, as"
2021.naacl-main.30,2020.lrec-1.86,0,0.102228,"code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, while AMR uses nodes to encode concepts and edges to express the semantic relations between them, UCCA proposes using text tokens as terminal nodes and building graphs on top of them. As a result of this heterogeneous landscape, often referred to as framework-specifi"
2021.naacl-main.30,2020.acl-main.119,0,0.196228,"ingual parsing and is capable of navigating through translation paths like IT → AMR,2 which it has never seen during training. The contributions of this work are therefore as follows: lingual setting introduced by Damonte and Cohen (2018) for the latter. Semantic Parsing Arguably among the formalisms that have drawn the most interest, AMR has seen the emergence of a rich yet dedicated literature, with recent approaches that can be roughly clustered into two groups. On the one hand, several graph-based solutions have been proposed (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020); among these solutions, Zhou et al. (2020) show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas Cai and Lam (2020) present an iterative method to build and refine AMR graphs incrementally. On the other hand, translation-based approaches, where seq2seq models are trained to translate from natural language text to linearized graphs, have been shown to reach competitive performances, despite the scarcity of training data (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019). Continuing this latter direction and arguably closest to our w"
2021.naacl-main.30,P13-2131,0,0.142011,"ns explained in §3.1 and Chinese simplification.12 Instead, we directly apply subword tokenization with a Unigram Model (Kudo, 2018). When working with Cross in a single-task setting on AMR or UCCA, we follow Ge et al. (2019) and use a vocabulary size of 20k subwords; instead, when working in the multilingual setting, we increase this value to 50k so as to better accommodate the increased amount of languages. Conversely, when using mBART, we always use the original vocabulary consisting of 250k subwords. 4.3 Evaluation We evaluate AMR and cross-lingual AMR parsing by using the Smatch score13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems’ performances, we also re11 See Appendix C for further details. We use the hanziconv library (https://github. com/berniey/hanziconv). 13 https://github.com/snowblink14/smatch 12 port the fine-grained scores as computed by the evaluation toolkit14 of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric15 of the shared task, conceptually similar to the Smatch score. 5 Results We now report the results SGL achieves focusing on the following translatio"
2021.naacl-main.30,K19-2007,0,0.0950833,"luding also these new datasets among the training data. t This model, which we denote by mBARTfmt + AP, surpasses further mBARTmt , clearly underlining the beneficial effect of this technique. Finally, following Sheth et al. (2021), we also report the results of SGL when evaluated on the machine-translated test set;18 similarly to their findings, we observe that, as the mismatch between the training and test set is reduced, our parser performs better in this setting than on the human-translated one. Model Type Score Oepen et al. (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) Che et al. (2019) multi-task single-task multi-task multi-task 41.0 82.1 73.1 82.6 Crossst Crossmt t Crossfmt single-task multi-task multi-task 55.7 72.0 75.1 mBARTst mBARTmt t mBARTfmt single-task multi-task multi-task 77.0 79.9 76.9 Table 3: UCCA results on The Little Prince. particular, we report the score of Che et al. (2019), the system that ranked first in both all-framework and UCCA parsing. First of all, we note the result of Crossst ; while its performance is far below the score Che et al. (2019) achieve, it still outperforms the original proposed baseline by more than 10 points. Furthermore, to the b"
2021.naacl-main.30,N18-1104,0,0.254552,"MR and UCCA as our cases in point to show the effectiveness of this framework. In particular, we show that, once the learning procedure also considers large parallel corpora coming from Machine Translation (MT), this configuration becomes an effective approach for framework-independent parsing via a single model. Even more interestingly, this model scales surprisingly well to cross-lingual parsing and is capable of navigating through translation paths like IT → AMR,2 which it has never seen during training. The contributions of this work are therefore as follows: lingual setting introduced by Damonte and Cohen (2018) for the latter. Semantic Parsing Arguably among the formalisms that have drawn the most interest, AMR has seen the emergence of a rich yet dedicated literature, with recent approaches that can be roughly clustered into two groups. On the one hand, several graph-based solutions have been proposed (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020); among these solutions, Zhou et al. (2020) show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas Cai and Lam (2020) present an iterative method to build and refine AMR gra"
2021.naacl-main.30,E17-1051,0,0.123596,"explore the possibility of using a single multilingual model. Cross-lingual AMR While most of the research effort in the AMR community has been focused on English only, the seminal work of Damonte and Cohen (2018) gave rise to an interesting new direction, i.e. exploring the extent to which AMR can act as an interlingua. The authors introduced a new problem, cross-lingual AMR parsing, and defined it as the task of recovering, given a sentence in any language, the AMR graph corresponding to its English translation. Using an adapted version of the transition-based parser originally proposed by Damonte et al. (2017) and training it on silver data generated through annotation projection, they examined whether AMR graphs could be recovered starting from non-English sentences. Even though their models fell short when compared to MT alternatives,3 their work showed promising results and suggested that, despite translation divergences, AMR could act effectively as an interlingua. Annotation projection has been focal in subsequent work as well. Blloshmi et al. (2020) propose an aligner-free cross-lingual parser, thus disposing of the need for word alignments in the annotation projection pipeline; their parser"
2021.naacl-main.30,W19-6721,0,0.0578342,"Missing"
2021.naacl-main.30,2020.acl-demos.35,0,0.147292,"(§3.2) and, finally, we present our multilingual framework (§3.3). 3.1 Graph Linearizations We now describe how we convert the considered meaning representations into translatable text sequences (linearization), along with their reverse process (delinearization). For AMR parsing, as in van Noord and Bos (2017), we first simplify AMR graphs by removing variables and wiki links. We then convert these stripped AMR graphs into trees by duplicating coreferring nodes. At this point, in order to obtain the final linearized version of a given AMR, we concatenate all the lines of its PENMAN notation (Goodman, 2020) together, replacing newlines and multiple spaces with single spaces (Figure 1a and 3 1b). Conversely, delinearization is performed by The input sentence is first translated towards English and, then, an English parser is used. assigning a variable to each predicted concept, per327 forming Wikification,4 restoring co-referring nodes and, where possible, repairing any syntactically malformed subgraph.5 For both phases, we use the scripts released by van Noord and Bos (2017).6 For UCCA parsing, we employ a Depth-First Search (DFS) approach: starting from the root, we navigate the graph, using sq"
2021.naacl-main.30,D18-1086,0,0.079629,"ented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, while AMR uses nodes to encode concepts and edges to express the semantic relations between them, UCCA proposes using text tokens as terminal nodes and building graphs on top of them. As a result of this heterogeneous la"
2021.naacl-main.30,P18-1035,0,0.0469404,"Missing"
2021.naacl-main.30,S19-2001,0,0.0209345,"h as IT → AMR and thus tackle tasks like crosslingual AMR parsing. 4 4.2 Experimental Setup We assess the effectiveness of our proposed approach by evaluating its performance on all translation paths where the target language is a graph formalism, the only exception being X → UCCA, with X any language but English. This choice is motivated by the fact that, differently from AMR where cross-lingual AMR aims to produce Englishbased meaning representations (Damonte and Cohen, 2018), UCCA builds graphs on top of its tokens which are, consequently, inherently in the same language as the input text (Hershcovich et al., 2019); we leave exploring this direction to future work. 4.1 Models We choose to use both Cross, a randomly initialized Transformer, and mBART, a multilingual pretrained Transformer, to better grasp the effects of this joint multilingual framework in different regimes. In particular, we explore the following configurations: • models trained only on a single semantic parsing task (AMR or UCCA parsing) and without considering any parallel data, denoted by Crossst and mBARTst ; • models trained on both semantic parsing tasks and the MT data, denoted by Crossmt and mBARTmt . t t we also consider Crossf"
2021.naacl-main.30,K19-2002,0,0.250938,"limited to showing the effectiveness lingual AMR parsing without ever seeing non- of transfer learning from related tasks to English AMR parsing. English to AMR examples at training time and Conversely, here we show that the benefits of push the current state of the art even further multilingual seq2seq frameworks are not limited once we include these examples; to English TEXT-to-AMR but, rather, that they en• On UCCA parsing, we reach competitive re- able astonishing performances on unseen translasults, outperforming a strong BERT-powered tion paths such as IT → AMR and competitive baseline (Hershcovich and Arviv, 2019). results on other frameworks, using UCCA as our case in point. In this sense, we continue the reWe release our code and our models for research cent cross-framework trend formally started by the purposes. shared task of Oepen et al. (2019), exploring the possibility of using translation-based approaches 2 Related Work for framework-independent parsing, as opposed to Our work is mainly concerned with semantic parsthe transition-based parsers proposed in that semiing in UCCA and AMR, considering also the crossnal work. Our findings are in line with the recent 1 By across languages, we mean that"
2021.naacl-main.30,P17-4012,0,0.0192948,"Missing"
2021.naacl-main.30,P17-1014,0,0.0241347,"ral graph-based solutions have been proposed (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020); among these solutions, Zhou et al. (2020) show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas Cai and Lam (2020) present an iterative method to build and refine AMR graphs incrementally. On the other hand, translation-based approaches, where seq2seq models are trained to translate from natural language text to linearized graphs, have been shown to reach competitive performances, despite the scarcity of training data (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019). Continuing this latter direction and arguably closest to our work, Xu et al. (2020a) and Bevilacqua et al. (2021) show that these mod• We reframe semantic parsing towards multiple els, once paired with adequate pre-training, can formalisms and from multiple languages as behave on par or better than dedicated and more somultilingual machine translation; phisticated graph-based alternatives, surpassing the performances of Cai and Lam (2020). In particular, • On AMR parsing, our framework achieves similarly to our work, Xu et al. (2020a) leverage a com"
2021.naacl-main.30,P18-1007,0,0.0329025,"Missing"
2021.naacl-main.30,2020.acl-main.703,0,0.0257808,"NG and SPRINGbart (Bevilacqua et al., 2021), highlighting the potential of fully pre-trained Transformer language models for translation-based approaches. mBARTmt and t mBARTfmt push performances further up, showing that the MT data are beneficial even in this pretrained setting and that the multi-task training set, which enables a single shared model to scale across formalisms and languages, is not detrimental to English AMR parsing. However, arguably more interesting is the comparison between the performances of mBART models and SPRING, which, in contrast, builds upon the English-only BART (Lewis et al., 2020). In particular, as SPRINGbart outperforms even t mBARTfmt , this finding suggests that, as expected, BART is more suitable than mBART when dealing with English AMR. However, as we show in §5.2, our choice is beneficial for cross-lingual AMR parsing and results in an interesting trade-off. Finally, we also evaluate SGL on AMR-3.0 t and report the results of Crossfmt , mBARTst and ft mBARTmt when trained on this dataset (Figure 1 bottom). Overall, we witness a similar trend compared to AMR-2.0. Model Sheth et al. (2021) t mBARTfmt + AP 66.9 73.3 69.6 73.9 71.0 73.4 − 64.9 Table 2: Smatch scores"
2021.naacl-main.30,2020.tacl-1.47,0,0.0210099,"re (Vaswani et al., 2017). This architecture is essentially composed of two building blocks, namely, a Transformer encoder and a Transformer decoder. The encoder is a stack of N identical layers, each made up of two sublayers: the first is a multi-head self-attention mechanism, while the second is a position-wise fully connected feed-forward network. The decoder follows a similar architecture, presenting, however, an additional sub-layer that performs multi-head attention over the output of the encoder. Within this work, we use two different kinds of Transformer architecture, Cross and mBART (Liu et al., 2020). Cross is a randomly initialized Transformer closely following the architecture depicted by Vaswani et al. (2017), except for a significant difference: we leverage a factorized embedding parameterization (Lan et al., 2020), that is, we decompose the large vocabulary embedding matrix into two smaller matrices. While the first of these represents the actual embedding matrix and projects one-hot vectors into an embedding space whose dimension is lower than the Transformer hidden size, the second one takes care of projecting these intermediate representations towards the actual Transformer hidden"
2021.naacl-main.30,P18-1037,0,0.0131779,"e interestingly, this model scales surprisingly well to cross-lingual parsing and is capable of navigating through translation paths like IT → AMR,2 which it has never seen during training. The contributions of this work are therefore as follows: lingual setting introduced by Damonte and Cohen (2018) for the latter. Semantic Parsing Arguably among the formalisms that have drawn the most interest, AMR has seen the emergence of a rich yet dedicated literature, with recent approaches that can be roughly clustered into two groups. On the one hand, several graph-based solutions have been proposed (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020); among these solutions, Zhou et al. (2020) show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas Cai and Lam (2020) present an iterative method to build and refine AMR graphs incrementally. On the other hand, translation-based approaches, where seq2seq models are trained to translate from natural language text to linearized graphs, have been shown to reach competitive performances, despite the scarcity of training data (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019)."
2021.naacl-main.30,2020.conll-shared.1,0,0.0589067,"Missing"
2021.naacl-main.30,K19-2001,0,0.0353898,"Missing"
2021.naacl-main.30,oepen-lonning-2006-discriminant,0,0.0409478,"e-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions,"
2021.naacl-main.30,2020.emnlp-demos.6,0,0.0451078,"Missing"
2021.naacl-main.30,2021.eacl-main.30,0,0.26107,"glish sentences. Even though their models fell short when compared to MT alternatives,3 their work showed promising results and suggested that, despite translation divergences, AMR could act effectively as an interlingua. Annotation projection has been focal in subsequent work as well. Blloshmi et al. (2020) propose an aligner-free cross-lingual parser, thus disposing of the need for word alignments in the annotation projection pipeline; their parser manages to outperform MT alternatives when both annotation projection and these baselines have access to comparable amounts of data. Conversely, Sheth et al. (2021) leverage powerful contextualized word embeddings to improve the foreign-text-to-English-AMR alignments, surpassing all previous approaches and, most importantly, the yet-unbeaten MT baselines that have access to larger amounts of data. We stand out from previous research and show that, as a matter of fact, annotation projection techniques are not needed to perform cross-lingual AMR parsing. By jointly training on parallel corpora from MT and the EN → SP data we have, we find that a multilingual model can navigate unseen translation paths such as IT → AMR effectively, outperforming all current"
2021.naacl-main.30,Q19-1002,0,0.0539188,"s are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, while AMR uses nodes to encode concepts and edges to express the semantic relations between them, UCCA proposes using text tokens as terminal nodes and building graphs on top"
2021.naacl-main.30,N18-1063,0,0.0384738,"Missing"
2021.naacl-main.30,tiedemann-2012-parallel,0,0.0445895,"Missing"
2021.naacl-main.30,2020.eamt-1.61,0,0.0607834,"Missing"
2021.naacl-main.30,2020.emnlp-main.196,0,0.042063,"Missing"
2021.naacl-main.30,2020.wmt-1.104,0,0.575623,"ienzaNLP/sgl. formalisms such as Abstract Meaning Representation (Banarescu et al., 2013, AMR), Elementary Dependency Structures (Oepen and Lønning, 2006, EDS), Prague Tectogrammatical Graphs (Hajiˇc et al., 2012, PTG), Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013, UCCA), inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation (Song et al., 2019), Text Summarization (Hardy and Vlachos, 2018), Human-Robot Interaction (Bonial et al., 2020), and as evaluation metrics (Sulem et al., 2018; Xu et al., 2020b). These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, while AMR uses nodes to encode concepts and edges to express the semantic relations between them, UCCA proposes using text tokens as terminal nodes and building graphs on top of them. As a result of this heterogeneous landscape, often referred to as framework-specific balkanization (Oepen et al., 2020), graph-based semantic parsi"
2021.naacl-main.30,P19-1009,0,0.036541,"Missing"
2021.naacl-main.30,D19-1392,0,0.0947796,"Missing"
2021.naacl-main.30,2020.acl-main.397,0,0.7804,"gly well to cross-lingual parsing and is capable of navigating through translation paths like IT → AMR,2 which it has never seen during training. The contributions of this work are therefore as follows: lingual setting introduced by Damonte and Cohen (2018) for the latter. Semantic Parsing Arguably among the formalisms that have drawn the most interest, AMR has seen the emergence of a rich yet dedicated literature, with recent approaches that can be roughly clustered into two groups. On the one hand, several graph-based solutions have been proposed (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020); among these solutions, Zhou et al. (2020) show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas Cai and Lam (2020) present an iterative method to build and refine AMR graphs incrementally. On the other hand, translation-based approaches, where seq2seq models are trained to translate from natural language text to linearized graphs, have been shown to reach competitive performances, despite the scarcity of training data (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019). Continuing this latter direction and argua"
2021.naacl-main.31,P15-1039,0,0.0212465,"7) made et al., 2008) predicate-argument structure inventory use of Graph Convolutional Networks (GCNs) to for Catalan and Spanish, the German Proposition better capture relations between neighboring nodes Bank which, differently from the other PropBanks, in syntactic dependency trees; Strubell et al. (2018) is derived from FrameNet (Hajic et al., 2009), and 339 PDT-Vallex (Hajic et al., 2003) for Czech. Many of these inventories are not aligned with each other as they follow and implement different linguistic theories which, in turn, may pose different challenges. Padó and Lapata (2009), and Akbik et al. (2015, 2016) worked around these issues by making the English PropBank act as a universal predicate sense and semantic role inventory and projecting PropBank-style annotations from English onto nonEnglish sentences by means of word alignment techniques applied to parallel corpora such as Europarl (Koehn, 2005). These efforts resulted in the creation of the Universal PropBank, a multilingual collection of semi-automatically annotated corpora for SRL, which is actively in use today to train and evaluate novel cross-lingual methods such as word alignment techniques (Aminian et al., 2019). In the absen"
2021.naacl-main.31,D16-1102,0,0.0406157,"Missing"
2021.naacl-main.31,W19-0417,0,0.0114148,"pata (2009), and Akbik et al. (2015, 2016) worked around these issues by making the English PropBank act as a universal predicate sense and semantic role inventory and projecting PropBank-style annotations from English onto nonEnglish sentences by means of word alignment techniques applied to parallel corpora such as Europarl (Koehn, 2005). These efforts resulted in the creation of the Universal PropBank, a multilingual collection of semi-automatically annotated corpora for SRL, which is actively in use today to train and evaluate novel cross-lingual methods such as word alignment techniques (Aminian et al., 2019). In the absence of parallel corpora, annotation projection techniques can still be applied by automatically translating an annotated corpus and then projecting the original labels onto the newly created silver corpus (Daza and Frank, 2020; Fei et al., 2020), whereas Daza and Frank (2019) have recently found success in training an encoder-decoder architecture to jointly tackle SRL and translation. 3 Model Description In the wake of recent work in SRL, our model falls into the broad category of end-to-end systems as it learns to jointly tackle predicate identification, predicate sense disambigu"
2021.naacl-main.31,W09-1206,0,0.0603409,"resources; • We automatically build and release a crosslingual mapping that aligns linguistic formalisms from diverse languages. We hope that our unified model will further advance cross-lingual SRL and represent a tool for the analysis and comparison of linguistic theories across multiple languages. 2 Related Work End-to-end SRL. The SRL pipeline is usually divided into four steps: predicate identification, predicate sense disambiguation, argument identification, and argument classification. While early research focused its efforts on addressing each step individually (Xue and Palmer, 2004; Björkelund et al., 2009; Zhao et al., 2009), recent work has successfully demonstrated that tackling some of these subtasks jointly with multitask learning (Caruana, 1997) is beneficial. In particular, He et al. (2018) and, subsequently, Cai et al. (2018), Li et al. (2019) and Conia et al. (2020), indicate that predicate sense signals aid the identification of predicateargument relations. Therefore, we follow this line and propose an end-to-end system for cross-lingual SRL. demonstrated the effectiveness of linguisticallyinformed self-attention layers in SRL; Cai and Lapata (2019b) observed that syntactic dependenci"
2021.naacl-main.31,N19-1075,0,0.0140003,"erogeneous linguistic resources. Our model may present significant structural differences from implicitly learns a high-quality mapping for language to language (Hajic et al., 2009). In the redifferent formalisms across diverse languages cent literature, it is standard practice to sidestep this without resorting to word alignment and/or translation techniques. We find that, not only is issue by training and evaluating a model on each our cross-lingual system competitive with the language separately (Cai and Lapata, 2019b; Chen current state of the art but that it is also robust et al., 2019; Kasai et al., 2019; He et al., 2019; Lyu to low-data scenarios. Most interestingly, our et al., 2019). Although this strategy allows a model unified model is able to annotate a sentence to adapt itself to the characteristics of a given forin a single forward pass with all the inventomalism, it is burdened by the non-negligible need ries it was trained with, providing a tool for for training and maintaining one model instance the analysis and comparison of linguistic theofor each language, resulting in a set of monolingual ries across different languages. We release our code and model at https://github.com/ syst"
2021.naacl-main.31,D18-1548,0,0.0254247,"quality and diversity of and NomBank (Meyers et al., 2004) to annotate Enthe information encoded by syntax is an enticing glish sentences, the Chinese Proposition Bank (Xue prospect that has resulted in a wide range of con- and Palmer, 2009) for Chinese, the AnCora (Taulé tributions: Marcheggiani and Titov (2017) made et al., 2008) predicate-argument structure inventory use of Graph Convolutional Networks (GCNs) to for Catalan and Spanish, the German Proposition better capture relations between neighboring nodes Bank which, differently from the other PropBanks, in syntactic dependency trees; Strubell et al. (2018) is derived from FrameNet (Hajic et al., 2009), and 339 PDT-Vallex (Hajic et al., 2003) for Czech. Many of these inventories are not aligned with each other as they follow and implement different linguistic theories which, in turn, may pose different challenges. Padó and Lapata (2009), and Akbik et al. (2015, 2016) worked around these issues by making the English PropBank act as a universal predicate sense and semantic role inventory and projecting PropBank-style annotations from English onto nonEnglish sentences by means of word alignment techniques applied to parallel corpora such as Europar"
2021.naacl-main.31,2005.mtsummit-papers.11,0,0.0281371,"derived from FrameNet (Hajic et al., 2009), and 339 PDT-Vallex (Hajic et al., 2003) for Czech. Many of these inventories are not aligned with each other as they follow and implement different linguistic theories which, in turn, may pose different challenges. Padó and Lapata (2009), and Akbik et al. (2015, 2016) worked around these issues by making the English PropBank act as a universal predicate sense and semantic role inventory and projecting PropBank-style annotations from English onto nonEnglish sentences by means of word alignment techniques applied to parallel corpora such as Europarl (Koehn, 2005). These efforts resulted in the creation of the Universal PropBank, a multilingual collection of semi-automatically annotated corpora for SRL, which is actively in use today to train and evaluate novel cross-lingual methods such as word alignment techniques (Aminian et al., 2019). In the absence of parallel corpora, annotation projection techniques can still be applied by automatically translating an annotated corpus and then projecting the original labels onto the newly created silver corpus (Daza and Frank, 2020; Fei et al., 2020), whereas Daza and Frank (2019) have recently found success in"
2021.naacl-main.31,taule-etal-2008-ancora,0,0.670347,"Missing"
2021.naacl-main.31,2020.emnlp-main.13,0,0.0198423,"ained language models such as ELMo (Peters not always be the case. Among the numerous stud- et al., 2018), BERT (Devlin et al., 2019) and XLMies that adopt the English PropBank as a universal RoBERTa (Conneau et al., 2020), inter alia, are predicate-argument structure inventory for cross- becoming the de facto input representation method, lingual SRL, the work of Mulcaire et al. (2018) thanks to their ability to encode vast amounts of stands out for proposing a bilingual model that is knowledge. Following recent studies (Hewitt and able to perform SRL according to two different Manning, 2019; Kuznetsov and Gurevych, 2020; inventories at the same time, although with signif- Conia and Navigli, 2020), which show that differicantly lower results compared to the state of the ent layers of a language model capture different art at the time. With our work, we go beyond cur- syntactic and semantic characteristics, our model rent approaches to cross-lingual SRL and embrace builds a contextual representation for an input word the diversity of the various representations made by concatenating the corresponding hidden states available in different languages. In particular, our of the four top-most inner layers of a langu"
2021.naacl-main.31,W04-2705,0,0.199391,"s-lingual SRL. A key challenge in performing cross-lingual SRL with a single unified model is the dissimilarity of predicate sense and semantic Multilingual SRL. Current work in multilingual role inventories between languages. For example, SRL revolves mainly around the development of the multilingual dataset distributed as part of the novel neural architectures, which fall into two CoNLL-2009 shared task (Hajic et al., 2009) adopts broad categories, syntax-aware and syntax-agnostic the English Proposition Bank (Palmer et al., 2005) ones. On one hand, the quality and diversity of and NomBank (Meyers et al., 2004) to annotate Enthe information encoded by syntax is an enticing glish sentences, the Chinese Proposition Bank (Xue prospect that has resulted in a wide range of con- and Palmer, 2009) for Chinese, the AnCora (Taulé tributions: Marcheggiani and Titov (2017) made et al., 2008) predicate-argument structure inventory use of Graph Convolutional Networks (GCNs) to for Catalan and Spanish, the German Proposition better capture relations between neighboring nodes Bank which, differently from the other PropBanks, in syntactic dependency trees; Strubell et al. (2018) is derived from FrameNet (Hajic et a"
2021.naacl-main.31,P18-2106,0,0.0454454,"Missing"
2021.naacl-main.31,J05-1004,0,0.140202,"by exploiting the information available in other, resource-richer languages. Cross-lingual SRL. A key challenge in performing cross-lingual SRL with a single unified model is the dissimilarity of predicate sense and semantic Multilingual SRL. Current work in multilingual role inventories between languages. For example, SRL revolves mainly around the development of the multilingual dataset distributed as part of the novel neural architectures, which fall into two CoNLL-2009 shared task (Hajic et al., 2009) adopts broad categories, syntax-aware and syntax-agnostic the English Proposition Bank (Palmer et al., 2005) ones. On one hand, the quality and diversity of and NomBank (Meyers et al., 2004) to annotate Enthe information encoded by syntax is an enticing glish sentences, the Chinese Proposition Bank (Xue prospect that has resulted in a wide range of con- and Palmer, 2009) for Chinese, the AnCora (Taulé tributions: Marcheggiani and Titov (2017) made et al., 2008) predicate-argument structure inventory use of Graph Convolutional Networks (GCNs) to for Catalan and Spanish, the German Proposition better capture relations between neighboring nodes Bank which, differently from the other PropBanks, in synta"
2021.naacl-main.371,2020.acl-main.463,0,0.0161074,"neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issues and reframe the WSD problem as a it can aid neural models to ground their represen- novel text extraction task, which we have called tations on structured resources and enable Natural Extractive Sense Comprehension (ESC). In this Language Understanding (Navigli, 2018). A task setting, a model receives as input a sentence with that is key to achieving this goal is Word Sense a target word and all its possible sense definitions. Disambiguation (WSD), where, given a sentence Then, we request the model to extract the text ∗ span associated with the gloss expressing the target Wor"
2021.naacl-main.371,2020.emnlp-main.585,1,0.780369,"Raganato et al., 2017b; Hadiwinoto et al., 2019), more recent approaches have started to exploit sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and relational information (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021). Sense definitions, in particular, have been shown to be effective for modeling word senses (Luo et al., 2018; Kumar et al., 2019), as they provide information orthogonal to that available in the training data. This has been further investigated under different perspectives by Huang et al. (2019, GlossBERT), Blevins and Zettlemoyer (2020, BEM) and Bevilacqua et al. (2020, Generationary). GlossBERT casts the WSD problem as a binary classification task where, given a word in context and one of its dictionary definitions, it determines whether this definition matches the word meaning expressed in the context. BEM employs a bi-encoder to represent the target word and its sense definitions within the same space. Generationary, instead, has predefined sense inventories at its disposal and directly generates a definition given a word in its context. The strength of these approaches lies in the fact that glosses allow senses that are under-represented within the trai"
2021.naacl-main.371,2020.acl-main.255,1,0.93934,"combining data annotated with distinct lexicographic resources. Besides its performance advantages, ESC also comes with other benefits: it does not require a large output vocabulary, and it eases the joint use of corpora annotated with different inventories. 2 Related Work synsets (concepts) and edges are typed semantic relations. While early neural models used WordNet as a mere repository of senses (Raganato et al., 2017b; Hadiwinoto et al., 2019), more recent approaches have started to exploit sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and relational information (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021). Sense definitions, in particular, have been shown to be effective for modeling word senses (Luo et al., 2018; Kumar et al., 2019), as they provide information orthogonal to that available in the training data. This has been further investigated under different perspectives by Huang et al. (2019, GlossBERT), Blevins and Zettlemoyer (2020, BEM) and Bevilacqua et al. (2020, Generationary). GlossBERT casts the WSD problem as a binary classification task where, given a word in context and one of its dictionary definitions, it determines whether this definition matches th"
2021.naacl-main.371,2020.acl-main.95,0,0.465089,"i-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issues and reframe the WSD problem as a it can aid neural"
2021.naacl-main.371,2021.eacl-main.286,1,0.770654,"distinct lexicographic resources. Besides its performance advantages, ESC also comes with other benefits: it does not require a large output vocabulary, and it eases the joint use of corpora annotated with different inventories. 2 Related Work synsets (concepts) and edges are typed semantic relations. While early neural models used WordNet as a mere repository of senses (Raganato et al., 2017b; Hadiwinoto et al., 2019), more recent approaches have started to exploit sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and relational information (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021). Sense definitions, in particular, have been shown to be effective for modeling word senses (Luo et al., 2018; Kumar et al., 2019), as they provide information orthogonal to that available in the training data. This has been further investigated under different perspectives by Huang et al. (2019, GlossBERT), Blevins and Zettlemoyer (2020, BEM) and Bevilacqua et al. (2020, Generationary). GlossBERT casts the WSD problem as a binary classification task where, given a word in context and one of its dictionary definitions, it determines whether this definition matches the word meaning expressed i"
2021.naacl-main.371,Q15-1038,1,0.809929,"l resources, achieving performances that were previously out of everyone’s reach. The model along with data is available at https://github.com/ SapienzaNLP/esc. 1 with a target word, a model has to predict its most suitable meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot"
2021.naacl-main.371,D19-1533,0,0.528688,"ned set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none o"
2021.naacl-main.371,D19-1355,0,0.456332,"een framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issues and reframe the WS"
2021.naacl-main.371,P19-1568,0,0.611188,"er, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issue"
2021.naacl-main.371,2020.acl-main.703,0,0.0481068,", which uses a vocabulary compression technique; EWISE (Kumar et al., 2019); GlossBERT (Huang et al., 2019); BEM8 (Blevins and Zettlemoyer, 2020) and EWISER (Bevilacqua and Navigli, 2020), which take advantage of external knowledge such as glosses and semantic relations. We note that EWISER uses a different development set, hence its results are not fully comparable with the others. Finally, we also consider two nearest-neighbour approaches based on synset embedding and vector similarity, i.e., LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020). E SCHER Setting We use BARTlarge (Lewis et al., 2020; Wolf et al., 2020) as transformer architecture9 owing to the fact that it is among the strongest models on reading comprehension tasks 5 A (lemma, part of speech) pair. We identify a sense as a pair (lexeme, definition). 7 Similarly to Blevins and Zettlemoyer (2020), we report the best results of the SVC single model trained on SemCor only. 8 BEM is the state-of-the-art model in this setting at the time of writing. 9 Please see Appendix A for experiments with different transformer pretrained models. 6 such as SQuAD (Rajpurkar et al., 2016) and it allows us to feed sequences up to 1024 subtok"
2021.naacl-main.371,N18-1121,0,0.0224243,"ons. Furthermore, E SCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone’s reach. The model along with data is available at https://github.com/ SapienzaNLP/esc. 1 with a target word, a model has to predict its most suitable meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains ling"
2021.naacl-main.371,2021.ccl-1.108,0,0.0697339,"Missing"
2021.naacl-main.371,P19-1569,0,0.0438551,"eeps BERT weights frozen and trains a gated linear unit on top of it; SVC7 (Vial et al., 2019), which uses a vocabulary compression technique; EWISE (Kumar et al., 2019); GlossBERT (Huang et al., 2019); BEM8 (Blevins and Zettlemoyer, 2020) and EWISER (Bevilacqua and Navigli, 2020), which take advantage of external knowledge such as glosses and semantic relations. We note that EWISER uses a different development set, hence its results are not fully comparable with the others. Finally, we also consider two nearest-neighbour approaches based on synset embedding and vector similarity, i.e., LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020). E SCHER Setting We use BARTlarge (Lewis et al., 2020; Wolf et al., 2020) as transformer architecture9 owing to the fact that it is among the strongest models on reading comprehension tasks 5 A (lemma, part of speech) pair. We identify a sense as a pair (lexeme, definition). 7 Similarly to Blevins and Zettlemoyer (2020), we report the best results of the SVC single model trained on SemCor only. 8 BEM is the state-of-the-art model in this setting at the time of writing. 9 Please see Appendix A for experiments with different transformer pretrained models. 6 such"
2021.naacl-main.371,P18-1230,0,0.0575795,"require a large output vocabulary, and it eases the joint use of corpora annotated with different inventories. 2 Related Work synsets (concepts) and edges are typed semantic relations. While early neural models used WordNet as a mere repository of senses (Raganato et al., 2017b; Hadiwinoto et al., 2019), more recent approaches have started to exploit sense definitions (Kumar et al., 2019; Blevins and Zettlemoyer, 2020) and relational information (Bevilacqua and Navigli, 2020; Conia and Navigli, 2021). Sense definitions, in particular, have been shown to be effective for modeling word senses (Luo et al., 2018; Kumar et al., 2019), as they provide information orthogonal to that available in the training data. This has been further investigated under different perspectives by Huang et al. (2019, GlossBERT), Blevins and Zettlemoyer (2020, BEM) and Bevilacqua et al. (2020, Generationary). GlossBERT casts the WSD problem as a binary classification task where, given a word in context and one of its dictionary definitions, it determines whether this definition matches the word meaning expressed in the context. BEM employs a bi-encoder to represent the target word and its sense definitions within the same"
2021.naacl-main.371,P19-1334,0,0.0141599,"ns (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issues and reframe the WSD problem as a it can aid neural models to ground their represen- novel text extraction task, which we have called tations on structured resources and enable Natural Extractive Sense Comprehension (ESC). In this Language Understanding (Navigli, 2018). A task setting, a model receives as input a sentence with that is key to achieving this goal is Word Sense a target word and all its possible sense definitions. Disambiguation (WSD), where, given a sentence Then, we request the model to extract the text ∗ span associated with the gloss"
2021.naacl-main.371,H93-1061,0,0.384076,"even when they were not available at the time of training. Word Sense Disambiguation (WSD) is one of the long-standing problems in lexical semantics, introduced for the first time in the context of Machine Translation by Weaver (1949). WSD aims at linking a word in context to its most suitable meaning in a predefined sense inventory, which is usually a dictionary where each entry defines a concept via a definition (gloss) and a set of examples. Most approaches to WSD rely on WordNet (Miller et al., 1990) as the underlying inventory of senses for the English language, and SemCor 3 Methodology (Miller et al., 1993) as training corpus. WordNet organizes lexical-semantic information by means In what follows, we first formalize the Extractive of a graph where sets of synonyms are grouped into Sense Comprehension task (Section 3.1), then in4662 [...] Move backwards from a certain position . [...] j=24 i=18 ArgMax Start Softmax Z End Softmax Ze s Linear Layer Transformer . s&gt; </ ce a rta i po n sit io n w ar ds fro m . e ov M ck ba a lig n at io n ob . < Re /s&gt; m ov on e es el f fro m t&gt; </ to <t &gt; ba ck do w n ha d <s &gt; Th e bu lly W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11 W12 W13 W14 W15 W16 W17 W18 W19 W20 W21 W"
2021.naacl-main.371,S15-2049,1,0.887163,"a We use the evaluation suite made available by Raganato et al. (2017a) for the English Word Sense Disambiguation task. It includes SemCor (Miller et al., 1993) for training, i.e., a corpus containing 33,362 sentences and 226,036 instances annotated manually with senses from WordNet 3.0. As common practice, we use SemEval-2007 (SE07; Pradhan et al., 2007) as development set. For testing, we consider all the remaining datasets in the suite, i.e., Senseval-2 (SE2; Edmonds and Cotton, 2001), Senseval-3 (SE3; Snyder and Palmer, 2004), SemEval-2013 (SE13; Navigli et al., 2013), SemEval-2015 (SE15; Moro and Navigli, 2015) and their concatenation (ALL).4 In order to measure the extent to which systems generalize to rare and unseen words and definitions (zero-shot settings), we also consider five other test sets that we created from the ALL dataset: While our approach already allows all the possible definitions of a word to be contextualized by jointly encoding them together with the context sentence, it may still suffer from the high unbalance in sense distribution (Kilgarriff, 2004) and be biased towards the most frequent definition regardless of its contextualization. Our framework allows this issue to be dea"
2021.naacl-main.371,S13-2040,1,0.862142,"work and neural architecture. 4.1 Setup Data We use the evaluation suite made available by Raganato et al. (2017a) for the English Word Sense Disambiguation task. It includes SemCor (Miller et al., 1993) for training, i.e., a corpus containing 33,362 sentences and 226,036 instances annotated manually with senses from WordNet 3.0. As common practice, we use SemEval-2007 (SE07; Pradhan et al., 2007) as development set. For testing, we consider all the remaining datasets in the suite, i.e., Senseval-2 (SE2; Edmonds and Cotton, 2001), Senseval-3 (SE3; Snyder and Palmer, 2004), SemEval-2013 (SE13; Navigli et al., 2013), SemEval-2015 (SE15; Moro and Navigli, 2015) and their concatenation (ALL).4 In order to measure the extent to which systems generalize to rare and unseen words and definitions (zero-shot settings), we also consider five other test sets that we created from the ALL dataset: While our approach already allows all the possible definitions of a word to be contextualized by jointly encoding them together with the context sentence, it may still suffer from the high unbalance in sense distribution (Kilgarriff, 2004) and be biased towards the most frequent definition regardless of its contextualizati"
2021.naacl-main.371,D19-1005,0,0.0391852,"Missing"
2021.naacl-main.371,S07-1016,0,0.1775,"at which frequent definitions are seen only as positive examples without overly affecting rare senses. 4 Standard WSD Evaluation In this Section we introduce the experimental setting we use to evaluate the proposed framework and neural architecture. 4.1 Setup Data We use the evaluation suite made available by Raganato et al. (2017a) for the English Word Sense Disambiguation task. It includes SemCor (Miller et al., 1993) for training, i.e., a corpus containing 33,362 sentences and 226,036 instances annotated manually with senses from WordNet 3.0. As common practice, we use SemEval-2007 (SE07; Pradhan et al., 2007) as development set. For testing, we consider all the remaining datasets in the suite, i.e., Senseval-2 (SE2; Edmonds and Cotton, 2001), Senseval-3 (SE3; Snyder and Palmer, 2004), SemEval-2013 (SE13; Navigli et al., 2013), SemEval-2015 (SE15; Moro and Navigli, 2015) and their concatenation (ALL).4 In order to measure the extent to which systems generalize to rare and unseen words and definitions (zero-shot settings), we also consider five other test sets that we created from the ALL dataset: While our approach already allows all the possible definitions of a word to be contextualized by jointl"
2021.naacl-main.371,Q18-1044,0,0.0213217,"E SCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone’s reach. The model along with data is available at https://github.com/ SapienzaNLP/esc. 1 with a target word, a model has to predict its most suitable meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpre"
2021.naacl-main.371,E17-1010,1,0.956635,"e meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their"
2021.naacl-main.371,D17-1120,1,0.95608,"e meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their"
2021.naacl-main.371,D16-1264,0,0.309864,"either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architectures (Kumar et al., 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020). Yet, despite their large improvements, none of these models attends all the possible definitions of a target word at once, and therefore each lacks the ability to represent both the input context and the candidate definitions together. Introduction Inspired by the Extractive Reading ComprehenBeing able to link a piece of raw text to a knowl- sion framework (Rajpurkar et al., 2016) in the edge base is fundamental in NLP (Navigli, 2009; field of Question Answering (QA), we cope with McCoy et al., 2019; Bender and Koller, 2020), as these issues and reframe the WSD problem as a it can aid neural models to ground their represen- novel text extraction task, which we have called tations on structured resources and enable Natural Extractive Sense Comprehension (ESC). In this Language Understanding (Navigli, 2018). A task setting, a model receives as input a sentence with that is key to achieving this goal is Word Sense a target word and all its possible sense definitions. Disa"
2021.naacl-main.371,2020.emnlp-main.285,1,0.821548,"a gated linear unit on top of it; SVC7 (Vial et al., 2019), which uses a vocabulary compression technique; EWISE (Kumar et al., 2019); GlossBERT (Huang et al., 2019); BEM8 (Blevins and Zettlemoyer, 2020) and EWISER (Bevilacqua and Navigli, 2020), which take advantage of external knowledge such as glosses and semantic relations. We note that EWISER uses a different development set, hence its results are not fully comparable with the others. Finally, we also consider two nearest-neighbour approaches based on synset embedding and vector similarity, i.e., LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020). E SCHER Setting We use BARTlarge (Lewis et al., 2020; Wolf et al., 2020) as transformer architecture9 owing to the fact that it is among the strongest models on reading comprehension tasks 5 A (lemma, part of speech) pair. We identify a sense as a pair (lexeme, definition). 7 Similarly to Blevins and Zettlemoyer (2020), we report the best results of the SVC single model trained on SemCor only. 8 BEM is the state-of-the-art model in this setting at the time of writing. 9 Please see Appendix A for experiments with different transformer pretrained models. 6 such as SQuAD (Rajpurkar et al., 2016"
2021.naacl-main.371,W04-0811,0,0.101517,"setting we use to evaluate the proposed framework and neural architecture. 4.1 Setup Data We use the evaluation suite made available by Raganato et al. (2017a) for the English Word Sense Disambiguation task. It includes SemCor (Miller et al., 1993) for training, i.e., a corpus containing 33,362 sentences and 226,036 instances annotated manually with senses from WordNet 3.0. As common practice, we use SemEval-2007 (SE07; Pradhan et al., 2007) as development set. For testing, we consider all the remaining datasets in the suite, i.e., Senseval-2 (SE2; Edmonds and Cotton, 2001), Senseval-3 (SE3; Snyder and Palmer, 2004), SemEval-2013 (SE13; Navigli et al., 2013), SemEval-2015 (SE15; Moro and Navigli, 2015) and their concatenation (ALL).4 In order to measure the extent to which systems generalize to rare and unseen words and definitions (zero-shot settings), we also consider five other test sets that we created from the ALL dataset: While our approach already allows all the possible definitions of a word to be contextualized by jointly encoding them together with the context sentence, it may still suffer from the high unbalance in sense distribution (Kilgarriff, 2004) and be biased towards the most frequent d"
2021.naacl-main.371,2019.gwc-1.14,0,0.0550971,"ciated with different lexemes. Comparison Systems As baselines, we consider the Most Frequent Sense computed on the training set (MFS SemCor) and two neural models featuring BERTlarge and BARTlarge as text encoders, with a linear classifier over the whole sense vocabulary on top. As for the BERTlarge baseline, we follow Blevins and Zettlemoyer (2020) and keep BERTlarge weights fixed, while for BARTlarge we finetune the whole model. As competitors, we consider the following models: GLU (Hadiwinoto et al., 2019), which keeps BERT weights frozen and trains a gated linear unit on top of it; SVC7 (Vial et al., 2019), which uses a vocabulary compression technique; EWISE (Kumar et al., 2019); GlossBERT (Huang et al., 2019); BEM8 (Blevins and Zettlemoyer, 2020) and EWISER (Bevilacqua and Navigli, 2020), which take advantage of external knowledge such as glosses and semantic relations. We note that EWISER uses a different development set, hence its results are not fully comparable with the others. Finally, we also consider two nearest-neighbour approaches based on synset embedding and vector similarity, i.e., LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020). E SCHER Setting We use BARTlarge ("
2021.naacl-main.371,J82-2005,0,0.556456,"ESC) for the WSD problem stands out from previous approaches inasmuch as it is the first to access the input context and all the target word’s definitions together, while, at the same time, dropping the requirement of a predefined sense inventory. Indeed, differently from its competitors, our proposed approach (E SCHER) can scale effectively across different lexical resources even when they were not available at the time of training. Word Sense Disambiguation (WSD) is one of the long-standing problems in lexical semantics, introduced for the first time in the context of Machine Translation by Weaver (1949). WSD aims at linking a word in context to its most suitable meaning in a predefined sense inventory, which is usually a dictionary where each entry defines a concept via a definition (gloss) and a set of examples. Most approaches to WSD rely on WordNet (Miller et al., 1990) as the underlying inventory of senses for the English language, and SemCor 3 Methodology (Miller et al., 1993) as training corpus. WordNet organizes lexical-semantic information by means In what follows, we first formalize the Extractive of a graph where sets of synonyms are grouped into Sense Comprehension task (Section 3"
2021.naacl-main.371,2020.emnlp-demos.6,0,0.0405598,"Missing"
2021.naacl-main.371,P19-1139,0,0.0133336,"ps://github.com/ SapienzaNLP/esc. 1 with a target word, a model has to predict its most suitable meaning from a predefined set of labels, i.e., its senses. WSD has not only considerably improved its performance with the advent of deep learning (by around 15 F1 points in 15 years), but it has also shown its benefits in downstream applications such as Neural Machine Translation (Liu et al., 2018; Pu et al., 2018) and Information Extraction (Moro and Navigli, 2013; Delli Bovi et al., 2015), while also being leveraged to enrich the contextual representations of neural models (Peters et al., 2019; Zhang et al., 2019). However, WSD has mostly been framed as a multi-label classification task (Raganato et al., 2017b; Hadiwinoto et al., 2019) over a very large vocabulary of discrete senses. This formulation may limit a model’s capabilities to properly represent word meanings, as each sense is only defined by means of its occurrences in a training set, while its inherent meaning remains linguistically unexpressed. Furthermore, rare or unseen senses are either poorly modeled or cannot be modeled at all. These problems have recently been mitigated by integrating sense definitions (glosses) within neural architec"
2021.semeval-1.3,2021.semeval-1.62,0,0.0289823,"elopment dataset. Then, bert-large-cased embeddings were fine-tuned using AdamW as optimizer with a learning rate equal to 1e-5. Each sentence was split by BertTokenizerFast into 118 tokens maximum. The model was trained for 4.5 epochs and stopped by Early Stopping with patience equal to 2. For each sentence, zhestyatsky took the embeddings of all sub-tokens corresponding to the target word and max pooled them into one embedding. Subsequently, zhestyatsky evaluated the cosine similarity of these embeddings and activated this value through ReLU. 6 Baselines MCL@IITK First, the MCL@IITK17 team (Gupta et al., 2021) pre-processed the sentences by adding a signal, either double quotes on both sides of the target word, or the target word itself appended to the end of the sentence. For En-En, MCL@IITK enriched the MCL-WiC training data using sentence reversal augmentation, WiC and SemCor. MCL@IITK obtained embeddings of the target words using the last hidden layer, and passed them to a logistic regression unit. MCL@IITK used ELECTRA, ALBERT, and XLM-RoBERTa as language models and submitted probability sum ensembles. For the non-English multilingual subtask, MCL@IITK used XLM-RoBERTa only and Following Ragan"
2021.semeval-1.3,P12-1092,0,0.0769424,"rios in which systems are tested in more than one language at the same time, thus highlighting the need for a new evaluation benchmark. Related Work 3 Several different tasks have been put forward which go beyond traditional WSD and drop the requirement of fixed sense inventories. Among the first alternatives we cite monolingual and cross-lingual Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010). Word-in-context similarity has also been proposed as a way to capture the dynamic nature of word meanings: the Stanford Contextual Word Similarities (SCWS) dataset, proposed by Huang et al. (2012), contains human judgements on pairs of words in context. Along these same lines, Armendariz et al. introduced CoSimLex, a dataset designed to evaluate the ability of models to capture word similarity judgements provided by humans. The Multilingual and Cross-lingual Word-in-Context Task In this Section, we present our SemEval task and describe a new dataset called Multilingual and Cross-lingual Word-in-Context (MCL-WiC). The task is divided into a multilingual and a crosslingual sub-task, each containing different datasets divided according to language combination. Each dataset instance is foc"
2021.semeval-1.3,P15-1010,1,0.88385,"Missing"
2021.semeval-1.3,2020.acl-main.747,0,0.524979,"Italy first.lastname@uniroma1.it Abstract a target word by clustering occurrences based on their context similarities (Neelakantan et al., 2015; Pelevina et al., 2016). In an effort to exploit the knowledge derived from lexical-knowledge bases, Iacobacci et al. (2015) introduced a new approach which allows sense representations to be linked to a predefined sense inventory. More recently, contextualized embeddings were proposed. These representations are obtained by means of neural language modeling, e.g. using LSTMs (Melamud et al., 2016) or the Transformer architecture (Devlin et al., 2019; Conneau et al., 2020), and are capable of representing words based on the context in which they occur. Contextualized representations have also been used to obtain effective sense embeddings (Loureiro and Jorge, 2019; Scarlini et al., 2020a,b; Calabrese et al., 2020). Although virtually all the above approaches can be evaluated in downstream applications, the inherent ability of the various embeddings to capture meaning distinctions still remains largely underinvestigated. While Word Sense Disambiguation (WSD), i.e. the task of determining the meaning of a word in a given context (Navigli, 2009), has long explored"
2021.semeval-1.3,2021.ccl-1.108,0,0.0572786,"Missing"
2021.semeval-1.3,P19-1569,0,0.140576,"t the knowledge derived from lexical-knowledge bases, Iacobacci et al. (2015) introduced a new approach which allows sense representations to be linked to a predefined sense inventory. More recently, contextualized embeddings were proposed. These representations are obtained by means of neural language modeling, e.g. using LSTMs (Melamud et al., 2016) or the Transformer architecture (Devlin et al., 2019; Conneau et al., 2020), and are capable of representing words based on the context in which they occur. Contextualized representations have also been used to obtain effective sense embeddings (Loureiro and Jorge, 2019; Scarlini et al., 2020a,b; Calabrese et al., 2020). Although virtually all the above approaches can be evaluated in downstream applications, the inherent ability of the various embeddings to capture meaning distinctions still remains largely underinvestigated. While Word Sense Disambiguation (WSD), i.e. the task of determining the meaning of a word in a given context (Navigli, 2009), has long explored the aforementioned ability, the task does not make it easy to test approaches that are not explicitly linked to existing sense inventories, such as WordNet (Miller et al., 1990) and BabelNet (Na"
2021.semeval-1.3,2021.semeval-1.103,0,0.0445154,"Missing"
2021.semeval-1.3,S07-1009,1,0.429625,"llowing for zero-shot settings. Despite their effectiveness, both the WiC and XL-WiC datasets are not manually created and do not cover all open-class parts of speech. Moreover, they do not consider cross-lingual evaluation scenarios in which systems are tested in more than one language at the same time, thus highlighting the need for a new evaluation benchmark. Related Work 3 Several different tasks have been put forward which go beyond traditional WSD and drop the requirement of fixed sense inventories. Among the first alternatives we cite monolingual and cross-lingual Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010). Word-in-context similarity has also been proposed as a way to capture the dynamic nature of word meanings: the Stanford Contextual Word Similarities (SCWS) dataset, proposed by Huang et al. (2012), contains human judgements on pairs of words in context. Along these same lines, Armendariz et al. introduced CoSimLex, a dataset designed to evaluate the ability of models to capture word similarity judgements provided by humans. The Multilingual and Cross-lingual Word-in-Context Task In this Section, we present our SemEval task and describe a new dataset called Multilingua"
2021.semeval-1.3,K16-1006,0,0.0341511,"pienza NLP Group Department of Computer Science Sapienza University of Rome, Italy first.lastname@uniroma1.it Abstract a target word by clustering occurrences based on their context similarities (Neelakantan et al., 2015; Pelevina et al., 2016). In an effort to exploit the knowledge derived from lexical-knowledge bases, Iacobacci et al. (2015) introduced a new approach which allows sense representations to be linked to a predefined sense inventory. More recently, contextualized embeddings were proposed. These representations are obtained by means of neural language modeling, e.g. using LSTMs (Melamud et al., 2016) or the Transformer architecture (Devlin et al., 2019; Conneau et al., 2020), and are capable of representing words based on the context in which they occur. Contextualized representations have also been used to obtain effective sense embeddings (Loureiro and Jorge, 2019; Scarlini et al., 2020a,b; Calabrese et al., 2020). Although virtually all the above approaches can be evaluated in downstream applications, the inherent ability of the various embeddings to capture meaning distinctions still remains largely underinvestigated. While Word Sense Disambiguation (WSD), i.e. the task of determining"
2021.semeval-1.3,2020.emnlp-main.285,1,0.861634,"om lexical-knowledge bases, Iacobacci et al. (2015) introduced a new approach which allows sense representations to be linked to a predefined sense inventory. More recently, contextualized embeddings were proposed. These representations are obtained by means of neural language modeling, e.g. using LSTMs (Melamud et al., 2016) or the Transformer architecture (Devlin et al., 2019; Conneau et al., 2020), and are capable of representing words based on the context in which they occur. Contextualized representations have also been used to obtain effective sense embeddings (Loureiro and Jorge, 2019; Scarlini et al., 2020a,b; Calabrese et al., 2020). Although virtually all the above approaches can be evaluated in downstream applications, the inherent ability of the various embeddings to capture meaning distinctions still remains largely underinvestigated. While Word Sense Disambiguation (WSD), i.e. the task of determining the meaning of a word in a given context (Navigli, 2009), has long explored the aforementioned ability, the task does not make it easy to test approaches that are not explicitly linked to existing sense inventories, such as WordNet (Miller et al., 1990) and BabelNet (Navigli and Ponzetto, 201"
2021.semeval-1.3,S10-1002,0,0.011713,"gs. Despite their effectiveness, both the WiC and XL-WiC datasets are not manually created and do not cover all open-class parts of speech. Moreover, they do not consider cross-lingual evaluation scenarios in which systems are tested in more than one language at the same time, thus highlighting the need for a new evaluation benchmark. Related Work 3 Several different tasks have been put forward which go beyond traditional WSD and drop the requirement of fixed sense inventories. Among the first alternatives we cite monolingual and cross-lingual Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010). Word-in-context similarity has also been proposed as a way to capture the dynamic nature of word meanings: the Stanford Contextual Word Similarities (SCWS) dataset, proposed by Huang et al. (2012), contains human judgements on pairs of words in context. Along these same lines, Armendariz et al. introduced CoSimLex, a dataset designed to evaluate the ability of models to capture word similarity judgements provided by humans. The Multilingual and Cross-lingual Word-in-Context Task In this Section, we present our SemEval task and describe a new dataset called Multilingual and Cross-lingual Word"
2021.semeval-1.3,2020.lrec-1.730,0,0.368187,"Missing"
2021.semeval-1.3,H93-1061,0,0.862639,"A (Clark et al., 2019) and ERNIE (Sun et al., 2020). The majority of participants made use of fine-tuned contextualized embeddings and used logistic regression to perform binary classification. Some participants used ensembles and majority voting. Data Multilingual sub-task As far as English is concerned, the majority of participating systems used the MCL-WiC training and development data. Some participants also used the data derived from WiC and XL-WiC. Furthermore, automaticallyconstructed WiC-like datasets were obtained by some participants, starting from semantic resources such as SemCor (Miller et al., 1993), WordNet and the Princeton WordNet Gloss Corpus (PWNG)12 , or by automatically translating available datasets into English. The available data was also enriched via sentence reversal augmentation (given a sentence pair, the two sentences were swapped). In some cases, the development and trial13 data was used to enrich the training data. As regards languages other than English, most participants used XL-WiC data, or new training and development datasets were obtained by splitting the MCL-WiC language-specific development Cross-lingual sub-task Also in this sub-task, XLM-RoBERTa was the most us"
2021.semeval-1.3,P10-1023,1,0.910749,"19; Scarlini et al., 2020a,b; Calabrese et al., 2020). Although virtually all the above approaches can be evaluated in downstream applications, the inherent ability of the various embeddings to capture meaning distinctions still remains largely underinvestigated. While Word Sense Disambiguation (WSD), i.e. the task of determining the meaning of a word in a given context (Navigli, 2009), has long explored the aforementioned ability, the task does not make it easy to test approaches that are not explicitly linked to existing sense inventories, such as WordNet (Miller et al., 1990) and BabelNet (Navigli and Ponzetto, 2010). This has two major drawbacks. First, sense inventories are not always available, especially for rare languages. Second, such requirement limits the evaluation of word and sense representations which are not bound to a sense inventory. To tackle this limitation, some benchmarks have recently been proposed. The CoSimLex dataset (Armendariz et al.) and the related SemEval-2020 Task 3 (Armendariz et al., 2020) focus on evaluating the similarity of word pairs which occur in the same context. More recently, the Word-in-Context (WiC) task (Pilehvar In this paper, we introduce the first SemEval task"
2021.semeval-1.3,W16-1620,0,0.0511935,"Missing"
2021.semeval-1.3,2021.semeval-1.93,0,0.0670201,"Missing"
2021.semeval-1.3,N19-1128,0,0.174132,"Missing"
2021.semeval-1.3,2021.semeval-1.96,0,0.0492398,"Missing"
2021.semeval-1.3,2021.semeval-1.17,0,0.0641755,"Missing"
2021.semeval-1.3,2020.emnlp-main.584,0,0.570455,"ous nature of words. To address this limitation, more sophisticated representations such as multi-prototype and contextualized embeddings have been put forward. Multi-prototype embeddings concentrate on the semantics which underlie 24 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 24–36 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics and Camacho-Collados, 2019), included in the SuperGLUE benchmark for Natural Language Understanding (NLU) systems (Wang et al., 2019) and its multilingual extension XL-WiC (Raganato et al., 2020), require systems to determine whether a word occurring in two different sentences is used with the same meaning, without relying on a predefined sense inventory. For instance, given the following sentence pair: Sub-task Multilingual Cross-lingual • the mouse eats the cheese, • click the right mouse button, Dev 500 500 500 500 500 - Test 500 500 500 500 500 500 500 500 500 Table 1: The MCL-WiC dataset: number of unique lexemes divided by sub-task and dataset. The second column (Dataset) indicates the available language combination. the ideal system should establish that the target word mouse i"
C04-1150,J04-2002,1,\N,Missing
C14-3003,P13-1133,0,0.0340689,"approaches both for WSD and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 5 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 5–7, Dublin, Ireland, August 23-29 2014. EL. We will then discuss the key aspects of partial matching for EL and, finally, we will describe multilingual inventories of word senses and named entities, among which Open Multilingual WordNet (Bond and Foster, 2013), Wikipedia1 , DBpedia (Auer et al., 2007), BabelNet (Navigli and Ponzetto, 2012). 3. State of the art in WSD and EL (75 mins) This session will introduce the key challenges to the tasks of WSD and EL and the well-known approaches, such as IMS (Zhong and Ng, 2010) and UKB (Agirre et al., 2013) for WSD, and Babelfy (Moro et al., 2014), AIDA (Hoffart et al., 2011; Hoffart et al., 2012), Tagme (Ferragina and Scaiella, 2010; Ferragina and Scaiella, 2012), Illinois Wikifier (Cheng and Roth, 2013) and DBpedia Spotlight (Mendes et al., 2011; Daiber et al., 2013), that can partially address them. Chal"
C14-3003,D13-1184,0,0.0323916,"ibe multilingual inventories of word senses and named entities, among which Open Multilingual WordNet (Bond and Foster, 2013), Wikipedia1 , DBpedia (Auer et al., 2007), BabelNet (Navigli and Ponzetto, 2012). 3. State of the art in WSD and EL (75 mins) This session will introduce the key challenges to the tasks of WSD and EL and the well-known approaches, such as IMS (Zhong and Ng, 2010) and UKB (Agirre et al., 2013) for WSD, and Babelfy (Moro et al., 2014), AIDA (Hoffart et al., 2011; Hoffart et al., 2012), Tagme (Ferragina and Scaiella, 2010; Ferragina and Scaiella, 2012), Illinois Wikifier (Cheng and Roth, 2013) and DBpedia Spotlight (Mendes et al., 2011; Daiber et al., 2013), that can partially address them. Challenges include: the lack of training data for non-English languages, the granularity of the sense inventory, the high level of ambiguity in EL, the most frequent sense baseline challenge, etc. 4. Evaluation measures and gold standard datasets (30 mins) We will conclude the tutorial by describing the standard performance measures for these tasks and well-known datasets for multilingual WSD and EL together with a discussion of the results. Speakers Roberto Navigli is an associate professor in"
C14-3003,D11-1072,0,0.353103,"Missing"
C14-3003,P10-4014,0,0.0864963,"14, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 5–7, Dublin, Ireland, August 23-29 2014. EL. We will then discuss the key aspects of partial matching for EL and, finally, we will describe multilingual inventories of word senses and named entities, among which Open Multilingual WordNet (Bond and Foster, 2013), Wikipedia1 , DBpedia (Auer et al., 2007), BabelNet (Navigli and Ponzetto, 2012). 3. State of the art in WSD and EL (75 mins) This session will introduce the key challenges to the tasks of WSD and EL and the well-known approaches, such as IMS (Zhong and Ng, 2010) and UKB (Agirre et al., 2013) for WSD, and Babelfy (Moro et al., 2014), AIDA (Hoffart et al., 2011; Hoffart et al., 2012), Tagme (Ferragina and Scaiella, 2010; Ferragina and Scaiella, 2012), Illinois Wikifier (Cheng and Roth, 2013) and DBpedia Spotlight (Mendes et al., 2011; Daiber et al., 2013), that can partially address them. Challenges include: the lack of training data for non-English languages, the granularity of the sense inventory, the high level of ambiguity in EL, the most frequent sense baseline challenge, etc. 4. Evaluation measures and gold standard datasets (30 mins) We will con"
C14-3003,Q14-1019,1,\N,Missing
C14-3003,J14-1003,0,\N,Missing
cucchiarelli-etal-2004-automatic,J04-2002,1,\N,Missing
cucchiarelli-etal-2004-automatic,J02-3001,0,\N,Missing
D10-1012,W06-3814,0,0.429135,"Missing"
D10-1012,P98-2127,0,0.290167,"enses previously induced (Section 3.2). 3.1 Word Sense Induction Word Sense Induction algorithms are unsupervised techniques aimed at automatically identifying the set of senses denoted by a word. These methods induce word senses from text by clustering word occurrences based on the idea that a given word – used in a specific sense – tends to co-occur with the same neighbouring words (Harris, 1954). Several approaches to WSI have been proposed in the literature (see Navigli (2009) for a survey), ranging from clustering based on context vectors (e.g., Sch¨utze (1998)) to word clustering (e.g., Lin (1998)) and co-occurrence graphs (e.g., Widdows and Dorow (2002)). Successful approaches such as HyperLex (V´eronis, 2004) – a graph algorithm based on the identification of hubs in co-occurrence graphs – have to cope with a high number of parameters to be tuned (Agirre et al., 2006). To deal with this issue we propose two variants of a simple, yet effective, graph-based algorithm for WSI, that we describe hereafter. The algorithm consists of two steps: graph construction and identification of word senses. 3.1.1 Graph construction Given a target query q, we build a co-occurrence graph Gq = (V, E) su"
D10-1012,W98-0704,0,0.172344,"ic Information Retrieval (SIR). A different direction consists of associating explicit semantics (i.e., word senses or concepts) with queries and documents, that is, performing Word Sense Disambiguation (WSD, see Navigli (2009)). SIR is performed by indexing and/or searching concepts rather than terms, thus potentially coping with two linguistic phenomena: expressing a single meaning with different words (synonymy) and using the same word to express various different meanings (polysemy). Over the years, different methods for SIR have been 117 proposed (Krovetz and Croft, 1992; Voorhees, 1993; Mandala et al., 1998; Gonzalo et al., 1999; Kim et al., 2004; Liu et al., 2005a, inter alia). However, contrasting results have been reported on the benefits of these techniques: it has been shown that WSD has to be very accurate to benefit Information Retrieval (Sanderson, 1994) – a result that was later debated (Gonzalo et al., 1999; Stokoe et al., 2003). Also, it has been reported that WSD has to be very precise on minority senses and uncommon terms, rather than on frequent words (Krovetz and Croft, 1992; Sanderson, 2000). SIR relies on the existence of a reference dictionary to perform WSD (typically, WordNet"
D10-1012,J98-1004,0,0.612178,"Missing"
D10-1012,C02-1114,0,0.292598,"Word Sense Induction Word Sense Induction algorithms are unsupervised techniques aimed at automatically identifying the set of senses denoted by a word. These methods induce word senses from text by clustering word occurrences based on the idea that a given word – used in a specific sense – tends to co-occur with the same neighbouring words (Harris, 1954). Several approaches to WSI have been proposed in the literature (see Navigli (2009) for a survey), ranging from clustering based on context vectors (e.g., Sch¨utze (1998)) to word clustering (e.g., Lin (1998)) and co-occurrence graphs (e.g., Widdows and Dorow (2002)). Successful approaches such as HyperLex (V´eronis, 2004) – a graph algorithm based on the identification of hubs in co-occurrence graphs – have to cope with a high number of parameters to be tuned (Agirre et al., 2006). To deal with this issue we propose two variants of a simple, yet effective, graph-based algorithm for WSI, that we describe hereafter. The algorithm consists of two steps: graph construction and identification of word senses. 3.1.1 Graph construction Given a target query q, we build a co-occurrence graph Gq = (V, E) such that V is a set of context words related to q and E is"
D10-1012,C98-2122,0,\N,Missing
D10-1012,W99-0624,0,\N,Missing
D12-1128,W09-2420,0,0.0477125,"Missing"
D12-1128,W11-0104,0,0.147473,"ea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language (Khapra et al., 2011). Yet the above two goals, i.e., disambiguating in an arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained. In this paper, we address both objectives and propose a graph-based approach to multilingual joint Word Sense Disambiguation. Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then"
D12-1128,S10-1054,0,0.0151949,"s and discussion. We report our results for CL-LS and CL-WSD in Tables 3 and 4. We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively. The evaluation scheme is based on the SemEval2007 English lexical substitution task (McCarthy and Navigli, 2009), and consists of an adaptation of the metrics of precision and recall for the translation setting. For each task, we compare our monolingual and multilingual approaches against the best performing SemEval systems for these tasks, namely UBA-T (Basile and Semeraro, 2010) and UVT-v (van Gompel, 2010) for CL-LS and CL-WSD, respectively, as well as a recent supervised proposal that exploits automatically generated multilingual features from parallel text and translated contexts (Lefever et al., 2011, Parasense). For each task we also report its official baseline, namely the first translation from an online-dictionary6 for CL-LS, and the most frequent word alignment obtained by 6 www.spanishdict.com 1407 applying GIZA++ to the Europarl data for CL-WSD. Our cross-lingual results confirm all trends of the English monolingual evaluation, namely that: a) our joint mu"
D12-1128,P06-1013,1,0.437409,"d Lapata, 2010; Ponzetto and Navigli, 2010). Inverse path length sum (PLength): We then developed a graph connectivity measure which scores each sense by summing over the inverse length of all paths which connect it to other senses in the graph: scorej = X p ∈ paths(sj ) 1 elength(p)−1 scorej = At the core of our algorithm lies the combination of the scores generated using the different translations of the target word w. For this purpose, we apply socalled ensemble methods, which have been shown to improve the performance of both supervised (Florian et al., 2002) and unsupervised WSD systems (Brody et al., 2006). Given |T |lexicalizations and |S |senses for w, the input to the combination component consists of a |T |× |S |matrix LScore, where each cell lScorei,j quantifies the empirical support for sense sj from a term ti ∈ T (see Section 3.2 for an example). The ensemble method computes from this translation-sense matrix a combined scoring, expressing the joint confidence across terms in different languages over the set of senses S. In this work, we use the ‘Probability Mixture’ (PMixture) method 1404 lScorei,j p(si,j ), p(si,j ) = P|S| . i=1 s=1 lScorei,s banknEN banconES BanknDE PMixture 3.5 Ensem"
D12-1128,P91-1034,0,0.217592,"al effects of exploiting multilingual knowledge in a joint fashion. 2 Related Work Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al., 1992; Chan and Ng, 2005; Zhong and Ng, 2009). Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform WSD using a similarity measure (Diab, 2003). A historical approach (Brown et al., 1991) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features. All the above approaches to multilingual or crosslingual WSD rely on bilingual corpora, including those which exploit existing multilingual WordNetlike resources (Ide et al., 2002), or use automatically induced multilingual co-occurrence graphs (Silberer 1400 and Ponzetto, 2010). However, this requirement is often very hard to satisfy, especially if we need wide coverage. To overcome this limitation, in this work we make use of Babel"
D12-1128,D07-1007,0,0.0154445,"use of BabelNet (Navigli and Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network. 3 Multilingual Joint WSD We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and"
D12-1128,P07-1005,0,0.022428,"nd Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network. 3 Multilingual Joint WSD We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and 3.5). 3.1 BabelNet Babe"
D12-1128,P91-1017,0,0.101892,"sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for"
D12-1128,W02-0811,0,0.017964,"etitive performance on various WSD tasks (Navigli and Lapata, 2010; Ponzetto and Navigli, 2010). Inverse path length sum (PLength): We then developed a graph connectivity measure which scores each sense by summing over the inverse length of all paths which connect it to other senses in the graph: scorej = X p ∈ paths(sj ) 1 elength(p)−1 scorej = At the core of our algorithm lies the combination of the scores generated using the different translations of the target word w. For this purpose, we apply socalled ensemble methods, which have been shown to improve the performance of both supervised (Florian et al., 2002) and unsupervised WSD systems (Brody et al., 2006). Given |T |lexicalizations and |S |senses for w, the input to the combination component consists of a |T |× |S |matrix LScore, where each cell lScorei,j quantifies the empirical support for sense sj from a term ti ∈ T (see Section 3.2 for an example). The ensemble method computes from this translation-sense matrix a combined scoring, expressing the joint confidence across terms in different languages over the set of senses S. In this work, we use the ‘Probability Mixture’ (PMixture) method 1404 lScorei,j p(si,j ), p(si,j ) = P|S| . i=1 s=1 lSc"
D12-1128,1992.tmi-1.9,0,0.779099,"like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Ling"
D12-1128,W02-0808,0,0.0358311,"Missing"
D12-1128,P11-1057,0,0.0458791,"and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language (Khapra et al., 2011). Yet the above two goals, i.e., disambiguating in an arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained. In this paper, we address both objectives and propose a graph-based approach to multilingual joint Word Sense Disambiguation. Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then combining this information in a synergistic way: each language provides a piece of sense evidence for the meani"
D12-1128,2005.mtsummit-papers.11,0,0.0112189,"ross-lingual WSD tasks cast disambiguation as a word translation problem: given an English polysemous noun in context as input, the system disambiguates it by providing a translation into another language (translations are deemed correct if they preserve the meaning of the source word in the target language). Their main difference, instead, lies in the range of translations which are assumed to be valid: that is, while CL-LS assumes no predefined sense inventory (i.e., any translation can be potentially correct), CL-WSD makes use of a sense inventory built on the basis of the Europarl corpus (Koehn, 2005). Our approach to lexical disambiguation involves two steps: first, given a target word in context, we disambiguate it as usual to the highest-ranked Babel synset; next, given the translations in the selected synset, we return the most suitable lexicalization in the language of interest. Since the selected synset can contain multiple translations in a target language for the input English word, we explore using an unsupervised strategy to select the most reliable translation from multiple candidates. To this end, we return for each test instance only the Algorithm P/R/F1 Baseline 23.80 Monolin"
D12-1128,S10-1094,0,0.0165637,"eshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingual lexical resource makes it possible to perform WSD in any of its languages. Accordingly, we complement our evaluation on English texts with a second set of experiments where we quantify the impact o"
D12-1128,S10-1003,0,0.334273,"umura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language ("
D12-1128,P11-2055,0,0.113447,"Missing"
D12-1128,P11-1033,0,0.039171,"Missing"
D12-1128,W04-0805,0,0.0171319,"rch in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) has always been focused mostly on English. Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of"
D12-1128,W04-0806,0,0.0760061,"Missing"
D12-1128,P11-1134,0,0.0172269,"Missing"
D12-1128,W09-2412,0,0.0550903,"Missing"
D12-1128,H93-1061,0,0.0223142,"ent languages would improve the quality of text understanding in arbitrary languages. However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) has always been focused mostly on English. Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yar"
D12-1128,nastase-etal-2010-wikinet,0,0.0289083,"Missing"
D12-1128,P10-1023,1,0.932453,"bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features. All the above approaches to multilingual or crosslingual WSD rely on bilingual corpora, including those which exploit existing multilingual WordNetlike resources (Ide et al., 2002), or use automatically induced multilingual co-occurrence graphs (Silberer 1400 and Ponzetto, 2010). However, this requirement is often very hard to satisfy, especially if we need wide coverage. To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007)"
D12-1128,P12-3012,1,0.815104,"on all disambiguation tasks. 5 Conclusions In this paper we presented a multilingual joint approach to WSD. Key to our methodology is the effective use of a wide-coverage multilingual knowledge base, BabelNet, which we exploit to perform graph-based WSD across languages and combine complementary sense evidence from translations in different languages using an ensemble method. This is the first proposal to exploit structured multilingual information within a joint, knowledge-rich framework for WSD. The APIs to perform multilingual WSD using BabelNet are freely available for research purposes (Navigli and Ponzetto, 2012b). Thanks to multilingual joint WSD we achieve state-of-the-art performance on three different gold standards. The good news about these results is that not only can further advances be achieved by using multilingual lexical knowledge, but, more importantly, that combining multilingual sense evidence from different languages at the same time yields consistent improvements over a monolingual apBaseline UvT-v Parasense French P/R/F1 21.25 N/A 24.54 German P/R/F1 13.16 N/A 16.88 Italian P/R/F1 15.18 N/A 18.03 Spanish P/R/F1 19.74 23.39 22.80 Monolingual graph Degree PLength 22.94 23.42 17.15 17."
D12-1128,J03-1002,0,0.00411186,"hm P/R/F1 Baseline 23.80 Monolingual Degree 30.52 graph PLength 30.64 Multilingual ensemble Degree PLength 32.21 32.47 UBA-T 32.17 Table 3: Performance on SemEval-2010 lexical substitution (best results are bolded). most frequent translation found in the Babel synset. Given that the two tasks make different assumptions on the sense inventory (no fixed inventory for CLLS vs. Europarl-based for CL-WSD), the frequency of a translation is calculated as either the number of Babel synsets in which it occurs (CL-LS), or its frequency of alignment with the target word, as obtained by applying GIZA++ (Och and Ney, 2003) to Europarl (CL-WSD). To provide an answer for all instances, we return this most frequent translation even when no sense assignment is attempted – i.e., no sense of the target word is connected to any other sense of the context words – or a tie occurs. Results and discussion. We report our results for CL-LS and CL-WSD in Tables 3 and 4. We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively. The evaluation scheme is based on the SemEval2007 English lexical substitution task (McCarthy an"
D12-1128,S07-1011,0,0.0197176,"Missing"
D12-1128,S01-1005,0,0.0449544,"we follow previous work (e.g., Navigli and Lapata (2010)) and explore a weakly-supervised setting where the system attempts no sense assignment if the highest score among those assigned to the senses of a target word is below a certain threshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual"
D12-1128,S10-1087,0,0.0141433,"der to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingual lexical resource makes it possible to perform WSD in any of its languages. Accordingly, we complement our evaluation on English texts with a second set of experiments where we quantify the impact of our approach on a lexical dis"
D12-1128,S10-1027,1,0.851206,"Missing"
D12-1128,W04-0811,0,0.058295,"i and Lapata (2010)) and explore a weakly-supervised setting where the system attempts no sense assignment if the highest score among those assigned to the senses of a target word is below a certain threshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingu"
D12-1128,S10-1053,0,0.0229438,"Missing"
D12-1128,S10-1013,0,\N,Missing
D12-1128,W09-2413,0,\N,Missing
D12-1128,P10-1154,1,\N,Missing
D12-1128,S10-1002,0,\N,Missing
D12-1128,S10-1012,0,\N,Missing
D12-1129,agirre-de-lacalle-2004-publicly,0,0.0803085,"Missing"
D12-1129,E09-1006,0,0.0433174,"Missing"
D12-1129,E09-1005,0,0.0868667,"Missing"
D12-1129,W09-2420,0,0.0958742,"Missing"
D12-1129,P06-1012,0,0.0222214,"Missing"
D12-1129,P07-1007,0,0.0884093,"Missing"
D12-1129,C08-1021,0,0.135879,"Missing"
D12-1129,W06-2609,0,0.0606664,"Missing"
D12-1129,D10-1044,0,0.0361412,"Missing"
D12-1129,P00-1062,0,0.28765,"Missing"
D12-1129,P05-1050,0,0.0129104,"07). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and 1412 Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD"
D12-1129,P10-1029,0,0.028484,"e inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles crossFigure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working with virtually any set of domains and minimizing the requirements by harvesting domain terms and definitions from the Web, bootstrapped using a small number of seeds. The existing approach closest to ours is that of Huang and Riloff (2010), who devised a bootstrapping approach to induce semantic class taggers from domain text. The semantic classes are associated with arbitrary NPs and must be established beforehand. Our objective, instead, is to perform domain disambiguation at the word level. To do this, we redefine the domain WSD problem as one of selecting the most suitable gloss from those available in our full-fledged multi-domain glossary. 3 A Minimally-Supervised Framework for Domain WSD In this section we present our new framework for performing domain WSD. The framework consists of two phases: glossary bootstrapping (S"
D12-1129,P10-1155,0,0.0376685,"anding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexi"
D12-1129,H05-1053,0,0.0620412,"Missing"
D12-1129,S10-1094,0,0.0310908,"Missing"
D12-1129,N07-1044,0,0.0574387,", in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbau"
D12-1129,magnini-cavaglia-2000-integrating,0,0.0294395,"Missing"
D12-1129,P04-1036,0,0.216395,"Missing"
D12-1129,J07-4005,0,0.0298302,"possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Mi"
D12-1129,N07-1025,0,0.0400871,"PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexicographic sense). In this paper we provide three main contributions: • We tackle the above issues by introducing a new framework based on the minimallysupervised acquisition of specialized glossaries for dozens of domains. 1411 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1411–1422, J"
D12-1129,E06-1016,0,0.010535,"be added at any time (either manually or automatically) to the multi-domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within"
D12-1129,P10-1134,1,0.581507,"ach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WS"
D12-1129,N06-2036,0,0.0299643,"Missing"
D12-1129,W04-0856,0,0.0141659,"semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and 1412 Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specifi"
D12-1129,N10-1133,0,\N,Missing
D12-1129,N10-1088,0,\N,Missing
D12-1129,S10-1013,0,\N,Missing
D13-1018,P13-1052,1,0.675172,"Missing"
D13-1018,N10-1088,0,0.0176589,"efinitions is not always immediate, especially if the target term pertains to a specialized domain. Indeed, not even well-known services such as Google Define are able to provide definitions for scientific or technical terms such as taxonomy learning or distant supervision in AI or figure-four leglock and suspended surfboard in wrestling. Domain-specific knowledge of a definitional nature is not only useful for humans, it is also useful for machines (Hovy et al., 2013). Examples include Natural Language Processing tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Velardi et al., 2013). Unfortunately, most of the Web dictionaries and glossaries available online comprise just a few hundred definitions, and they therefore provide only a partial view of a domain. This is also the case with manually compiled glossaries created by means of collaborative efforts, such as Wikipedia.1 The coverage issue is addressed by online aggregation services such as Google Define, which bring together definitions from several online dictionaries. However, these services do not classify textual definitions by domain: they"
D13-1018,D12-1129,1,0.848408,"ys immediate, especially if the target term pertains to a specialized domain. Indeed, not even well-known services such as Google Define are able to provide definitions for scientific or technical terms such as taxonomy learning or distant supervision in AI or figure-four leglock and suspended surfboard in wrestling. Domain-specific knowledge of a definitional nature is not only useful for humans, it is also useful for machines (Hovy et al., 2013). Examples include Natural Language Processing tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Velardi et al., 2013). Unfortunately, most of the Web dictionaries and glossaries available online comprise just a few hundred definitions, and they therefore provide only a partial view of a domain. This is also the case with manually compiled glossaries created by means of collaborative efforts, such as Wikipedia.1 The coverage issue is addressed by online aggregation services such as Google Define, which bring together definitions from several online dictionaries. However, these services do not classify textual definitions by domain: they just present the collected d"
D13-1018,P13-4018,1,0.813947,"Missing"
D13-1018,P00-1062,0,0.0197588,"ve efforts, such as Wikipedia.1 The coverage issue is addressed by online aggregation services such as Google Define, which bring together definitions from several online dictionaries. However, these services do not classify textual definitions by domain: they just present the collected definitions for all the possible meanings of a given term. In order to automatically obtain large domain glossaries, in recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Velardi et al., 2008; Fujii and Ishikawa, 2000). The methods involving corpora start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recu"
D13-1018,C92-2082,0,0.369379,"glossaries, in recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Velardi et al., 2008; Fujii and Ishikawa, 2000). The methods involving corpora start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recurring cue terms occurring 1 See http://en.wikipedia.org/wiki/Portal: Contents/Glossaries 170 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 170–181, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics within dictionary and encyclopedic resources can be automatically extracted and incorporat"
D13-1018,P10-1029,0,0.0610324,"Missing"
D13-1018,N10-1087,0,0.024753,"ected five seed hypernymy relations as the seed sets Sd input to Algorithm 1 (see Section 3.5). The seeds were selected by the authors on the basis of just two conditions: i) the seeds should cover different aspects of the domain and, indeed, should identify the domain implicitly; ii) at least 10,000 results should be returned by the search engine when querying it with the seeds plus the glossary keyword (see line 6 of Algorithm 1). The seed selection was not fine-tuned (i.e., it was not adjusted to improve performance), so it might well be that better seeds would provide better results (see (Kozareva and Hovy, 2010a)). However, such a study is beyond the scope of this paper. Chemistry Computing Environment Food Law Music 164 2493 3841 1174 1414 8137 421 3412 4186 3127 3662 32.0k 713 3009 3552 3644 4334 23.6k 946 1526 2175 1827 2601 5698 180 1836 4141 1773 4024 13.5k 218 1647 2729 1166 1249 84.1k Sport Business 1777 7370 9795 7639 8999 48.4k Physics Art Gold t/g 394 PTM t 4253 g 7386 BoW t 4012 g 5923 Wiki t,g 107.1k 315 146 3847 1696 5197 2938 4471 1990 6956 3425 33.8k 267.5k Table 1: Size of the gold standard and the automatically-acquired glossaries for 10 of the 30 selected domains (t: number of term"
D13-1018,D10-1108,0,0.0593178,"ected five seed hypernymy relations as the seed sets Sd input to Algorithm 1 (see Section 3.5). The seeds were selected by the authors on the basis of just two conditions: i) the seeds should cover different aspects of the domain and, indeed, should identify the domain implicitly; ii) at least 10,000 results should be returned by the search engine when querying it with the seeds plus the glossary keyword (see line 6 of Algorithm 1). The seed selection was not fine-tuned (i.e., it was not adjusted to improve performance), so it might well be that better seeds would provide better results (see (Kozareva and Hovy, 2010a)). However, such a study is beyond the scope of this paper. Chemistry Computing Environment Food Law Music 164 2493 3841 1174 1414 8137 421 3412 4186 3127 3662 32.0k 713 3009 3552 3644 4334 23.6k 946 1526 2175 1827 2601 5698 180 1836 4141 1773 4024 13.5k 218 1647 2729 1166 1249 84.1k Sport Business 1777 7370 9795 7639 8999 48.4k Physics Art Gold t/g 394 PTM t 4253 g 7386 BoW t 4012 g 5923 Wiki t,g 107.1k 315 146 3847 1696 5197 2938 4471 1990 6956 3425 33.8k 267.5k Table 1: Size of the gold standard and the automatically-acquired glossaries for 10 of the 30 selected domains (t: number of term"
D13-1018,P10-1134,1,0.941916,"his is also the case with manually compiled glossaries created by means of collaborative efforts, such as Wikipedia.1 The coverage issue is addressed by online aggregation services such as Google Define, which bring together definitions from several online dictionaries. However, these services do not classify textual definitions by domain: they just present the collected definitions for all the possible meanings of a given term. In order to automatically obtain large domain glossaries, in recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Velardi et al., 2008; Fujii and Ishikawa, 2000). The methods involving corpora start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of"
D13-1018,P06-1102,0,0.0782952,"Missing"
D13-1018,P06-1015,0,0.128624,"Missing"
D13-1018,P02-1006,0,0.0204028,"Missing"
D13-1018,W12-3206,0,0.213695,"Missing"
D13-1018,saggion-2004-identifying,0,0.0312146,"ly perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recurring cue terms occurring 1 See http://en.wikipedia.org/wiki/Portal: Contents/Glossaries 170 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 170–181, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics within dictionary and encyclopedic resources can be automatically extracted and incorporated into lexical patterns (Saggion, 2004). However, this approach is term-specific and does not scale to arbitrary terminologies and domains. The goal of the new approach outlined in this paper is to enable the automatic harvesting of largescale, full-fledged domain glossaries for dozens of domains, an outcome which should be very useful for both human activities and automatic tasks. We present ProToDoG (Probabilistic Topics for multi-Domain Glossaries), a framework for growing multi-domain glossaries which has three main novelties: i) minimal human supervision: a small set of hypernymy relation seeds for each domain is used to boots"
D13-1018,J13-3007,1,0.859317,"ns to a specialized domain. Indeed, not even well-known services such as Google Define are able to provide definitions for scientific or technical terms such as taxonomy learning or distant supervision in AI or figure-four leglock and suspended surfboard in wrestling. Domain-specific knowledge of a definitional nature is not only useful for humans, it is also useful for machines (Hovy et al., 2013). Examples include Natural Language Processing tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Velardi et al., 2013). Unfortunately, most of the Web dictionaries and glossaries available online comprise just a few hundred definitions, and they therefore provide only a partial view of a domain. This is also the case with manually compiled glossaries created by means of collaborative efforts, such as Wikipedia.1 The coverage issue is addressed by online aggregation services such as Google Define, which bring together definitions from several online dictionaries. However, these services do not classify textual definitions by domain: they just present the collected definitions for all the possible meanings of a"
D13-1018,P95-1026,0,0.515094,"Missing"
D15-1084,P12-1092,0,0.073049,"ON is included in the BabelNet synset Washington4bn . In contrast, unlinked (and potentially ambiguous) entities need an explicit disambiguation step (Figure 1(b)) connecting them to appropriate entries, i.e. synsets, in S: this is the case, in the above example, for the ambiguous mention washington that has to be linked to either the president, the city or the state. Therefore, our approach comprises two successive stages: S ENS E MBED is a knowledge-based approach for obtaining latent continuous representations of individual word senses. Unlike other sense-based embeddings approaches, like (Huang et al., 2012), which address the inherent polysemy of wordlevel representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of a large sense inventory along with the distributional information gathered from text corpora. In order to do this, S ENS E MBED requires a senseannotated corpus; for each target word sense, then, a representation is computed by maximizing the log likelihood of the word sense with respect to its context within the annotated text, similarly to the word-based embeddings model. Following Iacobacci et al. (2015), we trained S ENS E MBED using the Engli"
D15-1084,P15-1010,1,0.918398,"order to identify cross-resource alignments and merge relations sharing equivalent semantics into relation clusters (relation synsets). This process yields a unified set of relation synsets R∗ . The overall result is KB∗ = hE ∗ , R∗ , T ∗ i, where T ∗ is the set of all disambiguated triples redefined over E ∗ and R∗ . Figure 1: Unification algorithm workflow 4 Background beddings (Mikolov et al., 2013), are not suitable to our setting: they are constrained to surface word forms, and hence they inherently retain ambiguity of polysemous words and entity mentions. We thus leverage S ENS E MBED (Iacobacci et al., 2015), a novel semantically-enhanced approach to embeddings. S ENS E MBED is trained on a large annotated corpus and produces continuous representations for individual word senses (sense embeddings), according to an underlying sense inventory. The disambiguation stage of our approach is based on the interplay between two core components: a vector space model VS , as introduced in Section 3, which provides an unambiguous semantic representation for each item in S; and a Word Sense Disambiguation/Entity Linking system, working on the same sense inventory S, which discovers and disambiguates concepts"
D15-1084,D12-1082,0,0.0195471,"dge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DB PE DIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate F REEBASE types across R E V ERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates structured data with OIE data by learning latent feature vectors for entities and relations;"
D15-1084,P09-1113,0,0.0241203,"cessing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), F REEBASE (Bollacker et al., 2008) and DB PEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an"
D15-1084,Q14-1019,1,0.919088,"ses (sense embeddings), according to an underlying sense inventory. The disambiguation stage of our approach is based on the interplay between two core components: a vector space model VS , as introduced in Section 3, which provides an unambiguous semantic representation for each item in S; and a Word Sense Disambiguation/Entity Linking system, working on the same sense inventory S, which discovers and disambiguates concepts and entity mentions within a given input text. In this section we briefly describe our choice for these two components: S ENS E MBED (Iacobacci et al., 2015) and BABELFY (Moro et al., 2014). Figure 1 illustrates the workflow of our KB unification approach. Entities coming from any KBi ∈ KD can be directly (and unambiguously) mapped to the corresponding entries in S via BabelNet inter-resource linking (Figure 1(a)): in the above example, the entity Washington linked to the Wikipedia page G EORGE WASHINGTON is included in the BabelNet synset Washington4bn . In contrast, unlinked (and potentially ambiguous) entities need an explicit disambiguation step (Figure 1(b)) connecting them to appropriate entries, i.e. synsets, in S: this is the case, in the above example, for the ambiguous"
D15-1084,D11-1142,0,0.464845,"ge, and release output and evaluation data for the use and scrutiny of the community1 . 1 Introduction The breakthrough of the Open Information Extraction (OIE) paradigm opened up a research area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1 navigli@di.uniroma1.it http://lcl.uniroma1.it/kb-unify 726 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 726–736, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. The remainder of this paper is structured as follows: Se"
D15-1084,D12-1104,0,0.393137,"owledge Base Unification via Sense Embeddings and Disambiguation Roberto Navigli Luis Espinosa-Anke Claudio Delli Bovi Department of Department of Information and Department of Computer Science Communication Technologies Computer Science Sapienza University of Rome Universitat Pompeu Fabra Sapienza University of Rome dellibovi@di.uniroma1.it luis.espinosa@upf.edu Abstract from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), F REEBASE (Bollacker et al., 2008) and DB PEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into"
D15-1084,P14-1079,0,0.0115697,"tics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), F REEBASE (Bollacker et al., 2008) and DB PEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide links to some generalpu"
D15-1084,C14-1207,0,0.158416,"he concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DB PE DIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate F REEBASE types across R E V ERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates struct"
D15-1084,E12-1059,0,0.0567145,"surface level, which are suboptimal for our task, as will be discussed in Section 3. Since we require a common semantic framework for KB unification, we use vector representations based on word senses, which are mapped to a very large sense inventory. This shared sense inventory, then, constitutes the common ground in which disambiguation, alignment and final unification occurs. Related Work The integration of knowledge drawn from different sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), U BY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for"
D15-1084,N13-1008,0,0.118754,"l of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), F REEBASE (Bollacker et al., 2008) and DB PEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide link"
D15-1084,D12-1042,0,0.0256922,"d, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), F REEBASE (Bollacker et al., 2008) and DB PEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE s"
D15-1084,D13-1136,0,0.0210234,"Conference on Empirical Methods in Natural Language Processing, pages 726–736, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. The remainder of this paper is structured as follows: Section 2 reviews relevant related work; Sections 3, 4, 5 and 6 describe in detail each stage of the approach; Sections 7 and 8 describe the experiments carried out and the results obtained; and finally Section 9 summarizes our findings and discusses potential directions for future work. 2 application to tasks like relation extraction and KB completion (Socher et al., 2013; Weston et al., 2013; Bordes et al., 2013). These approaches, however, leverage embeddings at surface level, which are suboptimal for our task, as will be discussed in Section 3. Since we require a common semantic framework for KB unification, we use vector representations based on word senses, which are mapped to a very large sense inventory. This shared sense inventory, then, constitutes the common ground in which disambiguation, alignment and final unification occurs. Related Work The integration of knowledge drawn from different sources has received much attention over the last decade. Among the most notable"
D15-1084,P10-1013,0,0.0334245,"uations at each stage, and release output and evaluation data for the use and scrutiny of the community1 . 1 Introduction The breakthrough of the Open Information Extraction (OIE) paradigm opened up a research area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1 navigli@di.uniroma1.it http://lcl.uniroma1.it/kb-unify 726 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 726–736, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. The remainder of this paper is stru"
D17-1008,2005.mtsummit-papers.11,0,0.122714,"stances for each language of interest, a number that is not within reach for any language. In fact, no supervised system has been submitted in major multilingual WSD competitions for languages other than English (Navigli et al., 2013; Moro and Navigli, 2015). To overcome this problem, new methodologies have recently been developed which aim to create sense-tagged corpora automatically. Raganato et al. (2016) developed 7 heuristics to grow the number of hyperlinks in Wikipedia pages. Otegi et al. (2016) applied a different disambiguation pipeline for each language to parallel text in Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2015) in order to enrich them with semantic annotations. Taghipour and Ng (2015), the work closest to ours, exploits the alignment from English to Chinese sentences of State-of-the-art supervised systems include Support Vector Machines such as IMS (Zhong and Ng, 2010) and, more recently, LSTM neural networks with attention and multitask learning (Raganato et al., 2017b) as well as LSTMs paired with nearest neighbours classification (Melamud et al., 2016; Yuan et al., 2016). The latter also integrates a label propagation algorithm in order to enrich the sense annotat"
D17-1008,J98-1006,0,0.0935537,"-tag English sentences. The assumption is that the second language is less ambiguous than the first one and that hand-made translations of senses are available for each WordNet synset. This approach is, therefore, semi-automatic and relies on certain assumptions, in contrast to TrainO-Matic which is, instead, fully automatic and can be applied to any kind of corpus (and language) depending on the specific need. Earlier attempts at the automatic extraction of training samples were made by Agirre and De Lacalle (2004) and Fern´andez et al. (2004). Both exploited the monosemous relatives method (Leacock et al., 1998) in order to retrieve sentences from the Web which contained a given monosemous noun or a relative monosemous word (e.g., a synonym, a hypernym, etc.). As can be seen in (Fern´andez et al., 2004) this approach can lead to the retrieval of very accurate examples, but its main drawback lies in the number of senses covered. In fact, for all those synsets that do not have any monosemous relative, the system is unable to retrieve examples, thus heavily affecting the performance in terms of recall and F1. Knowledge-based WSD, instead, bypasses the heavy requirement of sense-annotated corpora by appl"
D17-1008,P16-1143,0,0.055563,"Missing"
D17-1008,S14-1005,0,0.0935635,"ally we used the output data to train IMS. Table 4: Number of nominal tokens for which at least one training example is provided by OMSTI or Train-O-Matic for each dataset. Dataset Senseval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL P 64.8 55.7 64.1 50.7 57.0 56.5 OMSTI R 28.5 31.0 35.9 23.4 26.7 27.0 F1 39.6 39.8 46.0 32.0 36.4 36.5 Results To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish. As can be seen from Table 8 TrainO-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All domains, Maths & Computer and Biomedicine). Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths & Computer in Spanish and on Biomedicine in Italian. This demonstrates the ability of Train-O-Matic to enable supervised WSD systems to surpass state-of-theart knowledge-based WSD approaches in lowresourced languages without relying on manually curated data for training. Train-O-Matic P R F1 69.5 65.5 67.4 66.1 63.1 64.6 59.8 59.8 59.8 61.3 53.3 57.0 67.0 62.3 64.6 65.1 59.7 62.3 Table 5: Precision, Recall and F1 of IMS trained on OMST"
D17-1008,K16-1006,0,0.0572512,"dia pages. Otegi et al. (2016) applied a different disambiguation pipeline for each language to parallel text in Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2015) in order to enrich them with semantic annotations. Taghipour and Ng (2015), the work closest to ours, exploits the alignment from English to Chinese sentences of State-of-the-art supervised systems include Support Vector Machines such as IMS (Zhong and Ng, 2010) and, more recently, LSTM neural networks with attention and multitask learning (Raganato et al., 2017b) as well as LSTMs paired with nearest neighbours classification (Melamud et al., 2016; Yuan et al., 2016). The latter also integrates a label propagation algorithm in order to enrich the sense annotated dataset. The main difference from our approach is its need for a manually annotated dataset to start the label propagation algorithm, whereas Train-O-Matic is fully automatic. An evaluation against this system would have been interesting, but neither the proprietary training data nor the code are available at the time of writing. In order to generalize effectively, these supervised systems require large numbers of training in85 approaches are barely able to surpass supervised W"
D17-1008,N15-1059,1,0.839355,"graph of BabelNet G = (V, E): ( 1 (s, s0 ) ∈ E(WordNet) w(s, s0 ) = W O(s, s0 ) otherwise (7) where E(WordNet) is the edge set of the original WordNet graph and W O(s, s0 ) is the weighted 2 3 overlap measure which calculates the similarity between two synsets: 0 W O(s, s ) = P|S| 1 2 −1 i=1 (ri + ri ) P|S| −1 i=1 (2i) where ri1 and ri2 are the rankings of the i-th synsets in the set S of the components in common between the vectors associated with s and s0 , respectively. Because at this stage we still have to calculate our synset vector representation, we use the precomputed NASARI vectors (Camacho-Collados et al., 2015) to calculate WO. This choice is due to WO’s higher performance over cosine similarity for vectors with explicit dimensions (Pilehvar et al., 2013). As a result, each row of the original adjacency matrix M of G will be replaced with the weights calculated in Formula 7 and then normalized in order to be ready for PPR calculation (see Formula 1). An idea of why a denser semantic network has more useful connections and thus leads to better results is provided by the example in Table 14 , where we show the highest-probability synsets in the PPR vectors calculated with Formula 1 for two different s"
D17-1008,H93-1061,0,0.640156,"Missing"
D17-1008,S01-1001,0,0.432986,"to IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009). Number of training"
D17-1008,Q14-1019,1,0.941927,"wledge-based WSD, takes a radically different approach: the idea is to exploit a general-purpose knowledge resource like WordNet (Fellbaum, 1998) to develop an algorithm which can take advantage of the structural and lexical-semantic information in the resource to choose among the possible senses of a target word occurring in context. For example, a PageRank-based algorithm can be developed to determine the probability of a given sense being reached starting from the senses of its context words. Recent approaches of this kind have been shown to obtain competitive results (Agirre et al., 2014; Moro et al., 2014). However, due to its inherent nature, knowledge-based WSD tends to adopt bag-of-word approaches which do not exploit the local lexical context of a target word, including function and collocation words, which limits this approach in some cases. In this paper we get the best of both worlds and present Train-O-Matic, a novel method for generating huge high-quality training sets for all the words in a language’s vocabulary. The approach is language-independent, thanks to its use of a multilingual knowledge resource, BabelNet (Navigli and Ponzetto, 2012), and it can be applied to any kind of corp"
D17-1008,S13-2040,1,0.944749,"performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009). Number of training sentences per sense Given a target word w, we sorted its senses Senses(w) following the WordNet ordering and se"
D17-1008,Q14-1035,1,0.860614,"ally-curated training data. T-O-M was shown to provide training data for virtually all the target ambiguous nouns, in marked contrast to alternatives like OMSTI, which covers in many cases around half of the tokens, resorting to the MFS otherwise. Moreover Train-O-Matic has proven to scale well to lowresourced languages, for which no manually annotated dataset exists, surpassing the current state of the art of knowledge-based systems. We believe that the ability of T-O-M to overcome the current paucity of annotated data for WSD, coupled with video games with a purpose for validation purposes (Jurgens and Navigli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at http://trainomatic.org. As future work we plan to extend our approach to verbs, adjectives and adverbs. Following Bennett et al. (2016) we will also experiment on more realistic estimates of P (s|w) in Formula 5 as well as other assumptions made in our work. Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No"
D17-1008,P14-1122,1,0.835197,"T-O-M was shown to provide training data for virtually all the target ambiguous nouns, in marked contrast to alternatives like OMSTI, which covers in many cases around half of the tokens, resorting to the MFS otherwise. Moreover Train-O-Matic has proven to scale well to lowresourced languages, for which no manually annotated dataset exists, surpassing the current state of the art of knowledge-based systems. We believe that the ability of T-O-M to overcome the current paucity of annotated data for WSD, coupled with video games with a purpose for validation purposes (Jurgens and Navigli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at http://trainomatic.org. As future work we plan to extend our approach to verbs, adjectives and adverbs. Following Bennett et al. (2016) we will also experiment on more realistic estimates of P (s|w) in Formula 5 as well as other assumptions made in our work. Acknowledgments The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487. 86 References"
D17-1008,L16-1483,0,0.142467,"stances annotated with senses for each target word occurrence. Overall, this amounts to millions of training instances for each language of interest, a number that is not within reach for any language. In fact, no supervised system has been submitted in major multilingual WSD competitions for languages other than English (Navigli et al., 2013; Moro and Navigli, 2015). To overcome this problem, new methodologies have recently been developed which aim to create sense-tagged corpora automatically. Raganato et al. (2016) developed 7 heuristics to grow the number of hyperlinks in Wikipedia pages. Otegi et al. (2016) applied a different disambiguation pipeline for each language to parallel text in Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2015) in order to enrich them with semantic annotations. Taghipour and Ng (2015), the work closest to ours, exploits the alignment from English to Chinese sentences of State-of-the-art supervised systems include Support Vector Machines such as IMS (Zhong and Ng, 2010) and, more recently, LSTM neural networks with attention and multitask learning (Raganato et al., 2017b) as well as LSTMs paired with nearest neighbours classification (Melamud et al., 2016; Yuan et"
D17-1008,vasilescu-etal-2004-evaluating,0,0.0185629,"et, which encodes the relational information that interconnects synsets via different kinds of relation. Approaches include variants of Personalized PageRank (Agirre et al., 2014) and densest subgraph approximation algorithms (Moro et al., 2014) which, thanks to the availability of multilingual resources such as BabelNet, can easily be extended to perform WSD in arbitrary languages. Other approaches to knowledge-based WSD exploit the definitional knowledge contained in a dictionary. The Lesk algorithm (Lesk, 1986) and its variants (Banerjee and Pedersen, 2002; Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004) aim to determine the correct sense of a word by comparing each wordsense definition with the context in which the target word appears. The limit of knowledge-based WSD, however, lies in the absence of mechanisms that can take into account the very local context of a target word occurrence, including non-content words such as prepositions and articles. Furthermore, recent studies seem to suggest that such 8 Conclusion In this paper we presented Train-O-Matic, a novel approach to the automatic construction of large training sets for supervised WSD in an arbitrary language. Train-O-Matic removes"
D17-1008,D16-1174,0,0.0506315,"Missing"
D17-1008,P10-4014,0,0.872816,"ion nor any aligned corpus. Corpora for sense annotation We used two different corpora to extract sentences: Wikipedia and the United Nations Parallel Corpus (Ziemski et al., 2016). The first is the largest and most up-to-date encyclopedic resource, containing definitional information, the second, on the other hand, is a public collection of parliamentary documents of the United Nations. The application of TrainO-Matic to the two corpora produced two senseannotated datasets, which we named T-O-MW iki and T-O-MU N , respectively. Reference system In all our experiments, we used It Makes Sense (Zhong and Ng, 2010, IMS), a state-of-the-art WSD system based on linear Support Vector Machines, as our reference system for comparing its performance when trained on TO-M, against the same WSD system trained on other sense-annotated corpora (i.e., SemCor and OMSTI). Following the WSD literature, unless stated otherwise, we report performance in terms of F1, i.e., the harmonic mean of precision and recall. We note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with perfo"
D17-1008,P13-1132,1,0.940871,"w-normalized adjacency matrix of the semantic network, the restart probability distribution is encoded by vector v (0) , and α is the well-known damping factor usually set to 0.85 (Brin and Page, 1998). If we set v (0) to a unit probability vector (0, . . . , 0, 1, 0, . . . , 0), i.e., restart is always on a given vertex, PPR outputs the probability of reaching every vertex starting from the restart vertex after a certain number of steps. This approach has been used in the literature to create semantic signatures (i.e., profiles) of individual concepts, i.e., vertices of the semantic network (Pilehvar et al., 2013), and then to determine the semantic similarity of concepts. As also done by Pilehvar and Collier (2016), we instead use the PPR vector as an estimate of the conditional probability of a word w0 given the target sense1 s ∈ V of word w: Building a Training Set from Scratch In this Section we present Train-O-Matic, a language-independent approach to the automatic construction of a sense-tagged training set. TrainO-Matic takes as input a corpus C (e.g., Wikipedia) and a semantic network G = (V, E). We assume a WordNet-like structure of G, i.e., V is the set of concepts (i.e., synsets) such that,"
D17-1008,L16-1561,0,0.171643,"(Navigli, 2009) to denote the k-th sense of word w with part-of-speech tag p. http://babelnet.org Retrieved on February 3rd, 2017. 81 to increase coverage, to keep a level playing field we excluded the latter from the corpus. iment for the animal, and operating system and Windows for the device sense, among others). 4 Experimental Setup We note that T-O-M, instead, is fully automatic and does not require any WSD-specific human intervention nor any aligned corpus. Corpora for sense annotation We used two different corpora to extract sentences: Wikipedia and the United Nations Parallel Corpus (Ziemski et al., 2016). The first is the largest and most up-to-date encyclopedic resource, containing definitional information, the second, on the other hand, is a public collection of parliamentary documents of the United Nations. The application of TrainO-Matic to the two corpora produced two senseannotated datasets, which we named T-O-MW iki and T-O-MU N , respectively. Reference system In all our experiments, we used It Makes Sense (Zhong and Ng, 2010, IMS), a state-of-the-art WSD system based on linear Support Vector Machines, as our reference system for comparing its performance when trained on TO-M, against"
D17-1008,J14-4005,1,0.884538,"h the sense annotated dataset. The main difference from our approach is its need for a manually annotated dataset to start the label propagation algorithm, whereas Train-O-Matic is fully automatic. An evaluation against this system would have been interesting, but neither the proprietary training data nor the code are available at the time of writing. In order to generalize effectively, these supervised systems require large numbers of training in85 approaches are barely able to surpass supervised WSD systems when they enrich their networks starting from a comparable amount of annotated data (Pilehvar and Navigli, 2014). With T-O-M, rather than further enriching an existing semantic network, we exploit the information available in the network to annotate raw sentences with sense information and train a state-of-the-art supervised WSD system without task-specific human annotations. the United Nation Parallel Corpus (Ziemski et al., 2016) to reduce the ambiguity of English words and sense-tag English sentences. The assumption is that the second language is less ambiguous than the first one and that hand-made translations of senses are available for each WordNet synset. This approach is, therefore, semi-automat"
D17-1008,S07-1016,0,0.156357,"lly automatize the WSD pipeline with performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009). Number of training sentences per sense Given a target word w, we sorted its senses Senses(w)"
D17-1008,E17-1010,1,0.854168,"note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive basel"
D17-1008,D17-1120,1,0.866053,"note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive basel"
D17-1008,W04-0811,0,0.748595,"or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art. Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3). Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns. Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009). Number of training sentences per sense Given a target wo"
D17-1008,K15-1037,0,0.495882,"z = 2 which were tuned on a separate small in-house development dataset5 . ki = • SemCor (Miller et al., 1993), a corpus containing about 226,000 words annotated manually with WordNet senses. 5 Results 5.1 Impact of syntagmatic relations The first result we report regards the impact of vanilla WordNet vs. our WordNet-induced subgraph of BabelNet (WordNetBN ) when calculating PPR vectors. As can be seen from Table 2 – which shows the performance of the T-O-MW iki corpora generated with the two semantic networks – using WordNet for PPR computation decreases • One Million Sense-Tagged Instances (Taghipour and Ng, 2015, OMSTI), a sense-annotated dataset obtained via a semi-automatic approach based on the disambiguation of a parallel corpus, i.e., the United Nations Parallel Corpus, performed by exploiting manually translated word senses. Because OMSTI integrates SemCor 5 82 50 word-sense pairs annotated manually. Dataset Senseval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL T-O-MW iki BN 70.5 67.4 59.8 65.5 68.6 67.3 T-O-MW iki WN 70.0 63.1 57.9 63.7 69.5 65.7 5.3 IMS uses the MFS as a backoff strategy when no sense can be output for a target word in context (Zhong and Ng, 2010). Consequently, the perf"
D17-1120,E17-1005,0,0.0205939,"dberg, 2016; Plank et al., 2016). Predicting the part-of-speech tag for a given token can also be informative for word senses, and help in dealing with cross-POS lexical ambiguities (e.g., book a flight vs. reading a good book); • Coarse-grained semantic labels (LEX) based on the WordNet (Miller et al., 1990) lexicographer files,4 i.e., 45 coarse-grained semantic categories manually associated with all the synsets in WordNet on the basis of both syntactic and logical groupings (e.g., noun.location, or verb.motion). These very coarse semantic labels, recently employed in a multitask setting by Alonso and Plank (2017), group together related senses and help the model to generalize, especially over senses less covered at training time. We follow previous work (Plank et al., 2016; Alonso and Plank, 2017) and define an auxiliary loss function for each additional task. The overall loss is then computed by summing the main loss (i.e., the one associated with word sense labels) and all the auxiliary losses taken into account. As regards the architecture, we consider both the models described in Sections 3.2 and 3.3 and modify them by adding two softmax layers in addition to the one in the original architecture."
D17-1120,W06-1670,0,0.264937,"Missing"
D17-1120,S15-1007,0,0.0418902,"Missing"
D17-1120,S01-1001,0,0.866693,"out an extensive experimental evaluation where we compare various neural architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, t"
D17-1120,N16-1077,0,0.0222648,"quence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP tasks (Zhou and Xu, 2015; Ma and Hovy, 2016; Wang and Chang, 2016). In particular, sequence-to-sequence models (Sutskever et al., 2014) have grown increasingly popular and are used extensively in, e.g., Machine Translation (Cho et al., 2014; Bahdanau et al., 2015), Sentence Representation (Kiros et al., 2015), Syntactic Parsing (Vinyals et al., 2015), Conversation Modeling (Vinyals and Le, 2015), Morphological Inflection (Faruqui et al., 2016) and Text Summarization (Gu et al., 2016). In line with this trend, we focus on the (so far unexplored) context of supervised WSD, and investigate state-of-the-art all-words approaches that are based on neural sequence learning and capable of disambiguating all target content words within an input text, a key feature in several knowledge-based approaches. 1157 1 he later r y1 he Softmax Layer 1 later r 1 check v the 3.1 3 y2 y3 y4 y5 o1 o2 o3 o4 o5 x1 x2 x3 x4 x5 x1 x2 x3 x4 x5 LSTM Layers he later checked the report Figure 1: Bidirectional LSTM sequence labeling architecture for WSD (2 hidden"
D17-1120,C14-1151,0,0.143081,"ur test sets, divided by part-of-speech tag. We compared against the best supervised and knowledge-based systems evaluated on the same framework. As supervised systems, we considered Context2Vec (Melamud et al., 2016) and It Makes Sense (Zhong and Ng, 2010, IMS), both the original implementation and the best configuration reported by Iacobacci et al. (2016, IMS+emb), which also integrates word embeddings using exponential decay.10 All these supervised systems were trained on the standardized version of SemCor. As knowledge-based systems we considered the embeddings-enhanced version of Lesk by Basile et al. (2014, Leskext +emb), UKB (Agirre et al., 2014) (UKBgloss w2w) , and Babelfy (Moro et al., 2014). All these systems relied on the Most Frequent Sense (MFS) baseline as back-off strategy.11 Overall, both BLSTM and Seq2Seq achieved results that are either state-of-the-art or statistically equivalent (unpaired t-test, p &lt; 0.05) to the best supervised system in each benchmark, performing on par with word experts tuned over explicitly engineered features (Iacobacci et al., 2016). Interestingly enough, BLSTM models tended consistently to outperform their Seq2Seq counterparts, suggesting that an encoder-d"
D17-1120,P16-1191,0,0.0127004,"in Natural Language Processing (NLP), Word Sense Disambiguation (Navigli, 2009, WSD) has received considerable attention over recent years. Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information Retrieval and Extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) to Machine Translation (Carpuat and Wu, 2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotat"
D17-1120,E17-2026,0,0.00656505,"r statistically equivalent (unpaired t-test, p &lt; 0.05) to the best supervised system in each benchmark, performing on par with word experts tuned over explicitly engineered features (Iacobacci et al., 2016). Interestingly enough, BLSTM models tended consistently to outperform their Seq2Seq counterparts, suggesting that an encoder-decoder architecture, despite being more powerful, might be suboptimal for WSD. Furthermore, introducing LEX (cf. Section 4) as auxiliary task was generally helpful; on the other hand, POS did not seem to help, corroborating previous findings (Alonso and Plank, 2017; Bingel and Søgaard, 2017). The overall performance by part of speech was consistent with the above analysis, showing that our models outperformed all knowledgebased systems, while obtaining results that are superior or equivalent to the best supervised mod10 We are not including Yuan et al. (2016), as their models are not available and not replicable on the standardized test sets, being based on proprietary data. 11 Since each system always outputs an answer, F-score equals both precision and recall, and statistical significance can be expressed with respect to any of these measures. els. It is worth noting that RNN-b"
D17-1120,C16-1333,0,0.026287,"Missing"
D17-1120,E17-1086,0,0.0104851,"dditional sense-annotated data (as we show in Section 6.2); second, we carry out an extensive experimental evaluation where we compare various neural architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng,"
D17-1120,D07-1007,0,0.0152287,"learning enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features. 1 Introduction As one of the long-standing challenges in Natural Language Processing (NLP), Word Sense Disambiguation (Navigli, 2009, WSD) has received considerable attention over recent years. Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information Retrieval and Extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) to Machine Translation (Carpuat and Wu, 2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raga"
D17-1120,P16-1154,0,0.0164885,"iter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP tasks (Zhou and Xu, 2015; Ma and Hovy, 2016; Wang and Chang, 2016). In particular, sequence-to-sequence models (Sutskever et al., 2014) have grown increasingly popular and are used extensively in, e.g., Machine Translation (Cho et al., 2014; Bahdanau et al., 2015), Sentence Representation (Kiros et al., 2015), Syntactic Parsing (Vinyals et al., 2015), Conversation Modeling (Vinyals and Le, 2015), Morphological Inflection (Faruqui et al., 2016) and Text Summarization (Gu et al., 2016). In line with this trend, we focus on the (so far unexplored) context of supervised WSD, and investigate state-of-the-art all-words approaches that are based on neural sequence learning and capable of disambiguating all target content words within an input text, a key feature in several knowledge-based approaches. 1157 1 he later r y1 he Softmax Layer 1 later r 1 check v the 3.1 3 y2 y3 y4 y5 o1 o2 o3 o4 o5 x1 x2 x3 x4 x5 x1 x2 x3 x4 x5 LSTM Layers he later checked the report Figure 1: Bidirectional LSTM sequence labeling architecture for WSD (2 hidden layers). We use the notation of Navigli"
D17-1120,D14-1110,0,0.0915758,"Missing"
D17-1120,P15-1010,1,0.842521,"ong-standing challenges in Natural Language Processing (NLP), Word Sense Disambiguation (Navigli, 2009, WSD) has received considerable attention over recent years. Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information Retrieval and Extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) to Machine Translation (Carpuat and Wu, 2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially diff"
D17-1120,P16-1085,1,0.389849,"dard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerful neural language model"
D17-1120,W16-5307,0,0.404492,"Missing"
D17-1120,S01-1004,0,0.0604105,"16) rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w; their instance-based system then compares that representation with those of example sentences annotated with the candidate meanings of w. Similarly, Context2Vec (Melamud et al., 2016) makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data. Finally, K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on bidirectional LSTM for the lexical sample task (Kilgarriff, 2001; Mihalcea et al., 2004). All these contributions have shown that supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and, to the best of our knowledge, very few attempts have been made to disambiguate a text jointly using sequence learning (Ciaramita and Altun, 2006). Sequence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established stand"
D17-1120,P16-1101,0,0.00605319,"ons have shown that supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and, to the best of our knowledge, very few attempts have been made to disambiguate a text jointly using sequence learning (Ciaramita and Altun, 2006). Sequence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP tasks (Zhou and Xu, 2015; Ma and Hovy, 2016; Wang and Chang, 2016). In particular, sequence-to-sequence models (Sutskever et al., 2014) have grown increasingly popular and are used extensively in, e.g., Machine Translation (Cho et al., 2014; Bahdanau et al., 2015), Sentence Representation (Kiros et al., 2015), Syntactic Parsing (Vinyals et al., 2015), Conversation Modeling (Vinyals and Le, 2015), Morphological Inflection (Faruqui et al., 2016) and Text Summarization (Gu et al., 2016). In line with this trend, we focus on the (so far unexplored) context of supervised WSD, and investigate state-of-the-art all-words approaches that are ba"
D17-1120,S07-1006,1,0.903703,"architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (T"
D17-1120,J16-2003,0,0.0118692,"words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerful neural language model to obtain a latent repr"
D17-1120,D17-1008,1,0.765324,": supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotators are involved (de Lacalle and Agirre, 2015), and as a consequence approaches based on unlabeled data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications. In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources (Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015). Such systems construct a model based only on the underlying resource, which is then able to handle multiple target wo"
D17-1120,K16-1006,0,0.0910152,"al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w; their instance-based system then compares that representation with those of example sentences annotated with the candidate meanings of w. Similarly, Context2Vec (Melamud et al., 2016) makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data. Finally, K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on bidirectional LSTM for the lexical sample task (Kilgarriff, 2001; Mihalcea et al., 2004). All these contributions have shown that supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and,"
D17-1120,J14-4005,1,0.470027,"d weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerfu"
D17-1120,W04-0807,0,0.0276139,"rful neural language model to obtain a latent representation for the whole sentence containing a target word w; their instance-based system then compares that representation with those of example sentences annotated with the candidate meanings of w. Similarly, Context2Vec (Melamud et al., 2016) makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data. Finally, K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on bidirectional LSTM for the lexical sample task (Kilgarriff, 2001; Mihalcea et al., 2004). All these contributions have shown that supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and, to the best of our knowledge, very few attempts have been made to disambiguate a text jointly using sequence learning (Ciaramita and Altun, 2006). Sequence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP task"
D17-1120,P16-2067,0,0.125931,"he first time step (Sutskever et al., 2014; Vinyals and Le, 2015), we condition each output symbol yt on c, allowing the decoder to peek into the input at every step, as in Cho et al. (2014). Finally, a fully-connected layer with softmax activation converts the current output vector of the last LSTM layer into a probability distribution over the output vocabulary O. The complete encoder-decoder architecture (including the attention mechanism) is shown in Figure 3. 4 Multitask Learning with Multiple Auxiliary Losses Several recent contributions (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Luong et al., 2016) have shown the effectiveness of multitask learning (Caruana, 1997, MTL) in a sequence learning scenario. In MTL the idea is that of improving generalization performance by leveraging training signals contained in related tasks, in order to exploit their commonalities and differences. MTL is typically carried out by training a single architecture using multiple loss functions and a shared representation, with the underlying intention of improving a main task by incorporating joint learning of one or more related auxiliary tasks. From a practical point of view, MTL works by"
D17-1120,H93-1061,0,0.838733,"ts (1024 units per direction). As regards multilingual all-words WSD (Section 6.2), we experimented, instead, with two different configurations of the embedding layer: the pre-trained bilingual embeddings by Mrkˇsi´c et al. (2017) for all the language pairs of interest (EN-IT, EN-FR, EN-DE, and EN-ES), and the pre-trained multilingual 512-dimensional embeddings for 12 languages by Ammar et al. (2016). 8 We followed Iacobacci et al. (2016) and used the Word2Vec (Mikolov et al., 2013) skip-gram model with 400 dimensions, 10 negative samples and a window size of 10. Training. We used SemCor 3.0 (Miller et al., 1993) as training corpus for all our experiments. Widely known and utilized in the WSD literature, SemCor is one of the largest corpora annotated manually with word senses from the sense inventory of WordNet (Miller et al., 1990) for all openclass parts of speech. We used the standardized version of SemCor as provided in the evaluation framework9 which also includes coarse-grained POS tags from the universal tagset. All models were trained for a fixed number of epochs E = 40 using Adadelta (Zeiler, 2012) with learning rate 1.0 and batch size 32. After each epoch we evaluated our models on the devel"
D17-1120,Q14-1019,1,0.893175,"based on unlabeled data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications. In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources (Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015). Such systems construct a model based only on the underlying resource, which is then able to handle multiple target words at the same time and disambiguate them jointly, whereas word experts are forced to treat each disambiguation target in isolation. In this paper our focus is on supervised WSD, but we depart from previous approaches and adopt a different perspective on the task: instead of framing a separate classification problem for each given word, we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem."
D17-1120,Q17-1022,0,0.00928244,"Missing"
D17-1120,E17-1010,1,0.762423,"2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotators are involved (de Lacalle and Agirre, 2015), and as a consequence approaches based on unlabeled data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be traine"
D17-1120,P15-1173,0,0.0322086,"Missing"
D17-1120,S13-1003,0,0.119233,"sted over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w; their instance-based system then compares that representation with those of example sentences annotated with the candidate meanings of w. Similarly, Context2Vec (Melamud et"
D17-1120,W04-0811,0,0.854914,"tal evaluation where we compare various neural architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more"
D17-1120,P16-2038,0,0.0596547,"le. Instead of feeding c to the decoder only at the first time step (Sutskever et al., 2014; Vinyals and Le, 2015), we condition each output symbol yt on c, allowing the decoder to peek into the input at every step, as in Cho et al. (2014). Finally, a fully-connected layer with softmax activation converts the current output vector of the last LSTM layer into a probability distribution over the output vocabulary O. The complete encoder-decoder architecture (including the attention mechanism) is shown in Figure 3. 4 Multitask Learning with Multiple Auxiliary Losses Several recent contributions (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Luong et al., 2016) have shown the effectiveness of multitask learning (Caruana, 1997, MTL) in a sequence learning scenario. In MTL the idea is that of improving generalization performance by leveraging training signals contained in related tasks, in order to exploit their commonalities and differences. MTL is typically carried out by training a single architecture using multiple loss functions and a shared representation, with the underlying intention of improving a main task by incorporating joint learning of one or more related auxiliary tasks. Fro"
D17-1120,P16-2034,0,0.00562978,"Vinyals he later checked the report et al., 2015), into the sequence labeling architecture of Section 3.1. The resulting attentive bidirectional LSTM tagger augments the original architecture with an attention layer, where a context vector c is computed from all the hidden states h1 , ..., hT of the bidirectional LSTM. The attentive tagger first reads the entire input sequence ~x to construct c, and then exploits c to predict the output label yj at each time step, by concatenating it with the output vector oj of the bidirectional LSTM (Figure 2). We follow previous work (Vinyals et al., 2015; Zhou et al., 2016) and compute c as the weighted sum of the hidden state vectors h1 , ..., hT . Formally, let H ∈ Rn × T be the matrix of hidden state vectors [ h1 , ..., hT ], where n is the hidden state dimension and T is the input sequence length (cf. Section 3). c is obtained as follows: 1 r 1 2 1 v 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 check v y3 3 the report n y4 y5 c Attention Layer o1 h1 o2 h2 o3 h3 o4 h4 o5 h5 LSTM Layers x1 x2 x3 x4 x5 x1 x2 x3 x4 x5 Embedding Layer he later checked the report Figure 2: Attentive bidirectional LSTM sequence labeling architecture for WSD (2 hidden layers). In the sequenc"
D17-1120,K15-1037,0,0.499633,"jority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotators are involved (de Lacalle and Agirre, 2015), and as a consequence approaches based on unlabeled data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications. In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources (Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015). Such systems construct a model based only"
D17-1120,N15-1035,0,0.338788,"jority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at the expense, however, of harder training and limited flexibility. First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotators are involved (de Lacalle and Agirre, 2015), and as a consequence approaches based on unlabeled data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications. In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources (Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015). Such systems construct a model based only"
D17-1120,J17-1002,0,0.0467631,"nguages without requiring additional sense-annotated data (as we show in Section 6.2); second, we carry out an extensive experimental evaluation where we compare various neural architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD. 2 Related Work The literature on WSD is broad and comprehensive (Agirre and Edmonds, 2007; Navigli, 2009): new models are continuously being developed (Yuan et al., 2016; Tripodi and Pelillo, 2017; Butnaru et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a cl"
D17-1120,P16-1218,0,0.0095764,"supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and, to the best of our knowledge, very few attempts have been made to disambiguate a text jointly using sequence learning (Ciaramita and Altun, 2006). Sequence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP tasks (Zhou and Xu, 2015; Ma and Hovy, 2016; Wang and Chang, 2016). In particular, sequence-to-sequence models (Sutskever et al., 2014) have grown increasingly popular and are used extensively in, e.g., Machine Translation (Cho et al., 2014; Bahdanau et al., 2015), Sentence Representation (Kiros et al., 2015), Syntactic Parsing (Vinyals et al., 2015), Conversation Modeling (Vinyals and Le, 2015), Morphological Inflection (Faruqui et al., 2016) and Text Summarization (Gu et al., 2016). In line with this trend, we focus on the (so far unexplored) context of supervised WSD, and investigate state-of-the-art all-words approaches that are based on neural sequence"
D17-1120,P15-1058,0,0.0834156,"data and semisupervised learning are emerging (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Yuan et al., 2016; Pasini and Navigli, 2017). Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications. In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources (Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015). Such systems construct a model based only on the underlying resource, which is then able to handle multiple target words at the same time and disambiguate them jointly, whereas word experts are forced to treat each disambiguation target in isolation. In this paper our focus is on supervised WSD, but we depart from previous approaches and adopt a different perspective on the task: instead of framing a separate classification problem for each given word, we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem. From this standpoint, WSD"
D17-1120,P14-1137,0,0.0166499,"versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features. 1 Introduction As one of the long-standing challenges in Natural Language Processing (NLP), Word Sense Disambiguation (Navigli, 2009, WSD) has received considerable attention over recent years. Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information Retrieval and Extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) to Machine Translation (Carpuat and Wu, 2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (Raganato et al., 2017), at"
D17-1120,P10-4014,0,0.548922,"et al., 2017) and tested over a wide variety of standard benchmarks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2007, 2013; Moro and Navigli, 2015). Moreover, the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks (Pilehvar and Navigli, 2014; Iacobacci et al., 2016; McCarthy et al., 2016; Raganato et al., 2017). As regards supervised WSD, traditional approaches are generally based on extracting local features from the words surrounding the target, and then training a classifier (Zhong and Ng, 2010; Shen et al., 2013) for each target lemma. In their latest developments, these models include more complex features based on word embeddings (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). The recent upsurge of neural networks has also contributed to fueling WSD research: Yuan et al. (2016) rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w; their instance-based system then compares that representation with those of example sentences annotated with the candidate meanings of w. Similarly, Con"
D17-1120,P12-1029,0,0.0611997,"er standard benchmarks and in multiple languages shows that sequence learning enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features. 1 Introduction As one of the long-standing challenges in Natural Language Processing (NLP), Word Sense Disambiguation (Navigli, 2009, WSD) has received considerable attention over recent years. Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information Retrieval and Extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) to Machine Translation (Carpuat and Wu, 2007; Xiong and Zhang, 2014; Neale et al., 2016). Recently, WSD has also been leveraged to build continuous vector representations for word senses (Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016). Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based. Supervised models have been shown to consis"
D17-1120,P15-1109,0,0.0076878,"ll these contributions have shown that supervised neural models can achieve state-of-the-art performances without taking advantage of external resources or language-specific features. However, they all consider each target word as a separate classification problem and, to the best of our knowledge, very few attempts have been made to disambiguate a text jointly using sequence learning (Ciaramita and Altun, 2006). Sequence learning, especially using LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005; Graves, 2013), has become a well-established standard in numerous NLP tasks (Zhou and Xu, 2015; Ma and Hovy, 2016; Wang and Chang, 2016). In particular, sequence-to-sequence models (Sutskever et al., 2014) have grown increasingly popular and are used extensively in, e.g., Machine Translation (Cho et al., 2014; Bahdanau et al., 2015), Sentence Representation (Kiros et al., 2015), Syntactic Parsing (Vinyals et al., 2015), Conversation Modeling (Vinyals and Le, 2015), Morphological Inflection (Faruqui et al., 2016) and Text Summarization (Gu et al., 2016). In line with this trend, we focus on the (so far unexplored) context of supervised WSD, and investigate state-of-the-art all-words app"
D17-1120,S15-2049,1,\N,Missing
D17-1120,S13-2040,1,\N,Missing
D19-1009,W19-4828,0,0.0136172,"n graphtheoretic principles to model the geometry of the data and on game theory to model the learning algorithm which disambiguates the words in a text. It represents the words as the players of a noncooperative game and their senses as the strategy that the players can select in order to play the games. The players are arranged in a graph whose edges determine the interactions and carry word similarity information. The payoff matrix is en91 last layer. The choice of the last layer is motivated by the fact that it stores semantic information and its attention distributions have high entropy (Clark et al., 2019). The first singular vector was removed from A in the case of word vectors whose length exceeded 500. This was done to reduce the redundancy of the representations in line with Arora et al. (2017). The distributions for each x were computed according to SemCor (Miller et al., 1993) and normalized using the softmax function. The replicator dynamics were run until a maximum number of iterations was reached (100) or the difference between two consecutive iterations wasPbelow a small threshold (10−3 ), calculated as ni=1 kxi (t − 1) − xi (t)k. The code of the model is available at https://github."
D19-1009,W18-2505,0,0.151662,"Missing"
D19-1009,W10-2304,0,0.0607499,"Missing"
D19-1009,J10-4006,0,0.0579258,"biguate all the words at the same time. As an example, WSDG can disambiguate 200 words (1650 senses) in 7 seconds, on a single CPU core. A generic representation of the model is proposed in Figure 1. Word embeddings As word embedding models we included 4 pre-word2vec models: the hierarchical log-bilinear model (Mnih and Hinton, 2007, HLBL), a probabilistic linear neural model which aims to predict the embedding of a word given the concatenation of the previous words; CW (Collobert and Weston, 2008), an embeddings model with a deep unified architecture for multitask NLP; Distributional Memory (Baroni and Lenci, 2010, DM), a semantically enriched countbased model; leskBasile (Basile et al., 2014), a model based on Latent Semantic Analysis reduced via Singular-Value Decomposition; 3 models obtained with word2vec: GoogleNews, a set of 300-dimensions vectors trained with the Google News dataset; BNC-*, vectors of different dimensions trained on the British National Corpus including POS information during training; and w2vR, trained with word2vec on the 2014 Implementation details The cosine similarity was used as similarity measure for both words and senses. The A matrix was treated as the adjacency matrix o"
D19-1009,N19-1423,0,0.0973599,"ch¨utze, 2015), instead, is initialized with a set of pretrained word embeddings, and induces sense and synset vectors in the same semantic space using an autoencoder. The vectors are induced by constraining their representation given the assumption that synsets are sums of their lexemes. CamachoCollados et al. (2015) presented NASARI, an approach that learns sense vectors by exploiting the hyperlink structure of the English Wikipedia, linking their representations to the semantic network of BabelNet (Navigli and Ponzetto, 2012). More recent works, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), are based on language models learned using complex neural network architectures. The advantage of these models is that they can produce different representations of words according to the context in which they appear. 4 u(xi , xj ) = xTi · Aij xj where Aij is the mi × mj payoff matrix between players i and j. In evolutionary game theory (Weibull, 1997), the games are played repeatedly and the players update their mixed strategy distributions over time until no player can improve the payoff obtained with the current mixed strategy. This situation corresponds to the equilibrium of the system."
D19-1009,C14-1151,0,0.0733681,"rds (1650 senses) in 7 seconds, on a single CPU core. A generic representation of the model is proposed in Figure 1. Word embeddings As word embedding models we included 4 pre-word2vec models: the hierarchical log-bilinear model (Mnih and Hinton, 2007, HLBL), a probabilistic linear neural model which aims to predict the embedding of a word given the concatenation of the previous words; CW (Collobert and Weston, 2008), an embeddings model with a deep unified architecture for multitask NLP; Distributional Memory (Baroni and Lenci, 2010, DM), a semantically enriched countbased model; leskBasile (Basile et al., 2014), a model based on Latent Semantic Analysis reduced via Singular-Value Decomposition; 3 models obtained with word2vec: GoogleNews, a set of 300-dimensions vectors trained with the Google News dataset; BNC-*, vectors of different dimensions trained on the British National Corpus including POS information during training; and w2vR, trained with word2vec on the 2014 Implementation details The cosine similarity was used as similarity measure for both words and senses. The A matrix was treated as the adjacency matrix of an undirected weighted graph and, to reduce the complexity of the model, the ed"
D19-1009,C04-1051,0,0.110536,"Missing"
D19-1009,Q17-1010,0,0.0610154,"token embeddings to obtain token-level representations. We also included three models which were built together with the sense embeddings introduced below. dump of the English Wikipedia, enriched with retrofitting (Faruqui et al., 2015), a technique to enhance pre-trained embeddings with semantic information. The enrichment was performed using retrofitting’s best configuration, based on the Paraphrase Database (Ganitkevitch et al., 2013, PPDB). We also tested GloVe (Pennington et al., 2014), trained with the concatenation of the 2014 dump of the English Wikipedia and Gigaword 5, and fastText (Bojanowski et al., 2017) trained on Wikipedia 2017, UMBC corpus and the statmt.org news dataset. Sense embeddings As sense embeddings, in addition to the three models introduced in Section 3 (AutoExtend, NASARI and SensEmbed), we included four models: Chen et al. (2014), a unified model which learns sense vectors by training a sense-annotated corpus disambiguated with a framework based on semantic similarity of WordNet sense definitions; meanBNC, created using a weighted combination of the words from WordNet glosses, using, as word vectors, the set of BNC200 mentioned earlier; DeConf (Pilehvar and Collier, 2016), als"
D19-1009,N15-1184,0,0.0817606,"Missing"
D19-1009,N15-1059,1,0.73511,"Missing"
D19-1009,N13-1092,0,0.0411504,"Missing"
D19-1009,D14-1110,0,0.0244773,"enhance pre-trained embeddings with semantic information. The enrichment was performed using retrofitting’s best configuration, based on the Paraphrase Database (Ganitkevitch et al., 2013, PPDB). We also tested GloVe (Pennington et al., 2014), trained with the concatenation of the 2014 dump of the English Wikipedia and Gigaword 5, and fastText (Bojanowski et al., 2017) trained on Wikipedia 2017, UMBC corpus and the statmt.org news dataset. Sense embeddings As sense embeddings, in addition to the three models introduced in Section 3 (AutoExtend, NASARI and SensEmbed), we included four models: Chen et al. (2014), a unified model which learns sense vectors by training a sense-annotated corpus disambiguated with a framework based on semantic similarity of WordNet sense definitions; meanBNC, created using a weighted combination of the words from WordNet glosses, using, as word vectors, the set of BNC200 mentioned earlier; DeConf (Pilehvar and Collier, 2016), also linked to WordNet, a model where sense vectors are inferred in the same semantic space of pre-trained word embeddings by decomposing the given word representation into its constituent senses; and finally SW2V (Mancini et al., 2017), a model lin"
D19-1009,P15-1010,1,0.782319,"s to exploit a neural language model which learns to predict a word occurrence given its surroundings. Another well-known word embedding model was presented by Pennington et al. (2014), which shares the idea of word2vec, but with the difference that it uses explicit latent representations obtained from statistical calculation on word co-occurrences. However, all word embedding models share a common issue: they cannot capture polysemy since they conflate the various word senses into a single vector representation. Several efforts have been presented so far to deal with this problem. SensEmbed (Iacobacci et al., 2015) uses a knowledge-based disambiguation system to build a sense-annotated corpus that, in its turn, is used to train a vector space model for word senses with word2vec. AutoExtend (Rothe and Sch¨utze, 2015), instead, is initialized with a set of pretrained word embeddings, and induces sense and synset vectors in the same semantic space using an autoencoder. The vectors are induced by constraining their representation given the assumption that synsets are sums of their lexemes. CamachoCollados et al. (2015) presented NASARI, an approach that learns sense vectors by exploiting the hyperlink struc"
D19-1009,P16-1085,1,0.83135,"efers to the MFS heuristic computed on SemCor on each dataset. The results are provided as F1 and the first result of the semi supervised systems with a statistically significant difference from the best of each dataset is marked with ∗ (χ2 , p &lt; 0.1). † indicates the same statistics but including also supervised models. introduced by Chaplot and Salakhutdinov (2018) (for this model we did not have the possibility to verify the results since its code is not available). In addition, we also report the performances of relevant supervised models, namely: It Makes Sense (Zhong and Ng, 2010, IMS), Iacobacci et al. (2016), Yuan et al. (2016), Raganato et al. (2017), Joulin et al. (2017) and Uslu et al. (2018). The results of our evaluation are shown in Table 1. As we can see our model achieves state-of-theart performances on four datasets and on S13 and S15 it performs better than many supervised systems. In general the gap between supervised and semi-supervised systems is reducing. This encourages new research in this direction. Our model fares particularly well on the disambiguation of nouns and verbs. However, the main gap between our models and supervised systems relies upon the disambiguation of verbs. NA"
D19-1009,C04-1162,0,0.152878,"Missing"
D19-1009,E17-2068,0,0.112046,"h sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – rather than predicting context words – predicts word senses. Knowledge-based models, instead, exploit the structural properties of a lexical-semantic knowledge base, and typically use the relational information between concepts in the semantic graph together with the lexical information contained therein (Navigli and Lapata, 2010). A popular algorithm used to select the sense of each word in this graph is PageRank (Page et al., 1999) that performs random walks over the network to identify the most important nodes (Haveliwala, 2002; Mihalcea et al., 2004; De Cao et al., 2010). An exten"
D19-1009,H93-1061,0,0.276127,"order to play the games. The players are arranged in a graph whose edges determine the interactions and carry word similarity information. The payoff matrix is en91 last layer. The choice of the last layer is motivated by the fact that it stores semantic information and its attention distributions have high entropy (Clark et al., 2019). The first singular vector was removed from A in the case of word vectors whose length exceeded 500. This was done to reduce the redundancy of the representations in line with Arora et al. (2017). The distributions for each x were computed according to SemCor (Miller et al., 1993) and normalized using the softmax function. The replicator dynamics were run until a maximum number of iterations was reached (100) or the difference between two consecutive iterations wasPbelow a small threshold (10−3 ), calculated as ni=1 kxi (t − 1) − xi (t)k. The code of the model is available at https://github. com/roccotrip/wsd_games_emb. ilarity. The strategy space of each player, i, is represented as a column vector of length m. It is initialized with: ( |mi |−1 if sense h is in Si , xhi = (4) 0 otherwise. This initialization is used in the case of unsupervised WSD, since it does not u"
D19-1009,C10-1066,0,0.0387258,"Missing"
D19-1009,C18-1030,0,0.11345,"ts of data 88 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 88–99, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics neural networks and especially long short-term memory (LSTM) networks, a type of recurrent neural network particularly suitable for handling arbitrary-length sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – rather than predicting context words – predicts word senses. Knowledge-based models, instead, exploit the structural properties of a lexical-semantic knowl"
D19-1009,Q14-1019,1,0.938354,"nowledge base, and typically use the relational information between concepts in the semantic graph together with the lexical information contained therein (Navigli and Lapata, 2010). A popular algorithm used to select the sense of each word in this graph is PageRank (Page et al., 1999) that performs random walks over the network to identify the most important nodes (Haveliwala, 2002; Mihalcea et al., 2004; De Cao et al., 2010). An extension of these models was proposed by Agirre et al. (2014) in which the Personalized PageRank algorithm is applied. Another knowledge-based approach is Babelfy (Moro et al., 2014), which defines a semantic signature for a given context and compares it with all the candidate senses in order to perform the disambiguation task. Chaplot and Salakhutdinov (2018) proposed a method that uses the whole document as the context for the words to be disambiguated, exploiting topical information (Ferret and Grau, 2002). It models word senses using a variant of the Latent Dirichlet Allocation framework (Blei et al., 2003), in which the topic distributions of the words are replaced with sense distributions modeled with a logistic normal distribution according to the frequencies obtai"
D19-1009,D15-1200,0,0.0210975,"lex ∆m , whose corners correspond to pure strategies. The intuition is that player i randomises over strategies according to the probabilities in xi . Each mixed strategy profile lives in the mixed strategy space of the game, given by the Cartesian product Θ = ∆m1 ×∆m2 × · · · × ∆m n . In a two-player game, a strategy profile can be defined as a pair (xi , xj ). The expected payoff for this strategy profile is computed as: on the quality of the input representations. Furthermore, the inclusion of semantic features, in addition to lexical ones, has been proven effective in many NLP approaches (Li and Jurafsky, 2015). Word embeddings, the current paradigm for lexical representation of words, were popularized with word2vec (Mikolov et al., 2013). The main idea is to exploit a neural language model which learns to predict a word occurrence given its surroundings. Another well-known word embedding model was presented by Pennington et al. (2014), which shares the idea of word2vec, but with the difference that it uses explicit latent representations obtained from statistical calculation on word co-occurrences. However, all word embedding models share a common issue: they cannot capture polysemy since they conf"
D19-1009,P18-1230,0,0.0115711,"s, a type of recurrent neural network particularly suitable for handling arbitrary-length sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – rather than predicting context words – predicts word senses. Knowledge-based models, instead, exploit the structural properties of a lexical-semantic knowledge base, and typically use the relational information between concepts in the semantic graph together with the lexical information contained therein (Navigli and Lapata, 2010). A popular algorithm used to select the sense of each word in this graph is PageRank (Page et al., 1999) that performs random walks over the network to identify the mo"
D19-1009,K17-1012,1,0.916798,"ed four models: Chen et al. (2014), a unified model which learns sense vectors by training a sense-annotated corpus disambiguated with a framework based on semantic similarity of WordNet sense definitions; meanBNC, created using a weighted combination of the words from WordNet glosses, using, as word vectors, the set of BNC200 mentioned earlier; DeConf (Pilehvar and Collier, 2016), also linked to WordNet, a model where sense vectors are inferred in the same semantic space of pre-trained word embeddings by decomposing the given word representation into its constituent senses; and finally SW2V (Mancini et al., 2017), a model linked to BabelNet which uses a shallow disambiguation step and which, by extending the word2vec architecture, learns word and sense vectors jointly in the same semantic space as an emerging feature. Results The results of these models are reported in Figure 2. One of the most interesting patterns that emerges from the heat map is that there are some combinations of word and sense embeddings that always work better than others. Sense vectors drive the performance of the system, contributing in great part to the accumulation of payoffs during the games. The sense vectors that maintain"
D19-1009,marelli-etal-2014-sick,0,0.0167365,"d text classification (see, e.g., (Pilehvar et al., 2017)), machine translation and topic modelling. Encouraged by the good results achieved in our exploratory studies, we plan to develop a new model for contextualised word embeddings based on a gametheoretic framework. Table 4: WSDGα results on the SICK dataset. similarity among the senses assigned to the target words is below a threshold (0.9), it classifies the pair as different senses, and as the same sense otherwise. As shown in Table 3 the disambiguation step has a big impact on the results. Sentence similarity We used the SICK dataset (Marelli et al., 2014) for this task. It consists of 9841 sentence pairs that had been annotated with relatedness scores on a 5-point rating scale. We used the test split of this dataset that contains 4906 sentence pairs. The aim of this experiment was to test if disambiguated sense vectors can provide a better representation of sentences than word vectors. We used a simple method to test the two representations: it consisted of representing a sentence as the sum of the disambiguated sense vectors in one case and as the sum of word vectors in the other case. Once the sentence representations had been obtained for b"
D19-1009,K16-1006,0,0.043553,"nce pairs in which a target word appears in two different contexts. The task consisted of predicting if a target word has the same sense in the two sentences or not. The aim of this experiment was twofold: we wanted to show the usefulness of contextualized word embeddings obtained from WSD systems and to demonstrate that the model was able to maintain the textual coherence. The experiments on this dataset were conducted on the development set (1400 sentence pairs). The comparison was conducted against state-of-theart models for contextualized word embeddings and sense embeddings: Context2Vec (Melamud et al., 2016) based on a bidirectional LSTM language model; ELMo1 , the first LSTM hidden state; ELMo3 , the weighted sum of the 3 LSTM layers; BERTbase ; BERTlarge . The results of these systems were taken from Pilehvar and CamachoCollados (2019). We note here that all these models, including WSDGα , do not use training data. They are based on a simple threshold-based classifier, tuned on the development set (638 sentence pairs). WSDGα disambiguates all the words in each pair of sentences separately and, if the cosine Figure 4: Correct and wrong answers given by WSDGα per sense rank. Priors Corroborating"
D19-1009,N18-1202,0,0.382173,"rd2vec. AutoExtend (Rothe and Sch¨utze, 2015), instead, is initialized with a set of pretrained word embeddings, and induces sense and synset vectors in the same semantic space using an autoencoder. The vectors are induced by constraining their representation given the assumption that synsets are sums of their lexemes. CamachoCollados et al. (2015) presented NASARI, an approach that learns sense vectors by exploiting the hyperlink structure of the English Wikipedia, linking their representations to the semantic network of BabelNet (Navigli and Ponzetto, 2012). More recent works, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), are based on language models learned using complex neural network architectures. The advantage of these models is that they can produce different representations of words according to the context in which they appear. 4 u(xi , xj ) = xTi · Aij xj where Aij is the mi × mj payoff matrix between players i and j. In evolutionary game theory (Weibull, 1997), the games are played repeatedly and the players update their mixed strategy distributions over time until no player can improve the payoff obtained with the current mixed strategy. This situation corresponds to"
D19-1009,J17-1002,1,0.933558,"di.uniroma1.it Abstract that are difficult to obtain. Furthermore, in the WSD context, the production of annotated data is even more complicated and excessively timeconsuming compared to other tasks. This arises because of the variability in lexical use. Furthermore, the number of different meanings to be considered in a WSD task is in the order of thousands, whereas classical classification tasks in machine learning have considerably fewer classes. We decided to adopt a semi-supervised approach to overcome the knowledge acquisition bottleneck and innovate the strand of research introduced by Tripodi and Pelillo (2017). These researchers developed a flexible game-theoretic WSD model that exploits word and sense similarity information. This combination of features allows the textual coherence to be maintained: in fact, in this model the disambiguation process is relational, and the sense assigned to a word must always be compatible with the senses of the words in the same text. It can be seen as a constraint satisfaction model which aims to find the best configuration of senses for the words in context. This is possible because the payoff function of the games is modeled in a way in which, when a game is pla"
D19-1009,L18-1168,0,0.0456736,"ent neural network particularly suitable for handling arbitrary-length sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – rather than predicting context words – predicts word senses. Knowledge-based models, instead, exploit the structural properties of a lexical-semantic knowledge base, and typically use the relational information between concepts in the semantic graph together with the lexical information contained therein (Navigli and Lapata, 2010). A popular algorithm used to select the sense of each word in this graph is PageRank (Page et al., 1999) that performs random walks over the network to identify the most important nodes ("
D19-1009,N19-1128,0,0.0254712,"how the usefulness of contextualized word embeddings obtained from WSD systems and to demonstrate that the model was able to maintain the textual coherence. The experiments on this dataset were conducted on the development set (1400 sentence pairs). The comparison was conducted against state-of-theart models for contextualized word embeddings and sense embeddings: Context2Vec (Melamud et al., 2016) based on a bidirectional LSTM language model; ELMo1 , the first LSTM hidden state; ELMo3 , the weighted sum of the 3 LSTM layers; BERTbase ; BERTlarge . The results of these systems were taken from Pilehvar and CamachoCollados (2019). We note here that all these models, including WSDGα , do not use training data. They are based on a simple threshold-based classifier, tuned on the development set (638 sentence pairs). WSDGα disambiguates all the words in each pair of sentences separately and, if the cosine Figure 4: Correct and wrong answers given by WSDGα per sense rank. Priors Corroborating the findings of Pilehvar and Navigli (2014), Postma et al. (2016) conducted a series of experiments to study the effect that the variation of sense distributions in the training set has on the performances of It makes sense (Zhong and"
D19-1009,D16-1174,0,0.0357297,"Missing"
D19-1009,C16-1130,0,0.0558431,"2009); therefore it is possible to use supervised learning techniques to solve the WSD problem. One drawback with this idea is that it requires large amounts of data 88 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 88–99, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics neural networks and especially long short-term memory (LSTM) networks, a type of recurrent neural network particularly suitable for handling arbitrary-length sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – ra"
D19-1009,J14-4005,1,0.839293,"rectional LSTM language model; ELMo1 , the first LSTM hidden state; ELMo3 , the weighted sum of the 3 LSTM layers; BERTbase ; BERTlarge . The results of these systems were taken from Pilehvar and CamachoCollados (2019). We note here that all these models, including WSDGα , do not use training data. They are based on a simple threshold-based classifier, tuned on the development set (638 sentence pairs). WSDGα disambiguates all the words in each pair of sentences separately and, if the cosine Figure 4: Correct and wrong answers given by WSDGα per sense rank. Priors Corroborating the findings of Pilehvar and Navigli (2014), Postma et al. (2016) conducted a series of experiments to study the effect that the variation of sense distributions in the training set has on the performances of It makes sense (Zhong and Ng, 2010). Specifically, they increased the volume of training examples (V) by enriching SemCor with senses inferred from BabelNet; increased the number of least frequent senses (LFS) (V+LFS); and overfitted the model constructing a training set proportional to the correct sense distribution of the test set (GOLD,GOLD+LFS). We used the same training sets to compute the priors for our system. The results o"
D19-1009,P17-1170,1,0.854164,"icular, it will be interesting to test new sense embedding models based on contextual embeddings. Thanks to the flexibility and scalability of our model, as future work we plan to explore in depth its use in different tasks, such as the creation of sentence (document) embeddings and lexical substitution. In fact, we believe that using disambiguated sense vectors, as shown in the contextsensitive embeddings and paraphrase detection studies, can offer a more accurate representation and improve the quality of downstream applications such as sentiment analysis and text classification (see, e.g., (Pilehvar et al., 2017)), machine translation and topic modelling. Encouraged by the good results achieved in our exploratory studies, we plan to develop a new model for contextualised word embeddings based on a gametheoretic framework. Table 4: WSDGα results on the SICK dataset. similarity among the senses assigned to the target words is below a threshold (0.9), it classifies the pair as different senses, and as the same sense otherwise. As shown in Table 3 the disambiguation step has a big impact on the results. Sentence similarity We used the SICK dataset (Marelli et al., 2014) for this task. It consists of 9841"
D19-1009,P10-4014,0,0.544337,"er than recent supervised models); 4. the use of disambiguated sense vectors to obtain contextualized word representations. 2 Word Sense Disambiguation WSD approaches can be divided into two main categories: supervised, which require human intervention in the creation of sense-annotated datasets, and the so-called knowledge-based approaches (Navigli, 2009), which require the construction of a task-independent lexical-semantic knowledge resource, but which, once that work is available, use models that are completely autonomous. As regards supervised systems, a popular system is It makes sense (Zhong and Ng, 2010), a model which takes advantage of standard WSD features such as POS-tags, word co-occurrences, and collocations and creates individual support vector machine classifiers for each ambiguous word. Newer supervised models exploit deep 3 Word and Sense Embeddings A good machine-interpretable representation of lexical features is fundamental for every NLP system. A system’s performance, however, depends 89 consists of a finite set of players N = (1, .., n), a finite set of pure strategies Si = {1, ..., mi } for each player i ∈ N , and a payoff (utility) function ui : S → R, that associates a payof"
D19-1009,C16-1330,0,0.021924,"; ELMo1 , the first LSTM hidden state; ELMo3 , the weighted sum of the 3 LSTM layers; BERTbase ; BERTlarge . The results of these systems were taken from Pilehvar and CamachoCollados (2019). We note here that all these models, including WSDGα , do not use training data. They are based on a simple threshold-based classifier, tuned on the development set (638 sentence pairs). WSDGα disambiguates all the words in each pair of sentences separately and, if the cosine Figure 4: Correct and wrong answers given by WSDGα per sense rank. Priors Corroborating the findings of Pilehvar and Navigli (2014), Postma et al. (2016) conducted a series of experiments to study the effect that the variation of sense distributions in the training set has on the performances of It makes sense (Zhong and Ng, 2010). Specifically, they increased the volume of training examples (V) by enriching SemCor with senses inferred from BabelNet; increased the number of least frequent senses (LFS) (V+LFS); and overfitted the model constructing a training set proportional to the correct sense distribution of the test set (GOLD,GOLD+LFS). We used the same training sets to compute the priors for our system. The results of this analysis are re"
D19-1009,E17-1010,1,0.90331,"al Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 88–99, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics neural networks and especially long short-term memory (LSTM) networks, a type of recurrent neural network particularly suitable for handling arbitrary-length sequences. Yuan et al. (2016) proposed a deep neural model trained with large amounts of data obtained in a semi-supervised fashion. This model was re-implemented by Le et al. (2018), reaching comparable results with a smaller training corpus. Raganato et al. (2017) introduced two approaches for neural WSD using models developed for machine translation and substituting translated words with sense-annotated ones. A recent work that combines labeled data and knowledge-based information has been proposed by Luo et al. (2018). Uslu et al. (2018) proposed fastSense, a model inspired by fastText (Joulin et al., 2017) which – rather than predicting context words – predicts word senses. Knowledge-based models, instead, exploit the structural properties of a lexical-semantic knowledge base, and typically use the relational information between concepts in the sema"
D19-1009,P15-1173,0,0.0376388,"Missing"
D19-1058,C18-1233,0,0.238843,"). The topmost output of the BiLSTM encoder is used together with the output of the PropBank predicate disambiguation layer to obtain a PropBank-style SRL output, and together with the output of the VerbAtlas frame disambiguation layer to obtain a VerbAtlas-style SRL output. • A frame disambiguation layer that, given the BiLSTM encoding yipred of a predicate wpred at the i-th encoder layer (with i = 2), disambiguates wpred with a VerbAtlas frame f , returning a trainable frame embedding ef : Baseline model Our baseline model in Figure 2 is built on top of the syntax-agnostic model proposed by Cai et al. (2018) in that it is mainly composed of a word representation layer, a sequence encoder and a biaffine attentional role scorer. A key difference is that our model features a multioutput layer that returns PropBank (PB) and NomBank (NB) labels (i.e., framesets and their roles) hipred = ReLU(W0 · yipred + b0 ) f = argmax(Wf · hipred + bf ) 633 Syntax-aware system • A predicate disambiguation layer that, given the BiLSTM encoding yjpred of a predicate wpred at the j-th encoder layer (with j = 4) and the frame embedding ef , disambiguates wpred with a PB or NB frameset p, returning a trainable predicate"
D19-1058,W05-0620,0,0.411844,"Missing"
D19-1058,S18-2028,0,0.0332918,"Missing"
D19-1058,lopez-de-lacalle-etal-2014-predicate,0,0.0925316,"Missing"
D19-1058,P98-1013,0,0.89173,"abeling of argument structures is a task pioneered by Gildea and Jurafsky (2002) called Semantic Role Labeling (SRL). SRL has become very popular thanks to its integration into other related NLP tasks such as machine translation (Liu and Gildea, 2010), visual semantic role labeling (Silberer and Pinkal, 2018) and information extraction (Bastianelli et al., 2013). In order to be performed, SRL requires the following core elements: 1) a verb inventory, and 2) a semantic role inventory. However, the current verb inventories used for this task, such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), are language-specific and lack highquality interoperability with existing knowledge bases. Furthermore, such resources provide low to medium coverage of the verbal lexicon (cf. Table 1), with PropBank showing the best figures, but still lower than other lexical inventories like WordNet (Fellbaum et al., 1998). Finally, the informativeness of the semantic roles defined in the various resources ranges from underspecified, as in PropBank’s roles, to overspecified, as in FrameNet’s frame elements. This poses multiple issues in terms of interpretability or cross-domain applicability. To overcome"
D19-1058,W11-4519,0,0.0553864,"to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et al., 2015), Turkish (Sahin ¸ and Adalı, 2018), Basque (Aldezabal et al., 2010), VerbNet addresses this limit by providing explicit, human-readable roles such as Agent, Patient, Experiencer, etc. Yet, VerbNet suffers from low coverage, in that it includes only 6791 verbs, which makes it a suboptimal resource for wide-coverage SRL. Another drawback of VerbNet is its organization into Levin’s classes (Levin, 1993), namely, 329 groups of verbs sharing the same syntactic behavior, independently of their meaning. As a consequence, its classes cannot be used straightforwardly"
D19-1058,W13-2322,0,0.0260248,"can be viewed as homologous: cluster type, argument roles and meaning units. WordNet does not provide argument structures except for sentence frames which, however, are syntactic and do not specify any roleset. make VerbAtlas suitable for NLP tasks that rely on PropBank, we also provide a mapping to its framesets. Finally, we prove through an SRL experiment that VerbAtlas is robust and enables state-of-theart performances on the CoNLL-2009 dataset. 2 among others. Its application goes well beyond the annotation of corpora: in fact, it was also adopted for the Abstract Meaning Representation (Banarescu et al., 2013), a semantic language that aims at abstracting away from cross-lingual syntactic idiosyncrasies, and NomBank (Meyers et al., 2004), a resource which provides argument structures for nouns. However, PropBank’s major drawback is that its roles do not explicitly mark the type of semantic relation with the verb, instead they just enumerate the arguments (i.e., Arg0, Arg1, etc.). Due to this, role labels do not preserve the same type of semantic relation across verbs, e.g., the first arguments of &quot;eat&quot; and &quot;feel&quot; are both labeled with Arg0 even if they express different relations (Agent and Experie"
D19-1058,W13-3820,0,0.0216629,"e Labeling Andrea Di Fabio♦♥ , Simone Conia♦ , Roberto Navigli♦ ♦ Department of Computer Science ♥ Department of Literature and Modern Cultures Sapienza University of Rome, Italy {difabio,conia,navigli}@di.uniroma1.it Abstract The automatic identification and labeling of argument structures is a task pioneered by Gildea and Jurafsky (2002) called Semantic Role Labeling (SRL). SRL has become very popular thanks to its integration into other related NLP tasks such as machine translation (Liu and Gildea, 2010), visual semantic role labeling (Silberer and Pinkal, 2018) and information extraction (Bastianelli et al., 2013). In order to be performed, SRL requires the following core elements: 1) a verb inventory, and 2) a semantic role inventory. However, the current verb inventories used for this task, such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), are language-specific and lack highquality interoperability with existing knowledge bases. Furthermore, such resources provide low to medium coverage of the verbal lexicon (cf. Table 1), with PropBank showing the best figures, but still lower than other lexical inventories like WordNet (Fellbaum et al., 1998). Finally, the informativeness of"
D19-1058,W09-3036,0,0.0174075,"Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et al., 2015), Turkish (Sahin ¸ and Adalı, 2018), Basque (Aldezabal et al., 2010), VerbNet addresses this limit by providing explicit, human-readable roles such as Agent, Patient, Experiencer, etc. Yet, VerbNet suffers from low coverage, in that it includes only 6791 verbs, which makes it a suboptimal resource for wide-coverage SRL. Another drawback of VerbNet is its organization into Levin’s classes (Levin, 1993), namely, 329 groups of verbs sharing the same syntactic behavior, independently of their meaning. As a consequenc"
D19-1058,J02-3001,0,0.430892,"this, role labels do not preserve the same type of semantic relation across verbs, e.g., the first arguments of &quot;eat&quot; and &quot;feel&quot; are both labeled with Arg0 even if they express different relations (Agent and Experiencer, respectively). Related work The most popular English verbal resources are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper-Schuler, 2005). Each resource is based on a different linguistic theory, which leads to different information being provided for each verb (cf. Table 1). FrameNet, in particular, was the first resource to be used for SRL (Gildea and Jurafsky, 2002): it is based on frame semantics, theorized by Fillmore (1976), which assumes different roles, i.e., frame elements, for different frames1 . This led to a proliferation of thousands of roles for only 5200 verbs. Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNL"
D19-1058,P13-1133,0,0.0351078,". To overcome the above limitations, in this paper we present VerbAtlas, a manually-crafted inventory of verbs and argument structures which provides several contributions: 1) full coverage of the English verbal lexicon, 2) prototypical argument structures for each cluster of synsets that define a semantically-coherent frame, 3) cross-domain explicit semantic roles, 4) the specification of refined semantic information and selectional preferences for the argument structure of frames, 5) linkage to WordNet and, as a result, to BabelNet (Navigli and Ponzetto, 2010) and Open Multilingual Wordnet (Bond and Foster, 2013), which in turn enable scalability across languages. Furthermore, to We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from WordNet into semantically-coherent frames. The frames define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to PropBank, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in terms of WordNet synsets, and is the first resource enr"
D19-1058,W09-1201,0,0.414628,"Missing"
D19-1058,E17-1045,0,0.025008,"er et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper-Schuler, 2005). Each resource is based on a different linguistic theory, which leads to different information being provided for each verb (cf. Table 1). FrameNet, in particular, was the first resource to be used for SRL (Gildea and Jurafsky, 2002): it is based on frame semantics, theorized by Fillmore (1976), which assumes different roles, i.e., frame elements, for different frames1 . This led to a proliferation of thousands of roles for only 5200 verbs. Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et a"
D19-1058,W13-5503,0,0.0420373,"Missing"
D19-1058,L18-1231,0,0.0233388,"Missing"
D19-1058,P18-1192,0,0.030548,"ence encoder and a biaffine attentional role scorer. A key difference is that our model features a multioutput layer that returns PropBank (PB) and NomBank (NB) labels (i.e., framesets and their roles) hipred = ReLU(W0 · yipred + b0 ) f = argmax(Wf · hipred + bf ) 633 Syntax-aware system • A predicate disambiguation layer that, given the BiLSTM encoding yjpred of a predicate wpred at the j-th encoder layer (with j = 4) and the frame embedding ef , disambiguates wpred with a PB or NB frameset p, returning a trainable predicate embedding ep : Roth and Lapata (2016) Marcheggiani and Titov (2017) He et al. (2018) Li et al. (2018) Syntax-agnostic system Marcheggiani et al. (2017) hjpred = ReLU(W00 · (yjpred ⊕ ef ) + b00 ) He et al. (2018) Cai et al. (2018) p = argmax(Wp · hjpred + bp ) This work • A biaffine attentional PB and NB role scorer that, given a BiLSTM encoding yi for a word wi and a predicate embedding ep , returns a vector spi of PB/NB role scores for wi with respect to p in a similar fashion to Cai et al. (2018). Formally: spi = y> i · Wrole PB ·e p + Urole PB (yi ⊕e p R 85.3 86.8 89.3 89.3 R 86.8 87.9 89.2 89.5 F1 86.7 88.0 89.5 89.8 F1 87.7 88.7 89.6 90.0 Table 5: Results on the English"
D19-1058,J15-4004,0,0.0393724,"require considerable human intervention in each new language. Finally, the above implies that if there is a verbal repository for one of the languages linked to the aforementioned resources, its argument structures can be seamlessly aligned with the PAS of VerbAtlas, as well as with VerbAtlas frames. 4 4.1 Creation of frames VerbAtlas frames were induced via semantic similarity between synsets. During multiple iterations over the verb inventory, if two or more synsets were perceived as similar, namely, if they shared features like the purpose of the action and the participants in the action (Hill et al., 2015) (e.g., &quot;dine&quot; and &quot;lunch&quot;), they were clustered together to form a new semantically-coherent frame. A one-synset-one-frame strategy was used to avoid any future mapping problem with other resources. The process is based on synset-by-synset human inspection. Two synsets are clustered together in the same frame if they express semanticallysimilar scenarios. For example, the verbs “kill” and “slaughter” share similar participants in the action (an Agent who kills/slaughters; a Patient who is killed/slaughtered) and purpose (Agent makes Patient die), so they are clustered into the K ILL frame. At"
D19-1058,palmer-etal-2008-pilot,0,0.0177226,"me elements, for different frames1 . This led to a proliferation of thousands of roles for only 5200 verbs. Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et al., 2015), Turkish (Sahin ¸ and Adalı, 2018), Basque (Aldezabal et al., 2010), VerbNet addresses this limit by providing explicit, human-readable roles such as Agent, Patient, Experiencer, etc. Yet, VerbNet suffers from low coverage, in that it includes only 6791 verbs, which makes it a suboptimal resource for wide-coverage SRL. Another drawback of VerbNet is its organization into Levin’s classes (Levin, 1993), namel"
D19-1058,N06-2015,0,0.0841733,"slaughters; a Patient who is killed/slaughtered) and purpose (Agent makes Patient die), so they are clustered into the K ILL frame. At the end of the first iteration, we checked the resulting frames and named them according to the common action implied by the synsets contained therein. For example, the E AT frame (Figure 1(b)) is composed of synsets that depict different kinds of action implying eating, like {devour, . . . , pig} and {gorge, . . . , glut}. In Table 2 we report the frames with the highest number of verb synsets. To validate the resulting frames we adopted a strategy similar to Hovy et al. (2006): we provided 3 linguists not involved in the clustering with a random sample of 1000 frame-synset pairs and asked them if the action expressed by the synset was implied by that expressed by the frame. We iterated over the inventory various times and moved the synsets from one frame to another until the Cohen’s Kappa coefficient (Eugenio and Glass, 2004) of their (yes or no) agreement was κ ≥ 0.80. Once this value was attained, we finalized the overall clustering and the resulting frames. Methodology Bottom-up approach The manual construction of VerbAtlas was performed in a bottom-up fashion."
D19-1058,J05-1004,0,0.932309,"The automatic identification and labeling of argument structures is a task pioneered by Gildea and Jurafsky (2002) called Semantic Role Labeling (SRL). SRL has become very popular thanks to its integration into other related NLP tasks such as machine translation (Liu and Gildea, 2010), visual semantic role labeling (Silberer and Pinkal, 2018) and information extraction (Bastianelli et al., 2013). In order to be performed, SRL requires the following core elements: 1) a verb inventory, and 2) a semantic role inventory. However, the current verb inventories used for this task, such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), are language-specific and lack highquality interoperability with existing knowledge bases. Furthermore, such resources provide low to medium coverage of the verbal lexicon (cf. Table 1), with PropBank showing the best figures, but still lower than other lexical inventories like WordNet (Fellbaum et al., 1998). Finally, the informativeness of the semantic roles defined in the various resources ranges from underspecified, as in PropBank’s roles, to overspecified, as in FrameNet’s frame elements. This poses multiple issues in terms of interpretability or cross-"
D19-1058,D14-1162,0,0.0821363,"t roles. Formally, our model is built on top of the following components: • A word representation layer that, given a sentence s = hw1 , w2 , . . . , wn i, builds a sequence of word representations x = hx1 , x2 , . . . , xn i where xi is an embedding representing wi . Each xi is the result of the concatenation of a pre-trained word embedding ept , and the following trainable vectors: a word embedding ew , a lemma embedding el , a POS embedding epos , and a predicate lemma embedding epred (active only if wi is a predicate). Formally: xi = ept ⊕ ew ⊕ el ⊕ epos ⊕ epred . We use GloVe embeddings (Pennington et al., 2014) as our underlying pre-trained word embeddings. • A densely-connected BiLSTM encoder that, given a sequence x of word representations, returns a sequence of encodings y = hyi = BiLSTM(xi ; x) : ∀i ∈ {1, . . . , n}i, where yi is a dynamic representation of xi with respect to the context defined by the whole sequence x. In a densely-connected BiLSTM encoder, the output of each layer is concatenated with the input of the same layer to mitigate the vanishing gradient problem. If hki is the encoding of the k-th layer for xi , then hk+1 = hki ⊕ LSTMf (hki ; hk1:i−1 ) ⊕ i b k k LSTM (hi ; hi+1:n ), a"
D19-1058,D18-1262,0,0.0378579,"Missing"
D19-1058,D17-1035,0,0.0217508,"Missing"
D19-1058,C10-1081,0,0.0436041,"Missing"
D19-1058,D19-1359,1,0.753916,"Missing"
D19-1058,D18-1282,0,0.0630232,"l Semantic Resource and Its Application to Semantic Role Labeling Andrea Di Fabio♦♥ , Simone Conia♦ , Roberto Navigli♦ ♦ Department of Computer Science ♥ Department of Literature and Modern Cultures Sapienza University of Rome, Italy {difabio,conia,navigli}@di.uniroma1.it Abstract The automatic identification and labeling of argument structures is a task pioneered by Gildea and Jurafsky (2002) called Semantic Role Labeling (SRL). SRL has become very popular thanks to its integration into other related NLP tasks such as machine translation (Liu and Gildea, 2010), visual semantic role labeling (Silberer and Pinkal, 2018) and information extraction (Bastianelli et al., 2013). In order to be performed, SRL requires the following core elements: 1) a verb inventory, and 2) a semantic role inventory. However, the current verb inventories used for this task, such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), are language-specific and lack highquality interoperability with existing knowledge bases. Furthermore, such resources provide low to medium coverage of the verbal lexicon (cf. Table 1), with PropBank showing the best figures, but still lower than other lexical inventories like WordNet (F"
D19-1058,W04-2705,0,0.115093,"sentence frames which, however, are syntactic and do not specify any roleset. make VerbAtlas suitable for NLP tasks that rely on PropBank, we also provide a mapping to its framesets. Finally, we prove through an SRL experiment that VerbAtlas is robust and enables state-of-theart performances on the CoNLL-2009 dataset. 2 among others. Its application goes well beyond the annotation of corpora: in fact, it was also adopted for the Abstract Meaning Representation (Banarescu et al., 2013), a semantic language that aims at abstracting away from cross-lingual syntactic idiosyncrasies, and NomBank (Meyers et al., 2004), a resource which provides argument structures for nouns. However, PropBank’s major drawback is that its roles do not explicitly mark the type of semantic relation with the verb, instead they just enumerate the arguments (i.e., Arg0, Arg1, etc.). Due to this, role labels do not preserve the same type of semantic relation across verbs, e.g., the first arguments of &quot;eat&quot; and &quot;feel&quot; are both labeled with Arg0 even if they express different relations (Agent and Experiencer, respectively). Related work The most popular English verbal resources are FrameNet (Baker et al., 1998), PropBank (Palmer et"
D19-1058,taule-etal-2008-ancora,0,0.263028,"ds of roles for only 5200 verbs. Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et al., 2015), Turkish (Sahin ¸ and Adalı, 2018), Basque (Aldezabal et al., 2010), VerbNet addresses this limit by providing explicit, human-readable roles such as Agent, Patient, Experiencer, etc. Yet, VerbNet suffers from low coverage, in that it includes only 6791 verbs, which makes it a suboptimal resource for wide-coverage SRL. Another drawback of VerbNet is its organization into Levin’s classes (Levin, 1993), namely, 329 groups of verbs sharing the same syntactic behavior, independently"
D19-1058,W03-1707,0,0.275214,"es1 . This led to a proliferation of thousands of roles for only 5200 verbs. Such domain specificity makes it difficult to scale to open-text SRL (Hartmann et al., 2017). PropBank challenges the issue of FrameNet’s roles with a repository of only 6 different core roles plus 19 modifiers for 10,687 framesets2 . This resource is the most widely adopted for SRL, as also attested by the popularity of datasets such as CoNLL-2005 (Carreras and Màrquez, 2005) and CoNLL-2009 (Hajiˇc et al., 2009). PropBank’s methodology was also used for other languages, such as Arabic (Palmer et al., 2008), Chinese (Xue and Palmer, 2003), Spanish and Catalan (Taulé et al., 2008), Hindi-Urdu (Bhatt et al., 2009), Brazilian Portuguese (Duran and Aluísio, 2011), Finnish (Haverinen et al., 2015), Turkish (Sahin ¸ and Adalı, 2018), Basque (Aldezabal et al., 2010), VerbNet addresses this limit by providing explicit, human-readable roles such as Agent, Patient, Experiencer, etc. Yet, VerbNet suffers from low coverage, in that it includes only 6791 verbs, which makes it a suboptimal resource for wide-coverage SRL. Another drawback of VerbNet is its organization into Levin’s classes (Levin, 1993), namely, 329 groups of verbs sharing t"
D19-1058,P10-1023,1,0.924721,"in terms of interpretability or cross-domain applicability. To overcome the above limitations, in this paper we present VerbAtlas, a manually-crafted inventory of verbs and argument structures which provides several contributions: 1) full coverage of the English verbal lexicon, 2) prototypical argument structures for each cluster of synsets that define a semantically-coherent frame, 3) cross-domain explicit semantic roles, 4) the specification of refined semantic information and selectional preferences for the argument structure of frames, 5) linkage to WordNet and, as a result, to BabelNet (Navigli and Ponzetto, 2010) and Open Multilingual Wordnet (Bond and Foster, 2013), which in turn enable scalability across languages. Furthermore, to We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from WordNet into semantically-coherent frames. The frames define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to PropBank, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in t"
D19-1359,I11-1077,0,0.0150507,"ehind the creation of such resources was substantiated in a knowledge-based WSD study conducted by Navigli and Lapata (2010), who hypothesized an improvement in performance by several points when enriching a semantic network with tens of lexical-semantic relations for each target word sense. To achieve this demanding goal, endeavors in the literature focused on the fully-automatic production of semantic combinations, such as those obtained by disambiguating topic signatures (Cuadros and Rigau, 2008; Cuadros et al., 2012, KnowNet and deepKnowNet) or by disentangling the concepts in ConceptNet (Chen and Liu, 2011). More recently, Espinosa-Anke et al. (2016) aimed at automatically enriching WordNet with collocational information by leveraging the relations between sense-level embedding spaces (ColWordNet), while Simov et al. (2016) addressed the enhancement of LKBs by exploiting relations over semantically-annotated corpora as contextual information. To the same end, Simov et al. (2018) employed grammatical role embeddings to gather new syntagmatic relations. The lack of syntagmatic information in semantic networks was also tackled by the extension of a lexical database by means of phrasets, i.e., sets"
D19-1359,cuadros-etal-2012-highlighting,0,0.0675874,"Missing"
D19-1359,C08-1021,0,0.116261,"pus (PWNG), which inherently included syntagmatic content, was subsequently also made available in 2008. The rationale behind the creation of such resources was substantiated in a knowledge-based WSD study conducted by Navigli and Lapata (2010), who hypothesized an improvement in performance by several points when enriching a semantic network with tens of lexical-semantic relations for each target word sense. To achieve this demanding goal, endeavors in the literature focused on the fully-automatic production of semantic combinations, such as those obtained by disambiguating topic signatures (Cuadros and Rigau, 2008; Cuadros et al., 2012, KnowNet and deepKnowNet) or by disentangling the concepts in ConceptNet (Chen and Liu, 2011). More recently, Espinosa-Anke et al. (2016) aimed at automatically enriching WordNet with collocational information by leveraging the relations between sense-level embedding spaces (ColWordNet), while Simov et al. (2016) addressed the enhancement of LKBs by exploiting relations over semantically-annotated corpora as contextual information. To the same end, Simov et al. (2018) employed grammatical role embeddings to gather new syntagmatic relations. The lack of syntagmatic inform"
D19-1359,D19-1058,1,0.753916,"Missing"
D19-1359,S01-1001,0,0.857992,"ne. Statistically-significant differences, according to a χ2 test (p < 0.01), compared to the baseline (first row), are underlined. The second column reports the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target word. LKBs For the purposes of our evaluation we measured the performance obtaine"
D19-1359,C16-1323,0,0.246731,"Missing"
D19-1359,S10-1095,0,0.232444,"Missing"
D19-1359,lemnitzer-etal-2008-enriching,0,0.301276,"atic expressions) and the term “lexicalsemantic combination” to refer to sense-annotated lexical combinations. 3 http://wordnetcode.princeton.edu/ glosstag.shtml 3534 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3534–3540, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics dramatically when employing an LKB with a larger number of high-quality lexical-semantic relations, i.e., more connections between concepts (Boyd-Graber et al., 2006; Lemnitzer et al., 2008; Ponzetto and Navigli, 2010). During the last two decades, a certain amount of work has been carried out aimed at enriching LKBs with new lexical-semantic relations. To this end, knowledge has been (semi-)automatically extracted from large collections of data and integrated into lexical resources such as WordNet. As far as semi-automatic approaches are concerned, Mihalcea and Moldovan (2001) conceived eXtended WordNet, a resource providing disambiguated glosses by means of a classification ensemble combined with human supervision. A set of manually disambiguated glosses, called the Princeton"
D19-1359,W02-0109,0,0.315238,"h of correlation between pairs of POS-tagged, lemmatized content words6 w1 , w2 , co-occurring within a sliding window of 3 words. Each candidate pair (w1 , w2 ) was weighted using Dice’s coefficient multiplied by a logarithmic factor of the co-occurrence frequency: score(w1 , w2 ) = log2 (1 + nw1 w2 ) 2nw1 w2 nw1 + nw2 (1) where nwi (i ∈ {1, 2}) is the frequency of wi and nw1 w2 is the frequency of the two words cooccurring within a window. Three filters were then applied in order to slim down the list of pairs: (i) we filtered out English stopwords according to the Natural Language Toolkit (Loper and Bird, 2002, NLTK 3.4); (ii) we discarded combinations between verbs and verbs; (iii) we discarded combinations not linked by any of the five most frequent dependencies in our list, namely: compound, dobj (direct object), iobj (indirect object), nsubj (nominal subject) and nmod (nominal modifier). Finally, we ranked the resulting lexical combination list according to the geometric mean 4 November 2018 English Wikipedia dump. According to the Universal Dependencies v2 (https://universaldependencies.org/u/ dep/all.html). 6 Restricted to nouns and verbs in the WordNet dictionary. 3535 5 word 1 runv runv run"
D19-1359,S15-2062,0,0.0377296,"Missing"
D19-1359,P14-5010,0,0.00394575,"n this work features: (i) wide coverage with a broad spectrum of possible lexical combinations, and (ii) high precision thanks to being entirely manually curated. 3 SyntagNet: a wide-coverage lexical-semantic combination resource In this Section, we present SyntagNet, a knowledge resource created starting from lexical combinations extracted from the English Wikipedia4 and the British National Corpus (Leech, 1992, BNC), and manually disambiguated according to the WordNet 3.0 sense inventory. 3.1 Methodology Lexical combination extraction First of all, we employed the Stanford CoreNLP pipeline (Manning et al., 2014) to extract the dependency trees5 for all the sentences in both Wikipedia and the BNC. Then, in order to identify relevant combinations, we determined the strength of correlation between pairs of POS-tagged, lemmatized content words6 w1 , w2 , co-occurring within a sliding window of 3 words. Each candidate pair (w1 , w2 ) was weighted using Dice’s coefficient multiplied by a logarithmic factor of the co-occurrence frequency: score(w1 , w2 ) = log2 (1 + nw1 w2 ) 2nw1 w2 nw1 + nw2 (1) where nwi (i ∈ {1, 2}) is the frequency of wi and nw1 w2 is the frequency of the two words cooccurring within a"
D19-1359,L18-1163,0,0.0578279,"Missing"
D19-1359,S15-2049,1,0.860016,"ts the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target word. LKBs For the purposes of our evaluation we measured the performance obtained with UKB when combined with different LKBs. As our baseline we used WordNet + PWNG, which is the best configuration of UKB according to its authors. We"
D19-1359,S13-2040,1,0.849149,"are underlined. The second column reports the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target word. LKBs For the purposes of our evaluation we measured the performance obtained with UKB when combined with different LKBs. As our baseline we used WordNet + PWNG, which is the best configu"
D19-1359,W04-0811,0,0.439795,"es, according to a χ2 test (p < 0.01), compared to the baseline (first row), are underlined. The second column reports the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target word. LKBs For the purposes of our evaluation we measured the performance obtained with UKB when combined with differen"
D19-1359,L18-1168,0,0.0911283,"Missing"
D19-1359,D17-1008,1,0.918481,"Missing"
D19-1359,P10-1154,1,0.780275,"Missing"
D19-1359,E17-1010,1,0.862544,"ults scored by a specific resource combined with the WNG (WordNet+PWNG) baseline. Statistically-significant differences, according to a χ2 test (p < 0.01), compared to the baseline (first row), are underlined. The second column reports the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target"
D19-1359,D17-1120,1,0.863369,"ults scored by a specific resource combined with the WNG (WordNet+PWNG) baseline. Statistically-significant differences, according to a χ2 test (p < 0.01), compared to the baseline (first row), are underlined. The second column reports the number of relations of the added resource. input LKB. We used its PPRw2w single-sentence context disambiguation method, which initializes the PPR vector using the context of the target word in a given sentence, while excluding the contribution of the target word itself. Evaluation benchmarks and measures We used five test sets standardized with WordNet 3.0 (Raganato et al., 2017a) including the English allwords tasks from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). To run experiments on multilingual WSD, we used the last two of the foregoing datasets, which also include German, Spanish, French and Italian, employing, as sense inventory, the synset lexicalizations provided in BabelNet 4.08 . As customary, we computed precision, recall and F1, which in our case coincided, due to UKB always outputting a sense for each target"
D19-1359,2018.gwc-1.33,0,0.0124433,"atic production of semantic combinations, such as those obtained by disambiguating topic signatures (Cuadros and Rigau, 2008; Cuadros et al., 2012, KnowNet and deepKnowNet) or by disentangling the concepts in ConceptNet (Chen and Liu, 2011). More recently, Espinosa-Anke et al. (2016) aimed at automatically enriching WordNet with collocational information by leveraging the relations between sense-level embedding spaces (ColWordNet), while Simov et al. (2016) addressed the enhancement of LKBs by exploiting relations over semantically-annotated corpora as contextual information. To the same end, Simov et al. (2018) employed grammatical role embeddings to gather new syntagmatic relations. The lack of syntagmatic information in semantic networks was also tackled by the extension of a lexical database by means of phrasets, i.e., sets of free combinations of words recurrently used to express a concept (Bentivogli and Pianta, 2004). Unfortunately, due to their (semi-)automatic nature, the aforementioned resources could not inherently offer wide coverage and high precision at the same time. Compared to other resources geared towards knowledge-based WSD, the novel resource we contribute in this work features:"
E06-1017,W04-0811,0,\N,Missing
E06-1017,J91-1002,0,\N,Missing
E06-1017,W04-0838,0,\N,Missing
E06-1017,C96-1005,0,\N,Missing
E06-1017,C04-1053,0,\N,Missing
E06-1017,W02-0817,0,\N,Missing
E06-1017,H93-1061,0,\N,Missing
E06-1017,W04-0827,0,\N,Missing
E06-1017,W04-0804,0,\N,Missing
E06-1017,W04-0864,0,\N,Missing
E06-2006,W04-0811,0,\N,Missing
E06-2006,P05-3019,0,\N,Missing
E06-2006,J91-1002,0,\N,Missing
E06-2006,W04-0838,0,\N,Missing
E06-2006,J04-2002,1,\N,Missing
E06-2006,W04-0827,0,\N,Missing
E06-2006,E06-1017,1,\N,Missing
E06-2006,W04-0861,0,\N,Missing
E09-1068,P98-1013,0,0.0108133,"sed on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy (Snow et al., 2006), structural approaches based on semantic clusters and distance metrics (Pennacchiotti and Pantel, 2006), supervised machine learning methods for the disambiguation of meronymy relations (Girju et al., 2003), etc. discovered, thus helping lexicographers improve the resources6 . An adaptation similar to that described for disambiguating the Ragazzini/Biagi can be employed for mapping pairs of lexical resources (e.g. FrameNet (Baker et al., 1998) to WordNet), thus contributing to the beneficial knowledge integration process. Following this direction, we are planning to further experiment on the mapping of FrameNet, VerbNet (Kipper et al., 2000), and other lexical resources. The graphs output by the CQC algorithm for our datasets are available from http://lcl.uniroma1.it/cqc. We are scheduling the release of a software package which includes our implementation of the CQC algorithm and allows its application to any resource for which a standard interface can be written. Finally, starting from the work of Budanitsky and Hirst (2006), we"
E09-1068,W04-0804,0,0.0245408,"Missing"
E09-1068,C88-2098,0,0.338369,"Work Word Sense Disambiguation is a large research field (see (Navigli, 2009) for an up-to-date overview). However, in this paper we focused on a specific kind of WSD, namely the disambiguation of dictionary definitions. Seminal works on the topic date back to the late 1970s, with the development of models for the identification of taxonomies from lexical resources (Litkowski, 1978; Amsler, 1980). Subsequent works focused on the identification of genus terms (Chodorow et al., 1985) and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (Nakamura and Nagao, 1988; Ide and V´eronis, 1993)). Kozima and Furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the Longman Dictionary of Contemporary English (LDOCE). In this direction, it is worth citing the work of Vanderwende (1996) and Richardson et al. (1998), who describe the construction of MindNet, a lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries. As a result, weighted relation paths are produced to infer the semantic similarity between pairs of words. Several heuristics"
E09-1068,P06-1100,0,0.0620564,"ontrast, the approach presented in this paper performs the disambiguation of ambiguous words by exploiting only the reference dictionary itself. Furthermore, as we showed in Section 3.3, our method does not rely on WordNet, and can be applied to any lexical knowledge resource, including bilingual dictionaries. Finally, methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy (Snow et al., 2006), structural approaches based on semantic clusters and distance metrics (Pennacchiotti and Pantel, 2006), supervised machine learning methods for the disambiguation of meronymy relations (Girju et al., 2003), etc. discovered, thus helping lexicographers improve the resources6 . An adaptation similar to that described for disambiguating the Ragazzini/Biagi can be employed for mapping pairs of lexical resources (e.g. FrameNet (Baker et al., 1998) to WordNet), thus contributing to the beneficial knowledge integration process. Following this direction, we are planning to further experiment on the mapping of FrameNet, VerbNet (Kipper et al., 2000), and other lexical resources. The graphs output by th"
E09-1068,P98-2180,0,0.0522623,"ent of models for the identification of taxonomies from lexical resources (Litkowski, 1978; Amsler, 1980). Subsequent works focused on the identification of genus terms (Chodorow et al., 1985) and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (Nakamura and Nagao, 1988; Ide and V´eronis, 1993)). Kozima and Furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the Longman Dictionary of Contemporary English (LDOCE). In this direction, it is worth citing the work of Vanderwende (1996) and Richardson et al. (1998), who describe the construction of MindNet, a lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries. As a result, weighted relation paths are produced to infer the semantic similarity between pairs of words. Several heuristics have been presented for the disambiguation of the genus of a dictionary definition (Wilks et al., 1996; Rigau et al., 1997). More recently, a set of heuristic techniques has been proposed to semantically annotate WordNet glosses, leading to the release of the eXtended WordNet (Harabagiu et al.,"
E09-1068,P06-1101,0,0.0664406,"al., 1998) to WordNet), thus contributing to the beneficial knowledge integration process. Following this direction, we are planning to further experiment on the mapping of FrameNet, VerbNet (Kipper et al., 2000), and other lexical resources. The graphs output by the CQC algorithm for our datasets are available from http://lcl.uniroma1.it/cqc. We are scheduling the release of a software package which includes our implementation of the CQC algorithm and allows its application to any resource for which a standard interface can be written. Finally, starting from the work of Budanitsky and Hirst (2006), we plan to experiment with the CQC algorithm when employed as a semantic similarity measure, and compare it with the most successful existing approaches. Although in this paper we focused on the disambiguation of dictionary glosses, the same approach can be applied for disambiguating collocations according to a dictionary of choice, thus providing a way to further enrich lexical resources with external knowledge. 6 Acknowledgments Conclusions The author is grateful to Ken Litkowski and the anonymous reviewers for their useful comments. He also wishes to thank Zanichelli and Macquarie for kin"
E09-1068,J96-3009,0,0.0179728,"he development of models for the identification of taxonomies from lexical resources (Litkowski, 1978; Amsler, 1980). Subsequent works focused on the identification of genus terms (Chodorow et al., 1985) and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (Nakamura and Nagao, 1988; Ide and V´eronis, 1993)). Kozima and Furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the Longman Dictionary of Contemporary English (LDOCE). In this direction, it is worth citing the work of Vanderwende (1996) and Richardson et al. (1998), who describe the construction of MindNet, a lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries. As a result, weighted relation paths are produced to infer the semantic similarity between pairs of words. Several heuristics have been presented for the disambiguation of the genus of a dictionary definition (Wilks et al., 1996; Rigau et al., 1997). More recently, a set of heuristic techniques has been proposed to semantically annotate WordNet glosses, leading to the release of the eXtend"
E09-1068,W04-0823,0,0.0185085,"e eXtended WordNet (Harabagiu et al., 1999; Moldovan and Novischi, 2004). Among the methods, the cross reference heuristic is the closest technique to our notion of cycles and quasi-cycles. Given a pair of words w and w0 , this heuristic is based on the occurrence of 600 w in the gloss of a sense s0 of w0 and, vice versa, of w0 in the gloss of a sense s of w. In other words, a graph cycle s → s0 → s of length 2 is sought. Based on the eXtended WordNet, a gloss disambiguation task was organized at Senseval-3 (Litkowski, 2004). Interestingly, the best performing systems, namely the TALP system (Castillo et al., 2004), and SSI (Navigli and Velardi, 2005), are knowledge-based and rely on rich knowledge resources: respectively, the Multilingual Central Repository (Atserias et al., 2004), and a proprietary lexical knowledge base. In contrast, the approach presented in this paper performs the disambiguation of ambiguous words by exploiting only the reference dictionary itself. Furthermore, as we showed in Section 3.3, our method does not rely on WordNet, and can be applied to any lexical knowledge resource, including bilingual dictionaries. Finally, methods in the literature more focused on a specific disambig"
E09-1068,P85-1037,0,0.888259,"Missing"
E09-1068,W06-1663,0,0.124046,"reover, while computational lexicons like WordNet contain semantically explicit information such as, among others, hypernymy and meronymy relations, most thesauri, glossaries, and machine-readable dictionaries are often just electronic transcriptions of their paper counterparts. As a result, for each entry (e.g. a word sense or thesaurus entry) they mostly provide implicit information in the form of free text. The production of semantically richer lexical resources can help alleviate the knowledge acquisition bottleneck and potentially enable advanced Natural Language Processing applications (Cuadros and Rigau, 2006). However, in order to reduce the high cost of manual annotation (Edmonds, 2000), and to avoid the repetition of this effort for each knowledge resource, this task must be supported by wide-coverage automated techniques which do not rely on the specific resource at hand. In this paper, we aim to make explicit large quantities of semantic information implicitly contained in the glosses of existing widecoverage lexical knowledge resources (specifically, machine-readable dictionaries and computational lexicons). To this end, we present a method for Gloss Word Sense Disambiguation (WSD), called th"
E09-1068,N03-1011,0,0.029129,"reference dictionary itself. Furthermore, as we showed in Section 3.3, our method does not rely on WordNet, and can be applied to any lexical knowledge resource, including bilingual dictionaries. Finally, methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy (Snow et al., 2006), structural approaches based on semantic clusters and distance metrics (Pennacchiotti and Pantel, 2006), supervised machine learning methods for the disambiguation of meronymy relations (Girju et al., 2003), etc. discovered, thus helping lexicographers improve the resources6 . An adaptation similar to that described for disambiguating the Ragazzini/Biagi can be employed for mapping pairs of lexical resources (e.g. FrameNet (Baker et al., 1998) to WordNet), thus contributing to the beneficial knowledge integration process. Following this direction, we are planning to further experiment on the mapping of FrameNet, VerbNet (Kipper et al., 2000), and other lexical resources. The graphs output by the CQC algorithm for our datasets are available from http://lcl.uniroma1.it/cqc. We are scheduling the r"
E09-1068,W99-0501,0,0.024619,"son et al. (1998), who describe the construction of MindNet, a lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries. As a result, weighted relation paths are produced to infer the semantic similarity between pairs of words. Several heuristics have been presented for the disambiguation of the genus of a dictionary definition (Wilks et al., 1996; Rigau et al., 1997). More recently, a set of heuristic techniques has been proposed to semantically annotate WordNet glosses, leading to the release of the eXtended WordNet (Harabagiu et al., 1999; Moldovan and Novischi, 2004). Among the methods, the cross reference heuristic is the closest technique to our notion of cycles and quasi-cycles. Given a pair of words w and w0 , this heuristic is based on the occurrence of 600 w in the gloss of a sense s0 of w0 and, vice versa, of w0 in the gloss of a sense s of w. In other words, a graph cycle s → s0 → s of length 2 is sought. Based on the eXtended WordNet, a gloss disambiguation task was organized at Senseval-3 (Litkowski, 2004). Interestingly, the best performing systems, namely the TALP system (Castillo et al., 2004), and SSI (Navigli a"
E09-1068,E93-1028,0,0.134037,"field (see (Navigli, 2009) for an up-to-date overview). However, in this paper we focused on a specific kind of WSD, namely the disambiguation of dictionary definitions. Seminal works on the topic date back to the late 1970s, with the development of models for the identification of taxonomies from lexical resources (Litkowski, 1978; Amsler, 1980). Subsequent works focused on the identification of genus terms (Chodorow et al., 1985) and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (Nakamura and Nagao, 1988; Ide and V´eronis, 1993)). Kozima and Furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the Longman Dictionary of Contemporary English (LDOCE). In this direction, it is worth citing the work of Vanderwende (1996) and Richardson et al. (1998), who describe the construction of MindNet, a lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries. As a result, weighted relation paths are produced to infer the semantic similarity between pairs of words. Several heuristics have been presented for the disambiguation of the genu"
E09-1068,J78-4003,0,0.874897,"Missing"
E09-1068,C98-2175,0,\N,Missing
E09-1068,C98-1013,0,\N,Missing
E09-1068,J06-1003,0,\N,Missing
E09-1068,P97-1007,0,\N,Missing
E14-1044,Q13-1013,0,0.034221,"rpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexical surface of text, while at the same time avoiding the need for training in a cross-language setting. To achieve this we leverage a multilingual semantic network, i.e., BabelNet, to obtain language-independent representations, which contain concepts together with semantic relations bet"
E14-1044,D09-1092,0,0.111807,"Missing"
E14-1044,J05-4003,0,0.0385517,"retrieval and categorization. 1 Introduction The huge amount of text that is available online is becoming ever increasingly multilingual, providing an additional wealth of useful information. Most of this information, however, is not easily accessible to the majority of users because of language barriers which hamper the cross-lingual search and retrieval of knowledge. Today’s search engines would benefit greatly from effective techniques for the cross-lingual retrieval of valuable information that can satisfy a user’s needs by not only providing (Landauer and Littman, 1994) and translating (Munteanu and Marcu, 2005) relevant results into different languages, but also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis"
E14-1044,W07-0711,0,0.0843323,"comparable document in the other language. We compared KBSim against the state-of-the-art supervised models S2Net, OPCA, CCA, and CLLSI (cf. Section 2). In contrast to these models, KBSim does not need a training step, so we applied it directly to the testing partition. In addition we also included the results of CL-ESA7 , CL-C3G8 and two simple vector-based models which translate all documents into English on a word-by-word basis and compared them using cosine similarity: the first model (CosSimE ) uses a statistical dictionary trained with Europarl using Wavelet-Domain Hidden Markov Models (He, 2007), a model similar to IBM Model 4; the second model (CosSimBN ) instead uses Algorithm 1 to translate the vectors with BabelNet. 0 KBSim(d, d ) = c(G)Sg (G, G ) + (1 − c(G))Sv (~ vLL0 , ~ vLL0 ), (8) where c(G) is an interpolation factor calculated as the edge density of knowledge graph G: c(G) = |E(G)| . |V (G)|(|V (G) |− 1) (9) Note that, using the factor c(G) to interpolate the two similarities in Eq. 8, we determine the relevance for the knowledge graphs and the multilingual vectors in a dynamic way. Indeed, c(G) makes the contribution of graph similarity depend on the richness of the knowl"
E14-1044,W11-0329,0,0.13096,"puter Science Sapienza Universit`a di Roma, Italy {francosalvador,navigli}@di.uniroma1.it 2 Natural Language Engineering Lab - PRHLT Research Center Universitat Polit`ecnica de Val`encia, Spain {mfranco,prosso}@dsic.upv.es Abstract 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multi"
E14-1044,D10-1025,0,0.0880472,"1 Department of Computer Science Sapienza Universit`a di Roma, Italy {francosalvador,navigli}@di.uniroma1.it 2 Natural Language Engineering Lab - PRHLT Research Center Universitat Polit`ecnica de Val`encia, Spain {mfranco,prosso}@dsic.upv.es Abstract 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador e"
E14-1044,W05-0822,0,0.0257559,"419 Model S2Net KBSim OPCA CosSimE CosSimBN CCA CL-LSI CL-ESA CL-C3G Dimension 2000 N/A 2000 N/A N/A 1500 5000 15000 N/A Accuracy 0.7447 0.7342 0.7255 0.7033 0.7029 0.6894 0.5302 0.2660 0.2511 MMR 0.7973 0.7750 0.7734 0.7467 0.7550 0.7378 0.6130 0.3305 0.3025 KBSim Full MT CosSimBN OPCA CCA CL-LSI CosSimE N/A 50 N/A 100 150 5000 N/A EN News Accuracy 0.8189 0.8483 0.8023 0.8412 0.8388 0.8401 0.8046 ES News Accuracy 0.6997 0.6484 0.6737 0.5954 0.5323 0.5105 0.4481 ble categories. In addition, each dataset of news is translated into the other four languages using the Portage translation system (Sadat et al., 2005). As a result, we have five different multilingual datasets, each containing source news documents in one language and four sets of translated documents in the other languages. Each of the languages has an independent vocabulary. Document vectors in the collection are created using TFIDFbased weighting. and CL-LSI, OPCA performs better thanks to its improved projection method using a noise covariance matrix, which enables it to obtain the main components in a low-dimensional space. CL-C3G and CL-ESA obtain the lowest results. Considering that English and Spanish do not have many lexical simila"
E17-1010,E09-1005,0,0.0315951,"account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2"
E17-1010,S10-1013,0,0.05303,"Missing"
E17-1010,S01-1001,0,0.929515,"Missing"
E17-1010,L16-1239,0,0.0353117,"Missing"
E17-1010,eisele-chen-2010-multiun,0,0.0122763,"604 802,443 30,441,386 #Annotations 2,282 1,850 455 1,644 1,022 226,036 911,134 #Sense types 1,335 1,167 375 827 659 33,362 3,730 #Word types 1,093 977 330 751 512 22,436 1,149 Ambiguity 5.4 6.8 8.5 4.9 5.5 6.8 8.9 Table 1: Statistics of the WSD datasets used in the evaluation framework (after standardization). • OMSTI (Taghipour and Ng, 2015a). OMSTI (One Million Sense-Tagged Instances) is a large corpus annotated with senses from the WordNet 3.0 inventory. It was automatically constructed by using an alignmentbased WSD approach (Chan and Ng, 2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OMSTI5 has already shown its potential as a training corpus by improving the performance of supervised systems which add it to existing training data (Taghipour and Ng, 2015a; Iacobacci et al., 2016). • SemEval-07 task 17 (Pradhan et al., 2007). This is the smallest among the five datasets, containing 455 sense annotations for nouns and verbs only. It was originally annotated using WordNet 2.1 sense inventory. • SemEval-13 task 12 (Navigli et al., 2013). This dataset includes thirteen documents from various domains. In this case the original sense inventory was WordNet 3.0,"
E17-1010,C14-1151,0,0.660668,"these approaches rely on the structure or content of manually-curated knowledge resources for disambiguation. One of the first approaches of this kind was Lesk (1986), which in its original version consisted of calculating the overlap between the context of the target word and its definitions as given by the sense inventory. Based on the same principle, various works have adapted the original algorithm by also taking into account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based"
E17-1010,P10-1156,0,0.0221383,"m related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its colloca"
E17-1010,P16-1085,1,0.413258,"ng systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0, and (3) preprocessing the datasets by consistently using the same pipeline. Second, we use this evaluation framework to perform a fair quantitative and qualitative empirical comparison of the main techniques proposed in the WSD literature, including the latest advances based on neural networks. 2 State of the Art The task of Word Sense Disambiguat"
E17-1010,P15-1072,1,0.718796,"Missing"
E17-1010,W16-5307,0,0.340237,"Missing"
E17-1010,L16-1269,1,0.877179,"Missing"
E17-1010,W02-1006,0,0.0338101,"urces (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised m"
E17-1010,D14-1110,0,0.140902,"Missing"
E17-1010,P14-5010,0,0.00371456,"SD datasets In this section we explain our pipeline for transforming any given evaluation dataset or senseannotated corpus into a preprocessed unified for100 Figure 1: Pipeline for standardizing any given WSD dataset. mat. In our pipeline we do not make any distinction between evaluation datasets and senseannotated training corpora, as the pipeline can be applied equally to both types. For simplicity we will refer to both evaluation datasets and training corpora as WSD datasets. Figure 1 summarizes our pipeline to standardize a WSD dataset. The process consists of four steps: CoreNLP toolkit (Manning et al., 2014) for Part-of-Speech (PoS) tagging3 and lemmatization. This step is performed in order to ensure that all systems use the same preprocessed data. 4. Finally, we developed a script to check that the final dataset conforms to the aforementioned guidelines. In this final verification we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro a"
E17-1010,S15-1007,0,0.0847564,"Missing"
E17-1010,K16-1006,0,0.13943,"rroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These approaches, which are often classified as semi-supervised, are targeted at overcoming the knowledge acquisition bottleneck of conventional supervised models (Pilehvar and Navigli, 2014). In fact, there is a line of research spec"
E17-1010,H94-1046,0,0.147967,"erable increase over the number of sense annotations of SemCor. However, SemCor is much more balanced in terms of unique senses covered (3,730 covered by OMSTI in contrast to over 33K covered by SemCor). Additionally, while OMSTI was constructed automatically, SemCor was manually built and, hence, its quality is expected to be higher. Finally, we calculated the ambiguity level of each dataset, computed as the total number of canSense-annotated training corpora We now describe the two WordNet senseannotated corpora used for training the supervised systems in our evaluation framework: • SemCor (Miller et al., 1994). SemCor4 is a manually sense-annotated corpus divided into 352 documents for a total of 226,040 sense annotations. It was originally tagged with senses from the WordNet 1.4 sense inventory. SemCor is, to our knowledge, the largest corpus manually annotated with WordNet senses, and is the main corpus used in the literature to train supervised WSD systems (Agirre et al., 2010b; Zhong and Ng, 2010). 5 In this paper we refer to the portion of sense-annotated data from the MultiUN corpus as OMSTI. Note that OMSTI was released along with SemCor. 6 Statistics included in Table 1: number of documents"
E17-1010,L16-1268,0,0.0631683,"Missing"
E17-1010,C12-1109,0,0.00980971,"oaches of this kind was Lesk (1986), which in its original version consisted of calculating the overlap between the context of the target word and its definitions as given by the sense inventory. Based on the same principle, various works have adapted the original algorithm by also taking into account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given"
E17-1010,S15-2049,1,0.750288,"2014) for Part-of-Speech (PoS) tagging3 and lemmatization. This step is performed in order to ensure that all systems use the same preprocessed data. 4. Finally, we developed a script to check that the final dataset conforms to the aforementioned guidelines. In this final verification we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro and Navigli, 2015) was developed for both WSD and Entity Linking and its format was especially designed for this latter task. Therefore, we decided to convert all datasets to a unified format. As unified format we use the XML scheme used for the SemEval-13 allwords WSD task (Navigli et al., 2013), where preprocessing information of a given corpus is also encoded. 4 Data In this section we summarize the WSD datasets used in the evaluation framework. To all these datasets we apply the standardization pipeline described in Section 3. First, we enumerate all the datasets used for the evaluation (Section 4.1). Secon"
E17-1010,P15-1173,0,0.0429977,"Missing"
E17-1010,Q14-1019,1,0.943174,"distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trai"
E17-1010,S13-1003,0,0.129919,"esentation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These"
E17-1010,W04-0811,0,0.878524,"Missing"
E17-1010,S07-1006,1,0.38032,"Missing"
E17-1010,K15-1037,0,0.625004,"wnstream NLP applications (de Lacalle and Agirre, 2015). In general the field does not have a clear path, partially owing to the fact that identifying real improvements over existing approaches becomes a hard task with current evaluation benchmarks. This is mainly due to the lack of a unified framework, which prevents direct and fair comparison among systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0,"
E17-1010,N15-1035,0,0.677673,"wnstream NLP applications (de Lacalle and Agirre, 2015). In general the field does not have a clear path, partially owing to the fact that identifying real improvements over existing approaches becomes a hard task with current evaluation benchmarks. This is mainly due to the lack of a unified framework, which prevents direct and fair comparison among systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0,"
E17-1010,S13-2040,1,0.749905,"ation we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro and Navigli, 2015) was developed for both WSD and Entity Linking and its format was especially designed for this latter task. Therefore, we decided to convert all datasets to a unified format. As unified format we use the XML scheme used for the SemEval-13 allwords WSD task (Navigli et al., 2013), where preprocessing information of a given corpus is also encoded. 4 Data In this section we summarize the WSD datasets used in the evaluation framework. To all these datasets we apply the standardization pipeline described in Section 3. First, we enumerate all the datasets used for the evaluation (Section 4.1). Second, we describe the sense-annotated corpora used for training (Section 4.2). Finally, we show some relevant statistics extracted from these resources (Section 4.3). 2. Once the dataset is converted to a unified format, we map the sense annotations from its original WordNet versio"
E17-1010,petrov-etal-2012-universal,0,0.0190664,"Missing"
E17-1010,P15-1058,0,0.0758502,"larity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora h"
E17-1010,J14-4005,1,0.844858,"architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These approaches, which are often classified as semi-supervised, are targeted at overcoming the knowledge acquisition bottleneck of conventional supervised models (Pilehvar and Navigli, 2014). In fact, there is a line of research specifically aimed at automatically obtaining large amounts of high-quality sense-annotated corpora 3 Standardization of WSD datasets In this section we explain our pipeline for transforming any given evaluation dataset or senseannotated corpus into a preprocessed unified for100 Figure 1: Pipeline for standardizing any given WSD dataset. mat. In our pipeline we do not make any distinction between evaluation datasets and senseannotated training corpora, as the pipeline can be applied equally to both types. For simplicity we will refer to both evaluation da"
E17-1010,P10-4014,0,0.240495,"over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled cor"
E17-1010,S07-1016,0,\N,Missing
E17-2036,D16-1057,0,0.0270607,"Missing"
E17-2036,P15-1010,1,0.829944,"utional) or not filtering training data by domains. hypernymy information is not encoded equally in different regions of distributional vector spaces, as it is stored differently depending on the domain. The hypernym discovery task consists of, given a term as input, finding its most appropriate hypernym. In this evaluation we followed the approach of Espinosa-Anke et al. (2016, TaxoEmbed), who provides a framework to train a domainwise transformation matrix (Mikolov et al., 2013) between the vector spaces of terms and hypernyms. As in the original work, we used the senselevel vector space of Iacobacci et al. (2015) and training data from Wikidata.18 We used the domain annotations of BabelDomains for clustering the training data by domain, and compared it with the domains obtained through the distributional step, as used in Espinosa-Anke et al. (2016). We additionally included a baseline which did not filter the training data by domain. The training data19 was composed of 20K term-hypernym pairs for the domain-filtered systems and 200K for the baseline, while the test data was composed of 250 randomly-extracted terms with their corresponding hypernyms in Wikidata. Table 4 shows the results of TaxoEmbed i"
E17-2036,W04-2214,0,0.648735,"a featured articles page. The BabelNet dataset is composed of 200 synsets randomly extracted from BabelNet 3.0 which were manually annotated with domains. As comparison systems we included a baseline based on Wikipedia (Wikipedia-idf). This baseline first constructs a tf-idf -weighted bag-ofword vector representation of Wikipedia pages and, similarly to our distributional approach, calculates its similarity with the concatenation of all Wikipedia pages associated with a domain in the Wikipedia featured articles page.15 We additionally compared with WN-Domains-3.2 (Magnini and Cavagli`a, 2000; Bentivogli et al., 2004), which is the latest released version of WordNet Domains16 . However, this approach involves manual curation, both in the selection of seeds and correction of errors. In order to enable a fair comparison, we report the results of a system based on its main automatic component. This baseline takes annotated synsets as input and propagates them through the WordNet taxonomy (WN-Taxonomy Prop.). Likewise, we report the results of the same baseline by propagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Eva"
E17-2036,P07-1034,0,0.125907,"ean Union inter-institutional terminology database. The domain labels of IATE are based on the Eurovoc thesaurus2 and were introduced manually. The fact that each of these approaches involves manual curation/intervention limits their extension to other resources, and therefore to downstream applications. Introduction Since the early days of Natural Language Processing (NLP) and Machine Learning, generalizing a given algorithm or technique has been extremely challenging. One of the main factors that has led to this issue in NLP has been the wide variety of domains for which data are available (Jiang and Zhai, 2007). Algorithms trained on the business domain are not to be expected to work well in biology, for example. Moreover, even if we manage to obtain a balanced training set across domains, our algorithm may not be as effective on some specific domain as if it had been trained on that same target domain. This issue has become even more challenging and significant with the rise of supervised learning techniques. These techniques are fed with large amounts of data and ought to be able generalize to various target domains. Several studies have proposed regularization frameworks for domain adaptation in"
E17-2036,D16-1095,0,0.112482,"Missing"
E17-2036,N15-1059,1,0.846306,"euristic is based on the BabelNet hypernymy structure, which is an integration of various taxonomies: WikiData, WordNet and MultiWiBi (Flati et al., 2016). The main intuition is that, in general, synsets connected by a hypernymy relation tend to share the same domain 4 https://en.wikipedia.org/wiki/ Wikipedia:Featured_articles 5 Biography domains are not considered. 6 For simplicity we refer to each domain with its first word (e.g., Geography to refer to Geography and Places). 7 http://lcl.uniroma1.it/nasari/ 8 Weighted Overlap has been proved to suit interpretable vectors better than cosine (Camacho-Collados et al., 2015). 9 This value was set through observation to increase precision but without drastically decreasing recall. 224 (Magnini and Cavagli`a, 2000).10 This taxonomybased heuristic is intended to both increase coverage and refine the quality of synsets annotated by the distributional approach. First, if all the hypernyms (at least two) of a given synset share the same top domain, this synset is annotated (or reannotated) with that domain. Second, if the top domain of an annotated synset is different from at least two of its hypernyms, this domain tag is removed. Distributional Taxonomy Labels Propaga"
E17-2036,magnini-cavaglia-2000-integrating,0,0.631538,"Missing"
E17-2036,P07-1033,0,0.105479,"Missing"
E17-2036,D16-1041,1,0.863988,"ropagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Evaluation One of the main applications of including domain information in sense inventories is to be able to cluster textual data by domain. Supervised systems may be particularly sensitive to this issue (Daum´e III, 2007), and therefore training data should be clustered accordingly. In particular, two recent studies found that clustering training data was essential for distributional hypernym discovery systems to perform accurately (Fu et al., 2014; Espinosa-Anke et al., 2016). They discovered that 15 For the annotation of WordNet we used the direct Wikipedia-WordNet mapping from BabelNet. 16 http://wndomains.fbk.eu/ 17 Defined as right granted by law or contract (especially a right to benefits). 226 BabelDomains Distributional Non-filtered Art 0.30 0.18 0.00 Bio 0.87 0.41 0.68 Edu 0.39 0.30 0.00 Geo 0.43 0.26 0.10 Hea 0.12 0.10 0.05 Med 0.71 0.46 0.25 Mus 0.42 0.43 0.11 Phy 0.20 0.08 0.00 Tra 0.63 0.56 0.34 War 0.13 0.11 0.00 Table 4: MRR (Mean Reciprocal Rank) performance of TaxoEmbed in the hypernym discovery task by filtering (BabelDomains and Distributional) o"
E17-2036,D12-1129,1,0.896578,"Missing"
E17-2036,P13-1132,1,0.8725,"each domain is first associated with a given vector. Then, the Wikipedia pages from the featured articles page are leveraged as follows. First, all Wikipedia pages associated with a given domain are concatenated into a single text. Second, a lexical vector is constructed for each text as in Camacho-Collados et al. (2016), by applying lexical specificity over the bag-of-word representation of the text. Finally, given a BabelNet synset s, the similarity between its respective NASARI lexical vector and the lexical vector of each domain is calculated using the Weighted Overlap comparison measure (Pilehvar et al., 2013).8 This enables us to obtain, for each BabelNet synset, scores for each domain label denoting their importance. For notational brevity, we will refer to the domain whose similarity score is highest across all domains as its top domain. For instance, the top domain of the BabelNet synset corresponding to rifle is Warfare, while its second domain is Engineering. In order to increase precision, initially we only tag those BabelNet synsets whose maximum score is higher than 0.35.9 Our goal is to enrich lexical resources with domain information. To this end, we rely on BabelNet 3.0, which merges bo"
E17-2036,P14-1113,0,0.217305,"ame baseline by propagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Evaluation One of the main applications of including domain information in sense inventories is to be able to cluster textual data by domain. Supervised systems may be particularly sensitive to this issue (Daum´e III, 2007), and therefore training data should be clustered accordingly. In particular, two recent studies found that clustering training data was essential for distributional hypernym discovery systems to perform accurately (Fu et al., 2014; Espinosa-Anke et al., 2016). They discovered that 15 For the annotation of WordNet we used the direct Wikipedia-WordNet mapping from BabelNet. 16 http://wndomains.fbk.eu/ 17 Defined as right granted by law or contract (especially a right to benefits). 226 BabelDomains Distributional Non-filtered Art 0.30 0.18 0.00 Bio 0.87 0.41 0.68 Edu 0.39 0.30 0.00 Geo 0.43 0.26 0.10 Hea 0.12 0.10 0.05 Med 0.71 0.46 0.25 Mus 0.42 0.43 0.11 Phy 0.20 0.08 0.00 Tra 0.63 0.56 0.34 War 0.13 0.11 0.00 Table 4: MRR (Mean Reciprocal Rank) performance of TaxoEmbed in the hypernym discovery task by filtering (Babel"
ehrmann-etal-2014-representing,C08-2017,0,\N,Missing
ehrmann-etal-2014-representing,van-assem-etal-2006-conversion,0,\N,Missing
ehrmann-etal-2014-representing,mccrae-etal-2012-collaborative,1,\N,Missing
ehrmann-etal-2014-representing,Q14-1019,1,\N,Missing
ehrmann-etal-2014-representing,E12-1059,0,\N,Missing
ehrmann-etal-2014-representing,P13-1133,0,\N,Missing
ehrmann-etal-2014-representing,P14-1044,1,\N,Missing
ehrmann-etal-2014-representing,S13-2040,1,\N,Missing
ehrmann-etal-2014-representing,P14-1089,1,\N,Missing
ehrmann-etal-2014-representing,P14-1122,1,\N,Missing
ehrmann-etal-2014-representing,francopoulo-etal-2006-lexical,0,\N,Missing
J04-2002,P99-1008,0,0.163946,"Missing"
J04-2002,P89-1010,0,0.242457,"he stand-alone procedure. 3.1 Phase 1: Terminology Extraction Terminology is the set of words or word strings that convey a single (possibly complex) meaning within a given community. In a sense, terminology is the surface appearance, in texts, of the domain knowledge of a community. Because of their low ambiguity and high specificity, these words are also particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology. Candidate terminological expressions are usually captured with more or less shallow techniques, ranging from stochastic methods (Church and Hanks 1989; Yamamoto and Church 2001) to more sophisticated syntactic approaches (Jacquemin 1997). 154 Navigli and Velardi 1 Learning Domain Ontologies terminology extraction candidate extraction domain corpus terminology filtering Natural Language Processor contrastive corpora 2 semantic interpretation semantic disambiguation Inductive learner WordNet identification of taxonomic relations Lexical Resources identification of conceptual relations 3 ontology integration and updating Domain Concept Forest Figure 3 The architecture of OntoLearn. Obviously, richer syntactic information positively influences"
J04-2002,J02-3001,0,0.0226129,"rland and Charniak (1999) propose a method for extracting whole-part relations from corpora and enrich an ontology with this information. Few papers propose methods of extensively enriching an ontology with domain terms. For example, Vossen (2001) uses statistical methods and string inclusion to create lexicalized trees, as we do (see Figure 4). However, no semantic disambiguation of terms is performed. Very often, in fact, ontology-learning papers regard domain terms as concepts. A statistical classifier for automatic identification of semantic roles between co-occuring terms is presented in Gildea and Jurafsky (2002). In order to tag texts with the appropriate semantic role, Gildea and Jurafsky use a training set of fifty thousand sentences manually annotated within the FrameNet semantic labeling project. Finally, in Maedche and Staab (2000, 2001), an architecture is presented to help ontology engineers in the difficult task of creating an ontology. The main contribution of this work is in the area of ontology engineering, although machine-learning methods are also proposed to automatically enrich the ontology with semantic relations. 6. Conclusions and Ongoing Developments We believe that the OntoLearn s"
J04-2002,magnini-cavaglia-2000-integrating,0,0.0141039,"of word senses is automatically built using a variety of knowledge source: 1. WordNet. In WordNet, in addition to synsets, the following information is provided: (a) (b) (c) (d) a textual sense definition (gloss); hyperonymy links (i.e., kind-of relations: for example, bus#1 is a kind of public transport#1); meronymy relations (i.e., part-of relations: for example, bus#1 has part roof#2 and window#2); other syntactic-semantic relations, as detailed later, not systematically provided throughout the lexical knowledge base. 2. Domain labels6 extracted by a semiautomatic methodology described in Magnini and Cavaglia (2000) for assigning domain information (e.g., tourism, zoology, sport) to WordNet synsets. 3. Annotated corpora providing examples of word sense usages in contexts: 6 Domain labels have been kindly made available by the IRST to our institution for research purposes. 159 Computational Linguistics (a) Volume 30, Number 2 SemCor7 is a corpus in which each word in a sentence is assigned a sense selected from the WordNet sense inventory for that word. Examples of a SemCor document are the following: Color#1 was delayed#1 until 1935, the widescreen#1 until the early#1 fifties#1. Movement#7 itself was#7 t"
J04-2002,W01-1005,1,0.570092,"Missing"
J04-2002,J01-1001,0,0.0563672,"e. 3.1 Phase 1: Terminology Extraction Terminology is the set of words or word strings that convey a single (possibly complex) meaning within a given community. In a sense, terminology is the surface appearance, in texts, of the domain knowledge of a community. Because of their low ambiguity and high specificity, these words are also particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology. Candidate terminological expressions are usually captured with more or less shallow techniques, ranging from stochastic methods (Church and Hanks 1989; Yamamoto and Church 2001) to more sophisticated syntactic approaches (Jacquemin 1997). 154 Navigli and Velardi 1 Learning Domain Ontologies terminology extraction candidate extraction domain corpus terminology filtering Natural Language Processor contrastive corpora 2 semantic interpretation semantic disambiguation Inductive learner WordNet identification of taxonomic relations Lexical Resources identification of conceptual relations 3 ontology integration and updating Domain Concept Forest Figure 3 The architecture of OntoLearn. Obviously, richer syntactic information positively influences the quality of the result t"
J04-2002,1993.mtsummit-1.10,0,0.0794304,"d in the task of identifying the key domain conceptualizations and describing them according to the organizational backbones established by the foundational ontology. The result of this effort is referred to as the core ontology (CO), which usually includes a few hundred application domain concepts. While many ontology projects eventually succeed in the task of defining a core ontology,1 populating the third level, which we call the specific domain ontology (SDO), is the actual barrier that very few projects have been able to overcome (e.g., WordNet [Fellbaum 1995], Cyc [Lenat 1993], and EDR [Yokoi 1993]), but they pay a price for this inability in terms of inconsistencies and limitations.2 It turns out that, although domain ontologies are recognized as crucial resources for the Semantic Web, in practice they are not available and when available, they are rarely used outside specific research environments. So which features are most needed to build usable ontologies? • Coverage: The domain concepts must be there; the SDO must be sufficiently (for the application purposes) populated. Tools are needed to extensively support the task of identifying the relevant concepts and the relations among"
J04-2002,alfonseca-manandhar-2002-improving,0,\N,Missing
J04-2002,J90-1003,0,\N,Missing
J04-2002,1995.mtsummit-1.17,0,\N,Missing
J06-2005,C96-1005,0,0.0452748,"nal Linguistics Volume 32, Number 2 2. Semantic Networks and Semantic Interconnection Patterns Semantic networks are a graphical notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. The availability of widecoverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically annotated corpora like SemCor (Miller et al. 1993), has certainly contributed to the exploration and exploitation of semantic graphs for several tasks like the analysis of lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi 2004), etc. Recently, a knowledge-based algorithm for word sense disambiguation called structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and Velardi 2004, 2005), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. Given a word context and a lexical knowledge base (LKB), obtained by integrating WordNet with annotated corpora and collocation resources (Navigli 2005), SSI selects a semantic graph in"
J06-2005,W02-0817,0,0.0561984,"Missing"
J06-2005,W04-0804,0,0.0193032,"careful crossing the street”), that a pedestrian crosses that part of the thoroughfare between the sidewalks. Though questionable, this is a subtlety made explicit in the dictionary and reinforced by the usage example of sense #2 above. The tool reflects this fact, showing that both senses are connected with other word senses in context, the first sense having a smaller degree of overall connectivity.2 As a second example, consider the WordNet definition of motorcycle: (b) Motorcycle: a motor vehicle with two wheels and a strong frame In the Gloss Word Sense Disambiguation task at Senseval-3 (Litkowski 2004), the human annotators assigned the first sense to the word frame (a structure supporting or containing something), unintentionally neglecting that the dictionary encodes a specific sense of frame concerning the structure of objects (e.g., vehicles, buildings, etc.). In fact, a chassis#3 is a kind of frame#6 (the internal supporting structure that gives an artifact its shape), and is also part of a motor vehicle#1. While regular polysemy holds between sense #1 and #6, there is no justification for the former choice, as it does not refer to vehicles at all (as reflected by the lack of semantic"
J06-2005,H93-1061,0,0.49846,"and Velardi 2005). ∗ Dipartimento di Informatica, Universit`a di Roma “La Sapienza,” Via Salaria, 113 - 00198 Roma, Italia. E-mail: navigli@di.uniroma1.it. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 2. Semantic Networks and Semantic Interconnection Patterns Semantic networks are a graphical notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. The availability of widecoverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically annotated corpora like SemCor (Miller et al. 1993), has certainly contributed to the exploration and exploitation of semantic graphs for several tasks like the analysis of lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi 2004), etc. Recently, a knowledge-based algorithm for word sense disambiguation called structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and Velardi 2004, 2005), has been shown to provide interesting insights into the choice of word senses by providing structural justificatio"
J06-2005,J91-1002,0,0.0415409,"ssociation for Computational Linguistics Computational Linguistics Volume 32, Number 2 2. Semantic Networks and Semantic Interconnection Patterns Semantic networks are a graphical notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. The availability of widecoverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically annotated corpora like SemCor (Miller et al. 1993), has certainly contributed to the exploration and exploitation of semantic graphs for several tasks like the analysis of lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi 2004), etc. Recently, a knowledge-based algorithm for word sense disambiguation called structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and Velardi 2004, 2005), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. Given a word context and a lexical knowledge base (LKB), obtained by integrating WordNet with annotated corpora and collocation resourc"
J06-2005,J04-2002,1,0.856411,"connection Patterns Semantic networks are a graphical notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. The availability of widecoverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically annotated corpora like SemCor (Miller et al. 1993), has certainly contributed to the exploration and exploitation of semantic graphs for several tasks like the analysis of lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi 2004), etc. Recently, a knowledge-based algorithm for word sense disambiguation called structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and Velardi 2004, 2005), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. Given a word context and a lexical knowledge base (LKB), obtained by integrating WordNet with annotated corpora and collocation resources (Navigli 2005), SSI selects a semantic graph including those word senses having a higher degree of interconnection, according"
J13-3007,P99-1008,0,0.0221969,"(on WordNet and MeSH sub-hierarchies). Section 5 is dedicated to concluding remarks. 2. Related Work Two main approaches are used to learn an ontology from text: rule-based and distributional approaches. Rule-based approaches use predefined rules or heuristic patterns to extract terms and relations. These approaches are typically based on lexico-syntactic patterns, first introduced by Hearst (1992). Instances of relations are harvested from text by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y). Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999; Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques (Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computational lexicons such as WordNet (Ponzetto and Navigli 2009). Distributional approache"
J13-3007,P13-1052,1,0.804884,"Missing"
J13-3007,W06-2609,0,0.0920079,"Missing"
J13-3007,D12-1129,1,0.834207,"Missing"
J13-3007,N12-1051,0,0.317925,"Missing"
J13-3007,J06-1005,0,0.0107522,"Missing"
J13-3007,C92-2082,0,0.631997,"nomyinduction algorithm in Section 3. In Section 4 we present our experiments, and discuss the results. Evaluation is both qualitative (on new A RTIFICIAL I NTELLIGENCE and F INANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Section 5 is dedicated to concluding remarks. 2. Related Work Two main approaches are used to learn an ontology from text: rule-based and distributional approaches. Rule-based approaches use predefined rules or heuristic patterns to extract terms and relations. These approaches are typically based on lexico-syntactic patterns, first introduced by Hearst (1992). Instances of relations are harvested from text by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y). Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999; Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques (Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collabo"
J13-3007,D10-1108,0,0.652252,"to existing ones in WordNet, with no evaluation of a full-fledged structured taxonomy and no restriction to a specific domain. A related, weakly supervised approach aimed at categorizing named entities, and attaching them to WordNet leaves, was proposed by Pasca (2004). Other approaches use formal concept analysis (Cimiano, Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn taxonomies from a folksonomy (Tang et al. 2009), and Markov logic networks and syntactic parsing applied to domain text (Poon and Domingos 2010). The work closest to ours is that presented by Kozareva and Hovy (2010). From an initial given set of root concepts and basic level terms, the authors first use Hearst-like lexico-syntactic patterns iteratively to harvest new terms from the Web. As a result a set of hyponym–hypernym relations is obtained. Next, in order to induce taxonomic relations between intermediate concepts, the Web is searched again with surface patterns. Finally, nodes from the resulting graph are removed if the out-degree is below a threshold, and edges are pruned by removing cycles and selecting the longest path in the case of multiple paths between concept pairs. Kozareva and Hovy’s met"
J13-3007,P08-1119,0,0.0180555,"Missing"
J13-3007,J04-2002,1,0.809207,"Navigli 2009). A quite recent challenge, referred to as ontology learning, consists of automatically or semi-automatically creating a lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al. 2011). As a result of ontology learning, the heavy requirements of manual ontology construction can be drastically reduced. In this paper we deal with the problem of learning a taxonomy (i.e., the backbone of an ontology) entirely from scratch. Very few systems in the literature address this task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this area. In OntoLearn taxonomy learning was accomplished in four steps: terminology extraction, derivation of term sub-trees via string inclusion, disambiguation of domain terms using a novel Word Sense Disambiguation algorithm, and combining the subtrees into a taxonomy. The use of a static, general-purpose repository of semantic knowledge, namely, WordNet (Miller et al. 1990; Fellbaum 1998), prevented the system from learning taxonomies in technical domains, however. In this paper we present OntoLearn Reloaded, a graph-based algorithm for learning"
J13-3007,W04-0844,1,0.79442,"Missing"
J13-3007,P10-1134,1,0.899033,"ed for the A RTIFICIAL I NTELLIGENCE and F INANCE domains. Seeing that we use high-level concepts, the set U can be considered domain-independent. Other choices are of course possible, especially if an upper ontology for a given domain is already available. For each term t ∈ T (i) (initially, i = 0), we first check whether t is an upper term (i.e., t ∈ U). If it is, we just skip it (because we do not aim at extending the taxonomy beyond an upper term). Otherwise, definition sentences are sought for t in the domain corpus and in a portion of the Web. To do so we use Word-Class Lattices (WCLs) (Navigli and Velardi 2010, introduced hereafter), which is a domain-independent machine-learned classifier that identifies definition sentences for the given term t, together with the corresponding hypernym (i.e., lexical generalization) in each sentence. For each term in our set T (i) , we then automatically extract definition candidates from the domain corpus, Web documents, and Web glossaries, by harvesting all the sentences that contain t. To obtain on-line glossaries we use a Web glossary extraction system (Velardi, Navigli, and D’Amadio 2008). Definitions can also be obtained via a lightweight bootstrapping proc"
J13-3007,J07-2002,0,0.0073904,"hiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computational lexicons such as WordNet (Ponzetto and Navigli 2009). Distributional approaches, instead, model ontology learning as a clustering or classification task, and draw primarily on the notions of distributional similarity (Pado and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such approaches are based on the assumption that paradigmatically-related concepts2 appear in similar contexts and their main advantage is that they are able to discover relations that do not explicitly appear in the text. They are typically less accurate, however, and the selection of feature types, notion of context, and similarity metrics vary considerably depending on the specific approach used. 1 http://lcl.uniroma1.it/ontolearn reloaded and ht"
J13-3007,P06-1015,0,0.0113948,"logy from text: rule-based and distributional approaches. Rule-based approaches use predefined rules or heuristic patterns to extract terms and relations. These approaches are typically based on lexico-syntactic patterns, first introduced by Hearst (1992). Instances of relations are harvested from text by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y). Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999; Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques (Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computational lexicons such as WordNet (Ponzetto and Navigli 2009). Distributional approaches, instead, model ontology learning as a clustering or classification task, and draw primarily on the notions of distributional similarity (Pado and Lap"
J13-3007,J11-2004,0,0.0241973,"Missing"
J13-3007,P10-1031,0,0.0211135,"manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computational lexicons such as WordNet (Ponzetto and Navigli 2009). Distributional approaches, instead, model ontology learning as a clustering or classification task, and draw primarily on the notions of distributional similarity (Pado and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such approaches are based on the assumption that paradigmatically-related concepts2 appear in similar contexts and their main advantage is that they are able to discover relations that do not explicitly appear in the text. They are typically less accurate, however, and the selection of feature types, notion of context, and similarity metrics vary considerably depending on the specific approach used. 1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org. 2 Because we are concerned with lexical taxonomies, in this paper w"
J13-3007,P06-1101,0,0.263873,"Missing"
J13-3007,storrer-wellinghoff-2006-automated,0,0.0367539,"In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF . [In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]R EST . [In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]R EST . [Myrtales]DF [are an order of]VF [ flowering plants]GF [placed as a basal group . . . ]R EST . 3.2.1 Word-Class Lattices. We now describe our WCL algorithm for the classification of definitional sentences and hypernym extraction. Our model is based on a formal notion of textual definition. Specifically, we assume a definition contains the following fields (Storrer and Wellinghoff 2006): r r r r The D EFINIENDUM field (DF): this part of the definition includes the definiendum (that is, the word being defined) and its modifiers (e.g., “In computer science, a pixel”); The D EFINITOR field (VF): which includes the verb phrase used to introduce the definition (e.g., “is”); The D EFINIENS field (GF): which includes the genus phrase (usually including the hypernym, e.g., “a dot”); The R EST field (RF): which includes additional clauses that further specify the differentia of the definiendum with respect to its genus (e.g., “that is part of a computer image”). To train our definiti"
J13-3007,W09-4410,0,0.0225445,"es additional clauses that further specify the differentia of the definiendum with respect to its genus (e.g., “that is part of a computer image”). To train our definition extraction algorithm, a data set of textual definitions was manually annotated with these fields, as shown in Table 4.8 Furthermore, the singleor multi-word expression denoting the hypernym was also tagged. In Table 4, for each sentence the definiendum and its hypernym are marked in bold and italics, respectively. Unlike other work in the literature dealing with definition extraction (Hovy et al. 2003; Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not only a variety of definition styles in our training set, in addition to the classic X is a Y pattern, but also a variety of domains. Therefore, our WCL algorithm requires no re-training when changing the application domain, as experimentally demonstrated by Navigli and Velardi (2010). Table 5 shows some non-trivial patterns for the VF field. 8 Available on-line at: http://lcl.uniroma1.it/wcl. 672 Velardi, Faralli, and Navigli OntoLearn Reloaded Table 5 Some nontrivial patterns for the VF field. is a term used to describe is the genus of is a term that re"
J13-3007,P09-1031,0,0.038379,"adigmatically-related concepts2 appear in similar contexts and their main advantage is that they are able to discover relations that do not explicitly appear in the text. They are typically less accurate, however, and the selection of feature types, notion of context, and similarity metrics vary considerably depending on the specific approach used. 1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org. 2 Because we are concerned with lexical taxonomies, in this paper we use the words concepts and terms interchangeably. 667 Computational Linguistics Volume 39, Number 3 Recently, Yang and Callan (2009) presented a semi-supervised taxonomy induction framework that integrates contextual, co-occurrence, and syntactic dependencies, lexico-syntactic patterns, and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. In their work, the authors assume that the set of ontological concepts C is known, therefore taxonomy learning is limited to finding relations between given pairs in C. In the experiments, they only use the word senses within a parti"
J13-3007,S10-1013,0,\N,Missing
J13-3008,W06-3814,0,0.0196041,"Missing"
J13-3008,W06-1669,0,0.306503,"Missing"
J13-3008,S07-1075,0,0.0179679,"ris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a large scale for many domains and languages, or based on large matrix ¨ computation methods such as context-group discrimination (Schutze 1998), non-negative matrix factorization (Van de Cruys and Apidianaki 2011) and Clustering by Committee (Lin and Pantel 2002). Instead, in our approach we aim to exploit the relational structure of word co-occurrences with lower requirements (i.e., using just a stopword list, a"
J13-3008,J10-4006,0,0.00690234,"0). Furthermore, the framework is independent of the target language, in that it just requires a large-enough corpus for co-occurrence extraction in that language and some basic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder). 20 http://www.cs.york.ac.uk/semeval-2013/task11/. 21 The data set is available at http://lcl.uniroma1.it/moresque/. 748 Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI As future work, the framework might be integrated with distributional semantics ´ and Pado´ 2010; Mitchell and models and techniques (Baroni and Lenci 2010; Erk, Pado, Lapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012, inter alia). Finally we note that, although in this article our framework was applied to polysemous queries only, nothing prevents it from being used to perform experiments at different levels of sense granularity. A qualitative evaluation of preliminary experiments in aspect identiﬁcation (cf. Section 2.6), which requires the detection of very ﬁne-grained subsenses of possibly monosemous queries, showed that WSI also seems to perform well in this task. Given the high number of monosemous queries"
J13-3008,W06-3812,0,0.466011,"les, and Diamonds (SquaT++), an algorithm that integrates two graph patterns previously exploited in the literature (Navigli and Crisafulli 2010), namely, squares and triangles, with a novel pattern called diamond. Balanced Maximum Spanning Tree Clustering (B-MST), an extension of a WSI algorithm based on the calculation of a Maximum Spanning Tree (Di Marco and Navigli 2011) that aims at balancing the number of co-occurrences in each sense cluster. HyperLex (V´eronis 2004), an algorithm based on the identiﬁcation of hubs (representing basic meanings) in co-occurrence graphs. Chinese Whispers (Biemann 2006), a randomized algorithm that partitions the graph vertices by iteratively transferring the mainstream message (i.e., word sense) to neighboring vertices. All of these graph algorithms for WSI consist of a common step, namely, cooccurrence graph construction (described in Section 3.2.1) and a second step, namely, the discovery of word senses, whose implementation depends on the speciﬁc algorithm adopted. We discuss the second phase of each algorithm separately (Section 3.2.2). 719 Computational Linguistics Volume 39, Number 3 Table 4 Example co-occurrences of word w = lion. word w animal vide"
J13-3008,J12-3005,0,0.0323339,"Missing"
J13-3008,E09-1013,0,0.196733,"{ get, fact, on, snow, leopard, snow leopard, endangered, species, endangered species, act, be, listed, as } { fact, endangered, species, endangered species, act, listed } Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI given word—used in a speciﬁc sense—tends to co-occur with the same neighboring words (Harris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a large scale for many domains and languages, or based on large matrix"
J13-3008,E06-1002,0,0.0145242,"ion of AMBIENT following guidelines provided by its authors.14 In fact, our aim was to study the behavior of Web search algorithms on queries of different lengths, ranging from one to four words. The AMBIENT data set, however, is composed in the main of one-word queries. MORESQUE provides dozens of queries of length 2, 3, and 4, together with the top 100 results from Yahoo! for each query annotated precisely as was done in the AMBIENT data set. We decided not to discontinue the use of Yahoo! mainly for homogeneity reasons. Wikipedia has already been used as a sense inventory by, among others, Bunescu and Pasca (2006), Mihalcea (2007), and Gabrilovich and Markovitch (2009). Santamar´ıa, Gonzalo, and Artiles (2010) have investigated in depth the beneﬁt of using Wikipedia as the sense inventory for diversifying search results, showing that Wikipedia offers much more sense coverage for search results than other resources such as WordNet. We report the statistics on the composition of the two data sets in Table 10. Given that the snippets could possibly be annotated with more than one Wikipedia subtopic, we also determined the average number of subtopics per snippet. This amounted to 1.01 for AMBIENT and 1.04"
J13-3008,J12-1002,0,0.00551296,"just requires a large-enough corpus for co-occurrence extraction in that language and some basic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder). 20 http://www.cs.york.ac.uk/semeval-2013/task11/. 21 The data set is available at http://lcl.uniroma1.it/moresque/. 748 Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI As future work, the framework might be integrated with distributional semantics ´ and Pado´ 2010; Mitchell and models and techniques (Baroni and Lenci 2010; Erk, Pado, Lapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012, inter alia). Finally we note that, although in this article our framework was applied to polysemous queries only, nothing prevents it from being used to perform experiments at different levels of sense granularity. A qualitative evaluation of preliminary experiments in aspect identiﬁcation (cf. Section 2.6), which requires the detection of very ﬁne-grained subsenses of possibly monosemous queries, showed that WSI also seems to perform well in this task. Given the high number of monosemous queries submitted to Web search engines, we believe that further investigation"
J13-3008,J10-4007,0,0.0210644,"Missing"
J13-3008,P09-3012,0,0.0197845,"ne does not “induce” the senses, but just classiﬁes (or labels) each snippet with the best-matching Wikipedia sense of the input query. 4.2 Experiment 1: Evaluation of the Clustering Quality 4.2.1 Evaluation Measures. In this ﬁrst experiment our goal is to evaluate the quality of the output produced by our search result clustering systems. Unfortunately, the clustering evaluation problem is a notably hard issue, and one for which there exists no unequivocal solution. Many evaluation measures have been proposed in the literature (Rand 1971; Zhao and Karypis 2004; Rosenberg and Hirschberg 2007; Geiss 2009, inter alia) so, in order to get exhaustive results, we tested three different clustering quality measures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which we introduce hereafter. Each of these measures M(C, G) calculates the quality of a clustering C, output for a given query q, against the gold standard clustering G for that query. We then determine the overall results on the entire set of queries Q in the test set according to the measure M by averaging the values of M(C, G) obtained for each single test query q ∈ Q. Adjusted Rand Index. Given a gold standard clustering G"
J13-3008,W99-0624,0,0.173562,"Missing"
J13-3008,S10-1079,0,0.119663,"oaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a large scale for many domains and languages, or based on large matrix ¨ computation methods such as context-group discrimination (Schutze 1998), non-negative matrix factorization (Van de Cruys and Apidianaki 2011) and Clustering by Committee (Lin and Pantel 2002). Instead, in our approach we aim to exploit the relational structure of word co-occurrences with lower requirements (i.e., using just a stopword list, a lemmatizer, and a compounder, cf."
J13-3008,P98-2127,0,0.545684,"es, act, esa, leopard, is, listed, as } { get, fact, on, snow, leopard, snow leopard, endangered, species, endangered species, act, be, listed, as } { fact, endangered, species, endangered species, act, listed } Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI given word—used in a speciﬁc sense—tends to co-occur with the same neighboring words (Harris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a large scale for m"
J13-3008,S10-1011,0,0.748025,"t a general evaluation framework for Web search result clustering, which we also exploit to perform a large-scale end-to-end experimental comparison of several graph-based WSI algorithms. In fact, the output of WSI (i.e., the automatically discovered senses) is evaluated in terms of both the quality of the corresponding search result clusters and the resulting ability to diversify search results. This is in contrast with most literature in the ﬁeld of Word Sense Induction, where experiments are mainly performed in vitro (i.e., not in the context of an everyday application; Matsuo et al. 2006; Manandhar et al. 2010). In order to test whether our results were strongly dependent on the evaluation measures we implemented in the framework, we complemented our extrinsic experimental evaluation with a qualitative analysis of the automatically induced senses. This study was performed via a manual evaluation carried out by several human annotators. We present novel versions of previously proposed WSI graph-based algorithms, namely, SquaT++ and Balanced Maximum Spanning Tree (B-MST) (the former is an enhancement of the original SquaT algorithm [Navigli and Crisafulli 2010], and the latter is a variant of MST [Di"
J13-3008,W98-0704,0,0.113998,"Missing"
J13-3008,W06-1664,0,0.0426115,"Missing"
J13-3008,N07-1025,0,0.0115114,"uidelines provided by its authors.14 In fact, our aim was to study the behavior of Web search algorithms on queries of different lengths, ranging from one to four words. The AMBIENT data set, however, is composed in the main of one-word queries. MORESQUE provides dozens of queries of length 2, 3, and 4, together with the top 100 results from Yahoo! for each query annotated precisely as was done in the AMBIENT data set. We decided not to discontinue the use of Yahoo! mainly for homogeneity reasons. Wikipedia has already been used as a sense inventory by, among others, Bunescu and Pasca (2006), Mihalcea (2007), and Gabrilovich and Markovitch (2009). Santamar´ıa, Gonzalo, and Artiles (2010) have investigated in depth the beneﬁt of using Wikipedia as the sense inventory for diversifying search results, showing that Wikipedia offers much more sense coverage for search results than other resources such as WordNet. We report the statistics on the composition of the two data sets in Table 10. Given that the snippets could possibly be annotated with more than one Wikipedia subtopic, we also determined the average number of subtopics per snippet. This amounted to 1.01 for AMBIENT and 1.04 for MORESQUE for"
J13-3008,D10-1012,1,0.934447,"f an everyday application; Matsuo et al. 2006; Manandhar et al. 2010). In order to test whether our results were strongly dependent on the evaluation measures we implemented in the framework, we complemented our extrinsic experimental evaluation with a qualitative analysis of the automatically induced senses. This study was performed via a manual evaluation carried out by several human annotators. We present novel versions of previously proposed WSI graph-based algorithms, namely, SquaT++ and Balanced Maximum Spanning Tree (B-MST) (the former is an enhancement of the original SquaT algorithm [Navigli and Crisafulli 2010], and the latter is a variant of MST [Di Marco and Navigli 2011] that produces more balanced clusters). We show how, thanks to our framework, WSI can be successfully integrated into real-world applications, such as Web search result 711 Computational Linguistics Volume 39, Number 3 Table 2 The top ﬁve categories returned by the Open Directory Project for the query snow leopard. ODP Category Science: Biology: Flora and Fauna: . . . Felidae: Uncia Kids and Teens: School Time: Science: . . . Leopards: Snow Leopards Science: Environment: Biodiversity: Conservation: Mammals: Felines Kids and Teens"
J13-3008,D07-1043,0,0.0955379,"ippets. Consequently the baseline does not “induce” the senses, but just classiﬁes (or labels) each snippet with the best-matching Wikipedia sense of the input query. 4.2 Experiment 1: Evaluation of the Clustering Quality 4.2.1 Evaluation Measures. In this ﬁrst experiment our goal is to evaluate the quality of the output produced by our search result clustering systems. Unfortunately, the clustering evaluation problem is a notably hard issue, and one for which there exists no unequivocal solution. Many evaluation measures have been proposed in the literature (Rand 1971; Zhao and Karypis 2004; Rosenberg and Hirschberg 2007; Geiss 2009, inter alia) so, in order to get exhaustive results, we tested three different clustering quality measures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which we introduce hereafter. Each of these measures M(C, G) calculates the quality of a clustering C, output for a given query q, against the gold standard clustering G for that query. We then determine the overall results on the entire set of queries Q in the test set according to the measure M by averaging the values of M(C, G) obtained for each single test query q ∈ Q. Adjusted Rand Index. Given a gold standard"
J13-3008,P10-1138,0,0.0229138,"Missing"
J13-3008,J98-1004,0,0.767139,"ts, on, snow, leopards, endangered, species, act, esa, leopard, is, listed, as } { get, fact, on, snow, leopard, snow leopard, endangered, species, endangered species, act, be, listed, as } { fact, endangered, species, endangered species, act, listed } Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI given word—used in a speciﬁc sense—tends to co-occur with the same neighboring words (Harris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are"
J13-3008,D12-1130,0,0.00488136,"a large-enough corpus for co-occurrence extraction in that language and some basic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder). 20 http://www.cs.york.ac.uk/semeval-2013/task11/. 21 The data set is available at http://lcl.uniroma1.it/moresque/. 748 Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI As future work, the framework might be integrated with distributional semantics ´ and Pado´ 2010; Mitchell and models and techniques (Baroni and Lenci 2010; Erk, Pado, Lapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012, inter alia). Finally we note that, although in this article our framework was applied to polysemous queries only, nothing prevents it from being used to perform experiments at different levels of sense granularity. A qualitative evaluation of preliminary experiments in aspect identiﬁcation (cf. Section 2.6), which requires the detection of very ﬁne-grained subsenses of possibly monosemous queries, showed that WSI also seems to perform well in this task. Given the high number of monosemous queries submitted to Web search engines, we believe that further investigation in this direction may wel"
J13-3008,J96-1001,0,0.110528,"Missing"
J13-3008,P11-1148,0,0.243323,"Missing"
J13-3008,C02-1114,0,0.247955,"t, endangered, species, endangered species, act, listed } Di Marco and Navigli Clustering and Diversifying Search Results with Graph-Based WSI given word—used in a speciﬁc sense—tends to co-occur with the same neighboring words (Harris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors ¨ (e.g., Schutze 1998) and word similarity (e.g., Lin 1998) to probabilistic frameworks (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002). In our work, we chose to focus on approaches based on co-occurrence graphs for two reasons: i) They have been shown to achieve state-of-the-art performance in standard evaluation tasks (Agirre et al. 2006b; Agirre and Soroa 2007; Korkontzelos and Manandhar 2010). ii) Other approaches are either based on syntactic dependency statistics (Lin 1998; Van de Cruys and Apidianaki 2011), which are hard to obtain on a large scale for many domains and languages, or based on large matrix ¨ computation methods such as context-group discrimination (Schutze 1998), non-negative matrix factorization (Van de"
J13-3008,H93-1052,0,0.669167,"odem pistol*stair*yacht*semantics potassium*razor*walrus*calendula monarchy*archery*google*locomotive*beach hyena*helium*soccer*ukulele*wife human*orchid*candela*colosseum*movie*guitar journey*harmonica*vine*mustache*rhino*police glossary*river*dad*kitchen*aikido*geranium*italy microbe*hug*ship*skull*beer*giraffe*mathematics 4.1.3 Tuning Set. Given that our graph construction step and our WSI algorithms have parameters, we created a data set to perform tuning. In order to ﬁx the parameter values independently of our application we created this data set by means of pseudowords ¨ (Schutze 1992; Yarowsky 1993). A pseudoword is an ambiguous artiﬁcial word created by concatenating two or more monosemous words. Each monosemous word represents a meaning of the pseudoword. For example, given the words pizza and blog we can create the pseudoword pizza*blog. The list of pseudowords we used is reported in Table 6. The powerful property of pseudowords is that they enable the automatic construction of sense-tagged corpora with virtually no effort. In fact, we automatically created our tuning data set as follows: 1. First, we collected the top 100 results retrieved by Yahoo! for each meaning (i.e., monosemous"
J13-3008,D10-1073,0,\N,Missing
J13-3008,J14-4005,1,\N,Missing
J13-3008,P10-1023,1,\N,Missing
J13-3008,P11-1120,0,\N,Missing
J13-3008,C98-2122,0,\N,Missing
J14-4005,N09-1003,0,0.040159,"Missing"
J14-4005,agirre-de-lacalle-2004-publicly,0,0.0376862,"Missing"
J14-4005,W04-3204,0,0.0823896,"Missing"
J14-4005,E09-1005,0,0.123671,"Our method can be considered an extension of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure. 845 Computational Linguistics Volume 40, Number 4 This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet. In order to measure semantic similarity we used the PPR (Haveliwala 2002) algorithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have been shown to require a widecoverage sense-tagged data set in order to"
J14-4005,P01-1005,0,0.140669,"Missing"
J14-4005,D08-1007,0,0.0258306,"Missing"
J14-4005,E06-1018,0,0.0430439,"kind semantically aware pseudowords, in that they aim at listing senses that are in specific relations to each other, thus mirroring the relations existing between the senses of real words in the lexicon. For example, the lack-insufficiency relation is encoded in the pseudoword for deficiency, which would not be possible if we generated a random pseudoword. Semantically aware pseudowords enable the generation of artificially annotated data sets that have similar properties to their real counterparts and this makes them particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006; Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated data set different senses of a word appear in distinct contexts. The extent of this distinction, however, depends on the semantic relatedness of the corresponding senses. The intuition behind semantically aware pseudowords is that they model each sense of an ambiguous word through a semantically similar monosemous representative that should appear naturally in contexts that are similar to those of its corresponding real sense. For this reason, these pseudowords should be expected to result in data sets w"
J14-4005,P10-1046,0,0.0468098,"Missing"
J14-4005,S07-1054,0,0.107006,"Missing"
J14-4005,W06-1663,0,0.0915187,"ith polysemy to a maximum of 35.9 at polysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise performance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees. B.2 Performance by Pseudosense Node Degree As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate disambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments"
J14-4005,S07-1015,0,0.221659,"Missing"
J14-4005,C08-1021,0,0.240614,"Cor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are ma"
J14-4005,E03-1071,0,0.0172557,"training data. Finally, in Section 6.6 we describe the evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such"
J14-4005,J13-3008,1,0.731833,"Missing"
J14-4005,P07-1028,0,0.0249181,"Missing"
J14-4005,J10-4007,0,0.027172,"Missing"
J14-4005,W00-1322,0,0.345091,"Missing"
J14-4005,D12-1129,1,0.78837,"hich the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest different distribution. To verify this, we studied the variability of sense distribution across texts belonging to different domains. We started from a data set of sense-annotated documents from 30 different domains provided by Faralli and Navigli (2012). We then estimated the average sense distribution of all nouns across documents, shown in Figure 3 (light columns) for polysemy degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average sense distribution across domains is not skewed, in contrast to the natural sense distribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set. Uni-Uni. This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in"
J14-4005,D07-1061,0,0.0182407,"Missing"
J14-4005,W11-2214,0,0.139974,"flating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, ¨ Church, and Yarowsky (1992a) and Schutze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unam¨ biguous words picked out to be in the same range of occurrence frequency (Schutze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an op"
J14-4005,P10-1155,0,0.0143506,"a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisition bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. T"
J14-4005,kilgarriff-rosenzweig-2000-english,0,0.0172803,"our experiments to cover a wide range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be th"
J14-4005,J98-1006,0,0.148303,"Missing"
J14-4005,W02-1006,0,0.034262,"representatives for the two mainstream WSD paradigms, that is, supervised and knowledge-based WSD. 6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and Ng 2010) as the representative supervised WSD system. IMS is a publicly available English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks.11 The system classifies words in context using linear support vector machines. The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (Lee and Ng 2002). For each of the four configurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB.12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB provides an implementa"
J14-4005,C00-1072,0,0.013541,"02 0.034 0.025 heroin drug hard cocaine user the list similarSynsets to select a pseudosense. However, the mode statistics in the table suggests that even when minFreq is set to a large value, most of the pseudosenses are picked out from the highest-ranking positions in the similarSynsets list. 3.3 Topic Signature-Based Pseudowords As an alternative means of finding suitable monosemous representatives for word senses with the PPR algorithm, we propose using automatically generated topic signatures. Topic signatures (TS) are weighted topical vectors that are associated with senses or concepts (Lin and Hovy 2000). The dimensions of these vectors are the words in the vocabulary and their weights determine the relatedness of each of these words to the target word sense. These vectors can be obtained automatically from large corpora or the Web with the help of monosemous relatives. In order to generate a TS-based pseudoword for a word w, we first sort the weighted vectors associated with the senses of w. Then, from each of these vectors, we select the monosemous word with highest relatedness (i.e., largest weight) which satisfies the minimum frequency constraint. The generation process of the TS-based ps"
J14-4005,W04-0804,0,0.0365061,"enses. In the following two sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemC"
J14-4005,P06-1058,0,0.147271,"ords in terms of disambiguation difficulty than random 3 http://www.nlm.nih.gov/mesh. 841 Computational Linguistics Volume 40, Number 4 pseudowords. However, this approach requires a specific hierarchical lexicon and falls short of creating many pseudowords with high polysemy. More recent work has focused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale e"
J14-4005,H94-1020,0,0.205432,"he evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such degrees given that very few pseudowords can be ge"
J14-4005,Q13-1013,0,0.0142671,"ments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, SemCor provi"
J14-4005,P04-1036,0,0.0101617,"range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest"
J14-4005,mihalcea-2002-bootstrapping,0,0.0561184,"f the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploit"
J14-4005,N07-1025,0,0.029214,"wledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a r"
J14-4005,W04-0807,0,0.0895033,"Missing"
J14-4005,H93-1061,0,0.69434,"ation for Computational Linguistics Computational Linguistics Volume 40, Number 4 difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins 1993). Unfortunately, providing knowledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzett"
J14-4005,moro-etal-2014-annotating,1,0.879132,"Missing"
J14-4005,Q14-1019,1,0.377703,"onary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amount"
J14-4005,D12-1128,1,0.768948,"t efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address t"
J14-4005,S13-2035,1,0.902507,"Missing"
J14-4005,otrusina-smrz-2010-new,0,0.194066,"cused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale evaluation framework for WSD, mainly because they suffer from coverage issues that prevent the creation of wide-coverage sense-annotated data sets. In this article we propose new pseudoword generation techniques that allow for the creation of thousands of artificial words having sufficient occurrence coverage"
J14-4005,P13-1132,1,0.4551,"Missing"
J14-4005,N13-1130,1,0.83194,"terms of disambiguation difficulty, representativeness, and distinguishability of the artificial senses. We leverage our semantically aware pseudowords to create, for the first time, a large-scale evaluation framework for WSD. Using this framework, we are able to perform an experimental comparison of state-of-the-art systems for supervised and knowledge-based WSD on a very large data set made up of millions of sense-tagged sentences. Our large-scale framework enables us to carry out an in-depth analysis of the factors and conditions that determine the systems’ performance. In our recent work (Pilehvar and Navigli 2013), we presented an approach for the generation of semantically aware pseudowords, called similarity-based pseudowords. At the core of this approach was the Personalized PageRank algorithm (Haveliwala 1 Although in our experiments we focus on nouns only, the same approach can potentially be used for any other open-class part of speech. 838 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2002) on the WordNet graph, which was utilized to find the most semantically similar monosemous representative for a given sense of a real ambiguous word. The main strength of the"
J14-4005,P10-1154,1,0.777793,". 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004;"
J14-4005,S07-1016,0,0.0431579,"Missing"
J14-4005,S13-1003,0,0.221754,"Missing"
J14-4005,D08-1027,0,0.120403,"Missing"
J14-4005,W04-0811,0,0.453984,". We then illustrate how we leverage our pseudowords to generate large sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based WSD is described in Section 6. Experimental results as well as the findings are presented and discussed in Section 7. Finally, we provide concluding remarks in Section 8. 2. Related Work 2.1 Supervised WSD and the Knowledge Acquisition Bottleneck Over the last few decades, WSD systems have been suffering from disappointingly low performance, especially in an all-words setting in which one has to cover the entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a). In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bo"
J14-4005,P14-1122,1,0.706979,"s in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with differ"
J14-4005,W13-0215,0,0.0273342,"Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2"
J14-4005,H05-1069,0,0.0328538,"eacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zho"
J14-4005,H93-1052,0,0.20931,"Missing"
J14-4005,P95-1026,0,0.298171,"In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have al"
J14-4005,P10-4014,0,0.844732,"dably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the sys"
J14-4005,N03-2023,0,\N,Missing
J14-4005,P07-1007,0,\N,Missing
J14-4005,J14-1003,0,\N,Missing
J14-4005,P10-2013,0,\N,Missing
K17-1012,N16-1163,0,0.0669838,"edge-Enhanced Training Massimiliano Mancini*, Jose Camacho-Collados*, Ignacio Iacobacci and Roberto Navigli Department of Computer Science Sapienza University of Rome mancini@dis.uniroma1.it {collados,iacobacci,navigli}@di.uniroma1.it Abstract Previous works have addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conf"
K17-1012,N09-1003,0,0.141055,"Missing"
K17-1012,K16-1026,0,0.0187522,"d Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings. These two approaches require multiple steps of training and make use of a relatively small resource like WordNet, which limits their coverage and applicability. Camacho-Collados et al. (2016) increased the coverage of these WordNetbased approaches by exploiting the complementary knowledge of WordNet and Wikipedia along with pre-trained word embeddings. Finally, Wang et al. (2014) and Fang et al. (2016) proposed a model to align vector spaces of words and entities from knowledge bases. However, these approaches are restricted to nominal instances only (i.e. Wikipedia pages or entities). In contrast, we propose a model which learns both words and sense embeddings from a single joint training phase, producing a common vector 3 Connecting words and senses in context In order to jointly produce embeddings for words and senses, SW2V needs as input a corpus where words are connected to senses1 in each given context. One option for obtaining such connections could be to take a sense-annotated corpu"
K17-1012,N15-1184,0,0.0592797,"Missing"
K17-1012,P16-1143,0,0.0260088,"Missing"
K17-1012,P16-1191,0,0.176883,"scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy between word pairs: clear antonyms, weak antonyms or neither. 16 For instance, the pairs sunset-sunrise and day-night are given, respectively, 1.88 and 2.47 gold scores in the 0-10 scale, while our model gives them a higher similarity score. In fact, both pairs appear as coordinate synsets in WordNet. 14 https://github.com/mfaruqui/ retrofitting 106 SW2V SensEmbed NAS"
K17-1012,D14-1067,0,0.0108602,"tively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature. Embedding words from large corpora into a lo"
K17-1012,J06-1003,0,0.0428959,"tity linking system based on BabelNet. We compare to both the default Babelfy system which • Benchmark. Word similarity has been one of the most popular benchmarks for in-vitro evaluation of vector space models (Pennington et al., 2014; Levy et al., 2015). For the analysis we use two word similarity datasets: the similarity portion (Agirre et al., 2009, WS-Sim) of the WordSim-353 dataset (Finkelstein et al., 2002) and RG-65 (Rubenstein and Goodenough, 1965). In order to compute the similarity of two words using our sense embeddings, we apply the standard closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006; Camacho-Collados 7 In this analysis we used the word similarity task for optimizing the sense embeddings, without caring about the performance of word embeddings or their interconnectivity. Therefore, this configuration may not be optimal for word embeddings and may be further tuned on specific applications. More information about different configurations in the documentation of the source code. 8 http://babelfy.org 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://babelnet.org 104 Input Words Senses Both Words WS-Sim RG-65 r ρ r ρ 0.49 0.48 0."
K17-1012,C14-1048,0,0.0262793,"er via Joint Knowledge-Enhanced Training Massimiliano Mancini*, Jose Camacho-Collados*, Ignacio Iacobacci and Roberto Navigli Department of Computer Science Sapienza University of Rome mancini@dis.uniroma1.it {collados,iacobacci,navigli}@di.uniroma1.it Abstract Previous works have addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without s"
K17-1012,P15-1072,1,0.709399,"Missing"
K17-1012,S13-1005,0,0.0204774,"ith this input setting. For instance, in the case of including both words and senses in the input layer, the co-occurrence information learned by the network would be duplicated for both words and senses. • Training model and hyperparameters. For evaluation purposes, we use the CBOW model of word2vec with standard hyperparameters: the dimensionality of the vectors is set to 300 and the window size to 8, and hierarchical softmax is used for normalization. These hyperparameter values are set across all experiments. • Corpus and semantic network. We use a 300M-words corpus from the UMBC project (Han et al., 2013), which contains English paragraphs extracted from the web.5 As semantic network we use BabelNet 3.06 , a large multilingual semantic network with over 350 million semantic connections, integrating resources such as Wikipedia and WordNet. We chose BabelNet owing to its wide coverage of named entities and lexicographic knowledge. 5.2 Disambiguation / Shallow word-sense connectivity algorithm In this section we evaluate the impact of our shallow word-sense connectivity algorithm (Section 3) by testing our model directly taking a predisambiguated text as input. In this case the network exploits t"
K17-1012,J15-4004,0,0.0127603,"vity algorithm achieves the best overall results. We believe that these results are due to the semantic connectivity ensured by our algorithm and to the possibility of associating words with more than one sense, which seems beneficial for training, making it more robust to possible disambiguation errors and to the sense granularity issue (Erk et al., 2013). The results are especially significant considering that our algorithm took a tenth of the time needed by Babelfy to process the corpus. 6 6.1 Word Similarity In this section we evaluate our sense representations on the standard SimLex-999 (Hill et al., 2015) and MEN (Bruni et al., 2014) word similarity datasets13 . SimLex and MEN contain 999 and 3000 word pairs, respectively, which constitute, to our knowledge, the two largest similar9 http://pan.baidu.com/s/1eQcPK8i We used the AutoExtend code (http://cistern. cis.lmu.de/˜sascha/AutoExtend/) to obtain sense vectors using W2V embeddings trained on UMBC (GoogleNews corpus used in their pre-trained models is not publicly available). We also tried the code to include BabelNet as lexical resource, but it was not easily scalable (BabelNet is two orders of magnitude larger than WordNet). 11 http://lcl."
K17-1012,D14-1110,0,0.350098,"Missing"
K17-1012,R13-1022,0,0.0519165,"nnotations (Bennett et al., 2016). Given an input word w, we compute the cosine similarity between w and all its candidate senses, picking the sense leading to the highest similarity: Table 4: Accuracy and F-Measure percentages of different systems on the SemEval Wikipedia sense clustering dataset. parison systems that use the Wikipedia corpus for training, in this experiment we report the results of our model trained on the Wikipedia corpus and using BabelNet as lexical resource only. For the evaluation we consider the two Wikipedia sense clustering datasets (500-pair and SemEval) created by Dandala et al. (2013). In these datasets sense clustering is viewed as a binary classification task in which, given a pair of Wikipedia pages, the system has to decide whether to cluster them into a single instance or not. To this end, we use our synset embeddings and cluster Wikipedia pages17 together if their similarity exceeds a threshold γ. In order to set the optimal value of γ, we follow Dandala et al. (2013) and use the first 500-pairs sense clustering dataset for tuning. We set the threshold γ to 0.35, which is the value leading to the highest F-Measure among all values from 0 to 1 with a 0.05 step size on"
K17-1012,J13-3008,1,0.913407,"Missing"
K17-1012,P12-1092,0,0.14184,"Missing"
K17-1012,J13-3003,0,0.0355605,"ed). We will refer to this latter version as Babelfy* and report the best configuration of each strategy according to our analysis. Table 2 shows the results of our model using the three different strategies on RG-65 and WSSim. Our shallow word-sense connectivity algorithm achieves the best overall results. We believe that these results are due to the semantic connectivity ensured by our algorithm and to the possibility of associating words with more than one sense, which seems beneficial for training, making it more robust to possible disambiguation errors and to the sense granularity issue (Erk et al., 2013). The results are especially significant considering that our algorithm took a tenth of the time needed by Babelfy to process the corpus. 6 6.1 Word Similarity In this section we evaluate our sense representations on the standard SimLex-999 (Hill et al., 2015) and MEN (Bruni et al., 2014) word similarity datasets13 . SimLex and MEN contain 999 and 3000 word pairs, respectively, which constitute, to our knowledge, the two largest similar9 http://pan.baidu.com/s/1eQcPK8i We used the AutoExtend code (http://cistern. cis.lmu.de/˜sascha/AutoExtend/) to obtain sense vectors using W2V embeddings trai"
K17-1012,P15-1010,1,0.913194,"pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature. Embedding words from large corpora into a lowdimensional vector space has been a popular task since the appearance of the probabilistic feedforward neural network language model (Bengio et al., 2003) and later developments such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, little research has focused on exploiting lexical resources to overcome the inherent ambiguity of word embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literat"
K17-1012,S13-2040,1,0.374752,"Missing"
K17-1012,P16-1085,1,0.0662351,"ally annotating large amounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network are taken as input and a"
K17-1012,N15-1070,0,0.0173292,"7) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpora. Word embeddings a"
K17-1012,D14-1113,0,0.154295,"Missing"
K17-1012,N09-2059,0,0.07871,"Missing"
K17-1012,P16-2074,0,0.0208263,"s, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy between word pairs: cle"
K17-1012,N15-1164,0,0.02838,"ble (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpo"
K17-1012,Q15-1016,0,0.0328712,"e the impact of our shallow word-sense connectivity algorithm (Section 3) by testing our model directly taking a predisambiguated text as input. In this case the network exploits the connections between each word and its disambiguated sense in context. For this comparison we used Babelfy8 (Moro et al., 2014), a state-of-the-art graph-based disambiguation and entity linking system based on BabelNet. We compare to both the default Babelfy system which • Benchmark. Word similarity has been one of the most popular benchmarks for in-vitro evaluation of vector space models (Pennington et al., 2014; Levy et al., 2015). For the analysis we use two word similarity datasets: the similarity portion (Agirre et al., 2009, WS-Sim) of the WordSim-353 dataset (Finkelstein et al., 2002) and RG-65 (Rubenstein and Goodenough, 1965). In order to compute the similarity of two words using our sense embeddings, we apply the standard closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006; Camacho-Collados 7 In this analysis we used the word similarity task for optimizing the sense embeddings, without caring about the performance of word embeddings or their interconnectivity. Therefore, this configuration may not"
K17-1012,D15-1200,0,0.062831,"Missing"
K17-1012,K16-1006,0,0.0149123,"ounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network are taken as input and a set of connected words"
K17-1012,E17-1009,0,0.012973,"addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Ja"
K17-1012,D14-1162,0,0.120226,"ense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017"
K17-1012,P15-2004,0,0.024573,"n SimLex-999 were produced on antonym pairs, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided th"
K17-1012,P17-1170,1,0.520897,"rd embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literature, various different methods have attempted to overcome this limitation. Chen et al. (2014) proposed a model for obtaining both word and sense representations based on a first training step of conventional word embeddings, a second disambiguation step based on sense definitions, and a final training phase which uses the disambiguated text as input. Likewise, Rothe and Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings."
K17-1012,H93-1061,0,0.176924,"Missing"
K17-1012,D16-1174,0,0.18825,"ich limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpora. Word embeddings are widely used in Natural Language Processing, mainly"
K17-1012,Q14-1019,1,0.959855,"lated work space of words and senses as an emerging feature. Embedding words from large corpora into a lowdimensional vector space has been a popular task since the appearance of the probabilistic feedforward neural network language model (Bengio et al., 2003) and later developments such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, little research has focused on exploiting lexical resources to overcome the inherent ambiguity of word embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literature, various different methods have attempted to overcome this limitation. Chen et al. (2014) proposed a model"
K17-1012,Q17-1022,0,0.00690929,"Missing"
K17-1012,D16-1018,0,0.062108,"Missing"
K17-1012,P10-4014,0,0.0871862,"input. However, manually annotating large amounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network"
K17-1012,E17-1010,1,0.0499564,"Missing"
K17-1012,D13-1141,0,0.040185,"rd and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of"
K17-1012,N10-1013,0,0.034831,"Missing"
K17-1012,P15-1173,0,0.0603118,"Missing"
K17-1012,J98-1004,0,0.301178,"Missing"
K17-1012,K15-1026,0,0.0122149,"roduced on antonym pairs, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy be"
K17-1012,S17-2008,0,0.0253672,"Missing"
K17-1012,N16-1160,0,0.0412252,"Missing"
K17-1012,C14-1016,0,0.0936118,"Missing"
K17-1012,N16-1151,0,0.042999,"Missing"
K17-1012,D14-1167,0,0.0271697,"put. Likewise, Rothe and Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings. These two approaches require multiple steps of training and make use of a relatively small resource like WordNet, which limits their coverage and applicability. Camacho-Collados et al. (2016) increased the coverage of these WordNetbased approaches by exploiting the complementary knowledge of WordNet and Wikipedia along with pre-trained word embeddings. Finally, Wang et al. (2014) and Fang et al. (2016) proposed a model to align vector spaces of words and entities from knowledge bases. However, these approaches are restricted to nominal instances only (i.e. Wikipedia pages or entities). In contrast, we propose a model which learns both words and sense embeddings from a single joint training phase, producing a common vector 3 Connecting words and senses in context In order to jointly produce embeddings for words and senses, SW2V needs as input a corpus where words are connected to senses1 in each given context. One option for obtaining such connections could be to take"
K17-1012,P15-1032,0,0.0169518,"he main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature"
K17-1012,P14-2089,0,0.0151258,"14; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge"
K17-1012,S07-1016,0,\N,Missing
L16-1269,E09-1005,0,0.603306,"ed corpus into their pipeline. Keywords: Word Sense Disambiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikida"
L16-1269,P13-1052,1,0.897374,"Missing"
L16-1269,N15-1059,1,0.834875,"Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a"
L16-1269,P15-1072,1,0.875453,"Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a"
L16-1269,D14-1110,0,0.0630207,"mbiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: i"
L16-1269,P15-2003,0,0.0581894,"Missing"
L16-1269,R13-1022,0,0.123122,"ularly suitable for enriching a semantic network. The rest of the default NASARI lexical pipeline for obtaining semantic representations (lexical specificity applied to the contextual information) remains unchanged. By integrating the high-precision disambiguated glosses into the NASARI pipeline, we obtain a new set of vector representations for BabelNet synsets, increasing its initial coverage (4.4M synsets covered by the default NASARI compared to 4.6M synsets covered by NASARI enriched with our disambiguated glosses). Experimental setup. We used the two sense clustering datasets created by Dandala et al. (2013). The task in these datasets consists of, given a pair of Wikipedia articles, to decide whether they should be merged into a single cluster or not. The first dataset (500-pair henceforth) contains 500 pairs of Wikipedia articles, while the second dataset (SemEval) consists of 925 pairs coming from a set of highly ambiguous words taken from disambiguation tasks of SemEval workshops. We follow the original setting of (Camacho-Collados et al., 2015a) and only cluster a pair of Wikipedia articles if their similarity, calculated by using the square-rooted Weighted Overlap comparison measure (Pilehv"
L16-1269,Q15-1038,1,0.850776,"om dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 . In any case, an accurate semantic analysis of a definition corpus is made difficult by the short and con"
L16-1269,fernandez-ordonez-etal-2012-unsupervised,0,0.0218343,"eline. Keywords: Word Sense Disambiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include"
L16-1269,P14-1089,1,0.854594,"on In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 ."
L16-1269,Q16-1002,0,0.0219232,"Missing"
L16-1269,W04-0804,0,0.113499,"Missing"
L16-1269,H93-1061,0,0.880223,"Missing"
L16-1269,Q14-1019,1,0.891396,"uld have to deal with the added difficulty of selecting context-appropriate synsets from an extremely large sense inventory. In fact, WordNet 3.0 comprises 117 659 synsets and a definition for each synset, while BabelNet 3.0 covers 13 801 844 synsets with a total of 40 328 194 definitions. Instead, in this paper we propose an automatic disambiguation approach which leverages multilinguality and cross-resource information along with a state-of-the-art 3 http://www.hlt.utdallas.edu/˜xwn/ http://wordnet.princeton.edu/glosstag. shtml 4 multilingual Word Sense Disambiguation/Entity Linking system (Moro et al., 2014) and a vector-based semantic representation of concepts and entities (Camacho-Collados et al., 2015a). By exploiting these features, we are able to produce a large-scale high-quality corpus of glosses, automatically disambiguated with BabelNet synsets5 . 3. Methodology The gist of our approach lies in the combination of different languages and resources for high-quality disambiguation. In fact, since many definitions are short and concise, the lack of meaningful context would negatively affect the performance of a Word Sense Disambiguation/Entity Linking system targeted at individual definitio"
L16-1269,P10-1134,1,0.865078,"Missing"
L16-1269,P13-1132,1,0.851297,"Missing"
L16-1269,P98-2180,0,0.257523,"e, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 . In any case, an accurate semantic analysis of a definition corpus is made diff"
L16-1269,N03-1033,0,0.0203355,"the combination of different languages and resources for high-quality disambiguation. In fact, since many definitions are short and concise, the lack of meaningful context would negatively affect the performance of a Word Sense Disambiguation/Entity Linking system targeted at individual definitions. To improve the data quality before the disambiguation step, we tokenize and Part-of-Speech (PoS) tag the definitions for a subset of languages: Tokenization. We use the tokenization system available from the polyglot project6 for 165 languages. Part-of-Speech tagging. We train the Stanford tagger (Toutanova et al., 2003), for 30 languages using the available data from the Universal Dependencies project7 (Nivre, 2015). Our disambiguation strategy is based on two steps: (1) all definitions are gathered together, grouped by definiendum and disambiguated using a multilingual disambiguation system (Section 3.1.); (2) the disambiguation output is then refined using semantic similarity (Section 3.2.). 3.1. Context-rich Disambiguation As an example, consider the following definition of castling in chess as provided by WordNet: Interchanging the positions of the king and a rook. (1) The context in (1) is limited and i"
L16-1269,J13-3007,1,0.846113,"l corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definitio"
L16-1269,C98-2175,0,\N,Missing
L18-1268,E09-1005,0,0.273858,"Missing"
L18-1268,L16-1269,1,0.883694,"Missing"
L18-1268,P17-2094,1,0.755113,"ry high thanks to the effort of specialized annotators, it is far from covering the whole English vocabulary of words and senses. Moreover, such manual resources need extra effort to be maintained and updated to integrate new senses and words appearing in everyday language. Thus, in order to overcome these issues, semi-automatic or fully automatic approaches have been proposed over the past years. Taghipour and Ng (2015) exploit a parallel corpus and the manual translations of senses to annotate the words in the corpus with senses. Similarly, but without the need for human intervention, Delli Bovi et al. (2017) and CamachoCollados et al. (2016), rely on aligned sentences in order to create a richer context that can be beneficial to their disambiguation. Raganato et al. (2016), instead, designed a set of heuristics which exploit the human effort of the Wikipedia community in order to propagate and add sense annotations to the Wikipedia pages. Similarly Pasini and Navigli (2017) exploit a knowledge base in order to annotate sentences with sense tags and uses a measure of confidence in order to select the most correct annotated sentences. They show that, relying on a multilingual semantic network as th"
L18-1268,S01-1001,0,0.666499,"mEval-13 and SemEval-15. Dataset Senseval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL Train-o-Matic 70.5 67.4 59.8 65.5 68.6 67.3 OMSTI 74.1 67.2 62.3 62.8 63.1 66.4 SemCor 76.8 73.8 67.3 65.5 66.1 70.4 MFS 72.1 72.0 65.4 63.0 66.3 67.6 Table 4: F1 of IMS trained on Train-o-Matic, OMSTI and SemCor, and MFS for the Senseval-2, Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets. The evaluation has been performed using the unified evaluation framework for Word Sense Disambiguation made available by Raganato et al. (2017), thus considering the following WSD shared tasks: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Navigli et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). We set the two Train-o-Matic parameters K to 500 and z to 2.0 experimentally, testing the models learned by IMS on a small in-house development set2 and choosing the one with the highest performance. Multilingual setup: For the other languages we tuned the two paramenter K and z in the same way we did for English. The corpora proved to be more effective with K set to 100, for all the languages, and z ranging in [2.0, 3.0]. To prove that the"
L18-1268,S15-2049,1,0.878792,"62.8 63.1 66.4 SemCor 76.8 73.8 67.3 65.5 66.1 70.4 MFS 72.1 72.0 65.4 63.0 66.3 67.6 Table 4: F1 of IMS trained on Train-o-Matic, OMSTI and SemCor, and MFS for the Senseval-2, Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets. The evaluation has been performed using the unified evaluation framework for Word Sense Disambiguation made available by Raganato et al. (2017), thus considering the following WSD shared tasks: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Navigli et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). We set the two Train-o-Matic parameters K to 500 and z to 2.0 experimentally, testing the models learned by IMS on a small in-house development set2 and choosing the one with the highest performance. Multilingual setup: For the other languages we tuned the two paramenter K and z in the same way we did for English. The corpora proved to be more effective with K set to 100, for all the languages, and z ranging in [2.0, 3.0]. To prove that the generated data in the other languages are also high quality we also report the performance of IMS when trained on Train-o-Matic corpora for Italian and S"
L18-1268,Q14-1019,1,0.916631,"st for English, a wide range of other issues still remain open. In fact, since supervised WSD systems need to be trained on a wordby-word basis, creating effective datasets requires a huge effort, which is beyond reach even for resource-rich languages like English. Clearly, this issue is even more severe for systems that need both lexicographic and encyclopedic knowledge (Schubert, 2006) and/or need to work in a multilingual or domain-specific setting. Knowledge-based WSD, on the other hand, exploits the knowledge contained in resources like WordNet to build algorithms (e.g. densest subgraph (Moro et al., 2014) or personalized page rank (Agirre and Soroa, 2009)) that can choose the sense of a word in context, thus not requiring training data but usually adopting bag-of-words approaches that neglect the lexical and syntactic context of the word (information that is more easily exploited by supervised systems), which may be essential in some scenarios. Furthermore, performances of both types of systems are highly affected by distribution of word senses that are usually different for each domain of application (Pasini and Navigli, 2018). In order to address these issues different solutions have been pr"
L18-1268,P10-1023,1,0.82266,"elect the most correct annotated sentences. They show that, relying on a multilingual semantic network as the underlying knowledge base, they are able to create high-quality sense-tagged corpora for any languages supported by the semantic network. Our work builds upon that of Pasini and Navigli (2017) in order to generate sense-tagged corpora for 5 major European languages (English, French, German, Spanish and Italian) and the most spoken language of Asia (Chinese) and paves the way for supervised Word Sense Disambiguation in multiple languages. Exploiting the knowledge contained in BabelNet (Navigli and Ponzetto, 2010; Navigli and Ponzetto, 2012) – a huge and multilingual semantic network containing both lexicographic and encyclopedic knowledge – and Wikipedia, we generated large corpora annotated with BabelNet senses for the 6 languages listed above. Experiments and statistics prove that these automatically created corpora are rich in terms of number of different lemmas annotated with a sense and number of sentences, and as such they can be a valuable resource for supervised WSD systems: in fact, systems trained on our datasets perform better or comparably to the state of the art across different language"
L18-1268,S07-1006,1,0.822935,"Missing"
L18-1268,S13-2040,1,0.945339,"Missing"
L18-1268,L16-1483,0,0.138839,"cross other languages: this is both because, for English, we set the value of the parameter K (see Section 2.1.) to 500 instead of 100, and because BabelNet, on average, contains more English senses compared to other languages. As can be seen, each language has an average of 75 different sentences for each sense in the corpus, with English having the highest number of sentences per sense. Note that the total number of distinct senses covered is not equal to the sum of distinct senses for each sense due to the fact that we use a language-independent sense inventory (i.e. BabelNet) similarly to Otegi et al. (2016) and Delli Bovi et al. (2017). Thus many senses are shared across languages. The average confidence score measures how confident the system was on average when annotating the given language, meaning that the resulting data is most likely better: this score depends on both the average ambiguity of each lemma and on the quality of the relations in BabelNet. As expected, the system confidence score is highest in languages that have the lowest polisemy, i.e. English and German, which have the lowest average number of senses for nouns. As regards the average number of sentences for each sense, it d"
L18-1268,D17-1008,1,0.857185,"been proposed over the past years. Taghipour and Ng (2015) exploit a parallel corpus and the manual translations of senses to annotate the words in the corpus with senses. Similarly, but without the need for human intervention, Delli Bovi et al. (2017) and CamachoCollados et al. (2016), rely on aligned sentences in order to create a richer context that can be beneficial to their disambiguation. Raganato et al. (2016), instead, designed a set of heuristics which exploit the human effort of the Wikipedia community in order to propagate and add sense annotations to the Wikipedia pages. Similarly Pasini and Navigli (2017) exploit a knowledge base in order to annotate sentences with sense tags and uses a measure of confidence in order to select the most correct annotated sentences. They show that, relying on a multilingual semantic network as the underlying knowledge base, they are able to create high-quality sense-tagged corpora for any languages supported by the semantic network. Our work builds upon that of Pasini and Navigli (2017) in order to generate sense-tagged corpora for 5 major European languages (English, French, German, Spanish and Italian) and the most spoken language of Asia (Chinese) and paves t"
L18-1268,E17-1010,1,0.925445,"call and F1 of IMS trained on Train-o-Matic, against the best performing system on SemEval-13 and SemEval-15. Dataset Senseval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL Train-o-Matic 70.5 67.4 59.8 65.5 68.6 67.3 OMSTI 74.1 67.2 62.3 62.8 63.1 66.4 SemCor 76.8 73.8 67.3 65.5 66.1 70.4 MFS 72.1 72.0 65.4 63.0 66.3 67.6 Table 4: F1 of IMS trained on Train-o-Matic, OMSTI and SemCor, and MFS for the Senseval-2, Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets. The evaluation has been performed using the unified evaluation framework for Word Sense Disambiguation made available by Raganato et al. (2017), thus considering the following WSD shared tasks: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Navigli et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). We set the two Train-o-Matic parameters K to 500 and z to 2.0 experimentally, testing the models learned by IMS on a small in-house development set2 and choosing the one with the highest performance. Multilingual setup: For the other languages we tuned the two paramenter K and z in the same way we did for English. The corpora proved to be more effective"
L18-1268,W04-0811,0,0.470102,"al-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15 ALL Train-o-Matic 70.5 67.4 59.8 65.5 68.6 67.3 OMSTI 74.1 67.2 62.3 62.8 63.1 66.4 SemCor 76.8 73.8 67.3 65.5 66.1 70.4 MFS 72.1 72.0 65.4 63.0 66.3 67.6 Table 4: F1 of IMS trained on Train-o-Matic, OMSTI and SemCor, and MFS for the Senseval-2, Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets. The evaluation has been performed using the unified evaluation framework for Word Sense Disambiguation made available by Raganato et al. (2017), thus considering the following WSD shared tasks: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Navigli et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). We set the two Train-o-Matic parameters K to 500 and z to 2.0 experimentally, testing the models learned by IMS on a small in-house development set2 and choosing the one with the highest performance. Multilingual setup: For the other languages we tuned the two paramenter K and z in the same way we did for English. The corpora proved to be more effective with K set to 100, for all the languages, and z ranging in [2.0, 3.0]. To prove that the generated data in the other languages"
L18-1268,K15-1037,0,0.552134,"corpora. One of the first attempt to produce a sense annotated corpus is SemCor (Miller et al., 1993), a collection of thousand sentences manually tagged with WordNet senses. While its quality is very high thanks to the effort of specialized annotators, it is far from covering the whole English vocabulary of words and senses. Moreover, such manual resources need extra effort to be maintained and updated to integrate new senses and words appearing in everyday language. Thus, in order to overcome these issues, semi-automatic or fully automatic approaches have been proposed over the past years. Taghipour and Ng (2015) exploit a parallel corpus and the manual translations of senses to annotate the words in the corpus with senses. Similarly, but without the need for human intervention, Delli Bovi et al. (2017) and CamachoCollados et al. (2016), rely on aligned sentences in order to create a richer context that can be beneficial to their disambiguation. Raganato et al. (2016), instead, designed a set of heuristics which exploit the human effort of the Wikipedia community in order to propagate and add sense annotations to the Wikipedia pages. Similarly Pasini and Navigli (2017) exploit a knowledge base in orde"
L18-1268,H93-1061,0,0.60706,"Missing"
moro-etal-2014-annotating,D11-1072,0,\N,Missing
moro-etal-2014-annotating,passonneau-etal-2010-word,1,\N,Missing
moro-etal-2014-annotating,D12-1128,1,\N,Missing
moro-etal-2014-annotating,ide-etal-2008-masc,1,\N,Missing
moro-etal-2014-annotating,H93-1061,0,\N,Missing
moro-etal-2014-annotating,J14-4005,1,\N,Missing
moro-etal-2014-annotating,Q14-1019,1,\N,Missing
moro-etal-2014-annotating,P11-3004,0,\N,Missing
moro-etal-2014-annotating,P10-1154,1,\N,Missing
moro-etal-2014-annotating,J14-1003,0,\N,Missing
moro-etal-2014-annotating,passonneau-etal-2012-masc,1,\N,Missing
moro-etal-2014-annotating,P13-1133,0,\N,Missing
moro-etal-2014-annotating,basile-etal-2012-developing,0,\N,Missing
moro-etal-2014-annotating,Q14-1025,1,\N,Missing
moro-etal-2014-annotating,ehrmann-etal-2014-representing,1,\N,Missing
N13-1130,E09-1005,0,0.0380075,"algorithm. PPR basically computes the probability according to which a random walker at a specific node in a graph would visit an arbitrary node in the same graph. The algorithm estimates, for a specific node in a graph, a probability distribution (called PPR vector) which determines the importance of any given node in the graph for that specific node. When applied to a semantic graph, this importance can be interpreted as semantic similarity. PPR has previously been used as a core component for semantic similarity1 (Hughes and Ramage, 2007; Agirre et al., 2009) and Word Sense Disambiguation (Agirre and Soroa, 2009). Algorithm 1 shows the procedure for the generation of our similarity-based pseudowords. The algorithm takes an ambiguous word w as input, and outputs its corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith sense of w, together with a confidence score which we detail below. Given w, the algorithm iterates over the synsets corresponding to its individual senses (lines 4-13) and finds the most suitable pseudosenses for Pw . For 1 Top-ranking synsets will contain words which are most likely similar to the target sense, whereas we move to a graded notion of relatedne"
N13-1130,N09-1003,0,0.0783016,"Missing"
N13-1130,P01-1005,0,0.167094,"Missing"
N13-1130,D08-1007,0,0.0339981,"ed data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficu"
N13-1130,E06-1018,0,0.218099,"sks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the nu"
N13-1130,P10-1046,0,0.0568983,"the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is c"
N13-1130,J13-3008,1,0.6768,"Missing"
N13-1130,P07-1028,0,0.0603859,"nse annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of"
N13-1130,J02-3001,0,0.00953271,"these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts. 1 Introduction A fundamental problem in computational linguistics is the paucity of manually annotated data, such as part-of-speech tagged sentences, treebanks, and logical forms, which exist only for few languages (Ide et al., 2010). A case in point is the lack of abundant sense annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order"
N13-1130,D07-1061,0,0.156147,"larity measure we selected the Personalized PageRank (Haveliwala, 2002, PPR) algorithm. PPR basically computes the probability according to which a random walker at a specific node in a graph would visit an arbitrary node in the same graph. The algorithm estimates, for a specific node in a graph, a probability distribution (called PPR vector) which determines the importance of any given node in the graph for that specific node. When applied to a semantic graph, this importance can be interpreted as semantic similarity. PPR has previously been used as a core component for semantic similarity1 (Hughes and Ramage, 2007; Agirre et al., 2009) and Word Sense Disambiguation (Agirre and Soroa, 2009). Algorithm 1 shows the procedure for the generation of our similarity-based pseudowords. The algorithm takes an ambiguous word w as input, and outputs its corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith sense of w, together with a confidence score which we detail below. Given w, the algorithm iterates over the synsets corresponding to its individual senses (lines 4-13) and finds the most suitable pseudosenses for Pw . For 1 Top-ranking synsets will contain words which are most likely"
N13-1130,P10-2013,0,0.0484543,"f pseudowords, i.e., artificial words which model real polysemous words. Our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets: semantic awareness and coverage. We evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts. 1 Introduction A fundamental problem in computational linguistics is the paucity of manually annotated data, such as part-of-speech tagged sentences, treebanks, and logical forms, which exist only for few languages (Ide et al., 2010). A case in point is the lack of abundant sense annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in o"
N13-1130,P06-1058,0,0.762166,"y proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously, and this has hind"
N13-1130,W04-0807,0,0.21108,"Missing"
N13-1130,N03-2023,0,0.834,"seudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously,"
N13-1130,otrusina-smrz-2010-new,0,0.483014,"e (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously, and this has hindered the generation of a large pseudosense-annotated"
N13-1130,H93-1052,0,0.0609921,"ccurrence frequency in the corpus C. This minimum frequency corresponds to the number of annotated sentences that are requested for the task of interest which will exploit the resulting annotated corpus. An immediate way of generating a pseudoword would be to randomly select its constituents from the set of all monosemous words given by a lexicon (e.g., WordNet). However, constructing a pseudoword by merely combining a random set of unambiguous words selected on the basis of their falling in the same range of occurrence frequency (Sch¨utze, 1992), or leveraging homophones and OCR ambiguities (Yarowsky, 1993), does not provide a suitable model of a real polysemous word (Gaustad, 2001; Nakov and Hearst, 2003). This is because in the real world different senses, unless they are homonymous, share some semantic or pragmatic relation. Therefore, random pseudowords will typically model only homonymous distinctions (such as the centimeter vs. curium senses of cm), while they will fall short of modeling systematic polysemy (such as the lack vs. insufficiency senses of deficiency). 2.1 Semantically-aware Pseudowords In order to cope with the above-mentioned limits of random pseudowords, an artificial word"
N13-1130,P10-4014,0,0.0806447,"Missing"
N15-1059,agirre-de-lacalle-2004-publicly,0,0.185389,"Missing"
N15-1059,N09-1003,0,0.248006,"Missing"
N15-1059,P13-4021,0,0.0165032,"Missing"
N15-1059,P14-1023,0,0.056742,"W (Pilehvar et al., 2013) are WordNet-based approaches that leverage the structural information of WordNet for the computation of semantic similarity. Most similar to our work are Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007, ESA), which represents a word in a high-dimensional space of Wikipedia articles, and Salient Semantic Analysis (Hassan and Mihalcea, 2011, SSA), which leverages the linking of concepts within Wikipedia articles for generating semantic profiles of words. Word2Vec (Mikolov et al., 2013) and PMI-SVD are the best predictive and cooccurrence models obtained by Baroni et al. (2014) on a 2.8 billion-token corpus that also includes the English Wikipedia.4 Word2Vec is based on neural network context prediction models (Mikolov et al., 2013), whereas PMI-SVD is a traditional cooccurrence based vector wherein weights are calculated by means of Pointwise Mutual Information (PMI) and the vector’s dimension is reduced to 500 by singular value decomposition (SVD). We use the DKProSimilarity (B¨ar et al., 2013) implementation of Lin and ESA in order to evaluate these measures on the WS-Sim dataset. 4.1.3 Results Table 1 shows the Pearson correlation of the different similarity mea"
N15-1059,F14-1032,1,0.865084,"Missing"
N15-1059,J06-1003,0,0.530067,"ty reduction and an effective weighting scheme, our representation attains state-of-the-art performance on multiple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by"
N15-1059,E06-1002,0,0.170695,"Missing"
N15-1059,R13-1022,0,0.205557,"kolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge available in both types of"
N15-1059,D10-1113,0,0.017686,"LP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained fr"
N15-1059,D08-1094,0,0.101817,"Missing"
N15-1059,P05-1045,0,0.00430959,"having, on average, 2.6 words. 571 3.2.2 Concept extraction If the two input words w1 and w2 are not found in the same synonym set in S, we proceed by obtaining their sets of senses Cw1 and Cw2 , respectively. Depending on the type of wi , we use two different resources for obtaining Cwi : the WordNet sense inventory and Wikipedia. WordNet words. When the word wi is defined in the WordNet sense inventory and is not a named entity (line 6 in Algorithm 1), we set Cwi as all the WordNet synsets that contain wi , i.e., Cwi = {synset s ∈ WordNet : wi ∈ s}. We use Stanford Named Entity Recognizer (Finkel et al., 2005) in our experiments. WordNet OOV and named entities. For named entities and words that do not exist in WordNet’s vocabulary (OOV) we construct the set Cwi by exploiting Wikipedia’s piped links (line 10 in Algorithm 1). To this end, we take as elements of Cwi the Wikipedia pages of the hyperlinks which have wi as their surface form, i.e., piped-links (wi ). If |Cwi |&gt; 5, we prune Cwi to its top-5 pages in terms of their number of ingoing links. Our choice of Wikipedia as a source for named entities is due to its higher coverage in comparison to WordNet. 4 Experiments We evaluated NASARI on two"
N15-1059,E14-1044,1,0.624945,"ea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Pilehvar et al., 2013), or Wikipedia (Gabrilovich and Markovitch, 2007; Mihalcea, 2007). None of these techniques, however, combine knowledge from multiple types of resource, making their representations resourcespecific and also prone to sparsity. In contrast, our method is based on the complementary knowledge of two different resources and their interlinking, leading to richer semantic representations that are also applicable across resources. Most similar to our combination of complementary knowledge is the work of Franco-Salvador et al. (2014) for crosslingual document retrieval. 575 Concept similarity. Concept similarity techniques are mainly limited to the knowledge that their underlying lexical resources provide. For instance, methods designed for measuring semantic similarity of WordNet synsets (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) usually leverage lexicographic or structural information in this lexical resource. Similarly, Wikipedia-based approaches (Hassan and Mihalcea, 2011; Strube and Ponzetto, 2006; Milne and Witten, 2008) do not usually benefit from the expert-based lexico-semant"
N15-1059,J15-4004,0,0.102299,"Missing"
N15-1059,P12-1092,0,0.261884,"ept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the know"
N15-1059,N07-1025,0,0.142037,"et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge avail"
N15-1059,D14-1113,0,0.0504506,"Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the knowledge derived from resource"
N15-1059,J07-2002,0,0.141103,"Missing"
N15-1059,P14-1044,1,0.829179,"ple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, u"
N15-1059,P13-1132,1,0.923416,"y using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation techniqu"
N15-1059,P10-1040,0,0.341448,"esource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 20"
N15-1059,D13-1136,0,0.0100905,"ntic similarity. Concept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their repres"
N15-3016,S12-1051,0,0.104282,"Missing"
N15-3016,P13-4021,0,0.0281113,"Missing"
N15-3016,P11-2087,0,0.0331341,"tion Semantic similarity quantifies the extent of shared semantics between two linguistics items, e.g., between deer and moose or cat and a feline mammal. Lying at the core of many Natural Language Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-ba"
N15-3016,J06-1003,0,0.0525352,"r correlations on RG-65 and accuracy, i.e., the number of correctly identified synonyms, on TOEFL. We show results for two sets of vectors: full vectors of size 118K and truncated vectors of size 5000 which are provided as a part of the package. As can be seen, despite reducing the space requirement by more than 15 times, our compressed vectors obtain high performance on both the datasets, matching those of the full vectors on the TOEFL dataset and also the cosine measure. 79 Related Work As the de facto standard lexical database, WordNet has been used widely in measuring semantic similarity. Budanitsky and Hirst (2006) provide an overview of WordNet-based similarity measures. WordNet::Similarity, a software developed by Pedersen et al. (2004), provides a Perl implementation of a number of these WordNet-based measures. UMLS::Similarity is an adaptation of WordNet::Similarity to the Unified Medical Language System (UMLS) which can be used for measuring the similarity and relatedness of terms in the biomedical domain (McInnes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measur"
N15-3016,P10-4006,0,0.0241543,"nes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measuring the semantic similarity of pairs of synsets with the same part of speech. ADW improves both issues by obtaining rich and unified representations for individual synsets, enabling effective comparison of arbitrary word senses or concepts, irrespective of their part of speech. Distributional semantic similarity measures have also attracted a considerable amount of research attention. The S-Space Package (Jurgens and Stevens, 2010) is an evaluation benchmark and a development framework for word space algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997). The package is integrated in DKProSimilarity (B¨ar et al., 2013), a more recently developed package geared towards semantic similarity of textual items. DKProSimilarity provides an opensource implementation of several semantic similarity techniques, from simple string-based measures such as character n-gram overlap, to more sophisticated vector-based measures such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). ADW was shown to impro"
N15-3016,P11-1076,0,0.0290749,"a feline mammal. Lying at the core of many Natural Language Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRan"
N15-3016,N04-3012,0,0.100431,"vectors: full vectors of size 118K and truncated vectors of size 5000 which are provided as a part of the package. As can be seen, despite reducing the space requirement by more than 15 times, our compressed vectors obtain high performance on both the datasets, matching those of the full vectors on the TOEFL dataset and also the cosine measure. 79 Related Work As the de facto standard lexical database, WordNet has been used widely in measuring semantic similarity. Budanitsky and Hirst (2006) provide an overview of WordNet-based similarity measures. WordNet::Similarity, a software developed by Pedersen et al. (2004), provides a Perl implementation of a number of these WordNet-based measures. UMLS::Similarity is an adaptation of WordNet::Similarity to the Unified Medical Language System (UMLS) which can be used for measuring the similarity and relatedness of terms in the biomedical domain (McInnes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measuring the semantic similarity of pairs of synsets with the same part of speech. ADW improves both issues by obtaining rich and u"
N15-3016,P14-1044,1,0.796084,"nguage Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRank (PPR) algorithm (Haveliwala et al., 2002) applied on the Wo"
N15-3016,P13-1132,1,0.909651,"t search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRank (PPR) algorithm (Haveliwala et al., 2002) applied on the WordNet graph (Miller et al., 1990), and can be used to compute the similarity between arbitrary linguistic items, all the way from word senses to texts. Pilehvar et al. (2013) reported state-of-the-art performance on multiple evaluation benchmarks belonging to different lexical levels: senses, words, and sentences. In this demonstration we present an open-source implementation of our system together with a Java API and a Web interface for online measurement of semantic similarity. We a"
navigli-2006-reducing,W04-0811,0,\N,Missing
navigli-2006-reducing,J91-1002,0,\N,Missing
navigli-2006-reducing,C94-2113,0,\N,Missing
navigli-2006-reducing,W02-0817,0,\N,Missing
navigli-2006-reducing,J04-2002,1,\N,Missing
navigli-2006-reducing,briscoe-carroll-2002-robust,0,\N,Missing
navigli-2006-reducing,magnini-cavaglia-2000-integrating,0,\N,Missing
navigli-etal-2010-annotated,degorski-etal-2008-definition,0,\N,Missing
navigli-etal-2010-annotated,P04-1030,0,\N,Missing
navigli-etal-2010-annotated,P99-1016,0,\N,Missing
navigli-etal-2010-annotated,W07-1706,0,\N,Missing
navigli-etal-2010-annotated,A92-1011,0,\N,Missing
navigli-etal-2010-annotated,P06-1101,0,\N,Missing
navigli-etal-2010-annotated,P09-1031,0,\N,Missing
navigli-etal-2010-annotated,P08-1115,0,\N,Missing
navigli-etal-2010-annotated,P10-1134,1,\N,Missing
navigli-etal-2010-annotated,N09-1046,0,\N,Missing
navigli-etal-2010-annotated,E09-1082,0,\N,Missing
navigli-etal-2010-annotated,storrer-wellinghoff-2006-automated,0,\N,Missing
navigli-velardi-2002-automatic,W01-1005,1,\N,Missing
P06-1013,W02-0811,0,0.0432469,"typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available (Yarowsky and Florian, 2002). The work presented here evaluates and compares the performance of well-established unsupervised WSD algorithms. We show that these algorithms yield sufficiently diverse outputs, thus motivating the use of combination methods for improving WSD performance. While combination approaches have been studied previously for supervised WSD (Florian et al., 2002), their use in an unsupervised setting is, to our knowledge, novel. We examine several existing and novel combination methods and demonstrate that our combined systems consistently outperform the 97 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 97–104, c Sydney, July 2006. 2006 Association for Computational Linguistics parametrized and can be adjusted to take the into account gloss length or to ignore function words. Distributional and WordNet Similarity McCarthy et al. (2004) propose a method for automatically ranking t"
P06-1013,P04-1036,0,0.658244,"sing the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art. 1 This paper focuses on unsupervised methods which we argue are useful for broad coverage sense disambiguation. Unsupervised WSD algorithms fall into two general classes: those that perform token-based WSD by exploiting the similarity or relatedness between an ambiguous word and its context (e.g., Lesk 1986); and those that perform type-based WSD, simply by assigning all instances of an ambiguous word its most frequent (i.e., predominant) sense (e.g., McCarthy et al. 2004; Galley and McKeown 2003). The predominant senses are automatically acquired from raw text without recourse to manually annotated data. The motivation for assigning all instances of a word to its most prevalent sense stems from the observation that current supervised approaches rarely outperform the simple heuristic of choosing the most common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the"
P06-1013,H05-1052,0,0.091332,"Missing"
P06-1013,H93-1061,0,0.0579067,"nss) with regard to si . The ranking score of sense si is then increased as a function of the WordNet similarity score and the distributional similarity score (dss) between the target word and the neighbor: state-of-the-art (e.g., McCarthy et al. 2004). Importantly, our WSD algorithms and combination methods do not make use of training material in any way, nor do they use the first sense information available in WordNet. In the following section, we briefly describe the unsupervised WSD algorithms considered in this paper. Then, we present a detailed comparison of their performance on SemCor (Miller et al., 1993). Next, we introduce our system combination methods and report on our evaluation experiments. We conclude the paper by discussing our results. 2 The Disambiguation Algorithms In this section we briefly describe the unsupervised WSD algorithms used in our experiments. We selected methods that vary along the following dimensions: (a) the type of WSD performed (i.e., token-based vs. type-based), (b) the representation and size of the context surrounding an ambiguous word (i.e., graph-based vs. word-based, document vs. sentence), and (c) the number and type of semantic relations considered for dis"
P06-1013,E06-1016,0,0.136393,"Missing"
P06-1013,J91-1002,0,0.014805,"thod presented above has four parameters: (a) the semantic space model representing the distributional properties of the target words (it is acquired from a large corpus representative of the domain at hand and can be augmented with syntactic relations such as subject or object), (b) the measure of distributional similarity for discovering neighbors (c) the number of neighbors that the ranking score takes into account, and (d) the measure of sense similarity. Lexical Chains Lexical cohesion is often represented via lexical chains, i.e., sequences of related words spanning a topical text unit (Morris and Hirst, 1991). Algorithms for computing lexical chains often perform WSD before inferring which words are semantically related. Here we describe one such disambiguation algorithm, proposed by Galley and McKeown (2003), while omitting the details of creating the lexical chains themselves. Galley and McKeown’s (2003) method consists of two stages. First, a graph is built representing all possible interpretations of the target words ∑ Overlap(context, Rel(sk )) Rel∈Relations where context is a simple (space separated) concatenation of all words wi for −n ≤ i ≤ n, i 6= 0 in a context window of length ±n around"
P06-1013,W97-0201,0,0.117829,"Missing"
P06-1013,H05-1051,0,0.0342821,"ost common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, holds promise for many NLP applications requiring broad-coverage language understanding. Examples include summarization, question answering, and text simplification. Recent studies have also shown that WSD can benefit machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005). Given the potential of WSD for many NLP tasks, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data, i.e., corpus occurrences of ambiguous words marked up with labels indicating the appropriate sense given the context (see Mihalcea and Edmonds 2004 and the references therein). A classifier automatically learns disambiguation cues from these hand-labeled examples. Although supervised methods typically achieve better performance than unsupervis"
P06-1013,J01-2002,0,0.079106,"Missing"
P06-1013,H05-1097,0,0.0120766,"outperform the simple heuristic of choosing the most common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, holds promise for many NLP applications requiring broad-coverage language understanding. Examples include summarization, question answering, and text simplification. Recent studies have also shown that WSD can benefit machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005). Given the potential of WSD for many NLP tasks, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data, i.e., corpus occurrences of ambiguous words marked up with labels indicating the appropriate sense given the context (see Mihalcea and Edmonds 2004 and the references therein). A classifier automatically learns disambiguation cues from these hand-labeled examples. Although supervised methods typically a"
P06-1013,briscoe-carroll-2002-robust,0,0.011318,"n the text. These weights were imported from Galley and McKeown into our implementation without modification. Because the SemCor corpus is relatively small (less than 700,00 words), it is not ideal for constructing a neighbor thesaurus appropriate for McCarthy et al.’s (2004) method. The latter requires each word to participate in a large number of cooccurring contexts in order to obtain reliable distributional information. To overcome this problem, we followed McCarthy et al. and extracted the neighbor thesaurus from the entire BNC. We also recreated their semantic space, using a RASPparsed (Briscoe and Carroll, 2002) version of the BNC and their set of dependencies (i.e., VerbObject, Verb-Subject, Noun-Noun and AdjectiveNoun relations). Similarly to McCarthy et al., we used Lin’s (1998) measure of distributional similarity, and considered only the 50 highest ranked |{w ∈ W f |fs (w) = fm (w)}| |W f | A baseline for this task can be easily defined for each word type by selecting a sense at random from its sense inventory and assuming that this is the predominant sense: Baselinesr = 1 1 |W f |w ∑ |senses(w)| ∈W f We evaluate the algorithms’ disambiguation performance by measuring the ratio of tokens for whi"
P06-1013,W04-0856,0,\N,Missing
P06-1014,magnini-cavaglia-2000-integrating,0,0.090895,"Missing"
P06-1014,W06-2503,0,0.367661,"Missing"
P06-1014,W04-0838,0,0.0118566,"Missing"
P06-1014,J91-1002,0,0.114595,"ntioned above (we adopt only direct relations to maintain a high precision). For example, some of the relations found and between concepts in dsem WN (race#n#3) sem dODE (race#n#1.1) are: i=1 at least one semantic interconnection between S and S 0 in the lexical knowledge base. A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created context-free grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts. The grammar consists of a small number of rules, inspired by the notion of lexical chains (Morris and Hirst, 1991). SSI performs disambiguation in an iterative fashion, by maintaining a set C of senses as a semantic context. Initially, C = V (the entire set of senses of words in C). At each step, for each sense S in C, the algorithm calculates a score of the degree of connectivity between S and the other senses in C: 3 P S 0 ∈C{S} i∈IC(S,S 0 ) race#n#3 speed#n#1 race#n#3 racing#n#1 race#n#3 relation related−to −→ related−to −→ kind−of −→ kind−of −→ race#n#1.1 vehicle#n#1 compete#v#1 sport#n#1 contest#n#1 contributing to the final value of the function on the two senses: matchSSI (race#n#3, race#n#1.1) ="
P06-1014,J04-2002,1,0.487859,"∈ SensesODE (w), we define matchSSI (S, S 0 ) as a function of the direct relations connecting sem 0 senses in dsem WN (S) and dODE (S ): Notice that unrelated senses can get a positive score because of an overlap of the sense descriptions. In the example, group#n, the hypernym of race#n#2, is also present in the definition of race#n#1.1. 2.3.2 Semantic matching Unfortunately, the very same concept can be defined with entirely different words. To match definitions in a semantic manner we adopted a knowledge-based Word Sense Disambiguation algorithm, Structural Semantic Interconnections (SSI, Navigli and Velardi (2004)). SSI3 exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing semantic relatedness between sense pairs. Collocations are acquired from existing resources (like the Oxford Collocations, the Longman Language Activator, collocation web sites, etc.). Each collocation is mapped to the WordNet sense inventory in a semi-automatic manner and transformed into a relatedness edge (Navigli and Velardi, 2005). Given a word context C = {w1 , ..., wn }, SSI builds a graph G = (V, E) such that V = n S SensesWN (wi ) and (S, S 0 ) ∈"
P06-1014,W99-0502,0,0.0514151,"Missing"
P06-1014,W04-0811,0,0.18253,"Missing"
P06-1014,H05-1051,0,0.0630823,"Missing"
P06-1014,W04-0856,0,0.0124655,"Missing"
P06-1014,N01-1010,0,0.085998,"Missing"
P06-1014,P05-1048,0,0.0294034,"Missing"
P06-1014,W04-0864,0,0.096334,"Missing"
P06-1014,briscoe-carroll-2002-robust,0,0.0217928,"s and 3 polysemous senses for the first homonym, while WordNet encodes a flat list of 6 senses, some of which strongly related (e.g. race#1 and race#3). Also, the ODE provides a sense (ginger Constructing Sense Descriptions For each word w, and for each sense S of w in a given dictionary D ∈ {W ORD N ET, ODE}, we construct a sense description dD (S) as a bag of words: dD (S) = def D (S) ∪ hyperD (S) ∪ domainsD (S) where: • def D (S) is the set of words in the textual definition of S (excluding usage examples), automatically lemmatized and partof-speech tagged with the RASP statistical parser (Briscoe and Carroll, 2002); • hyperD (S) is the set of direct hypernyms of S in the taxonomy hierarchy of D (∅ if hypernymy is not available); • domainsD (S) includes the set of domain labels possibly assigned to sense S (∅ when no domain is assigned). Specifically, in the case of WordNet, we generate def WN (S) from the gloss of S, hyperWN (S) from the noun and verb taxonomy, and domainsWN (S) from the subject field codes, i.e. domain labels produced semi-automatically by Magnini and Cavagli`a (2000) for each WordNet synset (we exclude the general-purpose label, called FACTOTUM). For example, for the first WordNet sen"
P06-1014,W02-0817,0,0.120287,"Missing"
P06-1014,W04-0827,0,0.0791546,"Missing"
P06-1014,C94-2113,0,0.482053,"Missing"
P06-1014,H05-1097,0,\N,Missing
P06-4004,W02-0817,0,0.0504271,"Missing"
P06-4004,H93-1061,0,0.0890273,"defined as follows: let w be a word in a sentence Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing seman1 SSI is available online at http://lcl.di.uniroma1.it/ssi. 13 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13–16, c Sydney, July 2"
P06-4004,J91-1002,0,0.127265,"volunteers on the web). Rather, to a large part it is due to the difficulty in making clear which are the real distinctions between close word senses in the WordNet inventory. Adjudicating sense choices, i.e. the task of validating word senses, is therefore critical in building a high-quality data set. The validation task can be defined as follows: let w be a word in a sentence Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in t"
P06-4004,J04-2002,1,0.823433,"plicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing seman1 SSI is available online at http://lcl.di.uniroma1.it/ssi. 13 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13–16, c Sydney, July 2006. 2006 Association for Computational Linguistics S → S 0 S1 |S 0 S2 |S 0 S3 (start rule) S 0 → enominalization |epertainymy | (part-of-speech jum"
P06-4004,E06-1017,1,0.819182,"Missing"
P06-4004,C96-1005,0,0.0261646,"is due to the difficulty in making clear which are the real distinctions between close word senses in the WordNet inventory. Adjudicating sense choices, i.e. the task of validating word senses, is therefore critical in building a high-quality data set. The validation task can be defined as follows: let w be a word in a sentence Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive"
P10-1023,E06-1002,0,0.0938351,"Missing"
P10-1023,D09-1030,0,0.0679928,"Missing"
P10-1023,J96-2004,0,0.0214704,"Missing"
P10-1023,W06-1663,0,0.0350835,"sting gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repo"
P10-1023,2007.mtsummit-papers.24,0,0.0161327,"ilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic"
P10-1023,P95-1032,0,0.0224447,"ordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations be"
P10-1023,J93-1004,0,0.121046,"(Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic"
P10-1023,P08-1088,0,0.0179436,"(Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have pre"
P10-1023,P10-1154,1,0.129943,"nstant independent of s. As a result, determining the most appropriate sense s consists of finding the sense s that maximizes the joint probability p(s, w). We estimate p(s, w) as: p(s, w) = score(s, w) X , score(s0 , w0 ) s0 ∈SensesWN (w), w0 ∈SensesWiki (w) where score(s, w) = |Ctx(s) ∩ Ctx(w) |+ 1 (we add 1 as a smoothing factor). Thus, in our algorithm we determine the best sense s by computing the intersection of the disambiguation contexts of s and w, and normalizing by the scores summed over all senses of w in Wikipedia and WordNet. More details on the mapping algorithm can be found in Ponzetto and Navigli (2010). 3.3 3.4 Example We now illustrate the execution of our methodology by way of an example. Let us focus on the Wikipage BALLOON ( AIRCRAFT ). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4"
P10-1023,W08-2231,0,0.0104265,"unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. Table 4: Precision of BabelNet on synonyms in WordNet (WN), Wikipedia (Wiki) and their intersection (WN ∩ Wiki): percentage and total number of words (in parentheses) are reported. not find translations in major editions of bilingual dictionarie"
P10-1023,2007.mtsummit-papers.53,0,0.0211011,"t al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions I"
P10-1023,P06-1101,0,0.107103,"ting wordnets in non-English languages. While BabelNet currently includes 6 languages, links to freely-available wordnets6 can immediately be established by utilizing the English WordNet as an interlanguage index. Indeed, BabelNet can be extended to virtually any language of interest. In fact, our translation method allows it to cope with any resource-poor language. As future work, we plan to apply our method to other languages, including Eastern European, Arabic, and Asian languages. We also intend to link missing concepts in WordNet, by establishing their most likely hypernyms – e.g., a` la Snow et al. (2006). We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon’s Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. Finally, we aim to apply BabelNet to a variety of applications which are known to benefit from a wide-coverage knowledge resource. We have already shown that the English-only subset of BabelNet allows simple knowledge-based algorithms to compete with supervised systems in standard coarse-grained and domain-specific WSD settings (Ponzetto and Navigli, 2010). We plan in th"
P10-1023,P07-2045,0,0.00504736,"age BALLOON ( AIRCRAFT ). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4 We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, we include w"
P10-1023,2005.mtsummit-papers.11,0,0.00843092,"ous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4 We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, we include words from their synsets, hypernyms, hy"
P10-1023,W09-2413,0,0.223667,"Missing"
P10-1023,P09-1030,0,0.0230873,"t al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wi"
P10-1023,H93-1061,0,0.490473,"ES , globusCA , pallone aerostaticoIT , ballonFR , montgolfi`ereFR high wind blow gas WordNet Figure 1: An illustrative overview of BabelNet. poor languages with the aid of Machine Translation. The result is an “encyclopedic dictionary”, that provides concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations. 2 using (a) the human-generated translations provided in Wikipedia (the so-called inter-language links), as well as (b) a machine translation system to translate occurrences of the concepts within sense-tagged corpora, namely SemCor (Miller et al., 1993) – a corpus annotated with WordNet senses – and Wikipedia itself (Section 3.3). We call the resulting set of multilingual lexicalizations of a given concept a babel synset. An overview of BabelNet is given in Figure 1 (we label vertices with English lexicalizations): unlabeled edges are obtained from links in the Wikipedia pages (e.g. BALLOON ( AIRCRAFT ) links to W IND), whereas labeled ones from WordNet3 (e.g. balloon1n haspart gasbag1n ). In this paper we restrict ourselves to concepts lexicalized as nouns. Nonetheless, our methodology can be applied to all parts of speech, but in that case"
P10-1023,E09-1068,1,0.453393,"resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These"
P10-1023,bel-etal-2000-simple,0,\N,Missing
P10-1023,D08-1080,0,\N,Missing
P10-1023,P96-1006,0,\N,Missing
P10-1023,kunze-lemnitzer-2002-germanet,0,\N,Missing
P10-1023,1995.mtsummit-1.17,0,\N,Missing
P10-1134,W09-4405,0,0.331994,"Missing"
P10-1134,P99-1016,0,0.0192162,"true definitions, but rather text fragments providing some relevant fact about a target term. For example, sentences like: “Bollywood is a Bombay-based film industry” and “700 or more films produced by India with 200 or more from Bollywood” are both “vital” answers for the question “Bollywood”, according to TREC classification, but the second sentence is not a definition. Hypernym Extraction. The literature on hypernym extraction offers a higher variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Pozna´nski, 1992; Ritter et al., 2009). One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifer based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them."
P10-1134,P04-1030,0,0.0495768,"eliminating redundant information. In computational linguistics, lattices have been used to model in a compact way many sequences of symbols, each representing an alternative hypothesis. Lattice-based methods differ in the types of nodes (words, phonemes, concepts), the interpretation of links (representing either a sequential or hierarchical ordering between nodes), their means of creation, and the scoring method used to extract the best consensus output from the lattice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface between speech recognition and understanding. Lat1318 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics tices are adopted also in Chinese word segmentation (Jiang et al., 2008), decompounding in German (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder et al., 2009). In more complex text processing tasks, such as information retrieval, information extraction and summari"
P10-1134,degorski-etal-2008-definition,0,0.295924,"Missing"
P10-1134,P08-1115,0,0.00917987,"tice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface between speech recognition and understanding. Lat1318 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics tices are adopted also in Chinese word segmentation (Jiang et al., 2008), decompounding in German (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder et al., 2009). In more complex text processing tasks, such as information retrieval, information extraction and summarization, the use of word lattices has been postulated but is considered unrealistic because of the dimension of the hypothesis space. To reduce this problem, concept lattices have been proposed (Carpineto and Romano, 2005; Klein, 2008; Zhong et al., 2008). Here links represent hierarchical relations, rather than the sequential order of symbols like in word/phoneme lattices, and nodes are clusters of salient words aggregated using synonymy, similarity, or subtrees of"
P10-1134,N09-1046,0,0.010217,"ation, and the scoring method used to extract the best consensus output from the lattice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface between speech recognition and understanding. Lat1318 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics tices are adopted also in Chinese word segmentation (Jiang et al., 2008), decompounding in German (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder et al., 2009). In more complex text processing tasks, such as information retrieval, information extraction and summarization, the use of word lattices has been postulated but is considered unrealistic because of the dimension of the hypothesis space. To reduce this problem, concept lattices have been proposed (Carpineto and Romano, 2005; Klein, 2008; Zhong et al., 2008). Here links represent hierarchical relations, rather than the sequential order of symbols like in word/phoneme lattices, and"
P10-1134,W06-2609,0,0.295431,"Missing"
P10-1134,C92-2082,0,0.725169,"s: http://trec.nist. gov 1319 ble2 . In fact, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. For example, sentences like: “Bollywood is a Bombay-based film industry” and “700 or more films produced by India with 200 or more from Bollywood” are both “vital” answers for the question “Bollywood”, according to TREC classification, but the second sentence is not a definition. Hypernym Extraction. The literature on hypernym extraction offers a higher variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Pozna´nski, 1992; Ritter et al., 2009). One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifer based on these features. Lexico-syntactic patterns are generated for"
P10-1134,C08-1049,0,0.0168628,"al ordering between nodes), their means of creation, and the scoring method used to extract the best consensus output from the lattice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface between speech recognition and understanding. Lat1318 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics tices are adopted also in Chinese word segmentation (Jiang et al., 2008), decompounding in German (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder et al., 2009). In more complex text processing tasks, such as information retrieval, information extraction and summarization, the use of word lattices has been postulated but is considered unrealistic because of the dimension of the hypothesis space. To reduce this problem, concept lattices have been proposed (Carpineto and Romano, 2005; Klein, 2008; Zhong et al., 2008). Here links represent hierarchical relations, rather than the sequential order of symb"
P10-1134,navigli-etal-2010-annotated,1,0.848242,"Missing"
P10-1134,E09-1068,1,0.907741,"efinitional, that is they provide a formal explanation for the term of interest. While it is not feasible to manually search texts for definitions, this task can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Automatic definition extraction is useful not only in the construction of glossaries, but also in many other NLP tasks. In ontology learning, definitions are used to create and enrich concepts with textual information (Gangemi et al., 2003), and extract taxonomic and non-taxonomic relations (Snow et al., 2004; Navigli and Velardi, 2006; Navigli, 2009a). Definitions are also harvested in Question Answering to deal with “what is” questions (Cui et al., 2007; Saggion, 2004). In eLearning, they are used to help students assimilate knowledge (Westerhout and Monachesi, 2007), etc. Much of the current literature focuses on the use of lexico-syntactic patterns, inspired by Hearst’s (1992) seminal work. However, these methods suffer both from low recall and precision, as definitional sentences occur in highly variable syntactic structures, and because the most frequent definitional pattern – X is a Y – is inherently very noisy. In this paper we pr"
P10-1134,saggion-2004-identifying,0,0.153579,"ch texts for definitions, this task can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Automatic definition extraction is useful not only in the construction of glossaries, but also in many other NLP tasks. In ontology learning, definitions are used to create and enrich concepts with textual information (Gangemi et al., 2003), and extract taxonomic and non-taxonomic relations (Snow et al., 2004; Navigli and Velardi, 2006; Navigli, 2009a). Definitions are also harvested in Question Answering to deal with “what is” questions (Cui et al., 2007; Saggion, 2004). In eLearning, they are used to help students assimilate knowledge (Westerhout and Monachesi, 2007), etc. Much of the current literature focuses on the use of lexico-syntactic patterns, inspired by Hearst’s (1992) seminal work. However, these methods suffer both from low recall and precision, as definitional sentences occur in highly variable syntactic structures, and because the most frequent definitional pattern – X is a Y – is inherently very noisy. In this paper we propose a generalized form of word lattices, called Word-Class Lattices (WCLs), as an alternative to lexico-syntactic pattern"
P10-1134,A92-1011,0,0.0897244,"Missing"
P10-1134,E09-1082,0,0.0308979,"tomata (NFA). The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant information. In computational linguistics, lattices have been used to model in a compact way many sequences of symbols, each representing an alternative hypothesis. Lattice-based methods differ in the types of nodes (words, phonemes, concepts), the interpretation of links (representing either a sequential or hierarchical ordering between nodes), their means of creation, and the scoring method used to extract the best consensus output from the lattice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface between speech recognition and understanding. Lat1318 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics tices are adopted also in Chinese word segmentation (Jiang et al., 2008), decompounding in German (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder"
P10-1134,storrer-wellinghoff-2006-automated,0,0.390114,"in order to demonstrate the independence of the method from the annotated dataset. WCLs are shown to generalize over lexico-syntactic patterns, and outperform well-known approaches to definition and hypernym extraction. The paper is organized as follows: Section 2 discusses related work, WCLs are introduced in Section 3 and illustrated by means of an example in Section 4, experiments are presented in Section 5. We conclude the paper in Section 6. 2 Related Work Definition Extraction. A great deal of work is concerned with definition extraction in several languages (Klavans and Muresan, 2001; Storrer and Wellinghoff, 2006; Gaudio and Branco, 2007; Iftene et al., 2007; Westerhout and Monachesi, 2007; Przepi´orkowski et al., 2007; Deg´orski et al., 2008). The majority of these approaches use symbolic methods that depend on lexico-syntactic patterns or features, which are manually crafted or semi-automatically learned (Zhang and Jiang, 2009; Hovy et al., 2003; Fahmi and Bouma, 2006; Westerhout, 2009). Patterns are either very simple sequences of words (e.g. “refers to”, “is defined as”, “is a”) or more complex sequences of words, parts of speech and chunks. A fully automated method is instead proposed by Borg et"
P10-1134,W09-4410,0,0.536955,"Missing"
P10-1134,W07-1706,0,\N,Missing
P10-1154,agirre-de-lacalle-2004-publicly,0,0.114731,"Missing"
P10-1154,E09-1005,0,0.248885,"ms to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++. In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance. We leave such exploration to future work. Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is Wikipedia, to perform disambiguation with the corresponding sense inventory (cf. the task of wikification proposed by Mihalcea and Csomai (2007) and Milne and Witten (2008b)). In this paper, we focused on English Word Sense Disambiguation. However, since WordNet++ is part of a multilingual semantic network (Navigli"
P10-1154,E06-1002,0,0.206374,"Missing"
P10-1154,J96-2004,0,0.0207335,"ding monosemous words). We selected a random sample of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annotation to provide the correct WordNet sense for each page title (an empty sense label was given if no correct mapping was possible). 505 non-empty mappings were found, i.e. Wikipedia pages with a corresponding WordNet sense. In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample. We computed the inter-annotator agreement using the kappa coefficient (Carletta, 1996) and found out that our annotators achieved an agreement coefficient κ of 0.9, indicating almost perfect agreement. Table 1 summarizes the performance of our disambiguation algorithm against the manually annotated dataset. Evaluation is performed in terms of standard measures of precision (the ratio of correct sense labels to the non-empty labels output by the mapping algorithm), recall (the ratio of correct sense labels to the total of non-empty laR ). bels in the gold standard) and F1 -measure ( P2P+R We also calculate accuracy, which accounts for 1526 Structure Gloss Structure + Gloss MFS B"
P10-1154,S07-1054,0,0.0177181,"Missing"
P10-1154,N09-1004,0,0.012826,"Missing"
P10-1154,W02-0817,0,0.0100596,"acted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the k"
P10-1154,P85-1037,0,0.197171,"better than supervised ones, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of c"
P10-1154,W06-1663,0,0.657423,"nd tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy. In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information Roberto Navigli Dipartimento di Informatica Sapienza Universit`a di Roma navigli@di.uniroma1.it contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010). However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009). The contributions of this paper are two-fold. First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend"
P10-1154,C08-1021,0,0.106657,"ased on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicat"
P10-1154,J06-1005,0,0.0117262,"ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of sta"
P10-1154,W99-0501,0,0.0910581,"Missing"
P10-1154,C92-2082,0,0.241669,"rform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and"
P10-1154,S07-1068,0,0.110289,"Missing"
P10-1154,H05-1053,0,0.0362288,"PT SSI MFS BL Random BL Nouns only P/R/F1 81.0 85.5 81.1 N/A 82.3 84.1 77.4 63.5 All words P/R/F1 79.1 81.7 77.0 73.6 82.5 83.2 78.9 62.7 Algorithm k-NN † Static PR † Personalized PR † ExtLesk Degree MFS BL Random BL Table 3: Performance on Semeval-2007 coarsegrained all-words WSD with MFS as a back-off strategy when no sense assignment is attempted. 6 The differences between the results in bold in each column of the table are not statistically significant at p &lt; 0.05. Finance P/R/F1 43.4 39.6 46.9 45.6 47.8 37.1 19.6 Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). 4.3 gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated o"
P10-1154,J03-4004,0,0.0147539,"psala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a v"
P10-1154,N07-1025,0,0.0615322,"kipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mappin"
P10-1154,H93-1061,0,0.907827,"Missing"
P10-1154,D08-1080,0,0.0123223,"ation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappi"
P10-1154,P10-1023,1,0.712486,"the corresponding semantic relation (soda2n , syrup1n ) to WordNet3 . Thus, WordNet++ represents an extension of WordNet which includes semantic associative relations between synsets. These are originally 3 Note that such relations are unlabeled. However, for our purposes this has no impact, since our algorithms do not distinguish between is-a and other kinds of relations in the lexical knowledge base (cf. Section 4.2). found in Wikipedia and then integrated into WordNet by means of our mapping. In turn, WordNet++ represents the English-only subset of a larger multilingual resource, BabelNet (Navigli and Ponzetto, 2010), where lexicalizations of the synsets are harvested for many languages using the so-called Wikipedia inter-language links and applying a machine translation system. 4 Experiments We perform two sets of experiments: we first evaluate the intrinsic quality of our mapping (Section 4.1) and then quantify the impact of WordNet++ for coarse-grained (Section 4.2) and domainspecific WSD (Section 4.3). 4.1 Evaluation of the Mapping Experimental setting. We first conducted an evaluation of the mapping quality. To create a gold standard for evaluation, we started from the set of all lemmas contained bot"
P10-1154,S07-1006,1,0.791308,"Missing"
P10-1154,E09-1068,1,0.876613,"In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets. 1 Introduction Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b). In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods. In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses. Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the"
P10-1154,P06-1100,0,0.00552179,"for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods repr"
P10-1154,W08-2231,0,0.0244515,"ing text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations connecting Wikipedia pages are transferred to WordNet. As a result, an extended version of WordNet is produced, that we call WordNet++."
P10-1154,P98-2181,0,0.13059,"Missing"
P10-1154,P09-1024,0,0.00803243,"the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet,"
P10-1154,P09-1051,0,0.00741371,"d Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on pre"
P10-1154,P06-1101,0,0.0556263,"resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-a"
P10-1154,C98-2176,0,\N,Missing
P10-1154,W01-0703,0,\N,Missing
P12-3012,W11-0104,0,0.0185717,"trated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query"
P12-3012,S07-1054,0,0.0230853,"Missing"
P12-3012,P11-1061,0,0.00401357,"– a wide-coverage multilingual lexical knowledge base – and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the"
P12-3012,S10-1026,0,0.0182807,": Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD evaluation, instead, using BabelNet allows us to surpass the best unsupervised system by a substantial margin, thus indicating the viability of high-performing WSD with a multilingual lexical knowledge base. While our performance still lags behind the application of supervised techniques to this task (cf. also results from Lefever a"
P12-3012,P11-1057,0,0.0592351,"2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query, retrieve and exploit its information for disambiguation. Nevertheless, to date, no integrated"
P12-3012,2005.mtsummit-papers.11,0,0.00989679,"n different languages. Since the selected Babel synset can contain multiple translations in a target language for the given English word, we use for this task an Algorithm NUS-PT SUSSX-FR Degree MFS BL Random BL Nouns only 82.3 81.1 84.7 77.4 63.5 All words 82.5 77.0 82.3 78.9 62.7 Dutch French German Italian Spanish Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD e"
P12-3012,S07-1068,0,0.0178267,"7 77.4 63.5 All words 82.5 77.0 82.3 78.9 62.7 Dutch French German Italian Spanish Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD evaluation, instead, using BabelNet allows us to surpass the best unsupervised system by a substantial margin, thus indicating the viability of high-performing WSD with a multilingual lexical knowledge base. While our performance still lags behind the"
P12-3012,P11-2055,0,0.0842271,"Missing"
P12-3012,P11-1033,0,0.0384722,"easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever an"
P12-3012,P11-1134,0,0.0373594,"aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEv"
P12-3012,W09-2412,0,0.202612,"Missing"
P12-3012,P10-1023,1,0.154385,"ive is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical k"
P12-3012,S07-1006,1,0.102631,"rring within this graph (line 5–10). Finally, we output the sense distributions of each word in lines 11–18. The disambiguation method, in turn, can be called by any other Java program in a way similar to the one highlighted by 70 the main method of lines 21–26, where we disambiguate the sample sentence ‘bank bonuses are paid in stocks’ (note that each input word can be written in any of the 6 languages, i.e. we could mix languages). 4 Experiments We benchmark our API by performing knowledgebased WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al., 2007, Coarse-WSD, henceforth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. For both experimental settings we use a standard graphbased algorithm, Degree (Navigli and Lapata, 2010), which has been previously shown to yield a highly competitive performance on different lexical disambiguation tasks (Ponzetto and Navigli, 2010). Given a semantic graph for the input context, Degree selects the sense of the target word with the highest vertex degree. In addition, in the CL-WSD setting we need to output appropriate lexicalization(s) in different languages. Since the sel"
P12-3012,E06-2006,1,0.832026,"cused on visual browsing of wide-coverage knowledge bases (Tylenda et al., 2011; Navigli and Ponzetto, 2012) by means of an API which allows the user to programmatically query and search BabelNet. This knowledge resource, in turn, can be used for eas71 Degree 15.52 22.94 17.15 18.03 22.48 T3-Coleur 10.56 21.75 13.05 14.67 19.64 UvT-v 17.70 − − − 23.39 Table 2: Performance on SemEval-2010 cross-lingual WSD (Lefever and Hoste, 2010). ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from W"
P12-3012,P05-3019,0,0.0122101,"n SemEval-2010 cross-lingual WSD (Lefever and Hoste, 2010). ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia include WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to BabelNet – i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet)."
P12-3012,N04-3012,0,0.00948591,"freely available to the research community on a multilingual scale. Previous endeavors are either not freely available (EuroWordNet (Vossen, 1998)), or are only accessible via a Web interface (cf. the Multilingual Research Repository (Atserias et al., 2004) and MENTA (de Melo and Weikum, 2010)), thus providing no programmatic access. And this is despite the fact that the availability of easy-to-use libraries for efficient information access is known to foster top-level research – cf. the widespread use of semantic similarity measures in NLP, thanks to the availability of WordNet::Similarity (Pedersen et al., 2004). With the present contribution we aim to fill this gap in multilingual tools, providing a multi-tiered contribution consisting of (a) an Application Programming Interface (API) for efficiently accessing the information available in BabelNet (Navigli and 67 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 67–72, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics bn:00008364n WIKIWN 08420278n 85 WN:EN:bank WIKI:EN:Bank WIKI:DE:Bank WIKI:IT:Banca WIKIRED:DE:Finanzinstitut WN:EN:banking_company WNTR:ES:banco WNTR"
P12-3012,N10-1135,0,0.0634823,"Missing"
P12-3012,S10-1053,0,0.141569,"Missing"
P12-3012,P10-4014,0,0.0116172,"performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia include WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to BabelNet – i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet). Acknowledgments The authors gratefully acknowledge the sup"
P12-3012,W09-2413,0,\N,Missing
P12-3012,P10-1154,1,\N,Missing
P12-3012,S10-1002,0,\N,Missing
P12-3012,nastase-etal-2010-wikinet,0,\N,Missing
P13-1052,D10-1108,0,0.171777,"Missing"
P13-1052,H05-1071,0,0.0832276,"Missing"
P13-1052,N10-1087,0,0.102621,"Missing"
P13-1052,P08-1119,0,0.222104,"Missing"
P13-1052,N10-1088,0,0.276225,"f information focused on specific areas of knowledge. However, it is not infrequent that, when reading a domainspecific text, we humans do not know the meaning of one or more terms. To help the human understanding of specialized texts, repositories of textual definitions for technical terms, called glossaries, are compiled as reference resources within each domain of interest. Interestingly, electronic glossaries have been shown to be key resources not only for humans, but also in Natural Language Processing (NLP) tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Navigli et al., 2011; Velardi et al., 2013). Today large numbers of glossaries are available on the Web. However most such glossaries are small-scale, being made up of just some hundreds of definitions. Consequently, individual glossaries typically provide a partial view of a given domain. Moreover, there is no easy way of retrieving the subset of Web glossaries which appertains to a domain of interest. Although online services such 1 http://en.wikipedia.org/wiki/Portal:Contents/Glossaries 528 Proceedings of the 51st Annual Meeting of the Ass"
P13-1052,U08-1013,0,0.0382289,"Missing"
P13-1052,D12-1129,1,0.754784,"on specific areas of knowledge. However, it is not infrequent that, when reading a domainspecific text, we humans do not know the meaning of one or more terms. To help the human understanding of specialized texts, repositories of textual definitions for technical terms, called glossaries, are compiled as reference resources within each domain of interest. Interestingly, electronic glossaries have been shown to be key resources not only for humans, but also in Natural Language Processing (NLP) tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Navigli et al., 2011; Velardi et al., 2013). Today large numbers of glossaries are available on the Web. However most such glossaries are small-scale, being made up of just some hundreds of definitions. Consequently, individual glossaries typically provide a partial view of a given domain. Moreover, there is no easy way of retrieving the subset of Web glossaries which appertains to a domain of interest. Although online services such 1 http://en.wikipedia.org/wiki/Portal:Contents/Glossaries 528 Proceedings of the 51st Annual Meeting of the Association for Computational L"
P13-1052,P09-1045,0,0.116037,"Missing"
P13-1052,P00-1062,0,0.0805904,"efforts are currently producing large-scale encyclopedias, such as Wikipedia, which are proving very useful in NLP (Hovy et al., 2013). Interestingly, wikipedias also include manually compiled glossaries. However, such glossaries still suffer from the same above-mentioned problems, i.e., being incomplete or over-specific,1 and hard to customize according to a user’s needs. To automatically obtain large domain glossaries, over recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Fujii and Ishikawa, 2000). The former methods start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Webbased methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recurring cue te"
P13-1052,C92-2082,0,0.244343,"tomatically obtain large domain glossaries, over recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Fujii and Ishikawa, 2000). The former methods start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Webbased methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recurring cue terms occurring within dictionary and encyclopedic resources can be automatically extracted and incorporated into lexical patterns (Saggion, 2004). However, this approach is term-specific and does not scale to arbitrary terminologies and domains. In this paper we propose GlossBoot, a novel approach which reduces human intervention to a bare minimum and exploits the Web"
P13-1052,P10-1134,1,0.853105,"ledged domain glossary is not a speedy operation. Collaborative efforts are currently producing large-scale encyclopedias, such as Wikipedia, which are proving very useful in NLP (Hovy et al., 2013). Interestingly, wikipedias also include manually compiled glossaries. However, such glossaries still suffer from the same above-mentioned problems, i.e., being incomplete or over-specific,1 and hard to customize according to a user’s needs. To automatically obtain large domain glossaries, over recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al., 2012) or the Web (Fujii and Ishikawa, 2000). The former methods start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. Webbased methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textua"
P13-1052,P10-1029,0,0.445172,"Missing"
P13-1052,P06-1102,0,0.205266,"Missing"
P13-1052,P06-1015,0,0.360829,"Missing"
P13-1052,C02-1142,0,0.207304,"Missing"
P13-1052,P02-1006,0,0.207116,"Missing"
P13-1052,W12-3206,0,0.19774,"Missing"
P13-1052,saggion-2004-identifying,0,0.571276,"ual definitions for these terms from the input corpus using a supervised system. Webbased methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. To address the low-recall issue, recurring cue terms occurring within dictionary and encyclopedic resources can be automatically extracted and incorporated into lexical patterns (Saggion, 2004). However, this approach is term-specific and does not scale to arbitrary terminologies and domains. In this paper we propose GlossBoot, a novel approach which reduces human intervention to a bare minimum and exploits the Web to learn a We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our exp"
P13-1052,W02-1028,0,0.293448,"Missing"
P13-1052,J13-3007,1,0.798293,"reading a domainspecific text, we humans do not know the meaning of one or more terms. To help the human understanding of specialized texts, repositories of textual definitions for technical terms, called glossaries, are compiled as reference resources within each domain of interest. Interestingly, electronic glossaries have been shown to be key resources not only for humans, but also in Natural Language Processing (NLP) tasks such as Question Answering (Cui et al., 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012) and ontology learning (Navigli et al., 2011; Velardi et al., 2013). Today large numbers of glossaries are available on the Web. However most such glossaries are small-scale, being made up of just some hundreds of definitions. Consequently, individual glossaries typically provide a partial view of a given domain. Moreover, there is no easy way of retrieving the subset of Web glossaries which appertains to a domain of interest. Although online services such 1 http://en.wikipedia.org/wiki/Portal:Contents/Glossaries 528 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 528–538, c Sofia, Bulgaria, August 4-9 2013. 2013"
P13-1052,P95-1026,0,\N,Missing
P13-1120,J12-1003,0,0.0169728,"Missing"
P13-1120,D08-1007,0,0.0146252,"t similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) an"
P13-1120,P10-2020,0,0.0208286,"erage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument structure analysis (Surdeanu e"
P13-1120,P10-1046,0,0.0140309,"e different semantic classes of predicate arguments is closely related to the task of identifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable coll"
P13-1120,W04-3205,0,0.0196309,"the arguments demonstrates the ability of our method to generalize to a variety of semantic classes. 6 Related work The availability of Web-scale corpora has led to the production of large resources of relations (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Carlson et al., 2010; Fader et al., 2011). However, these resources often operate purely at the lexical level, providing no information on the semantics of their arguments or relations. Several studies have examined adding semantics through grouping relations into sets (Yates and Etzioni, 2009), ontologizing the arguments (Chklovski and Pantel, 2004), or ontologizing the relations themselves (Moro and Navigli, 2013). However, analysis has largely been either limited to ontologizing a small number of relation types with a fixed inventory, which potentially limits coverage, or has used implicit definitions of semantic categories (e.g., clusters of arguments), which limits interpretability. For example, Mohamed et al. (2011) use the semantic categories of the NELL system (Carlson et al., 2010) to learn roughly 400 valid ontologized relations from over 200M web pages, whereas WiSeNet (Moro and Navigli, 2012) leverages Wikipedia to acquire rel"
P13-1120,W06-1670,0,0.0160167,"EURYSM and I NSTITUTE FOR A DVANCED S TUDY FACULTY); ii) categories are mostly structured in a directed acyclic graph with multiple parents per category (even worse, cycles are possible in principle); iii) there is no clear way of identifying core semantic classes from the large set of available categories. Although efforts towards the taxonomization of Wikipedia categories do exist in the literature (Ponzetto and Strube, 2011; Nastase and Strube, 2013), the results are of a lower quality than a hand-built lexical resource. Therefore, as was done in previous work (Mihalcea and Moldovan, 2001; Ciaramita and Altun, 2006; Izquierdo et al., 2009; Erk and McCarthy, 2009; Huang and Riloff, 2010), we pick out our semantic classes C from WordNet and leverage its manually-curated taxonomy to associate our arguments with the most suitable class. This way we avoid building a new taxonomy and shift the problem to that of projecting the Wikipedia pages – associated with annotated filling arguments – to synsets in WordNet. We address this problem in two steps: Wikipedia-WordNet mapping. We exploit an existing mapping implemented in BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual semantic network that"
P13-1120,J02-2003,0,0.050686,"existed to date that contains ontologized lexical predicates. In contrast, the present work provides a high-coverage method for learning argument supertypes from a broadcoverage ontology (WordNet), which can potentially be leveraged in relation extraction to ontolo1229 gize relation arguments. Our method for identifying the different semantic classes of predicate arguments is closely related to the task of identifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we ov"
P13-1120,P13-1052,1,0.885483,"Missing"
P13-1120,D09-1046,0,0.0536418,"; ii) categories are mostly structured in a directed acyclic graph with multiple parents per category (even worse, cycles are possible in principle); iii) there is no clear way of identifying core semantic classes from the large set of available categories. Although efforts towards the taxonomization of Wikipedia categories do exist in the literature (Ponzetto and Strube, 2011; Nastase and Strube, 2013), the results are of a lower quality than a hand-built lexical resource. Therefore, as was done in previous work (Mihalcea and Moldovan, 2001; Ciaramita and Altun, 2006; Izquierdo et al., 2009; Erk and McCarthy, 2009; Huang and Riloff, 2010), we pick out our semantic classes C from WordNet and leverage its manually-curated taxonomy to associate our arguments with the most suitable class. This way we avoid building a new taxonomy and shift the problem to that of projecting the Wikipedia pages – associated with annotated filling arguments – to synsets in WordNet. We address this problem in two steps: Wikipedia-WordNet mapping. We exploit an existing mapping implemented in BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual semantic network that integrates Wikipedia and WordNet.3 Based on a d"
P13-1120,P07-1028,0,0.0200046,"ntifying the different semantic classes of predicate arguments is closely related to the task of identifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a"
P13-1120,D11-1142,0,0.214119,"experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categories of interest (The"
P13-1120,P13-4018,1,0.831545,"e i-th sense of w in WordNet with part of speech p. 5 According to the Wikipedia guidelines, “The article should begin with a short declarative sentence, answering two questions for the nonspecialist reader: What (or who) is the subject? and Why is this subject notable?”, extracted from http://en.wikipedia.org/wiki/ 4 we extract the hypernym from the textual definition of p by applying Word-Class Lattices (Navigli and Velardi, 2010, WCL6 ), a domain-independent hypernym extraction system successfully applied to taxonomy learning from scratch (Velardi et al., 2013) and freely available online (Faralli and Navigli, 2013). If a hypernym h is successfully extracted and h is linked to a Wikipedia page p0 for which µ(p0 ) is defined, then we extend the mapping by setting µ(p) := µ(p0 ). For instance, the mapping provided by BabelNet does not provide any link for the page Peter Spence; thanks to WCL, though, we are able to set the page Journalist as its hypernym, and link it to the WordNet synset journalist1n . This way our mapping extension now covers 539,954 pages, i.e., more than an order of magnitude greater than the number of pages originally covered by the BabelNet mapping. 3.3.2 Populating the Semantic Clas"
P13-1120,J12-1005,0,0.00748327,"Missing"
P13-1120,N03-1011,0,0.00918087,"nd recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categories of interest (Thelen and Riloff, 2002; Igo and Riloff, 2009), large domain glossaries (De Benedictis et al., 2013) and lists of concepts (Katz et al., 2003). Recently, th"
P13-1120,P04-1048,0,0.0198518,"; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument structure analysis (Surdeanu et al., 2003; Yakushiji et al., 2006) are two areas that are also related to our work. But their generality goes beyond our intentions, as we focus on semantic predicates, which is much simpler and free from syntactic parsing. Another closely related work is that of Hanks (2013) concerning the Theory of Norms and Exploitations, where norms (exploitations) represent expected (unexpected) classes for a given lexical predicate. Although our semantified predicates do, indeed, provide explicit evidence of norms obtained from collective intelligen"
P13-1120,C92-2082,0,0.0606079,"Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categories of interest (Thelen and Riloff, 2002; Igo and Riloff, 2009), large domain glossaries (De Benedictis et al., 2013) and lis"
P13-1120,P10-1029,0,0.00817429,"tly structured in a directed acyclic graph with multiple parents per category (even worse, cycles are possible in principle); iii) there is no clear way of identifying core semantic classes from the large set of available categories. Although efforts towards the taxonomization of Wikipedia categories do exist in the literature (Ponzetto and Strube, 2011; Nastase and Strube, 2013), the results are of a lower quality than a hand-built lexical resource. Therefore, as was done in previous work (Mihalcea and Moldovan, 2001; Ciaramita and Altun, 2006; Izquierdo et al., 2009; Erk and McCarthy, 2009; Huang and Riloff, 2010), we pick out our semantic classes C from WordNet and leverage its manually-curated taxonomy to associate our arguments with the most suitable class. This way we avoid building a new taxonomy and shift the problem to that of projecting the Wikipedia pages – associated with annotated filling arguments – to synsets in WordNet. We address this problem in two steps: Wikipedia-WordNet mapping. We exploit an existing mapping implemented in BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual semantic network that integrates Wikipedia and WordNet.3 Based on a disambiguation algorithm,"
P13-1120,W09-1703,0,0.0210769,"13) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categories of interest (Thelen and Riloff, 2002; Igo and Riloff, 2009), large domain glossaries (De Benedictis et al., 2013) and lists of concepts (Katz et al., 2003). Recently, the availability of Wikipedia and other collaborative resources has considerably boosted research on several aspects of knowledge acquisition (Hovy et al., 2013), leading to the creation of several large-scale knowledge resources, such as DBPedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013), MENTA (de Melo and Weikum, 2010), to name but a few. This wealth of acquired knowledge is known to have a positive impact on important fields such as Infor"
P13-1120,E09-1045,0,0.0473923,"Missing"
P13-1120,E12-1038,0,0.014152,"antic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument structure analysis (Surdeanu et al., 2003; Yakushiji e"
P13-1120,P10-1150,0,0.180514,"knowledge resources still lack semantic information about language units such as phrases and collocations. For instance, which semantic classes are expected as a direct object of the verb break? What kinds of noun does the adjective amazing collocate with? Recognition of the need for systems that are aware of the selectional restrictions of verbs and, more in general, of textual expressions, dates back to several decades (Wilks, 1975), but today it is more relevant than ever, as is testified by the current interest in semantic class learning (Kozareva et al., 2008) and supertype acquisition (Kozareva and Hovy, 2010). These approaches leverage lexico-syntactic patterns and input seeds to recursively learn the semantic classes of relation arguments. However, they require the manual selection of one or more seeds for each pattern of interest, and this selection influences the amount and kind of semantic classes to be learned. Furthermore, the learned classes are not directly linked to existing resources such as WordNet (Fellbaum, 1998) or Wikipedia. The goal of our research is to create a largescale repository of semantic predicates whose lexical arguments are replaced by their semantic classes. For example"
P13-1120,P08-1119,0,0.00957108,"07), Information Extraction (Krause However, these knowledge resources still lack semantic information about language units such as phrases and collocations. For instance, which semantic classes are expected as a direct object of the verb break? What kinds of noun does the adjective amazing collocate with? Recognition of the need for systems that are aware of the selectional restrictions of verbs and, more in general, of textual expressions, dates back to several decades (Wilks, 1975), but today it is more relevant than ever, as is testified by the current interest in semantic class learning (Kozareva et al., 2008) and supertype acquisition (Kozareva and Hovy, 2010). These approaches leverage lexico-syntactic patterns and input seeds to recursively learn the semantic classes of relation arguments. However, they require the manual selection of one or more seeds for each pattern of interest, and this selection influences the amount and kind of semantic classes to be learned. Furthermore, the learned classes are not directly linked to existing resources such as WordNet (Fellbaum, 1998) or Wikipedia. The goal of our research is to create a largescale repository of semantic predicates whose lexical arguments"
P13-1120,J98-2002,0,0.026129,"scale resource has existed to date that contains ontologized lexical predicates. In contrast, the present work provides a high-coverage method for learning argument supertypes from a broadcoverage ontology (WordNet), which can potentially be leveraged in relation extraction to ontolo1229 gize relation arguments. Our method for identifying the different semantic classes of predicate arguments is closely related to the task of identifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferenc"
P13-1120,D11-1134,0,0.0108575,"providing no information on the semantics of their arguments or relations. Several studies have examined adding semantics through grouping relations into sets (Yates and Etzioni, 2009), ontologizing the arguments (Chklovski and Pantel, 2004), or ontologizing the relations themselves (Moro and Navigli, 2013). However, analysis has largely been either limited to ontologizing a small number of relation types with a fixed inventory, which potentially limits coverage, or has used implicit definitions of semantic categories (e.g., clusters of arguments), which limits interpretability. For example, Mohamed et al. (2011) use the semantic categories of the NELL system (Carlson et al., 2010) to learn roughly 400 valid ontologized relations from over 200M web pages, whereas WiSeNet (Moro and Navigli, 2012) leverages Wikipedia to acquire relation synsets for an open set of relations. Despite these efforts, no large-scale resource has existed to date that contains ontologized lexical predicates. In contrast, the present work provides a high-coverage method for learning argument supertypes from a broadcoverage ontology (WordNet), which can potentially be leveraged in relation extraction to ontolo1229 gize relation"
P13-1120,P10-1134,1,0.737494,"is issue, for each unmapped Wikipedia page p we obtain its textual definition as the first sentence of the page.5 Next, 3 http://babelnet.org We follow (Navigli, 2009) and denote with wpi the i-th sense of w in WordNet with part of speech p. 5 According to the Wikipedia guidelines, “The article should begin with a short declarative sentence, answering two questions for the nonspecialist reader: What (or who) is the subject? and Why is this subject notable?”, extracted from http://en.wikipedia.org/wiki/ 4 we extract the hypernym from the textual definition of p by applying Word-Class Lattices (Navigli and Velardi, 2010, WCL6 ), a domain-independent hypernym extraction system successfully applied to taxonomy learning from scratch (Velardi et al., 2013) and freely available online (Faralli and Navigli, 2013). If a hypernym h is successfully extracted and h is linked to a Wikipedia page p0 for which µ(p0 ) is defined, then we extend the mapping by setting µ(p) := µ(p0 ). For instance, the mapping provided by BabelNet does not provide any link for the page Peter Spence; thanks to WCL, though, we are able to set the page Journalist as its hypernym, and link it to the WordNet synset journalist1n . This way our ma"
P13-1120,N07-1071,0,0.0260684,"preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction ("
P13-1120,P06-1100,0,0.0258474,"contains ontologized lexical predicates. In contrast, the present work provides a high-coverage method for learning argument supertypes from a broadcoverage ontology (WordNet), which can potentially be leveraged in relation extraction to ontolo1229 gize relation arguments. Our method for identifying the different semantic classes of predicate arguments is closely related to the task of identifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class"
P13-1120,P10-1044,0,0.00588083,"o it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument"
P13-1120,P99-1014,0,0.048252,"ntifying selectional preferences. The most similar approaches to it are taxonomybased ones, which leverage the semantic types of the relations arguments (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pennacchiotti and Pantel, 2006). Nevertheless, despite their high quality sense-tagged data, these methods have often suffered from lack of coverage. As a result, alternative approaches have been proposed that eschew taxonomies in favor of rating the quality of potential relation arguments (Erk, 2007; Chambers and Jurafsky, 2010) or generating probability distributions over the arguments (Rooth et al., 1999; Pantel et al., 2007; Bergsma et al., 2008; Ritter et al., 2010; S´eaghdha, 2010; Bouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Ve"
P13-1120,P10-1045,0,0.0187805,"Missing"
P13-1120,P12-3013,0,0.00803867,"predicate (such as break ∗), our method produces a probability distribution over the set of semantic classes (thus covering the different expected meanings for the filling arguments) and is able to classify new instances with the most suitable class. Our experiments show generally high performance, also in comparison with previous work on argument supertyping. We hope that our semantic predicates will enable progress in different Natural Language Processing tasks such as Word Sense Disambiguation (Navigli, 2009), Semantic Role Labeling (F¨urstenau and Lapata, 2012) or even Textual Entailment (Stern and Dagan, 2012) – each of which is in urgent need of reliable semantics. While we focused on semantifying lexical predicates, as future work we will apply our method to the ontologization of large amounts of sequences of words, such as phrases or textual relations (e.g., considering Google n-grams appearing in Wikipedia). Notably, our method should, in principle, generalize to any semantically-annotated corpus (e.g., Wikipedias in other languages), provided lexical predicates can be extracted with associated semantic classes. In order to support future efforts we are releasing our semantic predicates as a fr"
P13-1120,P03-1002,0,0.010858,"ouma, 2010; Jang and Mostow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument structure analysis (Surdeanu et al., 2003; Yakushiji et al., 2006) are two areas that are also related to our work. But their generality goes beyond our intentions, as we focus on semantic predicates, which is much simpler and free from syntactic parsing. Another closely related work is that of Hanks (2013) concerning the Theory of Norms and Exploitations, where norms (exploitations) represent expected (unexpected) classes for a given lexical predicate. Although our semantified predicates do, indeed, provide explicit evidence of norms obtained from collective intelligence and would provide support for this theory, exploitations prese"
P13-1120,W02-1028,0,0.0589612,"011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categories of interest (Thelen and Riloff, 2002; Igo and Riloff, 2009), large domain glossaries (De Benedictis et al., 2013) and lists of concepts (Katz et al., 2003). Recently, the availability of Wikipedia and other collaborative resources has considerably boosted research on several aspects of knowledge acquisition (Hovy et al., 2013), leading to the creation of several large-scale knowledge resources, such as DBPedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013), MENTA (de Melo and Weikum, 2010), to name but a few. This wealth of acquired knowledge is known to have a positive impact on importa"
P13-1120,J13-3007,1,0.80568,"atch the predicate and abstract its arguments to general semantic classes (e.g., break B ODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to e"
P13-1120,P10-1013,0,0.166072,"EEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic lexicons for categor"
P13-1120,W06-1634,0,0.0262116,"stow, 2012) in order to obtain higher coverage of preferences. In contrast, we overcome the data sparsity of class-based models by leveraging the large quantity of collaboratively-annotated Wikipedia text in order to connect predicate arguments with their semantic class in WordNet using BabelNet (Navigli and Ponzetto, 2012); because we map directly to WordNet synsets, we provide a more readilyinterpretable collocation preference model than most similarity-based or probabilistic models. Verb frame extraction (Green et al., 2004) and predicate-argument structure analysis (Surdeanu et al., 2003; Yakushiji et al., 2006) are two areas that are also related to our work. But their generality goes beyond our intentions, as we focus on semantic predicates, which is much simpler and free from syntactic parsing. Another closely related work is that of Hanks (2013) concerning the Theory of Norms and Exploitations, where norms (exploitations) represent expected (unexpected) classes for a given lexical predicate. Although our semantified predicates do, indeed, provide explicit evidence of norms obtained from collective intelligence and would provide support for this theory, exploitations present a more difficult task,"
P13-1120,P95-1026,0,0.603607,"arguments, we need a large corpus of a definitional nature; second, we need wide-coverage semantic annotations of filling arguments. 3.2 Disambiguation of Filling Arguments The objective of the second step is to disambiguate as many arguments in Lπ as possible for the lex1223 2 We will also refer to l as the sense of a in sentence s. a a a a a a full [[bottle]] nice hot [[cup]] cold [[glass]] very big bottle brand constituent of milk of milk of milk of milk of milk of milk gets propagated to our ambiguous item as well. This heuristic mimes the one-sense-percollocation heuristic presented in (Yarowsky, 1995). • Trust the inventory: if Wikipedia provides only one sense for a, i.e., only one page title whose lemma is a, link a to that page. Consider the instance “At that point, Smith threw down a cup of Gatorade” in page Jimmy Clausen; there is only one sense for Gatorade in Wikipedia, so we link the unannotated occurrence to it. Table 1: An excerpt of the token sequences which match the lexical predicate a * of milk in Wikipedia (filling argument shown in the second column; following the Wikipedia convention we provide links in double square brackets). ical predicate π. We denote Dπ = {(a, s, l) :"
P13-1120,N07-4013,0,0.0715023,"ODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 1 Not only are these knowledge resources obtained by acquiring concepts and named entities, but they also provide semantic relations between them. These relations are extracted from unstructured or semi-structured text using ontology learning from scratch (Velardi et al., 2013) and Open Information Extraction techniques (Etzioni et al., 2005; Yates et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Moro and Navigli, 2013) which mainly stem from seminal work on is-a relation acquisition (Hearst, 1992) and subsequent developments (Girju et al., 2003; Pasca, 2004; Snow et al., 2004, among others). Introduction Acquiring semantic knowledge from text automatically is a long-standing issue in Computational Linguistics and Artificial Intelligence. Over the last decade or so the enormous abundance of information and data that has become available has made it possible to extract huge amounts of patterns and named entities (Etzioni et al., 2005), semantic l"
P13-1132,N09-1003,0,0.0486211,"Missing"
P13-1132,S12-1051,0,0.29581,"ky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, which enables meaningful similarity comparisons across different scales of text or lexical levels. Second, by operating at the sense level, a unified approach is able to identify the semantic similarities that exist independently of the text’s lexical forms and any semantic ambiguity therein. For example, consider the sentences: t1. A manager fired the worker. t2. An emplo"
P13-1132,S12-1059,0,0.0305138,"Missing"
P13-1132,C10-1005,0,0.0137954,"Missing"
P13-1132,J12-1003,0,0.0112526,"the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely"
P13-1132,P11-2087,0,0.0296674,", word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic simila"
P13-1132,J06-1003,0,0.937818,"2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense infor"
P13-1132,W05-1203,0,0.293606,"vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional features. As can be seen from Table 3, our alignmentbased disambiguatio"
P13-1132,J13-3008,1,0.741191,"Missing"
P13-1132,P08-2067,0,0.0268098,"an and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, w"
P13-1132,J12-1005,0,0.0406973,"Missing"
P13-1132,N06-2015,0,0.0347272,"(Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455 0.218 0.396 Noun 0.450 0.483 0.478 NA NA SE-2 Verb 0.465 0.482 0.473 NA NA Adj 0.484 0.531 0.501 0.473 0.371 Onto + SE"
P13-1132,D07-1061,0,0.146501,"andom walks beginning at that node will produce a frequency distribution over the nodes in the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala,"
P13-1132,O97-1002,0,0.0927604,"icks the most relevant sense of the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our"
P13-1132,S01-1004,0,0.0108658,"t::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455"
P13-1132,P98-2127,0,0.0658275,"f the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional feat"
P13-1132,magnini-cavaglia-2000-integrating,0,0.0196462,"Missing"
P13-1132,W06-2503,0,0.0213638,"my of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources s"
P13-1132,P06-1014,1,0.932281,"tories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic sig"
P13-1132,N04-3012,0,0.0975539,"respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs"
P13-1132,N13-1130,1,0.864485,"Missing"
P13-1132,W09-3204,0,0.101622,"the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses"
P13-1132,2003.mtsummit-papers.42,0,0.0659828,"ot include the mistakes made when the Jaccard measure was used as they vary with the k value. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be consistent with the previous literature (Hughes and Ramage, 2007; Agirre et al., 2009), we used Spearman’s rank correlation in our experiment. that of Rapp (2003) uses word senses, an approach that is outperformed by our method. The errors produced by our system were largely the result of sense locality in the WordNet network. Table 5 highlights the incorrect responses. The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness. For example, percentage may be interpreted colloquially as monetary value (e.g., “give me my percentage”) and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym. 4.1 4.3 Experimental Setup Our alignment-based sense disambiguatio"
P13-1132,D07-1107,0,0.352211,"ation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we describe our alignment-based disambiguation algorithm which enables us to produce sense-based semantic signatures for those lexical items (e.g., words or sentences) which are not sense annotated (Section 2.2). Finally, we propose three methods for comparin"
P13-1132,J11-2003,0,0.012897,"t multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Hal"
P13-1132,S12-1060,0,0.0804335,"Missing"
P13-1132,P95-1026,0,0.288603,"of the context words in both texts. To find this maximum we use an alignment procedure which, for each word type wi in item T1 , assigns wi to the sense that has the maximal similarity to any sense of the word types in the compared text T2 . Algorithm 1 formalizes the alignment process, which produces a sense disambiguated representation as a result. Senses are compared in terms of their semantic signatures, denoted as function R. We consider multiple definitions of R, defined later in Section 2.3. As a part of the disambiguation procedure, we leverage the one sense per discourse heuristic of Yarowsky (1995); given all the word types in two compared lexical items, each type is assigned a single sense, even if it is used multiple times. Additionally, if the same word type appears in both sentences, both will always be mapped to the same sense. Although such a sense assignment is potentially incorrect, assigning both types to the same sense results in a representation that does no worse than a surface-level comparison. We illustrate the alignment-based disambiguation procedure using the two example sentences t1 and t2 given in Section 1. Figure 1(a) illustrates example alignments of the first sense"
P13-1132,agirre-de-lacalle-2004-publicly,0,\N,Missing
P13-1132,C98-2122,0,\N,Missing
P13-4018,P10-1134,1,0.89376,"e is a directed acyclic graph, a subclass of non-deterministic finite state automata. The purpose of the lattice structure is to preserve (in a compact form) the salient differences among distinct sequences. In this paper we present a demonstration of Word-Class Lattices by providing a Java API and a Web application for online usage. Since multilinguality is a key need in today’s information society, and because WCLs have been tested overwhelmingly only with the English language, we provide experiments for three different languages, namely English, French and Italian. To do so, in contrast to Navigli and Velardi (2010), who created a manually annotated training set of definitions, we provide a heuristic method for the automatic acquisition of reliable training sets from Wikipedia, and use them to determine the robustness and generalization power of WCLs. We show high performance in definition and hypernym extraction for our three languages. In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated"
P13-4018,W09-4405,0,0.0512439,"Missing"
P13-4018,P99-1016,0,0.0641655,"is the string “classifier” which corresponds to the hypernym extracted by WCL for the input sentence “WCL is a kind of classifier”. 4.1 Web user interface We also release a Web interface to enable online usage of our WCLs for the English, French and Italian languages. In Figure 3 we show a screenshot of our Web interface. The user can type the Available from http://lcl.uniroma1.it/wcl 106 Figure 3: A screenshot of the WCL Web interface. Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, t"
P13-4018,P13-1052,1,0.785943,"Missing"
P13-4018,saggion-2004-identifying,0,0.615858,"odel used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that"
P13-4018,A92-1011,0,0.127819,"esponds to the hypernym extracted by WCL for the input sentence “WCL is a kind of classifier”. 4.1 Web user interface We also release a Web interface to enable online usage of our WCLs for the English, French and Italian languages. In Figure 3 we show a screenshot of our Web interface. The user can type the Available from http://lcl.uniroma1.it/wcl 106 Figure 3: A screenshot of the WCL Web interface. Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifier based on these featu"
P13-4018,N10-1088,0,0.17213,"automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manu"
P13-4018,D12-1129,1,0.869577,"Missing"
P13-4018,storrer-wellinghoff-2006-automated,0,0.0258223,"4-9 2013. 2013 Association for Computational Linguistics [In geography, a country]DF [is]V F [a political division]GF . [In finance, a bond]DF [is]V F [a negotiable certificate]GF [that that acknowledges. . . ]REST . [In poetry, a foot]DF [is]V F [a measure]GF [, consisting. . . ]REST . Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italics). geography finance poetry In NN1 division certificate measure , a hTARGETi a country bond foot JJ NN2 political negotiable Figure 1: The DF and GF Word-Class Lattices for the sentences in Table 1. following fields (Storrer and Wellinghoff, 2006): definiendum (DF), definitor (VF), definiens (GF) and rest (REST), where DF is the part of the definition including the word being defined (e.g., “In computer science, a closure”), VF is the verb phrase used to introduce the definition (e.g., “is”), GF usually includes the hypernym (e.g., “a firstclass function”, hypernym marked in italics) and RF includes additional clauses (e.g., “with free variables that are bound in the lexical environment”). Consider a set of training sentences T , each of which is automatically part-of-speech tagged and manually bracketed with the DF, VF, GF and REST fi"
P13-4018,P13-1120,1,0.836388,"se of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be au"
P13-4018,J13-3007,1,0.836701,"ernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of"
P13-4018,C92-2082,0,0.441042,"lp when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the 2 Word-Class Lattices In this section we briefly summarize Word-Class Lattices, originally introduced by Navigli and Velardi (2010). 2.1 Definitional Sentence Generalization WCL relies on a formal notion of textual definition. Specifically, given a definition, e.g.: “In computer science, a closure is a first-class function with free variables that are bound in the lexical environment”, we assume that it contains the"
P13-4018,W09-4410,0,0.0452283,"Missing"
P13-4018,P10-1150,0,0.134627,"tructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the 2 Word-Class Lattices In this section we briefly summarize Word-Class Lattices, originally introduced by Navigli and Velardi (2010). 2.1 Definitional Sentence Generalization WCL relies on a formal notion of textual definition. Specifically, given a definition, e.g.: “In computer science, a closure is a first-class function"
P14-1044,E09-1005,0,0.00560636,"nk (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective res"
P14-1044,D07-1061,0,0.0346618,"lk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first rep"
P14-1044,W98-0710,0,0.132083,"ely for aligning new pairs of resources for which no training data is available, with state-of-the-art performance. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying"
P14-1044,de-melo-weikum-2010-providing,0,0.0430328,"Missing"
P14-1044,R09-1039,0,0.0170718,"rs of resources for which no training data is available, with state-of-the-art performance. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two conce"
P14-1044,D12-1129,1,0.0403312,"and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective resources L1 and L2 (Figure 1(a"
P14-1044,Q13-1013,0,0.114181,"WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them. To do this, recent work has proposed graph construction by monosemous linking, where a concept is linked to all the concepts associated with the monosemous words in its definition (Matuschek and Gurevych, 2013). However, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated. In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure. Thanks to a novel modeling of the sense entries and an effective ontologization algorithm, our approach also fares well when resources lack r"
P14-1044,P14-1089,1,0.0410164,"raphs on the basis of their shortest distance. To gain more insight into the effectiveness of our 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for"
P14-1044,I11-1099,0,0.414615,"Robust Approach to Aligning Heterogeneous Lexical Resources Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {pilehvar,navigli}@di.uniroma1.it Abstract Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have f"
P14-1044,E12-1059,0,0.245016,"of a conifer”. The definition contains two content words: fruitn and conifern . The latter word is monosemous in Wiktionary, hence we directly connect cone4n to the only sense of conifern . The noun fruit, however, has 5 senses in Wiktionary. We therefore measure the similarity between the definition of cone4n and all the 5 definitions of fruit and introduce a link from cone4n to the sense of fruit which yields the maximal similarity value (defined as “(botany) The seedbearing part of a plant...”). (WP), Wiktionary (WT), and OmegaWiki (OW). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the information in the foregoing three resources. For WP, WT , OW we used the dump versions 20090822, 20131002, and 20131115, respectively. 4 For ontologizing WT and OW, the bag of content words W is given by the content words in sense definitions and, if available, additional related words obtained from lexicon relations (see Section 3). In WT, both of these are in word surface form and hence had to be disambiguated. For OW , however, the encoded relations, though relaEvaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evalu"
P14-1044,C12-1108,0,0.0563781,"Missing"
P14-1044,P06-1014,1,0.806119,"Missing"
P14-1044,W11-0122,0,0.189115,"g Heterogeneous Lexical Resources Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {pilehvar,navigli}@di.uniroma1.it Abstract Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into s"
P14-1044,P13-1132,1,0.379781,"alculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corres"
P14-1044,W08-2231,0,0.173234,"ormalizes the alignment process: the algorithm takes as input the semantic graphs G1 and G2 corresponding to the two resources, as explained above, and produces as output an alignment in the form of a set A of concept pairs. The algorithm iterates over all concepts c1 ∈ V1 and, for each of them, obtains the set of concepts C ⊂ V2 , which can be considered as alignment candidates for c1 (line 3). For a concept c1 , alignment candidates in G2 usually consist of every concept c2 ∈ V2 that shares at least one lexicalization with c1 in the same part of speech tag, i.e., LG1 (c1 ) ∩ LG2 (c2 ) 6= ∅ (Reiter et al., 2008; Meyer and Gurevych, 2011). Once the set of target candidates C for a source concept c1 is obtained, the alignment task can be cast as that of identifying those concepts in C to which c1 should be aligned. To do this, the algorithm calculates the similarity between c1 and each c2 ∈ C (line 5). If their similarity score exceeds a certain value denoted by θ 469 Figure 1: The process of measuring the similarity of a pair of concepts across two resources. The method consists of two components: definitional and structural similarities, each measuring a similarity score for the given concept pair."
P14-1089,P81-1030,0,0.338999,"in 3 phases: Fiona NNP Roberts NNP is an VBZ DT amod American JJ actress NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page J ULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in Figure 1. The noun involved in the copula relation i"
P14-1089,P13-1120,1,0.891779,"Missing"
P14-1089,C92-2082,0,0.279354,"us nature of the hypernyms, something that we only do for categories, due to their noisy network. While WikiNet and MENTA bring together the knowledge available both at the page and category level, like we do, they either achieve low precision and coverage of the taxonomical structure or exhibit overly general hypernyms, as we show in our experiments in the next section. Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of"
P14-1089,C73-2005,0,0.518574,"ikipedia bitaxonomy, i.e., a taxonomy of pages and categories, in 3 phases: Fiona NNP Roberts NNP is an VBZ DT amod American JJ actress NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page J ULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in"
P14-1089,D10-1108,0,0.405502,"either achieve low precision and coverage of the taxonomical structure or exhibit overly general hypernyms, as we show in our experiments in the next section. Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of gene"
P14-1089,C82-2013,0,0.751282,"iona NNP Roberts NNP is an VBZ DT amod American JJ actress NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page J ULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in Figure 1. The noun involved in the copula relation is actress and thus"
P14-1089,E09-1064,0,0.0104175,"an integrated taxonomy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a g"
P14-1089,nastase-etal-2010-wikinet,0,0.17255,"Missing"
P14-1089,P10-1134,1,0.728509,"es of semantic relation 945 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945–955, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics nsubj level are mutually beneficial for inducing a widecoverage and fine-grained integrated taxonomy. 2 Julia NNP We induce a Wikipedia bitaxonomy, i.e., a taxonomy of pages and categories, in 3 phases: Fiona NNP Roberts NNP is an VBZ DT amod American JJ actress NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular"
P14-1089,P14-1122,1,0.843337,"Missing"
P14-1089,N04-1041,0,0.0515668,"ng work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other"
P14-1089,J13-3007,1,0.623322,"onomical structure or exhibit overly general hypernyms, as we show in our experiments in the next section. Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedi"
P14-1089,W10-0911,0,0.0126249,"my of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuabl"
P14-1089,P06-1101,0,0.0495666,"a only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as"
P14-1122,J08-4004,0,0.0131135,"Missing"
P14-1122,J06-1003,0,0.00591089,"ated to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional"
P14-1122,W02-0817,0,0.0697425,"an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically a"
P14-1122,eom-etal-2012-using,0,0.0152073,"has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poes"
P14-1122,P14-1089,1,0.859176,"Missing"
P14-1122,P09-2053,0,0.542226,"Missing"
P14-1122,N06-1058,0,0.0155935,"ing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. Th"
P14-1122,P10-1023,1,0.342133,"propose a cost-effective method of validating and extending knowledge bases using video games with a purpose. Two video games were created to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build"
P14-1122,D12-1128,1,0.698726,"Missing"
P14-1122,W11-0122,0,0.00823163,"knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using"
P14-1122,P13-1132,1,0.654556,"image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify nove"
P14-1122,P06-1101,0,0.0307866,"tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream app"
P14-1122,W13-0215,0,0.254243,"te for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically acquired common sense relations using a slot machin"
P14-1122,D08-1056,0,0.0232994,"e most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game’s objectives, which potentially decreases motivation for answering correctly. Several works have proposed adapting existing word-based board game designs to create or validate common sense knowledge. von Ahn et al. (2006) generate common sense facts by using a game similar to TabooTM , where one player must list facts about a computer-selected lemma and a second player must guess the original lemma having seen only the facts. Similarly, Vickrey et al. (2008) gather free associations to a target word with the constraint, similar to TabooTM , where players cannot enter a small set of banned words. Vickrey et al. (2008) also present two games similar to the ScattergoriesTM , where players are given a category and then must list things in that category. The two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extendi"
P14-5012,J13-3008,1,0.891385,"Missing"
P14-5012,P98-2127,0,0.143022,", searchResults, wordClusters, AssociationMetric.DEGREE_OVERLAP); The first line obtains an instance of the class which performs the association between search result snippets and the word clusters obtained from the WSI algorithm. The second line calls the association method associateSnippet which inputs the target word, the search results obtained from the search engine, the word clusters and, finally, the kind of metric to use for the association. Three different association metrics are implemented in the toolkit: We also provide an implementation of a word clustering algorithm, i.e. Lin98 (Lin, 1998), which does not rely on co-occurrence graphs, but just on the word co-occurrence information to iteratively refine word clusters on the basis of their “semantic” relationships. A programmatic example of use of the B-MST WSI algorithm is as follows: BMST mst = new BMST(g); mst.makeClustering(); Clustering wordClusters = mst.getClustering(); • WORD OVERLAP performs the association by maximizing the size of the intersection between the word sets in each snippet and the word clusters; • DEGREE OVERLAP performs the association by calculating for each word cluster the sum where g is a co-occurrence"
P14-5012,D10-1012,1,0.926906,"ch Result Clustering and Diversification Daniele Vannella, Tiziano Flati and Roberto Navigli Dipartimento di Informatica Sapienza Universit`a di Roma {vannella,flati,navigli}@di.uniroma1.it Abstract Diversification and Web clustering algorithms, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity. Recently, it has been shown that the automatic acquisition of the meanings of a word of interest, a task referred to as Word Sense Induction, can be successfully integrated into search result clustering and diversification (Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013) so as to outperform non-semantic state-of-the-art Web clustering systems. In this demonstration we describe a new toolkit for Word Sense Induction, called WoSIT, which i) provides ready implementations of existing WSI algorithms; ii) can be extended with additional WSI algorithms; iii) enables the integration of WSI algorithms into search result clustering and diversification, thereby providing an extrinsic evaluation tool. As a result the toolkit enables the objective comparison of WSI algorithms within an end-user application in terms of the degree of diversific"
P14-5012,S13-2035,1,0.902798,"ery q, we build a co-occurrence graph Gq = (V, E) such that V is the set of words co-occurring with q and E is the set of undirected edges, each denoting a co-occurrence between pairs of words in V . In Figure 2 we show an example of a co-occurrence graph for the target word excalibur. WoSIT enables the creation of the cooccurrence graph either programmatically, by adding edges and vertices according to any userspecific algorithm, or starting from the statistics for co-occurring words obtained from a cooccurrence database (created, e.g., from a text corpus, as was done by Di Marco and Navigli (2013)). g.saveToTxt(fileName); g = WordGraph.loadFromTxt(fileName); We are now ready to provide our co-occurrence graph, created with just a few lines of code, as input to a WSI algorithm, as will be explained in the next section. 2.1.2 Discovery of Word Senses Once the co-occurrence graph for the query q is built, it can be input to any WSI algorithm which extends the GraphClusteringAlgorithm class in the toolkit. WoSIT comes with a number of ready-to-use such algorithms, among which: 68 King Arthur makeClustering method implements the induction algorithm and creates the word clusters, which can t"
P14-5012,W06-3812,0,0.0435657,"Excalibur 0.02 0.012 Book 0.013 0.006 0.005 Film Figure 2: Example of a co-occurrence graph for the word excalibur. public void makeClustering(); public Clustering getClustering(); • Balanced Maximum Spanning Tree (BMST) (Di Marco and Navigli, 2013), an extension of a WSI algorithm based on the calculation of a Maximum Spanning Tree (Di Marco and Navigli, 2011) aimed at balancing the number of co-occurrences in each sense cluster. • HyperLex (V´eronis, 2004), an algorithm which identifies hubs in co-occurrence graphs, thereby identifying basic meanings for the input query. • Chinese Whispers (Biemann, 2006), a randomized algorithm which partitions nodes by means of the iterative transfer of word sense information across the co-occurrence graph (Biemann, 2006). • Squares, Triangles and Diamonds (SquaT++) (Di Marco and Navigli, 2013), an extension of the SquaT algorithm (Navigli and Crisafulli, 2010) which exploits three cyclic graph patterns to determine and discard those vertices (or edges) with weak degree of connectivity in the graph. As a result, the new algorithm is readily integrated into the WoSIT toolkit. 2.2 Semantically-enhanced Search Result Clustering and Diversification We now move t"
P14-5012,C98-2122,0,\N,Missing
P15-1010,N09-1003,0,0.305209,"Missing"
P15-1010,N13-1092,0,0.0947633,"Missing"
P15-1010,P14-1023,0,0.389493,"of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dataset for the training of sense embeddings. of lexical-semantic knowledge bases, makes embeddings significan"
P15-1010,C14-1048,0,0.0182086,"yer and hence significantly speeding up the training process. Other related work includes GloVe (Pennington et al., 2014), which is an effort to make the vector dimensions in word embeddings explicit, and the approach of Bordes et al. (2013), which trains word embeddings on the basis of relationship information derived from WordNet. Several techniques have been proposed for transforming word embeddings to the sense level. Chen et al. (2014) leveraged word embeddings in Word Sense Disambiguation and investigated the possibility of retrofitting embeddings with the resulting disambiguated words. Guo et al. (2014) exploited parallel data to automatically generate sense-annotated data, based on the fact that different senses of a word are usually translated to different words in another language (Chan and Ng, 2005). The automatically-generated senseannotated data was later used for training sensespecific word embeddings. Huang et al. (2012) 6 Conclusions and Future Work We proposed an approach for obtaining continuous representations of individual word senses, referred to as sense embeddings. Based on the proposed sense embeddings and the knowledge obtained from a large-scale lexical resource, i.e., Bab"
P15-1010,J15-4004,0,0.0681504,"Missing"
P15-1010,P12-1092,0,0.556672,"Navigli Department of Computer Science Sapienza University of Rome {iacobacci,pilehvar,navigli}@di.uniroma1.it Abstract model distinct meanings of a word as they conflate the contextual evidence of different meanings of a word into a single vector; and (2) they base their representations solely on the distributional statistics obtained from corpora, ignoring the wealth of information provided by existing semantic resources. Several research works have tried to address these problems. For instance, basing their work on the original sense discrimination approach of Reisinger and Mooney (2010), Huang et al. (2012) applied K-means clustering to decompose word embeddings into multiple prototypes, each denoting a distinct meaning of the target word. However, the sense representations obtained are not linked to any sense inventory, a mapping that consequently has to be carried out either manually, or with the help of sense-annotated data. Another line of research investigates the possibility of taking advantage of existing semantic resources in word embeddings. A good example is the Relation Constrained Model (Yu and Dredze, 2014). When computing word embeddings, this model replaces the original co-occurre"
P15-1010,J06-1003,0,0.0667063,"a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α (Section 3.3.1) along with a graph vicinity factor flag (Section 3.3.2). 14: 15: 16: 17: 18: 3.3.1 Similarity measurement strategy We take two strategies for calculating the similarity of the given words w1 and w2 . Let Sw1 and Sw2 be the sets of senses associated with the two respective input words w1 and w2 , and let s~i be the sense embedding vector of the sense si . In the first strategy, which we refer to as closest, we follow the conventional approach (Budanitsky and Hirst, 2006) and measure the similarity of the two words as the similarity of their closest senses, i.e.: Simclosest (w1 , w2 ) = max T (s~1 , s~2 ) s1 ∈Sw1 s2 ∈Sw2 Sw1 ← getSenses(w1 ), Sw2 ← getSenses(w2 ) if Str is closest then sim ← -1 else sim ← 0 end if for each s1 ∈ Sw1 and s2 ∈ Sw2 do if Vic is true then tmp ← T ∗ (s~1 ,s~2 ) else tmp ← T (s~1 ,s~2 ) end if if Str is closest then sim ← max (sim, tmp) else sim ← sim + tmpα × d(s1 ) × d(s2 ) end if end for two words are assigned the similarity judgement of 6.27, which is slightly above the middle point in the similarity scale [0,10] of the dataset."
P15-1010,S12-1047,0,0.284454,"factor flag α parameter for the weighted strategy Output: The similarity between w1 and w2 Vector comparison For comparing vectors, we use the Tanimoto distance. The measure is a generalization of Jaccard similarity for real-valued vectors in [-1, 1]: T (w~1 , w~2 ) = kw~1 k2 w~1 · w~2 + kw~2 k2 − w~1 · w~2 (1) 1: where w~1 · w~2 is the dot product of the vectors w~1 and w~2 and kw~1 k is the Euclidean norm of w~1 . Rink and Harabagiu (2013) reported consistent improvements when using vector space metrics, in particular the Tanimoto distance, on the SemEval-2012 task on relational similarity (Jurgens et al., 2012) in comparison to several other measures that are designed for probability distributions, such as Jensen-Shannon divergence and Hellinger distance. 3.3 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Word similarity 13: We show in Algorithm 1 our procedure for measuring the semantic similarity of a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α (Section 3.3.1) along with a graph vicinity factor flag (Section 3.3.2). 14: 15: 16: 17: 18: 3.3.1 Similarity measurement strategy We take two strategies for calculating the sim"
P15-1010,W14-1618,0,0.0376758,"in terms of the closest word embeddings among all the corresponding synonyms obtained with the expansion procedure (cf. Section 3.1). A comparison of word and sense embeddings in the vanilla setting (with neither the expansion procedure nor graph vicinity factor) indicates the consistent advantage gained by moving from word Comparison systems. We compare our results against six other systems and the PMI baseline provided by the task organizers. As for systems that use word embeddings for measuring relational similarity, we report results for RNN-1600 (Mikolov et al., 2013c) and PairDirection (Levy and Goldberg, 2014). We also report results for UTD-NB and UTD-SVM (Rink and Harabagiu, 2012), which rely on lexical pattern classification based on Na¨ıve Bayes and Support Vector Machine classifiers, respectively. UTD-LDA (Rink and Harabagiu, 2013) is another system presented by the same authors that casts the task as a selectional preferences one. Finally, we show the performance of Com (Zhila et al., 2013), a system that combines Word2vec, lexical patterns, and knowledge base information. Similarly to the word similarity experiments, we also report a baseline based on word embeddings (Word2vec) trained on th"
P15-1010,D14-1110,0,0.812054,"ve semantic similarity measurement. We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for"
P15-1010,S12-1055,0,0.0233277,"yms obtained with the expansion procedure (cf. Section 3.1). A comparison of word and sense embeddings in the vanilla setting (with neither the expansion procedure nor graph vicinity factor) indicates the consistent advantage gained by moving from word Comparison systems. We compare our results against six other systems and the PMI baseline provided by the task organizers. As for systems that use word embeddings for measuring relational similarity, we report results for RNN-1600 (Mikolov et al., 2013c) and PairDirection (Levy and Goldberg, 2014). We also report results for UTD-NB and UTD-SVM (Rink and Harabagiu, 2012), which rely on lexical pattern classification based on Na¨ıve Bayes and Support Vector Machine classifiers, respectively. UTD-LDA (Rink and Harabagiu, 2013) is another system presented by the same authors that casts the task as a selectional preferences one. Finally, we show the performance of Com (Zhila et al., 2013), a system that combines Word2vec, lexical patterns, and knowledge base information. Similarly to the word similarity experiments, we also report a baseline based on word embeddings (Word2vec) trained on the same corpus and with the same settings as S ENS E MBED. 102 adopted a si"
P15-1010,W13-0118,0,0.0802148,"ow the nth sense of the word with part of speech x as wordxn . 6 97 3.2 Algorithm 1 Word Similarity Input: Two words w1 and w2 Str, the similarity strategy Vic, the graph vicinity factor flag α parameter for the weighted strategy Output: The similarity between w1 and w2 Vector comparison For comparing vectors, we use the Tanimoto distance. The measure is a generalization of Jaccard similarity for real-valued vectors in [-1, 1]: T (w~1 , w~2 ) = kw~1 k2 w~1 · w~2 + kw~2 k2 − w~1 · w~2 (1) 1: where w~1 · w~2 is the dot product of the vectors w~1 and w~2 and kw~1 k is the Euclidean norm of w~1 . Rink and Harabagiu (2013) reported consistent improvements when using vector space metrics, in particular the Tanimoto distance, on the SemEval-2012 task on relational similarity (Jurgens et al., 2012) in comparison to several other measures that are designed for probability distributions, such as Jensen-Shannon divergence and Hellinger distance. 3.3 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Word similarity 13: We show in Algorithm 1 our procedure for measuring the semantic similarity of a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α ("
P15-1010,N13-1090,0,0.830715,"aluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dataset for the t"
P15-1010,H93-1061,0,0.085931,"supervised process, usually based on neural networks. In contrast to word embeddings, which obtain a single model for potentially ambiguous words, sense embeddings are continuous representations of individual word senses. In order to be able to apply word embeddings techniques to obtain representations for individual word senses, large sense-annotated corpora have to be available. However, manual sense annotation is a difficult and time-consuming process, i.e., the so-called knowledge acquisition bottleneck. In fact, the largest existing manually sense annotated dataset is the SemCor corpus (Miller et al., 1993), whose creation dates back to more than two decades ago. In order to alleviate this issue, we leveraged a state-of-the-art Word Sense Disambiguation (WSD) algorithm to automatically generate large amounts of sense-annotated corpora. In the rest of Section 2, first, in Section 2.1, we describe the sense inventory used for S ENS E M BED . Section 2.2 introduces the corpus and the disambiguation procedure used to sense annotate this corpus. Finally in Section 2.3 we discuss how we leverage the automatically sense-tagged 2.2 Generating a sense-annotated corpus As our corpus we used the September-"
P15-1010,P10-1040,0,0.0492224,"ddings are vector space models (VSM) that represent words as real-valued vectors in a low-dimensional (relative to the size of the vocabulary) semantic space, usually referred to as the continuous space language model. The conventional way to obtain such representations is to compute a term-document occurrence matrix on large corpora and then reduce the dimensionality of the matrix using techniques such as singular value decomposition (Deerwester et al., 1990; Bullinaria and Levy, 2012, SVD). Recent predictive techniques (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Turian et al., 2010; Mikolov et al., 2013a) replace the conventional two-phase approach with a single supervised process, usually based on neural networks. In contrast to word embeddings, which obtain a single model for potentially ambiguous words, sense embeddings are continuous representations of individual word senses. In order to be able to apply word embeddings techniques to obtain representations for individual word senses, large sense-annotated corpora have to be available. However, manual sense annotation is a difficult and time-consuming process, i.e., the so-called knowledge acquisition bottleneck. In"
P15-1010,Q14-1019,1,0.573841,"Missing"
P15-1010,P14-2089,0,0.682129,"on the original sense discrimination approach of Reisinger and Mooney (2010), Huang et al. (2012) applied K-means clustering to decompose word embeddings into multiple prototypes, each denoting a distinct meaning of the target word. However, the sense representations obtained are not linked to any sense inventory, a mapping that consequently has to be carried out either manually, or with the help of sense-annotated data. Another line of research investigates the possibility of taking advantage of existing semantic resources in word embeddings. A good example is the Relation Constrained Model (Yu and Dredze, 2014). When computing word embeddings, this model replaces the original co-occurrence clues from text corpora with the relationship information derived from the Paraphrase Database1 (Ganitkevitch et al., 2013, PPDB), an automatically extracted dataset of paraphrase pairs. However, none of these techniques have simultaneously solved both above-mentioned issues, i.e., inability to model polysemy and reliance on text corpora as the only source of knowledge. We propose a novel approach, called S ENS E MBED, which addresses both drawbacks by exploiting semantic knowledge for modeling arbitrary word sens"
P15-1010,D14-1162,0,0.121745,"degree of analogy between the two pairs 1: Swa ← getSenses(wa ), Swb ← getSenses(wb ) 2: (s∗a , s∗b ) ← argmaxsa ∈Swa T (s~a , s~b ) 3: Swc ← getSenses(wc ), Swd ← getSenses(wd ) 4: sc , s~d ) (s∗c , s∗d ) ← argmax sc ∈Swc T (~ 5: 4.1.2 Comparison systems We compare the performance of our similarity measure against twelve other approaches. As regards traditional distributional models, we report the best results computed by Baroni et al. (2014) for PMI-SVD, a system based on Pointwise Mutual Information (PMI) and SVD-based dimensionality reduction. For word embeddings, we report the results of Pennington et al. (2014, GloVe) and Collobert and Weston (2008). GloVe is an alternative way for learning embeddings, in which vector dimensions are made explicit, as opposed to the opaque meaning of the vector dimensions in Word2vec. The approach of Collobert and Weston (2008) is an embeddings model with a deeper architecture, designed to preserve more complex knowledge as distant relations. We also show results for the word embeddings trained by Baroni et al. (2014). The authors first constructed a massive corpus by combining several large corpora. Then, they trained dozens of different Word2vec models by varying"
P15-1010,N13-1120,0,0.307465,"nal similarity Relational similarity evaluates the correspondence between relations (Medin et al., 1990). The task can be viewed as an analogy problem in which, given two pairs of words (wa , wb ) and (wc , wd ), the goal is to compute the extent to which the relations of wa to wb and wc to wd are similar. Sense embeddings are suitable candidates for measuring this type of similarity, as they represent relations between senses as linear transformations. Given this property, the relation between a pair of words can be obtained by subtracting their corresponding normalized embeddings. Following Zhila et al. (2013), the relational similarity between two pairs of word (wa , wb ) and (wc , wd ) is accordingly calculated as: ANALOGY (w ~a , w~b , w~c , w~d ) = T (w~b − w~a , w~d − w~c ) (6) We show the procedure for measuring the relational similarity in Algorithm 2. The algorithm first finds the closest senses across the two word pairs: s∗a and s∗b for the first pair and s∗c and s∗d for the second. The analogy vector representations are accordingly computed as the difference between the sense embeddings of the corresponding closest senses. Finally, the relational similarity is computed as the similarity o"
P15-1010,P13-1132,1,0.752755,"d2vec. The approach of Collobert and Weston (2008) is an embeddings model with a deeper architecture, designed to preserve more complex knowledge as distant relations. We also show results for the word embeddings trained by Baroni et al. (2014). The authors first constructed a massive corpus by combining several large corpora. Then, they trained dozens of different Word2vec models by varying the system’s training parameters and reported the best performance obtained on each dataset. As representatives for graph-based similarity techniques, we report results for the state-of-theart approach of Pilehvar et al. (2013) which is based on random walks on WordNet’s semantic network. Moreover, we present results for the graph-based approach of Zesch et al. (2008), which compares a pair of words based on the path lengths on Wiktionary’s semantic network. We also compare our word similarity measure against the multi-prototype models of Reisinger and Mooney (2010) and Huang et al. (2012), and against the approaches of Yu and Dredze (2014) and Chen et al. (2014), which enhance word embeddings with semantic knowledge derived from PPDB and WordNet, respectively. Finally, we report results for word embeddings, as our"
P15-1010,N10-1013,0,0.698982,"Missing"
P15-1072,E09-1005,0,0.756023,"not consider those BabelNet synsets that are not associated with Wikipedia pages. WordNet sense inventory. Similarly, when restricted to the WordNet inventory, we discard those BabelNet synsets that do not contain a WordNet synset. In this setting, we also leverage relations from WordNet’s semantic network and its disambiguated glosses3 in order to obtain a richer set of Wikipedia articles in the sub-corpus construction. The enrichment of the semantic network with the disambiguated glosses has been shown to be beneficial in various graph-based disambiguation tasks (Navigli and Velardi, 2005; Agirre and Soroa, 2009; Pilehvar et al., 2013). 4 Experiments We assess the reliability of M UFFIN in two standard evaluation benchmarks: semantic similarity (Section 4.1) and Word Sense Disambiguation (Section 4.2). 4.1 Datasets Semantic Similarity As our semantic similarity experiment we opted for word similarity, which is one of the most popular evaluation frameworks in lexical semantics. Given a pair of words, the task in word similarity is to automatically judge their semantic similarity and, ideally, this judgement should be close to that given by humans. 4.1.2 Comparison systems Monolingual. We benchmark our"
P15-1072,N09-1003,0,0.0629266,"Missing"
P15-1072,P14-1023,0,0.440805,"ine-interpretable form, is a fundamental problem in Natural Language Processing (NLP). The Vector Space Model (VSM) is a prominent approach for semantic representation, with widespread popularity in numerous NLP applications. The prevailing methods for the computation of a vector space representation are based on distributional semantics (Harris, 1954). However, these approaches, whether in their conventional co-occurrence based form (Salton et al., 1975; Turney and Pantel, 2010; Landauer and Dooley, 2002), or in their newer predictive branch (Collobert and Weston, 2008; Mikolov et al., 2013; Baroni et al., 2014), suffer from a major drawback: they are unable to model individual word senses or concepts, as they conflate 1. Multilingual: it enables sense representation in dozens of languages; 2. Unified: it represents a linguistic item, irrespective of its language, in a unified seman741 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 741–751, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Our procedure for constructing a multilingual vec"
P15-1072,F14-1032,1,0.762091,"on multiple datasets and settings in both frameworks, which confirms the reliability and flexibility of our representations. 2 2.2 Methodology Vector construction: lexical specificity Lexical specificity (Lafon, 1980) is a statistical measure based on the hypergeometric distribution. Due to its efficiency in extracting a set of highly relevant words from a sub-corpus, the measure has recently gained popularity in different NLP applications, such as textual data analysis (Lebart et al., 1998), term extraction (Drouin, 2003), and domain-based term disambiguation (Camacho-Collados et al., 2014; Billami et al., 2014). We leverage lexical specificity to compute the weights in our vectors. In our earlier work (Camacho-Collados et al., 2015), we conducted different experiments which demonstrated the improvement that lexical specificity can provide over the popular term frequency-inverse document frequency weighting scheme (Jones, 1972, tf-idf ). Lexical specificity computes the vector weights for an item, i.e., a word or a set of words, by comparing and contrasting its contextual information with a reference corpus. In our setting, we take the whole Wikipedia as our reference corpus RC (we use the October 20"
P15-1072,E09-1013,0,0.0487404,"al Semantic Representation of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches h"
P15-1072,D09-1124,0,0.0465708,"the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other languages while preserving the original similarity scores, has led to inconsistencies in the datasets. For instance, the Spanish dataset contains the identical pair mediodiamediodia with a similarity score of 3.42 (in the scale [0,4]). Additionally, the datasets contain several orth"
P15-1072,N15-1059,1,0.795724,"esentation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual"
P15-1072,D14-1110,0,0.232776,"Missing"
P15-1072,P12-1092,0,0.153897,"-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model indi"
P15-1072,D07-1061,0,0.0271844,"e of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Ou"
P15-1072,N15-1184,0,0.0879558,"Missing"
P15-1072,I05-1067,0,0.360848,"4.1.1 Input: a target word w and a document d (context of w) Output: cˆ, the intended sense of w 1: for each concept c ∈ Cw 2: scorec ← 0 3: for each lemma l ∈ d 4: if l ∈ lexc then −1 5: scorec ← scorec + rank(l, lexc ) 6: cˆ ← arg max scorec Monolingual. We picked the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other lang"
P15-1072,S13-2042,0,0.0480006,"Missing"
P15-1072,S10-1003,0,0.0437631,"vided with suitable amounts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniqu"
P15-1072,S13-2029,0,0.0397807,"ts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and s"
P15-1072,S13-2043,0,0.0202365,"pproaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and supervised 748 word senses. Thanks to its effective combination of distributional statistics and structured knowledge, the approach can compute efficient representations of arbitrary word senses, with high coverage and irrespect"
P15-1072,D14-1162,0,0.0877904,"lados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However, these approaches, either in their conventional co-occurrence based form (Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997; Turney and Pantel, 2010; Bullinaria and Levy, 2012), or the more recent predictive models (Mikolov et al., 2013; Collobert and Weston, 2008; Pennington et al., 2014), are restricted in two ways: (1) they cannot be used to compare word senses; and (2) they cannot be directly applied to cross-lingual semantic similarity. Though the first problem has been solved by multi-prototype models (Huang et al., 2012), or by the sense-specific representations obtained as a result of exploiting WordNet glosses (Chen et al., 2014), the second problem remains unaddressed. In contrast, our approach models word senses and concepts effectively, while providing a unified representation for different languages that enables cross-lingual semantic similarity. 6 Acknowledgments"
P15-1072,P14-1044,1,0.783795,"and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM techniques: Semantic representa"
P15-1072,P13-1132,1,0.862073,"andard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM te"
P15-1072,H93-1061,0,0.184777,"(SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five different languages: Italian, English, French, Spanish and German. There are on average 1123 words to disambiguate in each language’s dataset. As compa"
P15-1072,Q14-1019,1,0.944398,"and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five dif"
P15-1072,W09-3204,0,0.0161049,"Missing"
P15-1072,J91-1002,0,0.380594,"However, the approach is limited to the WSD and Entity Linking tasks. In contrast, our approach is global as it can be used in different NLP tasks, including WSD. Semantic similarity. Semantic similarity of word pairs is usually computed either on the basis of the structural properties of lexical databases and thesauri, or by comparing vectorial representations of words learned from massive text corpora. Structural approaches usually measure the similarity on the basis of the distance information on semantic networks, such as WordNet (Budanitsky and Hirst, 2006), or thesauri, such as Roget’s (Morris and Hirst, 1991; Jarmasz and Szpakowicz, 2003). The semantic network of WordNet has also been used in more sophisticated techniques such as those based on random graph walks (Ramage et al., 2009; Pilehvar et al., 2013), or coupled with the complementary knowledge from Wikipedia (Camacho-Collados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However,"
P15-1072,N10-1013,0,0.149374,"on of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully u"
P15-1072,W09-3206,0,0.0188389,"that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages"
P15-1072,S13-2040,1,0.922763,"oreover, M UFFINpivot attains the best results among the pivot systems on all datasets, confirming the reliability of our system in the monolingual setting. We note that since the cross-lingual datasets were built by translating the word pairs in the original English RG-65 dataset, the pivot-based comparison systems proved to be highly competitive, outperforming the CL-MSR2.0 system by a considerable margin. 4.2 4.2.1 4.2.2 WordNet As regards the WordNet disambiguation task, we take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2"
P15-1072,P10-4014,0,0.479385,"e take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Wor"
P15-1072,S07-1016,0,\N,Missing
P15-1072,W09-2413,0,\N,Missing
P15-1072,J06-1003,0,\N,Missing
P15-2001,agirre-de-lacalle-2004-publicly,0,0.0794886,"Missing"
P15-2001,J15-4004,0,0.0289188,"Missing"
P15-2001,P14-1023,0,0.0149352,"he automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-anno"
P15-2001,N15-1059,1,0.712727,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,S14-2003,1,0.67505,"ization of the approach which would be capable of automatically constructing reliable cross-lingual similarity datasets for any pair of languages. Scoring the dataset Twelve native Spanish speakers were asked to evaluate the similarity for the Spanish translations. In order to obtain a more global distribution of judges, we included judges both both Spain and Latin America. As far as the Farsi dataset was concerned, twelve Farsi native speakers scored the newly translated pairs. The guidelines provided to the annotators were based on the recent SemEval task on Cross-Level Semantic Similarity (Jurgens et al., 2014), which provides clear indications in order to distinguish similarity and relatedness. The annotators were allowed to give scores from 0 to 4, with a step size of 0.5. Table 1 shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG65 dataset. As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores. This is due to the fact that associated senses with words do not hold one-to-one correspondence across differe"
P15-2001,P15-1072,1,0.362867,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,E14-1044,1,0.813372,"e English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-l"
P15-2001,N07-1025,0,0.0174777,"Missing"
P15-2001,I05-1067,0,0.376725,"´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. W"
P15-2001,P11-1076,0,0.0297043,"Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction o"
P15-2001,D09-1124,0,0.216669,"@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an"
P15-2001,P14-1044,1,0.385004,"igned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction of the cross-lingual datasets. We report the results of the evaluation performed on the generated"
P15-2001,P13-1132,1,0.716354,"vide an evaluation of the automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al.,"
P16-1085,E09-1005,0,0.0130029,"hoCollados et al., 2015a). Babelfy is a multilingual knowledge-based WSD and Entity Linking algorithm based on the semantic network of BabelNet. Muffin is a multilingual sense representation technique that combines the structural knowledge derived from semantic networks with the distributional statistics obtained from text corpora. The system uses sense-based representations for performing WSD. Camacho-Collados et al. (2015a) also proposed a hybrid system that averages the disambiguation scores of IMS with theirs (shown as “Muffin + IMS” in our tables). We also report the results for UKB w2w (Agirre and Soroa, 2009), another knowledge-based WSD approach based on Personalized PageRank (Haveliwala, 2002). Finally, we also carried out experiments with the pre-trained models6 that are proSystem SE2 SE3 SE7 MFS baseline 71.6 70.3 65.8 Babelfy Muffin Muffin + IMS UBK w2w IMS (pre-trained models) IMS (SemCor) IMS (OMSTI) − − − − 77.5 73.0 76.6 68.3 − − 65.3 74.0 70.8 73.3 62.7 66.0 68.5 56.0 66.5 64.2 67.7 IMS + Word2vec (SemCor) 74.2 70.1 68.6 IMS + Word2vec (OMSTI) 77.7 74.1 71.5 Table 4: F1 performance in the nouns subsets of different all-words WSD datasets. vided with the IMS toolkit, as well as IMS traine"
P16-1085,W06-1669,0,0.0124011,"Missing"
P16-1085,P98-1013,0,0.118219,"of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and th"
P16-1085,P14-1023,0,0.0329689,"e highlight each cell according to the relative performance gain in comparison to the IMS baseline (top row in the table). ferent dimensionalities and learning techniques: Word2vec embeddings trained on Wikipedia, with the Skip-gram model for dimensionalities 50, 300 and 500 (for comparison reasons) and CBOW with 300 dimensions, Word2vec trained on the Google News corpus with 300 dimensions and the Skipgram model, the 300 dimensional embeddings of GloVe, and the 50 dimensional C&W embeddings. Additionally we include experiments on a non-embedding model, a PMI-SVD vector space model trained by Baroni et al. (2014). Table 6 lists the performance of our system with different word representations in vector space on the Senseval-2 English Lexical Sample task. The results corroborate the findings of Levy et al. (2015) that Skip-gram is more efficient in captur904 Word representations Dim. Concatenation Combination strategy Average Fractional Exponential Skip-gram - GoogleNews GloVe CBOW - Wiki Skip-gram - Wiki 300 300 300 300 65.5 61.7 65.1 65.2 65.5 66.3 65.4 65.6 69.4 66.7 68.9 68.9 69.6 68.3 68.8 69.7 PMI - SVD - Wiki Skip-gram - Wiki 500 500 65.5 65.1 65.3 65.6 67.3 69.1 66.8 69.9 50 50 58.6 65.0 67.3 6"
P16-1085,N13-1092,0,0.0156057,"Missing"
P16-1085,P10-1156,0,0.0196722,"upervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used exten"
P16-1085,E09-1013,0,0.0179041,"al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared meaning and induce senses. These methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were studied by Agirre et al. (2006), Brody and Lapata (2009), Manandhar et al. (2010), Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sen"
P16-1085,P12-1092,0,0.0166034,"ton et al. (2014, GloVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be in"
P16-1085,S07-1053,0,0.0186831,"f a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popular, syntactic relations have also been studied as a possible feature (Stetina et al., 1998) in WSD. More sophisticated features have also been studied. Examples are distributional semantic models, such as Latent Semantic Analysis (Van de Cruys and Apidianaki, 2011) and Latent Dirichlet Allocation (Cai et al., 2007). Inasmuch as they are the dominant distributional semantic model, word embeddings have also been applied as features to WSD systems. In this paper we study different methods through which word embeddings can be used as WSD features. 3.6 3.6.1 Concatenation Concatenation is our first strategy, which is inspired by the model of Bengio et al. (2003). This method consists of concatenating the vectors of the words surrounding a target word into a larger vector that has a size equal to the aggregated dimensions of all the individual embeddings. Let wij be the weight associated with the ith dimensio"
P16-1085,P15-1010,1,0.832227,"t features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e.,"
P16-1085,P15-1072,1,0.928323,"oVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexi"
P16-1085,N15-1059,1,0.852577,"oVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexi"
P16-1085,D14-1110,0,0.715913,"significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features. 1 Introduction 2 Embeddings represent words, or concepts in a low-dimensional continuous space. These vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. The ability of embeddings to capture knowledge has been exploited in several tasks, such as Machine Translation (Mikolov et al., 2013, MT), Sentiment Analysis (Socher et al., 2013), Word Sense Disambiguation (Chen et al., 2014, WSD) and Language Understanding (Mesnil et al., 2013). Supervised WSD is based on the hypothesis that contextual information provides a Word Embeddings An embedding is a representation of a topological object, such as a manifold, graph, or field, in a certain space in such a way that its connectivity or algebraic properties are preserved (Insall et al., 2015). Presented originally by Bengio et al. (2003), word embeddings aim at representing, i.e., embedding, the ideal semantic space of words in a real-valued continuous vector space. In contrast to traditional distributional techniques, such"
P16-1085,W02-1006,0,0.0190147,"ord embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popula"
P16-1085,Q15-1016,0,0.0471663,"kipedia, with the Skip-gram model for dimensionalities 50, 300 and 500 (for comparison reasons) and CBOW with 300 dimensions, Word2vec trained on the Google News corpus with 300 dimensions and the Skipgram model, the 300 dimensional embeddings of GloVe, and the 50 dimensional C&W embeddings. Additionally we include experiments on a non-embedding model, a PMI-SVD vector space model trained by Baroni et al. (2014). Table 6 lists the performance of our system with different word representations in vector space on the Senseval-2 English Lexical Sample task. The results corroborate the findings of Levy et al. (2015) that Skip-gram is more efficient in captur904 Word representations Dim. Concatenation Combination strategy Average Fractional Exponential Skip-gram - GoogleNews GloVe CBOW - Wiki Skip-gram - Wiki 300 300 300 300 65.5 61.7 65.1 65.2 65.5 66.3 65.4 65.6 69.4 66.7 68.9 68.9 69.6 68.3 68.8 69.7 PMI - SVD - Wiki Skip-gram - Wiki 500 500 65.5 65.1 65.3 65.6 67.3 69.1 66.8 69.9 50 50 58.6 65.0 67.3 65.7 62.9 68.3 64.3 68.6 Collobert & Weston Skip-gram - Wiki Table 6: F1 percentage performance on the Senseval-2 English Lexical Sample dataset with different word representations models, vector dimensio"
P16-1085,J13-3008,1,0.769446,"Missing"
P16-1085,S01-1001,0,0.935565,"cations which consist of 11 features around the target word. IMS uses a linear support vector machine (SVM) as its classifier. 4.2 Experiments 5.1 Lexical Sample WSD Experiment The lexical sample WSD tasks provide training datasets in which different occurrences of a small set of words are sense annotated. The goal is for a WSD system to analyze the contexts of the individual senses of these words and to capture clues that can be used for distinguishing different senses of a word from each other at the test phase. Datasets. As our benchmark for the lexical sample WSD, we chose the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-2007 (Pradhan et al., 2007) English Lexical Sample WSD tasks. The former two cover nouns, verbs and adjectives in their datasets whereas the latter task focuses on nouns and verbs Embedding Features We take the real-valued word embeddings as new features of IMS and introduce them into the system without performing any further modifications. 2 3 900 code.google.com/archive/p/word2vec/ http://ronan.collobert.com/senna/ Task Senseval-2 (SE2) Senseval-3 (SE3) SemEval-07 (SE7) Training Test noun verb adjective noun verb adjective 4851 3593 13287 356"
P16-1085,N15-1184,0,0.0796841,"ects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are b"
P16-1085,W04-0838,0,0.0214036,"methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were studied by Agirre et al. (2006), Brody and Lapata (2009), Manandhar et al. (2010), Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sense in the first language (Ng and Lee, 1996). Word Sense Disambiguation Natural language is inherently ambiguous. Most commonly-used words have several meanings. In order to identify the intended meaning of a word one has to analyze the context in which it appears by directly exploiting information from raw texts. The task of automatically assigning predefined meanings to wor"
P16-1085,W04-0807,0,0.145187,"ound the target word. IMS uses a linear support vector machine (SVM) as its classifier. 4.2 Experiments 5.1 Lexical Sample WSD Experiment The lexical sample WSD tasks provide training datasets in which different occurrences of a small set of words are sense annotated. The goal is for a WSD system to analyze the contexts of the individual senses of these words and to capture clues that can be used for distinguishing different senses of a word from each other at the test phase. Datasets. As our benchmark for the lexical sample WSD, we chose the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-2007 (Pradhan et al., 2007) English Lexical Sample WSD tasks. The former two cover nouns, verbs and adjectives in their datasets whereas the latter task focuses on nouns and verbs Embedding Features We take the real-valued word embeddings as new features of IMS and introduce them into the system without performing any further modifications. 2 3 900 code.google.com/archive/p/word2vec/ http://ronan.collobert.com/senna/ Task Senseval-2 (SE2) Senseval-3 (SE3) SemEval-07 (SE7) Training Test noun verb adjective noun verb adjective 4851 3593 13287 3566 3953 8987 755 314 − 1740 1807 2559"
P16-1085,P10-1154,1,0.425395,"Missing"
P16-1085,N10-1013,0,0.0253751,"Missing"
P16-1085,H94-1046,0,0.21226,"om word embeddings for improved WSD performance: (1) the system of Taghipour and Ng (2015), which combines word embeddings of Collobert and Weston (2008) using the concatenation strategy (cf. Section 3.6) and introduces the combined embeddings as a new feature in addition to the standard WSD features in IMS; and (2) AutoExtend (Rothe and Sch¨utze, 2015), which constructs a whole new set of features based on vectors made from words, senses and synsets of WordNet and incorporates them in IMS. Training corpus. As our training corpus we opted for two available resources: SemCor and OMSTI. SemCor (Miller et al., 1994) is a manually sense-tagged corpus created by the WordNet project team at Princeton University. The dataset is a subset of the English Brown Corpus and comprises around 360,000 words, providing annotations for more than 200K content words.4 OM5.1.1 Lexical sample WSD results Table 2 shows the F1 performance of the different systems on the three lexical sample datasets. As can be seen, the IMS + Word2vec system improves 4 We used automatic mappings to WordNet 3.0 provided in web.eecs.umich.edu/∼mihalcea/downloads.html. 901 STI5 (One Million Sense-Tagged for Word Sense Disambiguation and Inducti"
P16-1085,P15-1173,0,0.045228,"Missing"
P16-1085,C12-1109,0,0.00916756,"the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations repre"
P16-1085,S13-1003,0,0.272531,"Missing"
P16-1085,W04-0811,0,0.414476,"SD results Table 2 shows the F1 performance of the different systems on the three lexical sample datasets. As can be seen, the IMS + Word2vec system improves 4 We used automatic mappings to WordNet 3.0 provided in web.eecs.umich.edu/∼mihalcea/downloads.html. 901 STI5 (One Million Sense-Tagged for Word Sense Disambiguation and Induction) was constructed based on the DSO corpus (Ng and Lee, 1996) and provides annotations for around 42K different nouns, verbs, adjectives, and adverbs. Datasets. As benchmark for this experiment, we considered the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), and SemEval-2007 (Pradhan et al., 2007) English allwords tasks. There are 2474, 2041, and 465 words for which at least one of the occurrences has been sense annotated in the Senseval-2, Senseval-3 and SemEval-2007 datasets, respectively. System SE2 SE3 SE7 MFS baseline 60.1 62.3 51.4 IMS (Zhong and Ng, 2010) Taghipour and Ng (2015) IMS (pre-trained models) IMS (SemCor) IMS (OMSTI) 68.2 − 67.7 62.5 67.0 67.6 68.2 67.5 65.0 66.4 58.3 − 58.0 56.5 57.6 IMS + Word2vec (SemCor) 63.4 65.3 57.8 IMS + Word2vec (OMSTI) 68.3 68.2 59.1 Table 3: F1 performance on different English allwords WSD datasets."
P16-1085,E06-1016,0,0.0363847,"veraging word embeddings as WSD features in a supervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the ne"
P16-1085,D13-1170,0,0.00332672,"beddings alone, if designed properly, can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features. 1 Introduction 2 Embeddings represent words, or concepts in a low-dimensional continuous space. These vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. The ability of embeddings to capture knowledge has been exploited in several tasks, such as Machine Translation (Mikolov et al., 2013, MT), Sentiment Analysis (Socher et al., 2013), Word Sense Disambiguation (Chen et al., 2014, WSD) and Language Understanding (Mesnil et al., 2013). Supervised WSD is based on the hypothesis that contextual information provides a Word Embeddings An embedding is a representation of a topological object, such as a manifold, graph, or field, in a certain space in such a way that its connectivity or algebraic properties are preserved (Insall et al., 2015). Presented originally by Bengio et al. (2003), word embeddings aim at representing, i.e., embedding, the ideal semantic space of words in a real-valued continuous vector space. In contrast t"
P16-1085,Q14-1019,1,0.916961,"training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captu"
P16-1085,W98-0701,0,0.0818822,"s to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popular, syntactic relations have also been studied as a possible feature (Stetina et al., 1998) in WSD. More sophisticated features have also been studied. Examples are distributional semantic models, such as Latent Semantic Analysis (Van de Cruys and Apidianaki, 2011) and Latent Dirichlet Allocation (Cai et al., 2007). Inasmuch as they are the dominant distributional semantic model, word embeddings have also been applied as features to WSD systems. In this paper we study different methods through which word embeddings can be used as WSD features. 3.6 3.6.1 Concatenation Concatenation is our first strategy, which is inspired by the model of Bengio et al. (2003). This method consists of"
P16-1085,P96-1006,0,0.0718105,"Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sense in the first language (Ng and Lee, 1996). Word Sense Disambiguation Natural language is inherently ambiguous. Most commonly-used words have several meanings. In order to identify the intended meaning of a word one has to analyze the context in which it appears by directly exploiting information from raw texts. The task of automatically assigning predefined meanings to words in contexts, known as Word Sense Disambiguation, is a fundamental task in computational lexical semantics (Navigli, 2009). There are four conventional approaches to 1 Supervised methods 3.4 Knowledge-based methods These methods are based on existing lexical resou"
P16-1085,N15-1035,0,0.774649,"ivides each dimension by 2W since the number of context words is twice the window size: Word Embeddings as WSD features Word embeddings have become a prominent technique in distributional semantics. These methods leverage neural networks in order to model the contexts in which a word is expected to appear. Thanks to their ability in efficiently learning the semantics of words, word embeddings have been applied to a wide range of NLP applications. Several studies have also investigated their integration into the Word Sense Disambiguation setting. These include the works of Zhong and Ng (2010), Taghipour and Ng (2015), Rothe and Sch¨utze (2015), and Chen et al. (2014), which leverage embeddings for supervised (the former three) and ei = I+W X j=I−W j6=I wij 2W 3.6.3 Fractional decay Our third strategy for constructing a feature vector on the basis of the context word embeddings is inspired by the way Word2vec combines the words in the context. Here, the importance of a word 899 for our representation is assumed to be inversely proportional to its distance from the target word. Hence, surrounding words are weighted based on their distance from the target word: ei = I+W X wij j=I−W j6=I We carried out experi"
P16-1085,W06-2501,0,0.00847417,"tion of different methods of leveraging word embeddings as WSD features in a supervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-s"
P16-1085,P11-1148,0,0.0143733,"Missing"
P16-1085,D14-1162,0,0.119484,"utational Linguistics, pages 897–907, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics feed-forward neural network capable of predicting a word given the words preceding (i.e., leading up to) that word. Collobert and Weston (2008) presented a much deeper model consisting of several layers for feature extraction, with the objective of building a general architecture for NLP tasks. A major breakthrough occurred when Mikolov et al. (2013) put forward an efficient algorithm for training embeddings, known as Word2vec. A similar model to Word2vec was presented by Pennington et al. (2014, GloVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 20"
P16-1085,P14-2089,0,0.0162442,"s. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption"
P16-1085,J14-4005,1,0.814294,"et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and they require repetition of a comparable effort for each new language. Currently, the best performing WSD systems are those based on supervised learning. It Makes Sense (Zhong and Ng, 2010, IMS) and the system of Shen et al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared"
P16-1085,P10-4014,0,0.418016,"explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and they require repetition of a comparable effort for each new language. Currently, the best performing WSD systems are those based on supervised learning. It Makes Sense (Zhong and Ng, 2010, IMS) and the system of Shen et al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared meaning and induce senses. These methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were s"
P16-1085,S07-1016,0,\N,Missing
P16-1085,C98-1013,0,\N,Missing
P16-1085,J14-1003,0,\N,Missing
P16-1085,S10-1011,0,\N,Missing
P17-1170,R13-1022,0,0.0177951,"oth tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been prop"
P17-1170,baccianella-etal-2010-sentiwordnet,0,0.0519029,"Missing"
P17-1170,J15-2004,0,0.0193871,"ce of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie revi"
P17-1170,D15-1041,0,0.0141171,"s of their input. The word level functionality can affect the performance of these systems in two ways: (1) it can hamper their efficiency in handling words that are not encountered frequently during training, such as multiwords, inflections and derivations, and (2) it can restrict their semantic understanding to the level of words, with all their ambiguities, and thereby prevent accurate capture of the intended meanings. The first issue has recently been alleviated by techniques that aim to boost the generalisation power of NLP systems by resorting to sub-word or character-level information (Ballesteros et al., 2015; Kim et al., 2016). The second limitation, however, has not yet been studied sufficiently. A reasonable way to handle word ambiguity, and hence to tackle the second issue, is to semantify the input text: transform it from its surface-level semantics to the deeper level of word senses, i.e. their intended meanings. We take a step in this direction by designing a pipeline that enables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) a"
P17-1170,D16-1041,1,0.0551113,"a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embed"
P17-1170,W13-5003,0,0.0210547,"se distinctions can be beneficial to both tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representati"
P17-1170,N16-1163,0,0.0312474,"were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. C"
P17-1170,D14-1067,0,0.00713165,"onfigurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic netw"
P17-1170,L16-1269,1,0.895177,"bedding of its corresponding word. Owing to its reliance on WordNet’s semantic network, DeConf is limited to generating only those word senses that are covered by this lexical resource. We propose to use Wikipedia in order to expand the vocabulary of the computed word senses. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential"
P17-1170,E17-2036,1,0.840387,"Missing"
P17-1170,D14-1110,0,0.0816486,"Missing"
P17-1170,D13-1184,0,0.0200958,"biguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses i"
P17-1170,W16-2501,0,0.00621644,"ved over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the"
P17-1170,P16-1191,0,0.660744,"r many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimental setup. 5.1 Experimental set"
P17-1170,C14-1048,0,0.0139087,"al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmar"
P17-1170,P82-1020,0,0.792916,"Missing"
P17-1170,Q15-1023,0,0.0247776,"(ˆ s)} 11: return Disambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input d"
P17-1170,P12-1092,0,0.0165482,"unami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has o"
P17-1170,P11-1015,0,0.0188444,"rk for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even"
P17-1170,P15-1010,1,0.869344,"he shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to che"
P17-1170,N15-1070,0,0.0148647,"have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense em"
P17-1170,N15-1164,0,0.0284146,"he word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) propo"
P17-1170,N15-1011,0,0.00802726,"nses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th An"
P17-1170,P14-5010,0,0.00165544,"for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the input semantic network, N . Our algorithm then selects the best candidates iteratively. In each iteration, the 1 As defined in the underlying sense inventory, up to trigrams. We used Stanford CoreNLP (Manning et al., 2014) for tokenization, Part-of-Speech (PoS) tagging and lemmatization. 1858 Figure 2: Simplified graph-based representation of a sample sentence. Figure 3: Text classification model architecture. candidate sense that has the highest graph degree maxDeg is chosen as the winning sense: maxDeg = max |{(s, s0 ) ∈ E : s0 ∈ S}| s∈S (1) After each iteration, when a candidate sense sˆ is selected, all the possible candidate senses of the corresponding word (i.e. getLex(ˆ s)) are removed from E (line 10 in the algorithm). Figure 2 shows a simplified version of the graph for a sample sentence. The algorithm"
P17-1170,K16-1006,0,0.012425,"inking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in"
P17-1170,P14-1062,0,0.00336837,"nables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based mod"
P17-1170,D14-1181,0,0.123887,"of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 P"
P17-1170,D15-1200,0,0.0215655,"14; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to check the benefit that can be gained by replacing word embeddings with sense embeddings in multiple tasks. With the help of two simple disambiguation algorithms, unsupervised sense embeddings were integrated into various downstream applications, with varying degrees of success. Given the interdependency of sense representation and disambiguation in this model, it is very difficult to introduce alternative algorithms into its pipeline, either to benefit from the state of the art, or to carry out an evaluation. Instead, our pipeline provides the ad"
P17-1170,P16-1096,1,0.782735,"can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the i"
P17-1170,P06-1014,1,0.24475,"sent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embeddings. 4.2 Pre-trained Supersense Embeddings It has been argued that WordNet sense distinctions are too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. I"
P17-1170,D14-1113,0,0.0183534,"et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative"
P17-1170,P04-1035,0,0.00916293,"for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases"
P17-1170,P05-1015,0,0.141168,"sults in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose val"
P17-1170,D14-1162,0,0.119517,"mantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al."
P17-1170,D16-1174,1,0.850136,"Missing"
P17-1170,J14-4005,1,0.680885,"isambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retriev"
P17-1170,D16-1018,0,0.0905951,"ifferent NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the pr"
P17-1170,E17-1010,1,0.0268406,"3.6 83.2 IMDB 87.7 87.4 PL05 77.3 76.6 PL04 67.9 67.4 Stanford 91.8 91.3 Wikipedia 83.1 88.0 75.9† 67.1 91.0 WordNet Wikipedia 84.4 83.1 88.0 88.4∗ 75.9 75.8 66.2 69.3∗ 91.4† 91.0 85.5 88.3 80.2 72.5 93.1 83.4 88.3 79.2 69.7† 92.6 Wikipedia 83.8 87.0† 79.2 73.1 92.3 WordNet 85.2 88.8 79.5 73.8 92.7† Wikipedia 84.2 87.9 78.3† 72.6 92.2 Word Pre-trained Sense Supersense WordNet Table 4: Accuracy performance on five polarity detection datasets. Given that polarity datasets are balanced17 , we do not report F1 which would have been identical to accuracy. texts is a known issue (Moro et al., 2014; Raganato et al., 2017), the tackling of which remains an area of exploration. spective of the classification task. We attribute this to two main factors: 1. Sparsity: Splitting a word into multiple word senses can have the negative side effect that the corresponding training data for that word is distributed among multiple independent senses. This reduces the training instances per word sense, which might affect the classifier’s performance, particularly when senses are semantically related (in comparison to fine-grained senses, supersenses address this issue to some extent). 2. Disambiguation quality: As also ment"
P17-1170,N10-1013,0,0.011305,"nce of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research"
P17-1170,P15-1173,0,0.0610169,"Missing"
P17-1170,P11-1097,0,0.160583,"Missing"
P17-1170,N15-1099,0,0.0142975,"these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system by transforming its i"
P17-1170,J98-1004,0,0.448429,"Missing"
P17-1170,P13-2125,0,0.0526578,"re too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimenta"
P17-1170,P13-1045,0,0.0139818,"sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even distribution of positive and negative labels. 5.3.2 Results Table 4 lists accuracy performance of our classification model and all its varian"
P17-1170,N16-1160,0,0.0557682,"Missing"
P17-1170,D15-1167,0,0.00767,"applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th Annual Meeting of the"
P17-1170,C14-1016,0,0.0219186,"fter their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or"
P17-1170,J17-1002,0,0.0225265,"s. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012)."
P17-1170,D15-1243,0,0.0130565,"no improvement is observed over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification whi"
P17-1170,J14-2007,0,0.0411995,"word-level functionality of these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system"
P17-1170,P15-1032,0,0.018956,"ayer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic network of WordNet (Miller, 1995),"
P17-1170,E17-1109,0,0.0294874,"Missing"
P17-1170,P10-4014,0,0.0202993,"ph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges am"
P17-1170,D13-1141,0,0.0158915,"riments with two configurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach explo"
P17-1170,D07-1107,0,\N,Missing
P17-1170,K17-1012,1,\N,Missing
P17-2094,S13-2032,0,0.0227148,"however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requires a language-independent annotation framework that goes beyond monolingual WordNet-like sense inventories (Lefever"
P17-2094,P15-1010,1,0.854457,"ctically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus compris"
P17-2094,D16-1250,0,0.0741,"Missing"
P17-2094,P16-1085,1,0.780596,"endent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada,"
P17-2094,2016.gwc-1.8,0,0.0310773,"gest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requires a language-independent annotation framework that goes beyond mon"
P17-2094,W16-5307,0,0.0699806,"Missing"
P17-2094,2005.mtsummit-papers.11,0,0.113113,"een shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 European languages, with more than 743 million tokens overall. Apart from its prominent role in MT as a training set, the Europarl corpus has been used Parallel corpora are widely used in a variety of Natural Language Processing tasks, from Machine Translation to cross-lingual Word Sense Disambiguation, where parallel sentenc"
P17-2094,L16-1269,1,0.802688,"ual sense inventory of BabelNet, and covering all the 21 languages of the Europarl corpus. As such E URO S ENSE constitutes, to our knowledge, the largest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour an"
P17-2094,S13-2029,0,0.104183,"Missing"
P17-2094,P11-2055,0,0.0381769,"Missing"
P17-2094,K16-1006,0,0.0768567,"s from a languageindependent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–6"
P17-2094,D15-1131,0,0.0223863,"n systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language,"
P17-2094,H93-1061,0,0.940054,"ec. 67.5 Cov. 100 E URO S ENSE (full) E URO S ENSE (refined) 80.3 81.5 67.9 71.8 100 63.5 84.6 89.3 76.7 82.5 100 62.9 100 75.0 FR 100 53.8 ES Table 2: Precision (Prec.) and coverage (Cov.) of E URO S ENSE, manually evaluated on a random sample in 4 languages. Precision is averaged between the two judges, and coverage is computed assuming each content word in the sense inventory to be a valid disambiguation target. notations for English as a training set for a supervised all-words WSD system, It Makes Sense (Zhong and Ng, 2010, IMS). Following Taghipour and Ng (2015a), we started with SemCor (Miller et al., 1993) as initial training dataset, and then performed a subsampling of E URO S ENSE up to 500 additional training examples per word sense. We then trained IMS on this augmented training set and tested on the two most recent standard benchmarks for all-words WSD: the SemEval2013 task 12 (Navigli et al., 2013) and the SemEval-2015 task 13 (Moro and Navigli, 2015) test sets. As baselines we considered IMS trained on SemCor only and OMSTI, the sense-annotated dataset constructed by Taghipour and Ng (2015a) which also includes SemCor. Finally, we report the results of UKB, a knowledge-based system (Agir"
P17-2094,N16-1163,0,0.0217942,"ing of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language, but instead leverage all languages at the same time in a joint disambiguation procedure that is subsequently refined using distribut"
P17-2094,moro-etal-2014-annotating,1,0.912363,".it Abstract liable sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist i"
P17-2094,P16-1191,0,0.0741367,"both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 Europ"
P17-2094,Q14-1019,1,0.937008,".it Abstract liable sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist i"
P17-2094,C16-1256,0,0.0209036,"senborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting"
P17-2094,N16-1160,0,0.0567553,"Missing"
P17-2094,K15-1037,0,0.42081,"sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedeman"
P17-2094,P03-1058,0,0.0943459,"such E URO S ENSE constitutes, to our knowledge, the largest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requi"
P17-2094,tiedemann-2012-parallel,0,0.0448825,"Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 European languages, with more than 743 million tokens overall. Apart from its prominent role in MT as a training set, the Europarl corpus has been used Parallel corpora are widely used in a variety o"
P17-2094,P16-1024,0,0.0396579,"Missing"
P17-2094,N16-1142,0,0.0252696,"lly on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language, but instead leverage al"
P17-2094,passonneau-etal-2012-masc,0,0.0553585,"uage-independent sense annotations for a wide variety of concepts and named entities, which can be seamlessly mapped to individual semantic resources (e.g WordNet, Wikipedia, DBpedia) via BabelNet’s inter-resource mappings. Related Work Extending sense annotations to multiple languages is a demanding endeavor, especially when manual intervention is required. Despite the fact that sense-annotated corpora for a number of languages have been around for more than a decade (Petrolito and Bond, 2014), they either include few samples per word sense, or only cover a restricted set of ambiguous words (Passonneau et al., 2012); as a result, multilingual WSD was until recently almost exclusively tackled using knowledge-based approaches (Agirre et al., 2014; 2 3 Building E URO S ENSE Following Camacho-Collados et al. (2016a), our fully automatic disambiguation pipeline for constructing E URO S ENSE couples a graph-based multilingual joint WSD/EL system, Babelfy (Moro et al., 2014b)3 , and a language-independent vector representation of concepts and entities, NASARI (Camacho-Collados et al., 2016b).4 It comprises two stages: multilingual disambigua3 4 http://babelnet.org 595 http://babelfy.org http://lcl.uniroma1.it/n"
P17-2094,P15-1058,0,0.0318192,"Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2"
P17-2094,W14-0132,0,0.0559787,"riched context for a joint multilingual disambiguation. Using BabelNet, a unified multilingual sense inventory, we obtain language-independent sense annotations for a wide variety of concepts and named entities, which can be seamlessly mapped to individual semantic resources (e.g WordNet, Wikipedia, DBpedia) via BabelNet’s inter-resource mappings. Related Work Extending sense annotations to multiple languages is a demanding endeavor, especially when manual intervention is required. Despite the fact that sense-annotated corpora for a number of languages have been around for more than a decade (Petrolito and Bond, 2014), they either include few samples per word sense, or only cover a restricted set of ambiguous words (Passonneau et al., 2012); as a result, multilingual WSD was until recently almost exclusively tackled using knowledge-based approaches (Agirre et al., 2014; 2 3 Building E URO S ENSE Following Camacho-Collados et al. (2016a), our fully automatic disambiguation pipeline for constructing E URO S ENSE couples a graph-based multilingual joint WSD/EL system, Babelfy (Moro et al., 2014b)3 , and a language-independent vector representation of concepts and entities, NASARI (Camacho-Collados et al., 201"
P17-2094,N12-1078,0,0.0564615,"Missing"
P17-2094,J14-4005,1,0.914964,"Missing"
P17-2094,P10-4014,0,0.811237,"concepts and entities from a languageindependent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Shor"
P17-2094,E17-1010,1,0.798174,"12) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proc"
P17-2094,S13-2040,1,\N,Missing
P19-1069,W04-3204,0,0.438485,"Missing"
P19-1069,P17-2094,1,0.814447,"whereas they fall behind knowledgebased approaches when tested on other languages. Unfortunately, carrying out semantic annotations for a target language requires time, resources and expertise in the field. Thus, in the last few years new approaches have been developed to mitigate the burden of knowledge acquisition by providing automatically or semi-automatically tagged corpora. The main goal of such techniques is to infer the meaning of words occurring in raw sentences by leveraging information drawn from different sources of knowledge, i.e., parallel corpora (Taghipour and Ng, 2015; Delli Bovi et al., 2017), or semantic networks (Pasini and Navigli, 2017; Pasini et al., 2018). Although supervised models achieve competitive results when trained on The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automat"
P19-1069,Q15-1038,1,0.840626,"Language Processing (NLP). This problem has become even more critical with the advent of deep learning, as a bigger amount of data is needed to meet the requirements of more and more difficult tasks and increasingly complex models. Word Sense Disambiguation (WSD), i.e., the task of associating a word with its meaning in a context (Navigli, 2009), is one of the most affected research areas (Navigli, 2018). The interest in this field has grown remarkably due to the variety of applications that can benefit from it, such as Machine Translation (Neale et al., 2016) or Information Extraction (Delli Bovi et al., 2015). Most approaches to WSD are either supervised or knowledge-based. The former frames the problem 1 2 https://wordnet.princeton.edu https://babelnet.org 699 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 699–709 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatically and semi-automatically annotated datasets, a major limitation concerning these approaches is that they are strictly dependent on knowledge sources, which are in their turn difficult to harvest. In fact, on the one hand, parallel corpor"
P19-1069,S01-1001,0,0.238,"e other hyperparameters. Depending on the setting, English or multilingual, we chose the best-performing system on a development set: Senseval-2 for English and an inhouse development set for all the other languages8 . For both models, unless differently stated, we used the Most Frequent Sense (MFS) of a lemma, i.e., its first-ranked meaning in BabelNet, as backoff strategy when the system was not able to provide an answer. Test bed We used the evaluation framework for English all-words WSD made available by Raganato et al. (2017a). This comprises all the past test sets, including Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013), SemEval-2015 (Moro and Navigli, 2015) and ALL, i.e., the concatenation of all the aforementioned datasets. For the multilingual evaluation, instead, we used the allwords multilingual WSD tasks of SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). For both settings, we focused on nouns only, as NASARI vectors are available mainly for nominal concepts. Following the literature, we report the F1 measure on all the test sets unless stated differently. (3) j 0"
P19-1069,H92-1045,0,0.895949,"omatically and semi-automatically annotated datasets, a major limitation concerning these approaches is that they are strictly dependent on knowledge sources, which are in their turn difficult to harvest. In fact, on the one hand, parallel corpora require human intervention for translating a collection of texts into one or more different languages. On the other hand, semantic networks rely on manually-annotated lexical-semantic data for enriching the network itself. In this paper we tackle the knowledge acquisition bottleneck by extending the hypotheses introduced in the two seminal papers by Gale et al. (1992b, One Sense Per Discourse) and Yarowsky (1993, One Sense Per Collocation) to Wikipedia categories, thereby making the following four contributions: U NITED K INGDOM category groups together all the past and present monarchs of the country, e.g. Elisabeth II, Queen Victoria, etc. Based on this, in what follows we refer to the sentences of a category C as those sentences contained in all the pages of C, and we refer to the occurrences of a lemma in a category C as the occurrences of its inflected forms in the sentences of C. Automatically annotating Wikipedia Our approach aims at creating a sen"
P19-1069,S10-1095,0,0.181358,"Missing"
P19-1069,P18-1031,0,0.0286182,"vigli, 2015). For both settings, we focused on nouns only, as NASARI vectors are available mainly for nominal concepts. Following the literature, we report the F1 measure on all the test sets unless stated differently. (3) j 0 =1 where the second term is a smoothed version of the category rank reciprocal6 , i.e., it is normalised by the sum of the reciprocal of each category rank (from 1 to mi ). Once we have determined the number of examples to draw from each category, we sample the sentences according to their perplexity, which we compute with a Neural Language Model trained on WikiText103 (Howard and Ruder, 2018)7 . The result of the above three steps is a semantically-annotated corpus where each meaning s of each lemma l ∈ L is associated with a set of sentences in which l is tagged with s. 6 Recall that the categories associated with the sense s are sorted by weighted overlap. 7 http://files.fast.ai/models/wt103/ 8 The development set of each language comprises 50 manually-annotated word-sense pairs. 702 73.0 73.0 72.0 72.0 71.0 70.0 F1 F1 71.0 69.0 69.0 IMS Bi-LSTM MFS Trendline IMS Trendline Bi-LSTM 68.0 67.0 66.0 70.0 IMS Bi-LSTM MFS Trendline IMS Trendline Bi-LSTM 68.0 67.0 66.0 100 300 500 K 70"
P19-1069,2005.mtsummit-papers.11,0,0.0941892,"oach is its limited coverage. In fact, a training example can be provided only for those senses with at least one monosemous related concept. Raganato et al. (2016) presented in their paper a method for the automatic construction of a Semantically Enriched Wikipedia (SEW), where the number of hyperlink annotations was enlarged by means of a set of heuristics. As an outcome they released a corpus containing more than 200 million annotations for approximately 4 million concepts and named entities. Another approach was developed by Otegi et al. (2016) to enrich the multilingual text of Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2014) with several features, including semantic annotations in 6 different languages. Parallel corpora were exploited also in the more recent work of Taghipour and Ng (2015, OMSTI)11 , who presented a semi-automatic approach that creates a novel semantically-annotated dataset by leveraging the manual effort made to align senses across different languages. In contrast, recent methods have been able to fully automatise the whole process while simultaRelated Work Word Sense Disambiguation is a well-established task in the field of Natural Language Processing and it has"
P19-1069,L18-1268,1,0.819545,"ther languages. Unfortunately, carrying out semantic annotations for a target language requires time, resources and expertise in the field. Thus, in the last few years new approaches have been developed to mitigate the burden of knowledge acquisition by providing automatically or semi-automatically tagged corpora. The main goal of such techniques is to infer the meaning of words occurring in raw sentences by leveraging information drawn from different sources of knowledge, i.e., parallel corpora (Taghipour and Ng, 2015; Delli Bovi et al., 2017), or semantic networks (Pasini and Navigli, 2017; Pasini et al., 2018). Although supervised models achieve competitive results when trained on The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automaticallygenerated data consistently lead a supervised WSD model to state"
P19-1069,D17-1008,1,0.947779,"pproaches when tested on other languages. Unfortunately, carrying out semantic annotations for a target language requires time, resources and expertise in the field. Thus, in the last few years new approaches have been developed to mitigate the burden of knowledge acquisition by providing automatically or semi-automatically tagged corpora. The main goal of such techniques is to infer the meaning of words occurring in raw sentences by leveraging information drawn from different sources of knowledge, i.e., parallel corpora (Taghipour and Ng, 2015; Delli Bovi et al., 2017), or semantic networks (Pasini and Navigli, 2017; Pasini et al., 2018). Although supervised models achieve competitive results when trained on The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automaticallygenerated data consistently lead a supervi"
P19-1069,P18-1230,0,0.0677361,"angles over the past years. One of the major problems concerning WSD has been the so-called knowledge acquisition bottleneck (Gale et al., 1992a), i.e., the paucity of lexical-semantic data. In fact, semantic resources are mainly exploited by WSD models in one of two different ways: as structured knowledge to identify the meaning of a word in a context in knowledge-based models (Moro et al., 2014; Agirre et al., 2014; Chaplot and Salakhutdinov, 2018), and as training data to fit the parameters of a classifier in supervised models (Zhong and Ng, 2010; Yuan et al., 2016; Raganato et al., 2017b; Luo et al., 2018). On the one hand, knowledge-based models have proved to be more versatile when it comes to disambiguating less frequent words and texts in low-resourced languages, even though they suffer from the lack of statistical evidence of lexical context. On the other hand, supervised models have consistently attained higher results in English WSD (Raganato et al., 2017a), however at the cost of less flexibility and lower results when scal11 http://lcl.uniroma1.it/wsdeval/ training-data 706 Acknowledgments neously producing high-quality resources. For example, Delli Bovi et al. (2017) exploited an exte"
P19-1069,P13-1132,1,0.876616,"of l, we assign to (l, C) the sense that maximises the similarity with the category BOW as follows: Sense Assignment The second step aims at assigning a sense distribution to each lexeme-category pair. We exploit the BOW we computed and the NASARI lexical vectors (Camacho Collados et al., 2016) to represent categories and synsets, respectively. NASARI leverages Wikipedia pages to provide a sparse representation of BabelNet synsets, having words as their dimensions weighted by their lexical specificity (Lafon, 1980). NASARI has been used to compute the semantic similarity between two concepts (Pilevar et al., 2013) in combination with the Weighted Overlap (WO), which has proven to work better than cosine similarity for comparing sparse vectors. It takes as input two vectors v1 and v2 and computes their similarity by considering the ranks of the components shared by both vectors5 . However, as it takes into account only the common dimensions, it also gives a high similarity value when the two vectors share just a few dimensions with similar rankings. In light of this, we modified the original formula and added a weight factor Ψ as follows: P v1 v2 )−1 (rw + rw w∈O W O(v1 , v2 ) = Ψ (1) |O| P (2i)−1 sense"
P19-1069,H93-1061,0,0.217254,"e informative sentences are found for a given sense. Once K was set to 700 both for IMS and Bi-LSTM, we ran the same experiment varying z. As one can see in Figure 1 (right), IMS achieves the highest score when z = 2.1 while Bi-LSTM when z = 2.9. While IMS seems sensitive to this parameter, attaining better performance when the distribution of classes in training is more balanced, the neural model trend is almost constant, indicating it is less dependent on the sense distribution. Comparison systems We compared OneSeC with a manual, a semi-automatic and a fullyautomatic alternative: • SemCor (Miller et al., 1993): the most used training corpus in WSD, which provides more than 200K manual annotations. • OMSTI (Taghipour and Ng, 2015): a semi-automatic approach that extracts semantically-annotated data by exploiting parallel data to reduce the ambiguity of the target language. Since the resource contains SemCor by default, we considered only the semi-automatically generated examples in order to guarantee a fair comparison with OneSeC. • Train-O-Matic9 (Pasini and Navigli, 2017, TOM): a knowledge-based method for the automatic generation of sense-annotated data. Therefore, we chose IMS as our WSD referen"
P19-1069,E17-1010,1,0.802632,"rd Sense Disambiguation task to assess the quality of our automaticallygenerated corpus. Therefore, we trained a reference WSD model on the data generated by OneSeC and compared the results against those achieved by the same model trained on other resources. In what follows we introduce the reference Word Sense Disambiguation system, the test bed, the comparison systems and how we tuned the two parameters K and z. Reference system We carried out the evaluation with two different WSD models: the SVM-based system It Makes Sense (Zhong and Ng, 2010, IMS) and the Bi-LSTM-based model introduced by Raganato et al. (2017b). For the latter we used MUSE embeddings (Lample et al., 2018) in the input layer, a learning rate of 0.5 and followed Raganato et al. (2017b) for all the other hyperparameters. Depending on the setting, English or multilingual, we chose the best-performing system on a development set: Senseval-2 for English and an inhouse development set for all the other languages8 . For both models, unless differently stated, we used the Most Frequent Sense (MFS) of a lemma, i.e., its first-ranked meaning in BabelNet, as backoff strategy when the system was not able to provide an answer. Test bed We used"
P19-1069,Q14-1019,1,0.947991,"ing Multilingual Sense-Annotated Data Bianca Scarlini, Tommaso Pasini and Roberto Navigli Department of Computer Science Sapienza University of Rome {scarlini,pasini,navigli}@di.uniroma1.it Abstract as a classification (Zhong and Ng, 2010) or sequence learning (Raganato et al., 2017b) task, in which either a target word or all the content words in a sequence have to be tagged with one of their possible meanings. The latter, instead, exploits graph algorithms on knowledge bases, such as the Personalized PageRank method (Haveliwala, 2002; Agirre et al., 2014), or the densest subgraph heuristic (Moro et al., 2014). Hence, knowledgebased approaches rely on semantic networks such as WordNet1 (Miller et al., 1990), a manuallycurated resource where synonyms are grouped into so-called synsets, or BabelNet2 (Navigli and Ponzetto, 2010), a large multilingual encyclopedic dictionary that merges together different resources like WordNet, Wikipedia, Wikidata etc. Therefore, in one form or another both approaches to WSD need lexical-semantic data. This is especially crucial in the case of supervised systems, which have proved capable of attaining higher results on English, for which annotated data are available,"
P19-1069,D17-1120,1,0.924547,"rd Sense Disambiguation task to assess the quality of our automaticallygenerated corpus. Therefore, we trained a reference WSD model on the data generated by OneSeC and compared the results against those achieved by the same model trained on other resources. In what follows we introduce the reference Word Sense Disambiguation system, the test bed, the comparison systems and how we tuned the two parameters K and z. Reference system We carried out the evaluation with two different WSD models: the SVM-based system It Makes Sense (Zhong and Ng, 2010, IMS) and the Bi-LSTM-based model introduced by Raganato et al. (2017b). For the latter we used MUSE embeddings (Lample et al., 2018) in the input layer, a learning rate of 0.5 and followed Raganato et al. (2017b) for all the other hyperparameters. Depending on the setting, English or multilingual, we chose the best-performing system on a development set: Senseval-2 for English and an inhouse development set for all the other languages8 . For both models, unless differently stated, we used the Most Frequent Sense (MFS) of a lemma, i.e., its first-ranked meaning in BabelNet, as backoff strategy when the system was not able to provide an answer. Test bed We used"
P19-1069,W04-0811,0,0.157621,"he setting, English or multilingual, we chose the best-performing system on a development set: Senseval-2 for English and an inhouse development set for all the other languages8 . For both models, unless differently stated, we used the Most Frequent Sense (MFS) of a lemma, i.e., its first-ranked meaning in BabelNet, as backoff strategy when the system was not able to provide an answer. Test bed We used the evaluation framework for English all-words WSD made available by Raganato et al. (2017a). This comprises all the past test sets, including Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval2013 (Navigli et al., 2013), SemEval-2015 (Moro and Navigli, 2015) and ALL, i.e., the concatenation of all the aforementioned datasets. For the multilingual evaluation, instead, we used the allwords multilingual WSD tasks of SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). For both settings, we focused on nouns only, as NASARI vectors are available mainly for nominal concepts. Following the literature, we report the F1 measure on all the test sets unless stated differently. (3) j 0 =1 where the second term is a smoothe"
P19-1069,K15-1037,0,0.760702,"annotated data are available, whereas they fall behind knowledgebased approaches when tested on other languages. Unfortunately, carrying out semantic annotations for a target language requires time, resources and expertise in the field. Thus, in the last few years new approaches have been developed to mitigate the burden of knowledge acquisition by providing automatically or semi-automatically tagged corpora. The main goal of such techniques is to infer the meaning of words occurring in raw sentences by leveraging information drawn from different sources of knowledge, i.e., parallel corpora (Taghipour and Ng, 2015; Delli Bovi et al., 2017), or semantic networks (Pasini and Navigli, 2017; Pasini et al., 2018). Although supervised models achieve competitive results when trained on The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged wit"
P19-1069,P10-1023,1,0.808733,"ication (Zhong and Ng, 2010) or sequence learning (Raganato et al., 2017b) task, in which either a target word or all the content words in a sequence have to be tagged with one of their possible meanings. The latter, instead, exploits graph algorithms on knowledge bases, such as the Personalized PageRank method (Haveliwala, 2002; Agirre et al., 2014), or the densest subgraph heuristic (Moro et al., 2014). Hence, knowledgebased approaches rely on semantic networks such as WordNet1 (Miller et al., 1990), a manuallycurated resource where synonyms are grouped into so-called synsets, or BabelNet2 (Navigli and Ponzetto, 2010), a large multilingual encyclopedic dictionary that merges together different resources like WordNet, Wikipedia, Wikidata etc. Therefore, in one form or another both approaches to WSD need lexical-semantic data. This is especially crucial in the case of supervised systems, which have proved capable of attaining higher results on English, for which annotated data are available, whereas they fall behind knowledgebased approaches when tested on other languages. Unfortunately, carrying out semantic annotations for a target language requires time, resources and expertise in the field. Thus, in the"
P19-1069,H93-1052,0,0.466464,"ets, a major limitation concerning these approaches is that they are strictly dependent on knowledge sources, which are in their turn difficult to harvest. In fact, on the one hand, parallel corpora require human intervention for translating a collection of texts into one or more different languages. On the other hand, semantic networks rely on manually-annotated lexical-semantic data for enriching the network itself. In this paper we tackle the knowledge acquisition bottleneck by extending the hypotheses introduced in the two seminal papers by Gale et al. (1992b, One Sense Per Discourse) and Yarowsky (1993, One Sense Per Collocation) to Wikipedia categories, thereby making the following four contributions: U NITED K INGDOM category groups together all the past and present monarchs of the country, e.g. Elisabeth II, Queen Victoria, etc. Based on this, in what follows we refer to the sentences of a category C as those sentences contained in all the pages of C, and we refer to the occurrences of a lemma in a category C as the occurrences of its inflected forms in the sentences of C. Automatically annotating Wikipedia Our approach aims at creating a sense-annotated corpus in a target language by le"
P19-1069,P10-4014,0,0.854173,"Ksi j = Ksi j −1 m Pi 0−1 j Experimental Setup We exploited the Word Sense Disambiguation task to assess the quality of our automaticallygenerated corpus. Therefore, we trained a reference WSD model on the data generated by OneSeC and compared the results against those achieved by the same model trained on other resources. In what follows we introduce the reference Word Sense Disambiguation system, the test bed, the comparison systems and how we tuned the two parameters K and z. Reference system We carried out the evaluation with two different WSD models: the SVM-based system It Makes Sense (Zhong and Ng, 2010, IMS) and the Bi-LSTM-based model introduced by Raganato et al. (2017b). For the latter we used MUSE embeddings (Lample et al., 2018) in the input layer, a learning rate of 0.5 and followed Raganato et al. (2017b) for all the other hyperparameters. Depending on the setting, English or multilingual, we chose the best-performing system on a development set: Senseval-2 for English and an inhouse development set for all the other languages8 . For both models, unless differently stated, we used the Most Frequent Sense (MFS) of a lemma, i.e., its first-ranked meaning in BabelNet, as backoff strateg"
P19-1165,D14-1110,0,0.0156345,"eddings have paved the way for improvements in numerous NLP tasks (Goldberg, 2017), they still conflate the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings, i.e. embeddings which separate the various senses of each word in the vocabulary (Huang et al., 2012; Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016; Pilehvar and Collier, 2016; Mancini et al., 2017, among others). One of the weaknesses of these approaches, however, is that they do not take word ordering into account during the learning process. On the other hand, word-based approaches based on RNNs that consider sequence information have been presented, but they are not competitive in terms of speed or quality of the embeddings (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013). For example, in Figure 1 we show an excerpt of a t-SNE (Maaten and Hinton, 2008) projection"
P19-1165,D14-1179,0,0.0317113,"Missing"
P19-1165,P98-1013,0,0.507592,"roposed to mitigate this issue: Yu and Dredze (2014) presented an alternative way to train word embeddings by using, in addition to common features, words having some relation in a semantic resource, like PPDB (Ganitkevitch et al., 2013) or WordNet (Miller, 1995). Faruqui et al. (2015) presented a technique applicable to pre-processed embeddings, in which vectors are updated (“retrofitted”) in order to make them more similar to those which share a word type and less similar to those which do not. The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998). Melamud et al. (2016) introduced context2vec, a model based on a bidirectional LSTM for learning sentence and word embeddings. This model uses large raw text corpora to train a neural model that embeds entire sentential contexts and target words in the same lowdimensional space. Finally, Press and Wolf (2017) introduced a model, based on word2vec, where the embeddings are extracted from the output topmost weight matrix, instead of the input one, showing that those representations are also valid word embeddings. 2.2 Sense Embeddings In contrast to the above approaches, each of which aims to l"
P19-1165,S14-2098,0,0.0575149,"Missing"
P19-1165,W16-2508,1,0.855378,"l other approaches, including the word-based models. On the ESL task, LSTMEmbed is the runner-up approach across systems and only by a small margin. The performance of the remaining models is considerably below ours. Experiment 4: Outlier detection. Our second word-based evaluation was focused on outlier detection, a task intended to test the capability of the learned embeddings to create semantic clusters, that is, to test the assumption that the representation of related words should be closer than the representations of unrelated ones. We tested our model on the 8-8-8 dataset introduced by Camacho-Collados and Navigli (2016), containing eight clusters, each with eight words and eight possible outliers. In our case, we extended the 10 https://nlp.stanford.edu/projects/ glove/ 1691 Model Corpus Sense 8-8-8 OPP Acc. word2vec* UMBC Wikipedia GoogleNews - 92.6 73.4 93.8 70.3 94.7 70.3 GloVe* UMBC Wikipedia - 81.6 40.6 91.8 56.3 AutoExtend SensEmbed SW2V Nasari DeConf GoogleNews Wikipedia Wikipedia Wikipedia GoogleNews X X X X X 82.8 98.0 48.4 94.0 93.8 X 96.1 78.1 LSTMEmbed Wikipedia Objective word2vec GloVe - Dim. WS353 - random (baseline) 50 word2vec 50 word2vec + retro 50 LSTMEmbed GoogleNews 300 GloVe.6B 300 SensE"
P19-1165,N15-1059,1,0.73769,"sampling of frequent words set to 103 . 4.2 Sense-based Evaluation Our first set of experiments was aimed at showing the impact of our joint word and sense model in tasks where semantic, and not just lexical, relatedness is needed. We analyzed two tasks, namely Cross-Level Semantic Similarity and Most Frequent Sense Induction. Comparison systems. We compared the performance of LSTMEmbed against alternative approaches to sense embeddings: SensEmbed (Iacobacci et al., 2015), which obtained semantic representations by applying word2vec to the English Wikipedia disambiguated with Babelfy; Nasari (Camacho-Collados et al., 2015), a technique for rich semantic representation of arbitrary concepts present in WordNet and Wikipedia pages; AutoExtend (Rothe and Sch¨utze, 2015) which, starting from the word2vec word embeddings learned from GoogleNews9 , infers the representation of senses and synsets from WordNet; DeConf, an approach introduced by Pilehvar and Collier (2016) that decomposes a given word representation into its constituent sense representations by exploiting WordNet. 7 https://keras.io http://deeplearning.net/software/ theano/index.html 9 https://code.google.com/archive/p/ word2vec/ 1689 8 Model Pearson Spe"
P19-1165,N15-1184,0,0.0623616,"Missing"
P19-1165,P16-1191,0,0.017761,"in numerous NLP tasks (Goldberg, 2017), they still conflate the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings, i.e. embeddings which separate the various senses of each word in the vocabulary (Huang et al., 2012; Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016; Pilehvar and Collier, 2016; Mancini et al., 2017, among others). One of the weaknesses of these approaches, however, is that they do not take word ordering into account during the learning process. On the other hand, word-based approaches based on RNNs that consider sequence information have been presented, but they are not competitive in terms of speed or quality of the embeddings (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013). For example, in Figure 1 we show an excerpt of a t-SNE (Maaten and Hinton, 2008) projection of word and sense embeddings in the literature: 168"
P19-1165,N13-1092,0,0.0930132,"Missing"
P19-1165,Q16-1002,0,0.0198013,"ed in two separated regions without a clear correlation with (potentially ambiguous) words which are relevant to them. A more accurate representation would be to have word vectors distributed across all the space with defined clusters for each set of vectors related to each sense of a target word (Figure 2). Recently, the much celebrated Long-Short Term Memory (LSTM) neural network model has emerged as a successful model to learn representations of sequences, thus providing an ideal solution for many Natural Language Processing tasks whose input is sequence-based, e.g., sentences and phrases (Hill et al., 2016; Melamud et al., 2016; Peters et al., 2018). However, to date LSTMs have not been applied to the effective creation of sense embeddings linked to an explicit inventory. In this paper, we explore the capabilities of the architecture of LSTMs using sense-labeled corpora for learning semantic representations of words and senses. We present four main contributions: • We introduce LSTMEmbed, an RNN model based on a bidirectional LSTM for learning word and sense embeddings in the same semantic space, which – in contrast to the most popular approaches to the task – takes word ordering into account."
P19-1165,P82-1020,0,0.788366,"Missing"
P19-1165,P12-1092,0,0.190628,". But while word embeddings have paved the way for improvements in numerous NLP tasks (Goldberg, 2017), they still conflate the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings, i.e. embeddings which separate the various senses of each word in the vocabulary (Huang et al., 2012; Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016; Pilehvar and Collier, 2016; Mancini et al., 2017, among others). One of the weaknesses of these approaches, however, is that they do not take word ordering into account during the learning process. On the other hand, word-based approaches based on RNNs that consider sequence information have been presented, but they are not competitive in terms of speed or quality of the embeddings (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013). For example, in Figure 1 we show an excerpt of a t-SNE (Maaten and Hinto"
P19-1165,P15-1010,1,0.941849,"m of relation-specific vector offsets. Recent approaches, such as word2vec (Mikolov et al., 2013), and GloVe (Pennington et al., 2014), are capable of learning efficient word embeddings from large unannotated corpora. But while word embeddings have paved the way for improvements in numerous NLP tasks (Goldberg, 2017), they still conflate the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings, i.e. embeddings which separate the various senses of each word in the vocabulary (Huang et al., 2012; Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016; Pilehvar and Collier, 2016; Mancini et al., 2017, among others). One of the weaknesses of these approaches, however, is that they do not take word ordering into account during the learning process. On the other hand, word-based approaches based on RNNs that consider sequence information have been presented, but the"
P19-1165,N15-1070,0,0.0202709,"bed on tasks in need of sense information, we also carried out a second set of experiments focused on word-based evaluations with the objective of demonstrating the ability of our joint word and sense embedding model to tackle tasks traditionally approached with wordbased models. Experiment 3: Synonym Recognition. We first experimented with synonym recognition: given a target word and a set of alternative words, the objective of this task was to select the member from 1690 makes no assumption about the number of prototypes. Accuracy TOEFL-80 ESL-50 Model word2vec GloVe 87.00 88.75 62.00 60.00 Jauhar et al. (2015) MSSG Li and Jurafsky (2015) MUSE 80.00 78.26 82.61 88.41 73.33* 57.14 50.00 64.29 LSTMEmbed 92.50 72.00* • Li and Jurafsky (2015), a multi-sense embeddings model based on the Chinese Restaurant Process. • Jauhar et al. (2015), a multi-sense approach based on expectation-maximization style algorithms for inferring word sense choices in the training corpus and learning sense embeddings while incorporating ontological sources of information. Table 3: Synonym Recognition: accuracy (percentages). * Not statistically significant difference (χ2 , p < 0.05). • Modularizing Unsupervised Sense Embeddin"
P19-1165,S14-2003,1,0.843454,"0 0.356 LSTMEmbed 0.380* 0.400 Model P@1 P@3 P@5 AutoExtend SensEmbed SW2V Nasari DeConf 22.8 38.4 39.7 27.4 30.1 52.0 56.1 60.3 40.2 55.8 56.6 63.0 67.5 44.6 64.3 LSTMEmbed 39.0 59.2 66.0 Table 2: Precision on the MFS task (percentages). Table 1: Pearson and Spearman correlations on the CLSS word-to-sense similarity task. * Not statistically significant difference (χ2 , p < 0.05). Experiment 1: Cross-Level Semantic Similarity. To best evaluate the ability of embeddings to discriminate between the various senses of a word, we opted for the SemEval-2014 task on Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS), which includes word-to-sense similarity as one of its sub-tasks. The CLSS word-tosense similarity dataset comprises 500 instances of words, each paired with a short list of candidate senses from WordNet with human ratings for their word-sense relatedness. To compute the word-tosense similarity we used our shared vector space of words and senses, and calculated the similarity using the cosine distance. We included not only alternative sense-based representations but also the best performing approaches on this task: MeerkatMafia (Kashyap et al., 2014), which uses Latent Semantic Analysi"
P19-1165,W16-5307,0,0.0408835,"Missing"
P19-1165,S14-2072,0,0.0167231,"on Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS), which includes word-to-sense similarity as one of its sub-tasks. The CLSS word-tosense similarity dataset comprises 500 instances of words, each paired with a short list of candidate senses from WordNet with human ratings for their word-sense relatedness. To compute the word-tosense similarity we used our shared vector space of words and senses, and calculated the similarity using the cosine distance. We included not only alternative sense-based representations but also the best performing approaches on this task: MeerkatMafia (Kashyap et al., 2014), which uses Latent Semantic Analysis (Deerwester et al., 1990) and WordNet glosses to get word-sense similarity measurements; SemantiKLU (Proisl et al., 2014), an approach based on a distributional semantic model trained on a large Web corpus from different sources; SimCompass (Banea et al., 2014), which combines word2vec with information from WordNet. The results are given as Pearson and Spearman correlation scores in Table 1. LSTMEmbed achieves the state of the art by surpassing, in terms of Spearman correlation, alternative sense embedding approaches, as well as the best systems built spec"
P19-1165,D17-1034,0,0.0226096,"G Li and Jurafsky (2015) MUSE 80.00 78.26 82.61 88.41 73.33* 57.14 50.00 64.29 LSTMEmbed 92.50 72.00* • Li and Jurafsky (2015), a multi-sense embeddings model based on the Chinese Restaurant Process. • Jauhar et al. (2015), a multi-sense approach based on expectation-maximization style algorithms for inferring word sense choices in the training corpus and learning sense embeddings while incorporating ontological sources of information. Table 3: Synonym Recognition: accuracy (percentages). * Not statistically significant difference (χ2 , p < 0.05). • Modularizing Unsupervised Sense Embeddings (Lee and Chen, 2017, MUSE), an unsupervised approach that introduces a modularized framework to create sense-level representation learned with linear-time sense selection. the set which was most similar in meaning to the target word. The most likely synonym for a word w given the set of candidates Aw is calculated as: Syn (w, Aw ) = arg max Sim (w, v) v∈Aw (4) where Sim is the pairwise word similarity: Sim (w1 , w2 ) = max cosine (s~1 , s~2 ) s1 ∈Sw1 s2 ∈Sw2 (5) where Swi is the set of words and senses associated with the word wi . We consider all the inflected forms of every word, with and without all its possi"
P19-1165,D15-1200,0,0.180839,"ti-prototype embeddings rather than senses, due to the fact that these vectors are only identified as different from one another, while there is no clear identification of their inherent sense. Several approaches have used this idea: Huang et al. (2012) introduced a model which learned multi vectors per word by clustering word context representations. Neelakantan et al. (2014) extended word2vec and included a module which induced new sense vectors if the context in which a word occurred was too different from the previously seen contexts for the same word. A similar approach was introduced by Li and Jurafsky (2015), which used a Chinese Restaurant Process as a way to induce new senses. Finally, Peters et al. (2018) presented ELMo, a word-in-context representation model based on a deep bidirectional language model. In contrast to the other related approaches, ELMo does not have a token dictionary, but rather 1687 each token is represented by three vectors, two of which are contextual. These models are, in general, difficult to evaluate, due to their lack of linkage to a lexical-semantic resource. In marked contrast, LSTMEmbed, the neural architecture we present in this paper, aims to learn individual rep"
P19-1165,K17-1012,1,0.907918,"late the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings, i.e. embeddings which separate the various senses of each word in the vocabulary (Huang et al., 2012; Chen et al., 2014; Iacobacci et al., 2015; Flekova and Gurevych, 2016; Pilehvar and Collier, 2016; Mancini et al., 2017, among others). One of the weaknesses of these approaches, however, is that they do not take word ordering into account during the learning process. On the other hand, word-based approaches based on RNNs that consider sequence information have been presented, but they are not competitive in terms of speed or quality of the embeddings (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013). For example, in Figure 1 we show an excerpt of a t-SNE (Maaten and Hinton, 2008) projection of word and sense embeddings in the literature: 1685 Proceedings of the 57th Annual Meeting of the As"
P19-1165,K16-1006,0,0.315987,"regions without a clear correlation with (potentially ambiguous) words which are relevant to them. A more accurate representation would be to have word vectors distributed across all the space with defined clusters for each set of vectors related to each sense of a target word (Figure 2). Recently, the much celebrated Long-Short Term Memory (LSTM) neural network model has emerged as a successful model to learn representations of sequences, thus providing an ideal solution for many Natural Language Processing tasks whose input is sequence-based, e.g., sentences and phrases (Hill et al., 2016; Melamud et al., 2016; Peters et al., 2018). However, to date LSTMs have not been applied to the effective creation of sense embeddings linked to an explicit inventory. In this paper, we explore the capabilities of the architecture of LSTMs using sense-labeled corpora for learning semantic representations of words and senses. We present four main contributions: • We introduce LSTMEmbed, an RNN model based on a bidirectional LSTM for learning word and sense embeddings in the same semantic space, which – in contrast to the most popular approaches to the task – takes word ordering into account. • We present an innova"
P19-1165,H93-1061,0,0.534594,"lation, alternative sense embedding approaches, as well as the best systems built specifically for the CLSS word-to-sense similarity task. In terms of Pearson, LSTMEmbed is on a par with the current state of the art, i.e., MeerkatMafia. Experiment 2: Most Frequent Sense Induction. In a second experiment, we employed our representations to induce the most frequent sense (MFS) of the input words, which is known to be a hard-to-beat baseline for Word Sense Disambiguation systems (Navigli, 2009). The MFS is typically computed by counting the word sense pairs in an annotated corpus such as SemCor (Miller et al., 1993). To induce a MFS using sense embeddings, we identified – among all the sense embeddings of an ambiguous word – the sense which was closest to the word in terms of cosine similarity in the vector space. We evaluated all the sense embedding approaches on this task by comparing the induced most frequent senses against the MFS computed for all those words in SemCor which have a minimum number of 5 sense annotations (3731 words in total, that we release with the paper), so as to exclude words with insufficient gold-standard data for the estimates. We carried out our evaluation by calculating preci"
P19-1165,Q14-1019,1,0.788024,"(Section 4.2) and those concerned with the word level (Section 4.3). 4.1 Implementation Details Training data. We chose BabelNet (Navigli and Ponzetto, 2012) as our sense inventory.5 BabelNet is a large multilingual encyclopedic dictionary and semantic network, comprising approximately 16 million entries for concepts and named entities linked by semantic relations. As training corpus we used the English portion of BabelWiki,6 a multilingual corpus comprising the English Wikipedia (Scozzafava et al., 2015). The corpus was automatically annotated with named entities and concepts using Babelfy (Moro et al., 2014), a state-ofthe-art disambiguation and entity linking system, 5 We used version 4.0 as available from the website. http://lcl.uniroma1.it/ babelfied-wikipedia/ 6 based on the BabelNet semantic network. The English section of BabelWiki contains 3 billion tokens and around 3 million unique tokens. Learning embeddings. LSTMEmbed was built with the Keras7 library using Theano8 as backend. We trained our models with an Nvidia Titan X Pascal GPU. We set the dimensionality of the look-up table to 200 due to memory constraints. We discarded the 1,000 most frequent tokens and set the batch size to 2048"
P19-1165,D14-1113,0,0.0801335,"sense in the middle. Nevertheless, being based on word2vec, SW2V also lacks a notion of word ordering. Other approaches in the literature avoid the use of a predefined sense inventory. The vectors learned by such approaches are identified as multi-prototype embeddings rather than senses, due to the fact that these vectors are only identified as different from one another, while there is no clear identification of their inherent sense. Several approaches have used this idea: Huang et al. (2012) introduced a model which learned multi vectors per word by clustering word context representations. Neelakantan et al. (2014) extended word2vec and included a module which induced new sense vectors if the context in which a word occurred was too different from the previously seen contexts for the same word. A similar approach was introduced by Li and Jurafsky (2015), which used a Chinese Restaurant Process as a way to induce new senses. Finally, Peters et al. (2018) presented ELMo, a word-in-context representation model based on a deep bidirectional language model. In contrast to the other related approaches, ELMo does not have a token dictionary, but rather 1687 each token is represented by three vectors, two of wh"
P19-1165,D14-1162,0,0.0997544,"in NLP over ∗ Ignacio Iacobacci’s work was mainly done at the Sapienza University of Rome. the years, and latent vector-based representations, called embeddings, seem to be a good candidate for coping with ambiguity. Embeddings encode lexical and semantic items in a low-dimensional continuous space. These vector representations capture useful syntactic and semantic information of words and senses, such as regularities in the natural language, and relationships between them, in the form of relation-specific vector offsets. Recent approaches, such as word2vec (Mikolov et al., 2013), and GloVe (Pennington et al., 2014), are capable of learning efficient word embeddings from large unannotated corpora. But while word embeddings have paved the way for improvements in numerous NLP tasks (Goldberg, 2017), they still conflate the various meanings of each word and let its predominant sense prevail over all others in the resulting representation. Instead, when these embedding learning approaches are applied to senseannotated data, they are able to produce embeddings for word senses (Iacobacci et al., 2015). A strand of work aimed at tackling the lexical polysemy issue has proposed the creation of sense embeddings,"
P19-1165,P14-2089,0,0.0234383,"ector resulting from king − man + woman found to be very close to the induced vector of queen. GloVe (Pennington et al., 2014), an alternative approach trained on aggregated global word-word co-occurrences, obtained similar results. While these embeddings are surprisingly good for monosemous words, they fail to represent the non-dominant senses of words properly. For instance, the representations of bar 1686 and pub should be similar, as well as those of bar and stick, but having similar representations for pub and stick is undesirable. Several approaches were proposed to mitigate this issue: Yu and Dredze (2014) presented an alternative way to train word embeddings by using, in addition to common features, words having some relation in a semantic resource, like PPDB (Ganitkevitch et al., 2013) or WordNet (Miller, 1995). Faruqui et al. (2015) presented a technique applicable to pre-processed embeddings, in which vectors are updated (“retrofitted”) in order to make them more similar to those which share a word type and less similar to those which do not. The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998). Melamud et al. (2016) introduce"
P19-1165,N18-1202,0,0.350407,"ar correlation with (potentially ambiguous) words which are relevant to them. A more accurate representation would be to have word vectors distributed across all the space with defined clusters for each set of vectors related to each sense of a target word (Figure 2). Recently, the much celebrated Long-Short Term Memory (LSTM) neural network model has emerged as a successful model to learn representations of sequences, thus providing an ideal solution for many Natural Language Processing tasks whose input is sequence-based, e.g., sentences and phrases (Hill et al., 2016; Melamud et al., 2016; Peters et al., 2018). However, to date LSTMs have not been applied to the effective creation of sense embeddings linked to an explicit inventory. In this paper, we explore the capabilities of the architecture of LSTMs using sense-labeled corpora for learning semantic representations of words and senses. We present four main contributions: • We introduce LSTMEmbed, an RNN model based on a bidirectional LSTM for learning word and sense embeddings in the same semantic space, which – in contrast to the most popular approaches to the task – takes word ordering into account. • We present an innovative idea for taking a"
P19-1165,D16-1174,0,0.208048,"Missing"
P19-1165,E17-2025,0,0.0315058,"pplicable to pre-processed embeddings, in which vectors are updated (“retrofitted”) in order to make them more similar to those which share a word type and less similar to those which do not. The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998). Melamud et al. (2016) introduced context2vec, a model based on a bidirectional LSTM for learning sentence and word embeddings. This model uses large raw text corpora to train a neural model that embeds entire sentential contexts and target words in the same lowdimensional space. Finally, Press and Wolf (2017) introduced a model, based on word2vec, where the embeddings are extracted from the output topmost weight matrix, instead of the input one, showing that those representations are also valid word embeddings. 2.2 Sense Embeddings In contrast to the above approaches, each of which aims to learn representations of lexical items, sense embeddings represent individual word senses as separate vectors. One of the main approaches for learning sense embeddings is the so-called knowledge-based approach, which relies on a predefined sense inventory such as WordNet, BabelNet1 (Navigli and Ponzetto, 2012) o"
P19-1165,S14-2093,0,0.0153455,"ity dataset comprises 500 instances of words, each paired with a short list of candidate senses from WordNet with human ratings for their word-sense relatedness. To compute the word-tosense similarity we used our shared vector space of words and senses, and calculated the similarity using the cosine distance. We included not only alternative sense-based representations but also the best performing approaches on this task: MeerkatMafia (Kashyap et al., 2014), which uses Latent Semantic Analysis (Deerwester et al., 1990) and WordNet glosses to get word-sense similarity measurements; SemantiKLU (Proisl et al., 2014), an approach based on a distributional semantic model trained on a large Web corpus from different sources; SimCompass (Banea et al., 2014), which combines word2vec with information from WordNet. The results are given as Pearson and Spearman correlation scores in Table 1. LSTMEmbed achieves the state of the art by surpassing, in terms of Spearman correlation, alternative sense embedding approaches, as well as the best systems built specifically for the CLSS word-to-sense similarity task. In terms of Pearson, LSTMEmbed is on a par with the current state of the art, i.e., MeerkatMafia. Experime"
P19-1165,P15-1173,0,0.0363429,"Missing"
Q14-1019,E09-1005,0,0.0195463,"Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account the structural properties of the knowledge base (Mihalcea, 2005). Other approaches, instead, leverage the structural properties of the knowledge base 232 by exploiting centrality and connectivity measures (Sinha and Mihalcea, 2007; Tsatsaronis et al., 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010). One of the key steps of many knowledge-based WSD algorithms is the creation of a graph representing the semantic interpretations of the input text. Two main strategies to build this graph have been proposed: i) exploiting the direct connections, i.e., edges, between the considered sense candidates; ii) populating the graph according to (shortest) paths between them. In our approach we manage to unify these two strategies by automatically creating edges between sense candidates performing Random Walk with Restart (Tong et al., 2006). The recent upsurge of interest i"
Q14-1019,W06-1669,0,0.0243355,"Missing"
Q14-1019,W11-0104,0,0.0385072,"d sense candidates; ii) populating the graph according to (shortest) paths between them. In our approach we manage to unify these two strategies by automatically creating edges between sense candidates performing Random Walk with Restart (Tong et al., 2006). The recent upsurge of interest in multilinguality has led to the development of cross-lingual and multilingual approaches to WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality has been exploited in different ways, e.g., by using parallel corpora to build multilingual contexts (Guo and Diab, 2010; Banea and Mihalcea, 2011; Lefever et al., 2011) or by means of ensemble methods which exploit complementary sense evidence from translations in different languages (Navigli and Ponzetto, 2012b). In this work, we present a novel exploitation of the structural properties of a multilingual semantic network. 2.2 Entity Linking Entity Linking (Erbs et al., 2011; Rao et al., 2013; Cornolti et al., 2013) encompasses a set of similar tasks, which include Named Entity Disambiguation (NED), that is the task of linking entity mentions in a text to a knowledge base (Bunescu and Pasca, 2006; Cucerzan, 2007), and Wikification, i.e"
Q14-1019,E09-1013,0,0.00864966,"s. 2 Related Work 2.1 Word Sense Disambiguation Word Sense Disambiguation (WSD) is the task of choosing the right sense for a word within a given context. Typical approaches for this task can be classified as i) supervised, ii) knowledge-based, and iii) unsupervised. However, supervised approaches require huge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account the structural properties of the knowledge base (Mihalcea, 2005). Other approaches, instead, leverage the structural properties of the knowledge base 232 by exploiting centrality and connectivity measures (Sinha and"
Q14-1019,E06-1002,0,0.00909906,"ilingual contexts (Guo and Diab, 2010; Banea and Mihalcea, 2011; Lefever et al., 2011) or by means of ensemble methods which exploit complementary sense evidence from translations in different languages (Navigli and Ponzetto, 2012b). In this work, we present a novel exploitation of the structural properties of a multilingual semantic network. 2.2 Entity Linking Entity Linking (Erbs et al., 2011; Rao et al., 2013; Cornolti et al., 2013) encompasses a set of similar tasks, which include Named Entity Disambiguation (NED), that is the task of linking entity mentions in a text to a knowledge base (Bunescu and Pasca, 2006; Cucerzan, 2007), and Wikification, i.e., the automatic annotation of text by linking its relevant fragments of text to the appropriate Wikipedia articles. Mihalcea and Csomai (2007) were the first to tackle the Wikification task. In their approach they disambiguate each word in a sentence independently by exploiting the context in which it occurs. However, this approach is local in that it lacks a collective notion of coherence between the selected Wikipedia pages. To overcome this problem, Cucerzan (2007) introduced a global approach based on the simultaneous disambiguation of all the terms"
Q14-1019,S07-1054,0,0.00704452,"Missing"
Q14-1019,D13-1184,0,0.15652,"Missing"
Q14-1019,D07-1074,0,0.0420424,"d Diab, 2010; Banea and Mihalcea, 2011; Lefever et al., 2011) or by means of ensemble methods which exploit complementary sense evidence from translations in different languages (Navigli and Ponzetto, 2012b). In this work, we present a novel exploitation of the structural properties of a multilingual semantic network. 2.2 Entity Linking Entity Linking (Erbs et al., 2011; Rao et al., 2013; Cornolti et al., 2013) encompasses a set of similar tasks, which include Named Entity Disambiguation (NED), that is the task of linking entity mentions in a text to a knowledge base (Bunescu and Pasca, 2006; Cucerzan, 2007), and Wikification, i.e., the automatic annotation of text by linking its relevant fragments of text to the appropriate Wikipedia articles. Mihalcea and Csomai (2007) were the first to tackle the Wikification task. In their approach they disambiguate each word in a sentence independently by exploiting the context in which it occurs. However, this approach is local in that it lacks a collective notion of coherence between the selected Wikipedia pages. To overcome this problem, Cucerzan (2007) introduced a global approach based on the simultaneous disambiguation of all the terms in a text and th"
Q14-1019,J13-3008,1,0.272604,"Missing"
Q14-1019,P10-1156,0,0.143512,"vised. However, supervised approaches require huge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account the structural properties of the knowledge base (Mihalcea, 2005). Other approaches, instead, leverage the structural properties of the knowledge base 232 by exploiting centrality and connectivity measures (Sinha and Mihalcea, 2007; Tsatsaronis et al., 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010). One of the key steps of many knowledge-based WSD algorithms is the creation of a graph representing the semantic interpretations of the input text. Two main strateg"
Q14-1019,S13-2042,0,0.047507,"Missing"
Q14-1019,D11-1072,0,0.479102,"Missing"
Q14-1019,S10-1003,0,0.0440739,"eation of a graph representing the semantic interpretations of the input text. Two main strategies to build this graph have been proposed: i) exploiting the direct connections, i.e., edges, between the considered sense candidates; ii) populating the graph according to (shortest) paths between them. In our approach we manage to unify these two strategies by automatically creating edges between sense candidates performing Random Walk with Restart (Tong et al., 2006). The recent upsurge of interest in multilinguality has led to the development of cross-lingual and multilingual approaches to WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality has been exploited in different ways, e.g., by using parallel corpora to build multilingual contexts (Guo and Diab, 2010; Banea and Mihalcea, 2011; Lefever et al., 2011) or by means of ensemble methods which exploit complementary sense evidence from translations in different languages (Navigli and Ponzetto, 2012b). In this work, we present a novel exploitation of the structural properties of a multilingual semantic network. 2.2 Entity Linking Entity Linking (Erbs et al., 2011; Rao et al., 2013; Cornolti et al., 2013) encompasses"
Q14-1019,P11-2055,0,0.018708,"Missing"
Q14-1019,S13-2043,0,0.0134027,"d the same parameters on all the other WSD datasets. As for EL, we used the training part of AIDA-CoNLL (Hoffart et al., 2011) to set µ = 5 and θ = 0.0. 8.1 Systems Multilingual WSD. We evaluated our system on the SemEval-2013 task 12 by comparing it with the participating systems: • UMCC-DLSI (Guti´errez et al., 2013) a stateof-the-art Personalized PageRank-based approach that exploits the integration of different sources of knowledge, such as WordNet Domains/Affect (Strapparava and Valitutti, 2004), SUMO (Zouaq et al., 2009) and the eXtended WordNet (Mihalcea and Moldovan, 2001); • DAEBAK! (Manion and Sainudiin, 2013) which performs WSD on the basis of peripheral diversity within subgraphs of BabelNet; • GETALP (Schwab et al., 2013) which uses an Ant Colony Optimization technique together with the classical measure of Lesk (1986). We also compared with UKB w2w (Agirre and Soroa, 2009), a state-of-the-art approach for knowledge-based WSD, based on Personalized PageRank (Haveliwala, 2002). We used the same mapping from words to senses that we used in our approach, default parameters7 and BabelNet as the input graph. Moreover, we compared our system with IMS (Zhong and Ng, 2010), a state-of-theart supervised"
Q14-1019,H05-1052,0,0.00658717,"w domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account the structural properties of the knowledge base (Mihalcea, 2005). Other approaches, instead, leverage the structural properties of the knowledge base 232 by exploiting centrality and connectivity measures (Sinha and Mihalcea, 2007; Tsatsaronis et al., 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010). One of the key steps of many knowledge-based WSD algorithms is the creation of a graph representing the semantic interpretations of the input text. Two main strategies to build this graph have been proposed: i) exploiting the direct connections, i.e., edges, between the considered sense candidates; ii) populating the graph according to (shortest) paths"
Q14-1019,H93-1061,0,0.939372,"GETALP (Schwab et al., 2013) which uses an Ant Colony Optimization technique together with the classical measure of Lesk (1986). We also compared with UKB w2w (Agirre and Soroa, 2009), a state-of-the-art approach for knowledge-based WSD, based on Personalized PageRank (Haveliwala, 2002). We used the same mapping from words to senses that we used in our approach, default parameters7 and BabelNet as the input graph. Moreover, we compared our system with IMS (Zhong and Ng, 2010), a state-of-theart supervised English WSD system which uses an SVM trained on sense-annotated corpora, such as SemCor (Miller et al., 1993) and DSO (Ng and Lee, 1996), among others. We used the IMS model out-of-the-box with Most Frequent Sense (MFS) as backoff routine since the model obtained using the task trial data performed worse. We followed the original task formulation and evaluated the synsets in three different settings, i.e., 7 ./ukb wsd -D dict.txt -K kb.bin --ppr w2w ctx.txt 238 when using BabelNet senses, Wikipedia senses and WordNet senses, thanks to BabelNet being a superset of the other two inventories. We ran our system on a document-by-document basis, i.e., disambiguating each document at once, so as to test its"
Q14-1019,C12-1109,0,0.00674235,"ge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account the structural properties of the knowledge base (Mihalcea, 2005). Other approaches, instead, leverage the structural properties of the knowledge base 232 by exploiting centrality and connectivity measures (Sinha and Mihalcea, 2007; Tsatsaronis et al., 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010). One of the key steps of many knowledge-based WSD algorithms is the creation of a graph representing the semantic interpretations of the input text. Two main strategies to build this graph have been proposed: i) ex"
Q14-1019,D12-1128,1,0.30773,"Missing"
Q14-1019,P96-1006,0,0.0520549,"hich uses an Ant Colony Optimization technique together with the classical measure of Lesk (1986). We also compared with UKB w2w (Agirre and Soroa, 2009), a state-of-the-art approach for knowledge-based WSD, based on Personalized PageRank (Haveliwala, 2002). We used the same mapping from words to senses that we used in our approach, default parameters7 and BabelNet as the input graph. Moreover, we compared our system with IMS (Zhong and Ng, 2010), a state-of-theart supervised English WSD system which uses an SVM trained on sense-annotated corpora, such as SemCor (Miller et al., 1993) and DSO (Ng and Lee, 1996), among others. We used the IMS model out-of-the-box with Most Frequent Sense (MFS) as backoff routine since the model obtained using the task trial data performed worse. We followed the original task formulation and evaluated the synsets in three different settings, i.e., 7 ./ukb wsd -D dict.txt -K kb.bin --ppr w2w ctx.txt 238 when using BabelNet senses, Wikipedia senses and WordNet senses, thanks to BabelNet being a superset of the other two inventories. We ran our system on a document-by-document basis, i.e., disambiguating each document at once, so as to test its effectiveness on long cohe"
Q14-1019,J14-4005,1,0.471041,"n the available semantic interpretations of the input text to perform a joint disambiguation with both concepts and named entities. Our experiments show the benefits of our synergistic approach on six gold-standard datasets. 2 Related Work 2.1 Word Sense Disambiguation Word Sense Disambiguation (WSD) is the task of choosing the right sense for a word within a given context. Typical approaches for this task can be classified as i) supervised, ii) knowledge-based, and iii) unsupervised. However, supervised approaches require huge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches marginally take into account"
Q14-1019,S07-1016,0,0.181288,"of mentions to be disambiguated roughly ranges from 1K to 2K per language in the different setups. • The SemEval-2007 task 7 dataset for coarsegrained English all-words WSD (Navigli et al., 2007). We take into account only nominal mentions obtaining a dataset containing 1107 nouns to be disambiguated using WordNet. t := t + 1 return G?I i.e., avgdeg(GI ) = 8 . Finally, we select as the densest subgraph of the initial semantic interpretation graph GI the graph G?I that maximizes the average degree (see lines 14–15). 237 • The SemEval-2007 task 17 dataset for finegrained English all-words WSD (Pradhan et al., 2007). We considered only nominal mentions resulting in 158 nouns annotated with WordNet synsets. • The Senseval-3 dataset for English all-words WSD (Snyder and Palmer, 2004), which contains 899 nouns to be disambiguated using WordNet. • KORE50 (Hoffart et al., 2012), which consists of 50 short English sentences (mean length of 14 words) with a total number of 144 mentions manually annotated using YAGO2, for which a Wikipedia mapping is available. This dataset was built with the idea of testing against a high level of ambiguity for the EL task. • AIDA-CoNLL6 (Hoffart et al., 2011), which consists o"
Q14-1019,P11-1138,0,0.045134,"simultaneous disambiguation of all the terms in a text and the use of lexical context to disambiguate the mentions. To maximize the semantic agreement Milne and Witten (2008) introduced the analysis of the semantic relations between the candidate senses and the unambiguous context, i.e., words with a single sense candidate. However, the performance of this algorithm depends heavily on the number of links incident to the target senses and on the availability of unambiguous words within the input text. To overcome this issue a novel class of approaches have been proposed (Kulkarni et al., 2009; Ratinov et al., 2011; Hoffart et al., 2011) that exploit global and local features. However, these systems either rely on a difficult NP-hard formalization of the problem which is infeasible for long text, or exploit popularity measures which are domain-dependent. In contrast, we show that the semantic network structure can be leveraged to obtain state-of-the-art performance by synergistically disambiguating both word senses and named entities at the same time. Recently, the explosion of on-line social networking services, such as Twitter and Facebook, have contributed to the development of new methods for the ef"
Q14-1019,S13-2041,0,0.0274038,"Missing"
Q14-1019,S13-1003,0,0.073601,"ubgraph heuristic on the available semantic interpretations of the input text to perform a joint disambiguation with both concepts and named entities. Our experiments show the benefits of our synergistic approach on six gold-standard datasets. 2 Related Work 2.1 Word Sense Disambiguation Word Sense Disambiguation (WSD) is the task of choosing the right sense for a word within a given context. Typical approaches for this task can be classified as i) supervised, ii) knowledge-based, and iii) unsupervised. However, supervised approaches require huge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some of these approaches"
Q14-1019,W04-0811,0,0.151242,"rds WSD (Navigli et al., 2007). We take into account only nominal mentions obtaining a dataset containing 1107 nouns to be disambiguated using WordNet. t := t + 1 return G?I i.e., avgdeg(GI ) = 8 . Finally, we select as the densest subgraph of the initial semantic interpretation graph GI the graph G?I that maximizes the average degree (see lines 14–15). 237 • The SemEval-2007 task 17 dataset for finegrained English all-words WSD (Pradhan et al., 2007). We considered only nominal mentions resulting in 158 nouns annotated with WordNet synsets. • The Senseval-3 dataset for English all-words WSD (Snyder and Palmer, 2004), which contains 899 nouns to be disambiguated using WordNet. • KORE50 (Hoffart et al., 2012), which consists of 50 short English sentences (mean length of 14 words) with a total number of 144 mentions manually annotated using YAGO2, for which a Wikipedia mapping is available. This dataset was built with the idea of testing against a high level of ambiguity for the EL task. • AIDA-CoNLL6 (Hoffart et al., 2011), which consists of 1392 English articles, for a total of roughly 35K named entity mentions annotated with YAGO concepts separated in development, training and test sets. We exploited the"
Q14-1019,strapparava-valitutti-2004-wordnet,0,0.0143633,"and θ = 0.8 by optimizing F 1 on the trial dataset of the SemEval-2013 task on multilingual WSD (Navigli et al., 2013). We used the same parameters on all the other WSD datasets. As for EL, we used the training part of AIDA-CoNLL (Hoffart et al., 2011) to set µ = 5 and θ = 0.0. 8.1 Systems Multilingual WSD. We evaluated our system on the SemEval-2013 task 12 by comparing it with the participating systems: • UMCC-DLSI (Guti´errez et al., 2013) a stateof-the-art Personalized PageRank-based approach that exploits the integration of different sources of knowledge, such as WordNet Domains/Affect (Strapparava and Valitutti, 2004), SUMO (Zouaq et al., 2009) and the eXtended WordNet (Mihalcea and Moldovan, 2001); • DAEBAK! (Manion and Sainudiin, 2013) which performs WSD on the basis of peripheral diversity within subgraphs of BabelNet; • GETALP (Schwab et al., 2013) which uses an Ant Colony Optimization technique together with the classical measure of Lesk (1986). We also compared with UKB w2w (Agirre and Soroa, 2009), a state-of-the-art approach for knowledge-based WSD, based on Personalized PageRank (Haveliwala, 2002). We used the same mapping from words to senses that we used in our approach, default parameters7 and"
Q14-1019,N03-1033,0,0.0262374,"ists of 50 short English sentences (mean length of 14 words) with a total number of 144 mentions manually annotated using YAGO2, for which a Wikipedia mapping is available. This dataset was built with the idea of testing against a high level of ambiguity for the EL task. • AIDA-CoNLL6 (Hoffart et al., 2011), which consists of 1392 English articles, for a total of roughly 35K named entity mentions annotated with YAGO concepts separated in development, training and test sets. We exploited the POS tags already available in the SemEval and Senseval datasets, while we used the Stanford POS tagger (Toutanova et al., 2003) for the English sentences in the last two datasets. 6 We used AIDA-CoNLL as it is the most recent and largest available dataset for EL (Hachey et al., 2013). The TAC KBP datasets are available only to participants. Parameters. We fixed the parameters of RWR (Section 5) to the values α = .85, η = 100 and n = 1M which maximize F1 on a manually created tuning set made up of 10 gold-standard semantic signatures. We tuned our two disambiguation parameters µ = 10 and θ = 0.8 by optimizing F 1 on the trial dataset of the SemEval-2013 task on multilingual WSD (Navigli et al., 2013). We used the same"
Q14-1019,P11-1148,0,0.00522898,"Missing"
Q14-1019,P14-1122,1,0.554456,"Missing"
Q14-1019,P10-4014,0,0.963914,"it uses a densest subgraph heuristic on the available semantic interpretations of the input text to perform a joint disambiguation with both concepts and named entities. Our experiments show the benefits of our synergistic approach on six gold-standard datasets. 2 Related Work 2.1 Word Sense Disambiguation Word Sense Disambiguation (WSD) is the task of choosing the right sense for a word within a given context. Typical approaches for this task can be classified as i) supervised, ii) knowledge-based, and iii) unsupervised. However, supervised approaches require huge amounts of annotated data (Zhong and Ng, 2010; Shen et al., 2013; Pilehvar and Navigli, 2014), an effort which cannot easily be repeated for new domains and languages, while unsupervised ones suffer from data sparsity and an intrinsic difficulty in their evaluation (Agirre et al., 2006; Brody and Lapata, 2009; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011; Di Marco and Navigli, 2013). On the other hand, knowledge-based approaches are able to obtain good performance using readily-available structured knowledge (Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). Some"
Q14-1019,S07-1006,1,\N,Missing
Q14-1019,W09-2413,0,\N,Missing
Q14-1019,P10-1154,1,\N,Missing
Q14-1019,J14-1003,0,\N,Missing
Q14-1019,S13-2040,1,\N,Missing
Q14-1019,S10-1011,0,\N,Missing
Q14-1035,J08-4004,0,0.066065,"ncrete noun senses, whereas Puzzle Racer annotates all parts of speech and both concrete and abstract senses. Furthermore, Puzzle Racer’s output enables new visual games for tasks using word senses such as Word Sense Disambiguation, frame detection, and selectional preference acquisition. The second game, Ka-boom!, performs Word Sense Disambiguation (WSD) to identify the meaning of a word in context by players interacting with pictures. Sense annotation is regarded to be one of the most challenging NLP annotation tasks (Fellbaum et al., 1998; Edmonds and Kilgarriff, 2002; Palmer et al., 2007; Artstein and Poesio, 2008), so we view it as a challenging application for testing the limits of visual NLP games. Our work provides the following four contributions. First, we present a new game-centric design methodology for NLP games with a purpose. Second, we demonstrate with the first game that video games can produce linguistic annotations equal in quality to those of experts and at a cost reduction from gathering the same annotations via crowdsourcing; with the second game we show that video games provide a statistically significant performance improvement over a current state-of-the-art nonvideo game with a pur"
Q14-1035,P98-1013,0,0.163072,"stroy, causing them to lose interest. Extensibility Ka-boom! contains two core mechanics: (1) instructions on which pictures should be destroyed and which should be spared, and (2) series of images shown to the player during game play. As with Puzzle Racer, the Ka-boom! mechanics can be modified to extend the game to new types of annotation. For example, instructions could display picture examples and ask players to destroy either similar or opposite-meaning ideas in order to annotate synonyms or antonyms. In another setting, images can be associated with semantic frames (e.g., from FrameNet (Baker et al., 1998)) and players must spare images showing the frame of the game’s 458 sentence in order to provide frame annotations. 6 Ka-boom! Annotation Analysis Ka-boom! is intended to provide a complementary and more-enjoyable method for sense annotation using only pictures. To test its effectiveness, we perform a direct comparison with the state-of-the-art GWAP for sense annotation, Wordrobe (Venhuizen et al., 2013), which is not a video game. 6.1 Experimental Setup Organizers of the Wordrobe project (Venhuizen et al., 2013) provided a data set of 111 recentlyannotated contexts having between one and nine"
Q14-1035,W02-0817,0,0.1261,"Missing"
Q14-1035,D09-1046,0,0.0234013,"layers and 16,000 images. Two experiments were performed. First, we directly compared the quality of the game-based annotations with those of crowdsourcing. Second, we compared the difference in quality between expertbased gold standard images and the highest-ranked images rated by players. 4.1 Experimental Setup To test the potential of our approach, we selected a range of 23 polysemous noun, verb, and adjective lemmas, shown in Table 1. Lemmas had 4-10 senses each, for a total of 132 senses. Many lemmas have both abstract and concrete senses and some are known to have highly-related senses (Erk and McCarthy, 2009). Hence, given their potential annotation difficulty, we view performance on these lemmas as a lower bound. For all lemmas, during the image generation process (Sec. 3.2) annotators were able to produce queries for all but one sense, expect2v ;2 this produced 1356 gold images in G and 16,656 unrated images 2 The sense expect2v has the definition, “consider obligatory; request and expect.” Annotators were able to formulate many queries that could have potentially shown images of this definition, but the images results of such queries were consistently unrelated to the meaning. interest1n : a se"
Q14-1035,P13-1120,1,0.830288,"on-related mechanics: (1) an initial set of instructions on how players are to interact with images, (2) multiple series of images shown during game play, and (3) an open-ended question at the end of the game. These mechanics can be easily extended to other types of annotation where players must choose between several concepts shown as options in the puzzle gates. For example, the instructions could show players a phrase such as “a bowl of *” and ask players to race over images of things that might fit the “*” argument in order to obtain selectional preference annotations of the phrase (`a la Flati and Navigli (2013)); the lemmas or senses associated with the selected images can be aggregated to identify the types of arguments preferred by players for the game’s provided phrase. Similarly, the instructions could be changed to provide a set of keywords or phrases (instead of images associated with a sense) and ask players to navigate over images of the words in order to perform image labeling. Nouns Verbs Adjectives disc4n : a flat circular plate circular plate dish plate argument, arm, atmosphere, bank, difficulty, disc, interest, paper, party, shelter activate, add, climb, eat, encounter expect, rule, sm"
Q14-1035,P09-2053,0,0.456664,"Missing"
Q14-1035,W11-0404,0,0.0483446,"is crowdsourcing word sense annotations. Despite initial success in performing WSD using crowdsourcing (Snow et al., 2008), many approaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to"
Q14-1035,N13-1062,1,0.571609,"sense annotations. Despite initial success in performing WSD using crowdsourcing (Snow et al., 2008), many approaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to associate lexica"
Q14-1035,H93-1061,0,0.533758,"Sense disambiguation accuracies fluent English speakers and were free to recruit other players. A total of 19 players participated. Unlike Puzzle Racer, players were not compensated. Each context was seen in at least six games. WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two (Navigli, 2009); because all items are annotated, precision and recall are equivalent and we report performance as accuracy. Performance is measured relative to two baselines: (1) a baseline that picks the sense of the lemma that is most frequent in SemCor (Miller et al., 1993), denoted as MFS, and (2) a baseline equivalent to performance if players had randomly clicked on images, denoted as Random.5 6.2 Results Two analyses were performed. Because Kaboom! continuously revises the annotation during gameplay based on which pictures players spare, the first analysis assesses how the accuracy changes 5 This baseline is similar to random sense selection but takes into account differences in the number of pictures per sense. 459 with respect to the length of one Ka-boom! game. The second analysis measures the accuracy with respect to the number of games played per contex"
Q14-1035,W13-2323,0,0.0161494,"roaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to associate lexical items with images. Following, we first describe the design, annotation process, and extensibility of the game, and then discus"
Q14-1035,J14-4005,1,0.812208,"contest period. The difference in collection time reflects an important difference in the current resources: while crowdsourcing has established platforms with on-demand workers, no central platforms exist for games with a purpose with an analogous pool of game players. However, although the current games were released in a limited fashion, later game releases to larger venues such as Facebook may attract more players and significantly decrease both collection times and overall annotation cost. 5 Game 2: Ka-boom! Building large-scale sense-annotated corpora is a long-standing objective (see (Pilehvar and Navigli, 2014)) and has sparked significant interest in developing effective crowdsourcing annotation and GWAP strategies (cf. Sec. 2). Therefore, we propose a second video game, Ka-boom!, that produces sense annotations from game play. A live demonstration of the game is available online.4 Design and Game Play Ka-boom! is an action game in the style of the popular Fruit Ninja game: pictures are tossed on screen from the boundaries of the screen, which the player must then selectively destroy in order to score points. The game’s challenge stems from rapidly identifying which pictures should be destroyed or"
Q14-1035,D08-1027,0,0.470986,"Missing"
Q14-1035,P14-1122,1,0.71668,"Missing"
Q14-1035,W13-0215,0,0.0858123,"ed by linguistic experts or trained annotators. However, such effort is often very time- and cost-intensive, and as a result creating large-scale annotated datasets remains a longstanding bottleneck for many areas of NLP. As an alternative to requiring expert-based annotations, many studies used untrained, online workers, commonly known as crowdsourcing. When Within NLP, gamified annotation tasks include anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009) and disambiguation (Seemakurty et al., 2010; Venhuizen et al., 2013). The games’ interfaces typically incorporate common game elements such as scores, leaderboards, or difficulty levels. However, the game itself remains largely text-based, with a strong resemblance to a traditional annotation task, and little resemblance to games most people actively play. In the current work, we propose a radical shift in NLP-focused GWAP design, building graphical, dynamic games that achieve the same result as traditional annotation. Rather than embellish an annota449 Transactions of the Association for Computational Linguistics, 2 (2014) 449–463. Action Editor: Mirella Lapa"
Q14-1035,J13-3003,0,\N,Missing
Q14-1035,C98-1013,0,\N,Missing
Q15-1038,J07-4004,0,0.017085,"of the paper we identify BabelNet concepts or entities using a subscript-superscript notation where, for instance, bandibn refers to the i-th BabelNet sense for the English word band. 2.1 Textual Definition Processing The first step of the process is the automatic extraction of syntactic information (typed dependencies) and semantic information (word senses and named entity mentions) from each textual definition. Each definition undergoes the following steps: Syntactic Analysis. Each textual definition d is parsed to obtain a dependency graph Gd (Figure 1a). Parsing is carried out using C&C (Clark and Curran, 2007), a log-linear parser based on Combinatory Categorial Grammar (CCG). Although our algorithm seamlessly works with any syntactic formalism, CCG rules are especially suited to longer definitions and linguistic phenomena like coordinating conjunctions (Steedman, 2000). Semantic Analysis. Semantic analysis is based on Babelfy (Moro et al., 2014), a joint, stateof-the-art approach to entity linking and word sense disambiguation. Given a lexicalized semantic network as underlying structure, Babelfy uses a dense subgraph algorithm to identify high-coherence semantic interpretations of words and multi"
Q15-1038,J02-2003,0,0.0630009,"al relation patterns (Nakashole et al., 2012), or to improve the quality of surface pattern realizations (Moro and Navigli, 2013). Phenomena like synonymy and polysemy have been addressed with kernel-based similarity measures and soft clustering techniques (Min et al., 2012; Moro and Navigli, 2013), or exploiting the semantic types of relation arguments (Nakashole et al., 2012; Moro and Navigli, 2012). An appropriate modeling of semantic types (e.g. selectional preferences) constitutes a line of research by itself, rooted in earlier works like (Resnik, 1996) and focused on either class-based (Clark and Weir, 2002), or similarity-based (Erk, 2007), approaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation semantic types from OIE relations (Ritter et al., 2010), or to directly learn an ontological structure from a starting set of relation instances (Movshovitz-Attias and Cohen, 2015). However, the knowledge generated is often hard to interpret and integrate with existing knowledge bases without human intervention (Ritter et al., 2010). In this respect"
Q15-1038,D15-1084,1,0.813278,"yntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013) and tackled challenging linguistic phenomena like synonymy and polysemy. However, these issues have not yet been addressed in their entirety. Relation strings are still bound to surface text, lacking actual semantic content. Furthermore, most OIE systems do not have a clear and unified ontological structure and require additional processing steps, such as statistical inference mappings (Dutta et al., 2014), graphbased alignments of relational phrases (Grycner and Weikum, 2014), or knowledge base unification procedures (Delli Bovi et al., 2015), in order for their potential to be exploitable in real applications. In D EF IE the key idea is to leverage the linguistic analysis of recent semantically-enhanced OIE techniques while moving from open text to smaller corpora of dense prescriptive knowledge. The aim is 529 Transactions of the Association for Computational Linguistics, vol. 3, pp. 529–543, 2015. Action Editor: Sebastian Riedel. Submission batch: 5/2015; Revision batch: 8/2015; Published 10/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Figure 1: Syntactic-semantic graph construc"
Q15-1038,P07-1028,0,0.0166928,"), or to improve the quality of surface pattern realizations (Moro and Navigli, 2013). Phenomena like synonymy and polysemy have been addressed with kernel-based similarity measures and soft clustering techniques (Min et al., 2012; Moro and Navigli, 2013), or exploiting the semantic types of relation arguments (Nakashole et al., 2012; Moro and Navigli, 2012). An appropriate modeling of semantic types (e.g. selectional preferences) constitutes a line of research by itself, rooted in earlier works like (Resnik, 1996) and focused on either class-based (Clark and Weir, 2002), or similarity-based (Erk, 2007), approaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation semantic types from OIE relations (Ritter et al., 2010), or to directly learn an ontological structure from a starting set of relation instances (Movshovitz-Attias and Cohen, 2015). However, the knowledge generated is often hard to interpret and integrate with existing knowledge bases without human intervention (Ritter et al., 2010). In this respect, the semantic predicates propose"
Q15-1038,D11-1142,0,0.0896868,"oped for many years and are continuously being improved. A great deal of research also focuses on enriching available semi-structured resources, most notably Wikipedia, thereby creating taxonomies (Ponzetto and Strube, 2011; Flati et al., 2014), ontologies (Mahdisoltani et al., 2015) and semantic networks (Navigli and Ponzetto, 2012; Nastase and Strube, 2013). These solutions, however, 1 http://lcl.uniroma1.it/defie are inherently constrained to small and often prespecified sets of relations. A more radical approach is adopted in systems like T EXT RUNNER (Etzioni et al., 2008) and R E V ERB (Fader et al., 2011), which developed from the Open Information Extraction (OIE) paradigm (Etzioni et al., 2008) and focused on the unconstrained extraction of a large number of relations from massive unstructured corpora. Ultimately, all these endeavors were geared towards addressing the knowledge acquisition problem and tackling long-standing challenges in the field, such as Machine Reading (Mitchell, 2005). While earlier OIE approaches relied mostly on dependencies at the level of surface text (Etzioni et al., 2008; Fader et al., 2011), more recent work has focused on deeper language understanding at the level"
Q15-1038,P13-1120,1,0.851256,"roaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation semantic types from OIE relations (Ritter et al., 2010), or to directly learn an ontological structure from a starting set of relation instances (Movshovitz-Attias and Cohen, 2015). However, the knowledge generated is often hard to interpret and integrate with existing knowledge bases without human intervention (Ritter et al., 2010). In this respect, the semantic predicates proposed by Flati and Navigli (2013) seem to be more promising. A novelty in our approach is that issues like polysemy and synonymy are explicitly addressed with a unified entity linking and disambiguation algorithm. By incorporating explicit semantic content in our relation patterns, not only do we make relations less ambiguous, but we also abstract away from specific lexicalizations of the content words and merge together many patterns conveying the same semantics. Rather than using plain dependencies we also inject explicit semantic content into the dependency graph to generate a unified syntactic-semantic representation. Pre"
Q15-1038,P14-1089,1,0.629372,"cent years have witnessed the massive exploitation of collaborative, semi-structured information as the ideal middle ground between high-quality, fully-structured resources and the larger amount of cheaper (but noisy) unstructured text (Hovy et al., 2013). Collaborative projects, like Freebase (Bollacker et al., 2008) and Wikidata (Vrandeˇci´c, 2012), have been being developed for many years and are continuously being improved. A great deal of research also focuses on enriching available semi-structured resources, most notably Wikipedia, thereby creating taxonomies (Ponzetto and Strube, 2011; Flati et al., 2014), ontologies (Mahdisoltani et al., 2015) and semantic networks (Navigli and Ponzetto, 2012; Nastase and Strube, 2013). These solutions, however, 1 http://lcl.uniroma1.it/defie are inherently constrained to small and often prespecified sets of relations. A more radical approach is adopted in systems like T EXT RUNNER (Etzioni et al., 2008) and R E V ERB (Fader et al., 2011), which developed from the Open Information Extraction (OIE) paradigm (Etzioni et al., 2008) and focused on the unconstrained extraction of a large number of relations from massive unstructured corpora. Ultimately, all these"
Q15-1038,C14-1207,0,0.391829,"nt work has focused on deeper language understanding at the level of both syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013) and tackled challenging linguistic phenomena like synonymy and polysemy. However, these issues have not yet been addressed in their entirety. Relation strings are still bound to surface text, lacking actual semantic content. Furthermore, most OIE systems do not have a clear and unified ontological structure and require additional processing steps, such as statistical inference mappings (Dutta et al., 2014), graphbased alignments of relational phrases (Grycner and Weikum, 2014), or knowledge base unification procedures (Delli Bovi et al., 2015), in order for their potential to be exploitable in real applications. In D EF IE the key idea is to leverage the linguistic analysis of recent semantically-enhanced OIE techniques while moving from open text to smaller corpora of dense prescriptive knowledge. The aim is 529 Transactions of the Association for Computational Linguistics, vol. 3, pp. 529–543, 2015. Action Editor: Sebastian Riedel. Submission batch: 5/2015; Revision batch: 8/2015; Published 10/2015. c 2015 Association for Computational Linguistics. Distributed un"
Q15-1038,P11-1055,0,0.00600762,"iest days, OIE systems had to cope with the dimension and heterogeneity of huge unstructured sources of text. The first systems employed statistical techniques and relied heavily on information redundancy. Then, as soon as semistructured resources came into play (Hovy et al., 2013), researchers started developing learning systems based on self-supervision (Wu and Weld, 2007) and distant supervision (Mintz et al., 2009; Krause et al., 2012). Crucial issues in distant supervision, like noisy training data, have been addressed in various ways: probabilistic graphical models (Riedel et al., 2010; Hoffmann et al., 2011), sophisticated multi-instance learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class"
Q15-1038,D12-1093,0,0.0276217,"abstract away from specific lexicalizations of the content words and merge together many patterns conveying the same semantics. Rather than using plain dependencies we also inject explicit semantic content into the dependency graph to generate a unified syntactic-semantic representation. Previous works (Moro et al., 2013) used similar semantic graph representations to produce filtering rules for relation extraction, but they required a starting set of relation patterns and did not exploit syntactic information. A joint approach of syntacticsemantic analysis of text was used in works such as (Lao et al., 2012), but they addressed a substantially different task (inference for knowledge base completion) and assumed a radically different setting, with a predefined starting set of semantic relations from a given knowledge base. As we enforce an OIE approach, we do not have such requirements and directly process the input text via parsing and disambiguation. This enables D EF IE to generate relations already integrated with resources like WordNet and Wikipedia, without additional alignment steps (Grycner and Weikum, 2014), or semantic type propagations (Lin et al., 2012). As shown in Section 6.3, explic"
Q15-1038,D12-1082,0,0.0140151,"text was used in works such as (Lao et al., 2012), but they addressed a substantially different task (inference for knowledge base completion) and assumed a radically different setting, with a predefined starting set of semantic relations from a given knowledge base. As we enforce an OIE approach, we do not have such requirements and directly process the input text via parsing and disambiguation. This enables D EF IE to generate relations already integrated with resources like WordNet and Wikipedia, without additional alignment steps (Grycner and Weikum, 2014), or semantic type propagations (Lin et al., 2012). As shown in Section 6.3, explicit semantic content within relation patterns underpins a rich and high-quality relation taxonomy, whereas generalization in (Nakashole et al., 2012) is limited to support set inclusion and leads to sparser and less accurate results. 8 Conclusion and Future Work We presented D EF IE, an approach to OIE that, thanks to a novel unified syntactic-semantic analysis of text, harvests instances of semantic relations from a corpus of textual definitions. D EF IE extracts knowledge on a large scale, reducing data sparsity and disambiguating both arguments and relation p"
Q15-1038,D12-1094,0,0.0142884,"terature) seems rather the opposite, namely, to target Web-scale corpora. In contrast, we manage to extract a large amount of high-quality information by combining an OIE unsupervised approach with definitional data. A deeper linguistic analysis constitutes the focus of many OIE approaches. Syntactic dependencies are used to construct general relation patterns (Nakashole et al., 2012), or to improve the quality of surface pattern realizations (Moro and Navigli, 2013). Phenomena like synonymy and polysemy have been addressed with kernel-based similarity measures and soft clustering techniques (Min et al., 2012; Moro and Navigli, 2013), or exploiting the semantic types of relation arguments (Nakashole et al., 2012; Moro and Navigli, 2012). An appropriate modeling of semantic types (e.g. selectional preferences) constitutes a line of research by itself, rooted in earlier works like (Resnik, 1996) and focused on either class-based (Clark and Weir, 2002), or similarity-based (Erk, 2007), approaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation sema"
Q15-1038,P09-1113,0,0.0091846,"ings might also propagate this information across other knowledge bases and rephrase semantic relations in terms of, e.g., automatically generated Wikipedia hyperlinks. 7 Related Work From the earliest days, OIE systems had to cope with the dimension and heterogeneity of huge unstructured sources of text. The first systems employed statistical techniques and relied heavily on information redundancy. Then, as soon as semistructured resources came into play (Hovy et al., 2013), researchers started developing learning systems based on self-supervision (Wu and Weld, 2007) and distant supervision (Mintz et al., 2009; Krause et al., 2012). Crucial issues in distant supervision, like noisy training data, have been addressed in various ways: probabilistic graphical models (Riedel et al., 2010; Hoffmann et al., 2011), sophisticated multi-instance learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines W"
Q15-1038,Q14-1019,1,0.315483,"(word senses and named entity mentions) from each textual definition. Each definition undergoes the following steps: Syntactic Analysis. Each textual definition d is parsed to obtain a dependency graph Gd (Figure 1a). Parsing is carried out using C&C (Clark and Curran, 2007), a log-linear parser based on Combinatory Categorial Grammar (CCG). Although our algorithm seamlessly works with any syntactic formalism, CCG rules are especially suited to longer definitions and linguistic phenomena like coordinating conjunctions (Steedman, 2000). Semantic Analysis. Semantic analysis is based on Babelfy (Moro et al., 2014), a joint, stateof-the-art approach to entity linking and word sense disambiguation. Given a lexicalized semantic network as underlying structure, Babelfy uses a dense subgraph algorithm to identify high-coherence semantic interpretations of words and multi-word expressions across an input text. We apply Babelfy to each definition d, obtaining a sense mapping Sd from surface text (words and entity mentions) to word senses and named entities (Figure 1b). As a matter of fact, any disambiguation or entity linking strategy can be used at this stage. However, a knowledge-based unified approach like"
Q15-1038,P15-1140,0,0.0123952,"iate modeling of semantic types (e.g. selectional preferences) constitutes a line of research by itself, rooted in earlier works like (Resnik, 1996) and focused on either class-based (Clark and Weir, 2002), or similarity-based (Erk, 2007), approaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation semantic types from OIE relations (Ritter et al., 2010), or to directly learn an ontological structure from a starting set of relation instances (Movshovitz-Attias and Cohen, 2015). However, the knowledge generated is often hard to interpret and integrate with existing knowledge bases without human intervention (Ritter et al., 2010). In this respect, the semantic predicates proposed by Flati and Navigli (2013) seem to be more promising. A novelty in our approach is that issues like polysemy and synonymy are explicitly addressed with a unified entity linking and disambiguation algorithm. By incorporating explicit semantic content in our relation patterns, not only do we make relations less ambiguous, but we also abstract away from specific lexicalizations of the content"
Q15-1038,D12-1104,0,0.0968902,"Missing"
Q15-1038,P10-1134,1,0.686749,", while other OIE systems exploit massive corpora like Wikipedia (typically more than 1.5 billion tokens), ClueWeb (more than 33 billion tokens), or the Web itself. Furthermore, our semantic analysis based on Babelfy enables the discovery of semantic connections between both general concepts and named entities, with the potential to enrich existing structured and semi-structured resources, as we showed in a preliminary study on BabelNet (cf. Section 6.7). As the next step, we plan to apply D EF IE to open text and integrate it with definition extraction and automatic gloss finding algorithms (Navigli and Velardi, 2010; Dalvi et al., 2015). Also, by further exploiting the underlying knowledge base, inference and learning techniques (Lao et al., 2012; Wang et al., 2015) can be applied to complement our model, generating new triples or correcting wrong ones. Finally, another future perspective is to leverage the increasingly large variety of multilingual resources, like BabelNet, and move towards the modeling of language-independent relations. Acknowledgments The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234. This research was also partially supported by Google thro"
Q15-1038,P14-2119,0,0.0176763,"ly on information redundancy. Then, as soon as semistructured resources came into play (Hovy et al., 2013), researchers started developing learning systems based on self-supervision (Wu and Weld, 2007) and distant supervision (Mintz et al., 2009; Krause et al., 2012). Crucial issues in distant supervision, like noisy training data, have been addressed in various ways: probabilistic graphical models (Riedel et al., 2010; Hoffmann et al., 2011), sophisticated multi-instance learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class-attribute pairs; R E N OUN (Yahya et al., 2014) in turn exploits B IPERPEDIA to extract facts expressed as noun phrases. D EF IE focuses, instead, on smaller and denser corpor"
Q15-1038,P98-2180,0,0.0372511,"erent strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class-attribute pairs; R E N OUN (Yahya et al., 2014) in turn exploits B IPERPEDIA to extract facts expressed as noun phrases. D EF IE focuses, instead, on smaller and denser corpora of prescriptive knowledge. Although early works, such as MindNet (Richardson et al., 1998), had already highlighted the potential of textual definitions for extracting reliable semantic information, no OIE approach to the best of our knowledge has exploited definitional data to extract and disambiguate a large knowledge base of semantic relations. The direction of most papers (especially in the recent OIE literature) seems rather the opposite, namely, to target Web-scale corpora. In contrast, we manage to extract a large amount of high-quality information by combining an OIE unsupervised approach with definitional data. A deeper linguistic analysis constitutes the focus of many OIE"
Q15-1038,N13-1008,0,0.0209296,"loyed statistical techniques and relied heavily on information redundancy. Then, as soon as semistructured resources came into play (Hovy et al., 2013), researchers started developing learning systems based on self-supervision (Wu and Weld, 2007) and distant supervision (Mintz et al., 2009; Krause et al., 2012). Crucial issues in distant supervision, like noisy training data, have been addressed in various ways: probabilistic graphical models (Riedel et al., 2010; Hoffmann et al., 2011), sophisticated multi-instance learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class-attribute pairs; R E N OUN (Yahya et al., 2014) in turn exploits B IPERPEDIA to extract facts expressed as noun phrases. D EF IE"
Q15-1038,P10-1044,0,0.0150287,"xploiting the semantic types of relation arguments (Nakashole et al., 2012; Moro and Navigli, 2012). An appropriate modeling of semantic types (e.g. selectional preferences) constitutes a line of research by itself, rooted in earlier works like (Resnik, 1996) and focused on either class-based (Clark and Weir, 2002), or similarity-based (Erk, 2007), approaches. However, these methods are used to model the semantics of verbs rather than arbitrary patterns. More recently some strategies based on topic modeling have been proposed, either to infer latent relation semantic types from OIE relations (Ritter et al., 2010), or to directly learn an ontological structure from a starting set of relation instances (Movshovitz-Attias and Cohen, 2015). However, the knowledge generated is often hard to interpret and integrate with existing knowledge bases without human intervention (Ritter et al., 2010). In this respect, the semantic predicates proposed by Flati and Navigli (2013) seem to be more promising. A novelty in our approach is that issues like polysemy and synonymy are explicitly addressed with a unified entity linking and disambiguation algorithm. By incorporating explicit semantic content in our relation pa"
Q15-1038,D12-1042,0,0.00599018,"huge unstructured sources of text. The first systems employed statistical techniques and relied heavily on information redundancy. Then, as soon as semistructured resources came into play (Hovy et al., 2013), researchers started developing learning systems based on self-supervision (Wu and Weld, 2007) and distant supervision (Mintz et al., 2009; Krause et al., 2012). Crucial issues in distant supervision, like noisy training data, have been addressed in various ways: probabilistic graphical models (Riedel et al., 2010; Hoffmann et al., 2011), sophisticated multi-instance learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class-attribute pairs; R E N OUN (Yahya et al., 2014) in turn exploits B IPERPE"
Q15-1038,D14-1038,0,0.0179151,"e learning algorithms (Surdeanu et al., 2012), matrix factorization techniques (Riedel et al., 2013), labeled data infusion (Pershina et al., 2014) or crowd-based human computing (Kondreddi et al., 539 2014). A different strategy consists of moving from open text extraction to more constrained settings. For instance, the K NOWLEDGE VAULT (Dong et al., 2014) combines Web-scale extraction with prior knowledge from existing knowledge bases; B IPER PEDIA (Gupta et al., 2014) relies on schema-level attributes from the query stream in order to create an ontology of class-attribute pairs; R E N OUN (Yahya et al., 2014) in turn exploits B IPERPEDIA to extract facts expressed as noun phrases. D EF IE focuses, instead, on smaller and denser corpora of prescriptive knowledge. Although early works, such as MindNet (Richardson et al., 1998), had already highlighted the potential of textual definitions for extracting reliable semantic information, no OIE approach to the best of our knowledge has exploited definitional data to extract and disambiguate a large knowledge base of semantic relations. The direction of most papers (especially in the recent OIE literature) seems rather the opposite, namely, to target Web-"
Q15-1038,C98-2175,0,\N,Missing
R19-1015,P18-1031,0,0.1389,"y data-hungry, it appears unlikely that there will be much progress in WSD unless either more data is available, or less data is needed. Between the two directions, we believe efforts towards the latter will prove more fruitful, firstly, because of scalability considerations, and secondly, and more importantly, because of the recent growth in the use of transfer learning, as exemplified by contextualized embeddings. Contextualized embeddings have been shown to produce much better results on downstream tasks compared to end-to-end training, even when less data is provided (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; He et al., 2018; Akbik et al., 2018). Contextualized embeddings that use words as tokenization units, such as ELMo (Peters et al., 2018), are most suited to WSD. They are usually trained through selfsupervised Causal Language Modeling (CLM) (Lample and Conneau, 2019): given a word sequence w1 , w2 , . . . , wn the system has to use w1 to predict w2 , the sequence w1:2 to predict w3 and so on. CLM is inherently unidirectional, as the model must not be able to “peek” at the word it has to predict. Thus to encode the left and the right contexts two separate networks have to"
R19-1015,C18-1139,0,0.0722068,"Missing"
R19-1015,P16-1085,1,0.864255,"d Work Despite the limited availability of training data, the WSD systems offering the best performances are supervised ones. Many of the approaches are still end-to-end, i.e. they only make use of the information learned during the WSD training. End-to-end WSD Systems In WSD traditional machine learning techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019), the architecture we hereby propose has two main components, which we wil"
R19-1015,W16-5307,0,0.0549943,"Missing"
R19-1015,N19-1423,0,0.646505,"s unlikely that there will be much progress in WSD unless either more data is available, or less data is needed. Between the two directions, we believe efforts towards the latter will prove more fruitful, firstly, because of scalability considerations, and secondly, and more importantly, because of the recent growth in the use of transfer learning, as exemplified by contextualized embeddings. Contextualized embeddings have been shown to produce much better results on downstream tasks compared to end-to-end training, even when less data is provided (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; He et al., 2018; Akbik et al., 2018). Contextualized embeddings that use words as tokenization units, such as ELMo (Peters et al., 2018), are most suited to WSD. They are usually trained through selfsupervised Causal Language Modeling (CLM) (Lample and Conneau, 2019): given a word sequence w1 , w2 , . . . , wn the system has to use w1 to predict w2 , the sequence w1:2 to predict w3 and so on. CLM is inherently unidirectional, as the model must not be able to “peek” at the word it has to predict. Thus to encode the left and the right contexts two separate networks have to be used, even if the"
R19-1015,C18-1030,0,0.0117271,"the following contributions: Transfer Learning WSD Systems One of the best performing WSD systems (Yuan et al., 2016) employs a semi-supervised neural architecture, whereby a unidirectional LSTM was trained to predict a masked token on huge amounts of unlabeled data (over 100B tokens). The trained LSTM was used to produce contextualized embeddings for tagged tokens in SemCor; then kNN or a more sophisticated label propagation algorithm was used to predict a sense. The size of the training data makes replication difficult – a reimplementation attempt with a smaller corpus led to worse results (Le et al., 2018). A similar approach using ELMo contextualized embeddings has been presented by Peters et al. (2018), but the results were underwhelming. Another attempt at using transfer learning in WSD has been carried out by Melacci et al. (2018). The authors enhanced IMS with context2vec (Melamud et al., 2016), obtaining performance roughly on a par with Yuan et al. (2016). • we introduce the BiTransformer, a novel Transformer-based (Vaswani et al., 2017) coattentive layer allowing deeper bidirectionality; • we introduce QBERT (Quasi Bidirectional Encoder Representations from Transformers), a novel Transf"
R19-1015,S01-1001,0,0.67851,"doresearch/ flair 4 There are no further specifications about the composition of the training corpus. 126 5 Evaluation tasks Training and Test Data For each comparison system we train two WSD classifiers, one using only SemCor as training corpus and the other using the concatenation of SemCor and the corpus of WordNet’s Tagged Glosses5 (WTG). WTG includes 117659 manually disambiguated WordNet synset glosses, with 496776 annotated tokens. We test the performance of the models on the English all-words evaluation datasets from the SensEval and SemEval WSD evaluation campaigns, namely Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) and their concatenation (ALL). We use SemEval-2015 as our development set, to select the best epoch of the run. We use the version of SemCor and the evaluation datasets included in the WSD framework6 of Raganato et al. (2017b). As the first and main experiment we train and evaluate a WSD Transformer classifier (Section 5.1.1) using QBERT and comparison contextualized embeddings. To corroborate the results, as further experiments we evaluate the perfor"
R19-1015,P18-1230,0,0.164068,"ing techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019), the architecture we hereby propose has two main components, which we will refer to 123 QBERT Encoder Past Transformer Stack BiTransformer K Q e3 e4 e3 V 1 2 1 p 1 1 p 2 K Q 1 p 3 1 p e4 4 1 p 3 V P≫ f 4 1 pastmask. 1 Q e3 e3 e4 e4 V f 1 2 f 1 3 f 1 4 1 f n p 2 2 f n n p 3 3 f n 1 2 PAD f f 1 3 f n 2 K V 3 n f 4 K PAD n p 4 f n 3 n p 1 K Q F f n 4 PAD 1 4 n p 2 CLM"
R19-1015,S15-2049,1,0.885658,"a For each comparison system we train two WSD classifiers, one using only SemCor as training corpus and the other using the concatenation of SemCor and the corpus of WordNet’s Tagged Glosses5 (WTG). WTG includes 117659 manually disambiguated WordNet synset glosses, with 496776 annotated tokens. We test the performance of the models on the English all-words evaluation datasets from the SensEval and SemEval WSD evaluation campaigns, namely Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) and their concatenation (ALL). We use SemEval-2015 as our development set, to select the best epoch of the run. We use the version of SemCor and the evaluation datasets included in the WSD framework6 of Raganato et al. (2017b). As the first and main experiment we train and evaluate a WSD Transformer classifier (Section 5.1.1) using QBERT and comparison contextualized embeddings. To corroborate the results, as further experiments we evaluate the performance of the contextualized embeddings on the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) (Section 5.2). 5.1 5.1.1 Word Sense Dis"
R19-1015,E17-1010,1,0.937432,"ed English computational lexicon in NLP, the following two senses are associated (among many others) to the verb to serve: 1. devotion: devote (part of) one’s life or efforts to, as of countries, institutions, or ideas; 2. food: help to some food; help with some food or drink; The WSD system, in this case, would be tasked to associate the target word with the correct meaning – i.e. the devotion sense. 122 Proceedings of Recent Advances in Natural Language Processing, pages 122–131, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_015 framing of WSD as a tagging task (Raganato et al., 2017a; Vial et al., 2018). Consider the sentence (1) above. It features attractors, i.e. words or phrases pushing the sense interpretation in one direction or the other, with the left context providing a strong cue for the food sense and the right for the devotion sense. In this paper, we propose a modification of the usual CLM architecture for transfer learning that enables us to train a high-performance WSD system. In this context, we make the following contributions: Transfer Learning WSD Systems One of the best performing WSD systems (Yuan et al., 2016) employs a semi-supervised neural archite"
R19-1015,P16-1162,0,0.0221191,"embeddings, outperforming on the standard evaluation datasets both the previously established state of the art (by a large margin) and a comparable model using ELMo; Contextualized Embeddings Most of the approaches to contextualized embeddings involve CLM pretraining of directional (either attentive or recurrent) networks. Very successful CLM-based models include ELMo, in which two separate directional LSTMs are fed the output of a shared character-based Convolutional Neural Network (CNN) encoder (Peters et al., 2018), and OpenAI GPT, using Transformers instead of LSTMs and a BPE vocabulary (Sennrich et al., 2016) with regular embeddings instead of the CNN encoder (Radford et al., 2018). Another popular approach, Flair, features character-level LSTMs, outputting hidden states at word boundaries (Akbik et al., 2018). As CLM architectures are normally unidirectional, one alternative in order to guarantee a joint encoding of the context is the Masked Language Modeling (MLM) of BERT (Devlin et al., 2019), which, however, requires a variety of tricks at training time. • we use QBERT to beat ELMo on the recently established Word-in-Context (WiC) task (Pilehvar and Camacho-Collados, 2019). 2 Related Work Desp"
R19-1015,D17-2018,0,0.0125593,"training data, the WSD systems offering the best performances are supervised ones. Many of the approaches are still end-to-end, i.e. they only make use of the information learned during the WSD training. End-to-end WSD Systems In WSD traditional machine learning techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019), the architecture we hereby propose has two main components, which we will refer to 123 QBERT Encoder Past Transform"
R19-1015,W04-0811,0,0.154188,"r specifications about the composition of the training corpus. 126 5 Evaluation tasks Training and Test Data For each comparison system we train two WSD classifiers, one using only SemCor as training corpus and the other using the concatenation of SemCor and the corpus of WordNet’s Tagged Glosses5 (WTG). WTG includes 117659 manually disambiguated WordNet synset glosses, with 496776 annotated tokens. We test the performance of the models on the English all-words evaluation datasets from the SensEval and SemEval WSD evaluation campaigns, namely Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval07 (Pradhan et al., 2007), SemEval-13 (Navigli et al., 2013), SemEval-15 (Moro and Navigli, 2015) and their concatenation (ALL). We use SemEval-2015 as our development set, to select the best epoch of the run. We use the version of SemCor and the evaluation datasets included in the WSD framework6 of Raganato et al. (2017b). As the first and main experiment we train and evaluate a WSD Transformer classifier (Section 5.1.1) using QBERT and comparison contextualized embeddings. To corroborate the results, as further experiments we evaluate the performance of the contextualized embeddings"
R19-1015,D17-1008,1,0.919316,"Missing"
R19-1015,K15-1037,0,0.163951,"Missing"
R19-1015,D14-1162,0,0.0875958,"Missing"
R19-1015,L18-1168,0,0.531142,"ional machine learning techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019), the architecture we hereby propose has two main components, which we will refer to 123 QBERT Encoder Past Transformer Stack BiTransformer K Q e3 e4 e3 V 1 2 1 p 1 1 p 2 K Q 1 p 3 1 p e4 4 1 p 3 V P≫ f 4 1 pastmask. 1 Q e3 e3 e4 e4 V f 1 2 f 1 3 f 1 4 1 f n p 2 2 f n n p 3 3 f n 1 2 PAD f f 1 3 f n 2 K V 3 n f 4 K PAD n p 4 f n 3 n p 1 K Q F f n 4"
R19-1015,N18-1202,0,0.442365,"NLP, are particularly data-hungry, it appears unlikely that there will be much progress in WSD unless either more data is available, or less data is needed. Between the two directions, we believe efforts towards the latter will prove more fruitful, firstly, because of scalability considerations, and secondly, and more importantly, because of the recent growth in the use of transfer learning, as exemplified by contextualized embeddings. Contextualized embeddings have been shown to produce much better results on downstream tasks compared to end-to-end training, even when less data is provided (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; He et al., 2018; Akbik et al., 2018). Contextualized embeddings that use words as tokenization units, such as ELMo (Peters et al., 2018), are most suited to WSD. They are usually trained through selfsupervised Causal Language Modeling (CLM) (Lample and Conneau, 2019): given a word sequence w1 , w2 , . . . , wn the system has to use w1 to predict w2 , the sequence w1:2 to predict w3 and so on. CLM is inherently unidirectional, as the model must not be able to “peek” at the word it has to predict. Thus to encode the left and the right contexts two s"
R19-1015,N19-1128,0,0.0662924,"Missing"
R19-1015,P15-1058,0,0.0228602,"ervised ones. Many of the approaches are still end-to-end, i.e. they only make use of the information learned during the WSD training. End-to-end WSD Systems In WSD traditional machine learning techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019), the architecture we hereby propose has two main components, which we will refer to 123 QBERT Encoder Past Transformer Stack BiTransformer K Q e3 e4 e3 V 1 2 1 p 1 1 p 2 K Q 1 p 3 1 p e4"
R19-1015,E17-2025,0,0.0306109,"the WSD and Word-in-Context experiments. Finally, we report the setup and results of the experiments in Section 5. 4.1 Comparison Systems 2. Off-the-shelf pretrained Flair (Akbik et al., 2018). We employ the models the project’s page3 refers to as mix-forward and mix-backward, pretrained on “Web, Wikipedia, Subtitles”4 . We concatenate their contextualized embeddings. QBERT Encoder Pretraining CLM Prediction Head and Hyperparameters To train the QBERT Encoder on CLM we use an Adaptive Softmax (Grave et al., 2017) layer as Prediction Head. Following Baevski and Auli (2018), we tie the weights (Press and Wolf, 2017) of the embedding matrices but not the projective weights. Both Adaptive Input and Adaptive Softmax use a vocabulary of 400k words, with cutoffs set to 35k, 100k, 200k and a shrinking factor of 4. The past and future stacks as well as the BiTransformer feature an input and output size of 512, while the first layer of the internal feedforward projects the input to 2048 dimensions, the same as the base configuration in Vaswani et al. (2017). The masked Transformer stacks are both 5-layer deep. 3. SBERT (Shallowly Bidirectional Encoder Representations from Transformers), a baseline featuring the"
R19-1015,D17-1120,1,0.903352,"ed English computational lexicon in NLP, the following two senses are associated (among many others) to the verb to serve: 1. devotion: devote (part of) one’s life or efforts to, as of countries, institutions, or ideas; 2. food: help to some food; help with some food or drink; The WSD system, in this case, would be tasked to associate the target word with the correct meaning – i.e. the devotion sense. 122 Proceedings of Recent Advances in Natural Language Processing, pages 122–131, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_015 framing of WSD as a tagging task (Raganato et al., 2017a; Vial et al., 2018). Consider the sentence (1) above. It features attractors, i.e. words or phrases pushing the sense interpretation in one direction or the other, with the left context providing a strong cue for the food sense and the right for the devotion sense. In this paper, we propose a modification of the usual CLM architecture for transfer learning that enables us to train a high-performance WSD system. In this context, we make the following contributions: Transfer Learning WSD Systems One of the best performing WSD systems (Yuan et al., 2016) employs a semi-supervised neural archite"
R19-1015,P10-4014,0,0.0953681,"ricks at training time. • we use QBERT to beat ELMo on the recently established Word-in-Context (WiC) task (Pilehvar and Camacho-Collados, 2019). 2 Related Work Despite the limited availability of training data, the WSD systems offering the best performances are supervised ones. Many of the approaches are still end-to-end, i.e. they only make use of the information learned during the WSD training. End-to-end WSD Systems In WSD traditional machine learning techniques are still very competitive because they are not as data-hungry as neural networks. The very popular It Makes Sense (IMS) system (Zhong and Ng, 2010), based on Support Vector Machines and hand-crafted features, performs very well when word embeddings are used as additional features (Iacobacci et al., 2016); the classifier by Papandrea et al. (2017) also gets competitive results. The system of Weissenborn et al. (2015) attains very high performances, but only disambiguates nouns. More recently, neural models have been developed (Kageb¨ack and Salomonsson, 2016; Uslu et al., 2018; Luo et al., 2018). Some of the most successful offer an intuitive 3 The QBERT Architecture Similarly to other LM-based approaches to contextualized embeddings (Pet"
S07-1006,W02-0817,0,0.0564553,"Missing"
S07-1006,N06-2015,0,0.143704,"Missing"
S07-1006,H93-1061,0,0.482724,"We calculated two baselines for the test corpus: a random baseline, in which senses are chosen at random, and the most frequent baseline (MFS), in which we assign the first WordNet sense to each word in the dataset. Formally, the accuracy of the random baseline was calculated as follows: |T | BLRand = 1 X 1 |T |i=1 |CoarseSenses(wi )| 32 1 X δ(wi , 1) |T |i=1 where δ(wi , k) equals 1 when the k-th sense of word wi belongs to the cluster(s) manually associated by the lexicographer to word wi (0 otherwise). Notice that our calculation of the MFS is based on the frequencies in the SemCor corpus (Miller et al., 1993), as we exploit WordNet sense rankings. 4 Results 12 teams submitted 14 systems overall (plus two systems from a 13th withdrawn team that we will not report). According to the SemEval policy for task organizers, we remark that the system labelled as U O R-SSI was submitted by the first author (the system is based on the Structural Semantic Interconnections algorithm (Navigli and Velardi, 2005) with a lexical knowledge base composed by WordNet and approximately 70,000 relatedness edges). Even though we did not specifically enrich the algorithm’s knowledge base on the task at hand, we list the s"
S07-1006,P06-1014,1,0.759012,"Missing"
S07-1006,W04-0811,0,0.096086,"Missing"
S07-1009,P06-1057,0,0.102043,"Missing"
S07-1009,P99-1004,0,0.00750796,"target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC frequency data. 4. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of all synsets of the target, ranked with the BNC frequency data. We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin (Lin, 1998) and αSD (Lee, 1999) 4 . We took the word with the largest similarity (or smallest distance for αSD and l1) for best and the top 10 for oot. For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word. 4 Systems 9 teams registered and 8 participated, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, U"
S07-1009,W02-0816,1,0.782675,"st researchers believe that it will ultimately prove useful for applications which need some degree of semantic interpretation, the jury is still out on this point. One problem is that WSD systems have been tested on fine-grained inventories, rendering the task harder than it need be for many applications (Ide and Wilks, 2006). Another significant problem is that there is no clear choice of inventory for any given task (other than the use of a parallel corpus for a specific language pair for a machine translation application). The lexical substitution task follows on from some previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by researchers on a task which has potential for NLP applications. Finding alternative words that can occur in given contexts would potentially be useThe task involves a lexical sample of nouns, verbs, adjectives and adverbs. Both annotators and systems select one or more substitutes for the target word in the context of a sentence. The data was selected from the English Internet Corpus of English produced by Sharoff (2006) from the Internet (http://corpus.leeds.ac.uk/internet.html). This is a balanced corpus similar in flavour to the BNC, thou"
S07-1009,H93-1061,0,0.832213,"multiword response from a majority of at least 2 annotators. Let mwi ∈ M W be the multiword identified by majority vote for item i. Let M W sys be the subset of T for which there is a multiword response from the system and mwsysi be a multiword specified by the system for item i. detection P = P if mwi exists at i |M W sys| mwsysi ∈M W sys 1 (9) detection R = P mwsysi ∈M W 1 if mwi exists at i |M W | (10) identif ication P = P if mwsysi = mwi |M W sys| mwsysi ∈M W sys 1 (11) identif ication R = P 3.1 mwsysi ∈M W 1 if mwsysi = mwi |M W | (12) Baselines We produced baselines using WordNet 2.1 (Miller et al., 1993a) and a number of distributional similarity measures. For the WordNet best baseline we found the best ranked synonym using the criteria 1 to 4 below in order. For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1. Synonyms from the first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC freque"
S13-2035,S07-1002,0,0.261696,"Missing"
S13-2035,J13-3008,1,0.296144,"Missing"
S13-2035,S10-1011,0,0.208757,"sk described in this paper1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems. The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre 1 http://www.cs.york.ac.uk/semeval-2013/task11/ 193 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 193–201, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics and Soroa, 2007; Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms. Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning). For instance, consider the target query apple and the following 3 search result snippets: Figure 1: An example of search result for the apple query, including: page title, URL and snippet. query length AOL logs our dataset 1 45"
S13-2035,D10-1012,1,0.109008,"luation is actually an instance of the more general and difficult problem of evaluating clustering algorithms. Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively. A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds. Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results. The Semeval-2013 task described in this paper1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems. The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre 1 http://www.cs.york.ac.uk/semeval-2013/task11/ 193 S"
S13-2035,W04-0811,0,0.0264255,"identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008). On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses. In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output. Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clusterin"
S13-2040,E09-1005,0,0.281923,"ts parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et al., 2011), which enriches the WordNet semantic network using edges from multiple lexical resources, such as WordNet Domains and the eXtended WordNet. WSD was then performed using the ISR-WN network in combination with the algorithm of Guti´errez (2012), which is an extension of the Personalized PageRank algorithm for WSD (Agirre and Soroa, 2009) which includes senses frequency. The algorithm requires initializing the PageRank algorithm with a set of seed synsets (vertices) in the network; this initialization represents the key variation among UMCC’s three approaches. The RUN -1 system performs WSD using all noun instances from the sentence context. In contrast, the RUN -2 works at the discourse level and initializes the PageRank using the synsets of all Team System English French German Italian Spanish DAEBAK! GETALP GETALP UMCC-DLSI UMCC-DLSI UMCC-DLSI PD BN-1 BN-2 RUN -1 RUN -2 RUN -3 0.604 0.263 0.266 0.677 0.685 0.680 0.538 0.261"
S13-2040,S01-1001,0,0.86219,"pating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010"
S13-2040,D09-1046,0,0.0512076,"Missing"
S13-2040,S13-2049,1,0.473907,"Missing"
S13-2040,S10-1003,0,0.0606125,"focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has en"
S13-2040,W04-0807,0,0.170117,"troduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation"
S13-2040,W09-2412,0,0.0262841,"Missing"
S13-2040,D12-1128,1,0.887659,"Missing"
S13-2040,P12-3012,1,0.854178,"Missing"
S13-2040,S07-1006,1,0.783276,"d limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and"
S13-2040,S07-1016,0,0.745774,"en more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution ("
S13-2040,C12-1146,0,0.0336153,"DAEBAK! submitted one system called PD (Peripheral Diversity) based on BabelNet path indices from the BabelNet synset graph. Using a ±5 sentence window around the target word, a graph is constructed for all senses of co-occurring lemmas following the procedure proposed by Navigli and Lapata (2010). The final sense is selected based on measuring connectivity to the synsets of neighboring lemmas. The MFS is used as a backoff strategy when no appropriate sense can be picked out. GETALP GETALP submitted three systems, two for BabelNet and one for WordNet, all based on the ant-colony algorithm of (Schwab et al., 2012), which uses the sense inventory network structure to identify paths connecting synsets of the target lemma to the synsets of other lemmas in context. The algorithm requires setting several parameters for the weighting of the structure of the contextbased graph, which vary across the three systems. The BN1 system optimizes its parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et"
S13-2040,W04-0811,0,0.892093,"s future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Wo"
S13-2040,P95-1026,0,0.301495,". Finally, the RUN -3 system initializes using all words in the sentence. UMCC-DLSI Run-2 0.9 5 0.8 Results and Discussion 0.7 227 0.6 WSD F1 All teams submitted at least one system using the BabelNet inventory, shown in Table 3. The UMCCDLSI systems were consistently able to outperform the MFS baseline (a notoriously hard-to-beat heuristic) in all languages except German. Additionally, the DAEBAK! system outperformed the MFS baseline on French and Italian. The UMCC-DLSI RUN 2 system performed the best for all languages. Notably, this system leverages the single-sense per discourse heuristic (Yarowsky, 1995), which uses the same sense label for all occurrences of a lemma in a document. UMCC-DLSI submitted the only three systems to use Wikipedia-based senses. Table 4 shows their performance. Of the three sense inventories, Wikipedia had the most competitive MFS baseline, scoring at least 0.694 on all languages. Notably, the Wikipedia-based system has the lowest recall of all systems. Despite having superior precision to the MFS baseline, the low recall brought the resulting F1 measure below the MFS. Two teams submitted four total systems for WordNet, shown in Table 5. The UMCC-DLSI RUN -2 system w"
S13-2040,W09-2413,0,\N,Missing
S13-2040,S10-1002,0,\N,Missing
S14-2003,S12-1051,0,0.573556,"alone. 1 Introduction Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning. Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval (Hliaoutakis et al., 2006) or paraphrasing (Glickman and Dagan, 2003). Semantic similarity evaluations have largely focused on comparing similar types of lexical items. Most recently, tasks in SemEval (Agirre et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Nav"
S14-2003,S14-2001,0,0.0683247,"Missing"
S14-2003,J08-4004,0,0.0124731,"e average similarity value of non-OOV items. Baseline scores were made public after the evaluation period ended. Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation period concluded. Unlike LCS, GST better handles the transpositions of tokens across the two texts and can still report high similarity when encountering reordered text. The minimum match length for GST was set to 6. inter-annotator correlations of 0.377–0.832. However, we note that Pearson correlation and Krippendorff’s α are not directly comparable (Artstein and Poesio, 2008), as annotators’ scores may be correlated, but completely disagree. Second, the two-phase construction process produced values that were evenly distributed across the rating scale, shown in Figure 1 as the distribution of the values for all data sets. However, we note that this creation procedure was very resource intensive and, therefore, semi-automated or crowdsourcing-based approaches for producing high-quality data will be needed to expand the size of the data in future CLSS-based evaluations. Nevertheless, as a pilot task, the manual effort was essential for ensuring a rigorouslyconstruct"
S14-2003,W13-3806,0,0.0852038,"se 1 were rated for their similarity according to the scale described in Section 2.2. An initial pilot study showed that crowdsourcing was only moderately effective for producing these ratings with high agreement. Furthermore, the texts used in Task 3 came from a variety of genres, such as scientific domains, which some workers had difficulty understanding. While we note that crowdsourcing has been used in prior STS tasks for generating similarity scores (Agirre et al., 2012; Agirre et al., 2013), both tasks’ efforts encountered lower worker score correlations on some portions of the dataset (Diab, 2013), suggesting that crowdsourcing may not be reliable for judging the similarity of certain types of text. See Section 3.5 for additional details. Therefore, to ensure high quality, the first two organizers rated all items independently. Because the sentence-to-phrase and phrase-to-word comparisons contain slang and idiomatic language, a third American English mother tongue annotator was added for those data sets. The third annotator was compensated e250 for their assistance. Annotators were allowed to make finer-grained distinctions in similarity using multiples of 0.25. For all items, when any"
S14-2003,P06-1014,1,0.050394,"Missing"
S14-2003,N13-1092,0,0.0642978,"Missing"
S14-2003,J14-4005,1,0.692332,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,P14-1044,1,0.162584,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,ide-suderman-2004-american,0,0.00893705,"from specific domains, social media, and text with idiomatic or slang language. Table 3 summarizes the corpora and their distribution across the test and training sets for each comparison type, with a high-level description of the genre of the data. We briefly describe the corpora next. The WikiNews, Reuters 21578, and Microsoft Research (MSR) Paraphrase corpora are all drawn from newswire text, with WikiNews being authored by volunteer writers and the latter two corpora written by professionals. Travel Guides was drawn from the Berlitz travel guides data in the Open American National Corpus (Ide and Suderman, 2004) and includes very verbose sentences 1 Annotation materials along with all training and test data are available on the task website http://alt.qcri. org/semeval2014/task3/. 18 4 – Very Similar The two items have very similar meanings and the most important ideas, concepts, or actions in the larger text are represented in the smaller text. Some less important information may be missing, but the smaller text is a very good summary of the larger text. 3 – Somewhat Similar The two items share many of the same important ideas, concepts, or actions, but include slightly different details. The smalle"
S14-2003,S12-1046,0,0.00660937,"download?” is captured in the phrase “streaming vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high perform"
S14-2003,S12-1047,1,0.484906,"l task for evaluating the capabilities of systems at measuring all types of semantic similarity, independently of the size of the text. To accomplish this objective, systems were presented with items from four comparison types: (1) paragraph to sentence, (2) sentence to phrase, (3) phrase to word, and (4) word to sense. Given a pair of items, a system must assess the degree to which the meaning of the larger item is captured in the smaller item. WordNet 3.0 was chosen as the sense inventory (Fellbaum, 1998). 2.2 Task Data Rating Scale 3.1 Following previous SemEval tasks (Agirre et al., 2012; Jurgens et al., 2012), Task 3 recognizes that two items’ similarity may fall within a range of similarity values, rather than having a binary notion of similar or dissimilar. Initially a six-point (0–5) scale similar to that used in the STS tasks was considered (Agirre et al., 2012); however, annotators found difficulty in deciding between the lower-similarity options. After multiple revisions and feedback from a group of initial annotators, we developed a five-point Likert scale for rating a pair’s similarity, shown in Table 1.1 The scale was designed to systematically order a broad range of semantic relations: s"
S14-2003,S01-1004,0,0.0226851,"Missing"
S14-2003,S10-1004,0,0.00761061,"vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high performance for multiple comparison types. Further,"
S14-2003,2005.mtsummit-papers.11,0,0.00244139,"ce on Wikipedia. Food reviews were drawn from the SNAP Amazon Fine Food Reviews data set (McAuley and Leskovec, 2013) and are customer-authored reviews for a variety of food items. Fables were taken from a collection of Aesop’s Fables. The Yahoo! Answers corpus was derived from the Yahoo! Answers data set, which is a collection of questions and answers from the Community Question Answering (CQA) site; the data set is notable for having the highest degree of ungrammaticality in our test set. SMT Europarl is a collection of texts from the English-language proceedings of the European parliament (Koehn, 2005); Europarl data was also used in the PPDB corpus (Ganitkevitch et al., 2013), from which phrases were extracted. Wikipedia was used to generate two phrase data sets from (1) extracting the definitional portion of an article’s initial sentence, e.g., “An [article name] is a [definition],” and (2) captions for an article’s images. Web queries were gathered from online sources of realworld queries. Last, the first and second authors generated slang and idiomatic phrases based on expressions contained in Wiktionary. 3.2 Annotation Process A two-phase process was used to produce the test and traini"
S14-2003,S13-1004,0,\N,Missing
S15-1021,W11-2501,0,0.37889,"tion about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, where the systems have to classify word pairs from a different domain than the domains in the training set. The objective is to assess the importance of the domain-aware training instances for the classification. The K&H dataset contains only instances from three domains and is imbalanced between the number of instances across domains and relation types. Therefore, our second experiment tests each method on the BLESS dataset (Baroni and Lenci, 2011), which spans 17 topical domains and includes five relation types, the three in K&H and (a) attributes of concepts, a relation holding between nouns and adjectives, and (b) actions performed by/to concepts a relation holding between nouns and verbs. In total, the BLESS dataset contains 14400 positive instances and an equal number of negative instances. This experiment measures the generalizability of each system and tests the capabilities of the systems for lexical-semantic relation types other than taxonomic relations. Domain-aware training instances To show the importance of the domain-aware"
S15-1021,P14-1023,0,0.0665731,"nstances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target seman"
S15-1021,P99-1008,0,0.0152874,"ties among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisitio"
S15-1021,W03-0415,0,0.0316758,"es of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and"
S15-1021,P06-1038,0,0.0330214,"ttern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined"
S15-1021,J13-3008,1,0.128812,"Missing"
S15-1021,P14-1113,0,0.167986,"jective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semantic relations. Given a set of target semantic relations R = {r1 , . . . , rn }, and a set of word pairs W = {(x, y)1 , . . . , (x, y)n }, the task is to label each word pair (x, y)"
S15-1021,J06-1005,0,0.0169068,"ation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relat"
S15-1021,C92-2082,0,0.691702,"been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techn"
S15-1021,W09-0205,0,0.151917,"e to requiring co-occurrence, other works have classified the relation of a word pair using lexical similarity, i.e., the similarity of the concepts themselves. Given two word pairs, (w1 , w2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even wh"
S15-1021,S12-1047,1,0.748516,"to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. The patterns retained are then used as features to train an SVM classifier over the set of possible relation types. DSZhila & DSLevy Word embeddings have previously been shown to accurately measure relational similarity; Zhila et al. (2013) demonstrate state-ofthe-art performance on SemEval-2012 Task 2 (Jurgens et al., 2012) which measures word pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken 187 as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings prod"
S15-1021,D10-1108,0,0.00836648,"64.7 F 8.7 14.2 59.0 21.0 14.2 73.1 74.2 73.5 76.4 Wikipedia P R F 77.0 11.7 20.4 89.4 16.2 27.5 94.0 75.5 83.7 32.8 22.6 26.8 27.7 15.6 20.0 96.8 87.7 92.0 97.6 89.3 93.2 95.4 86.1 90.5 96.7 88.4 92.4 Table 3: Aggregated results obtained for the indomain setup with the K&H dataset. Detailed results are presented in the Appendix A. occur in text. Therefore, in the first experiment, we test whether the recall of classification systems is improved when the word pair representation encodes information about lexical and relational similarity. As an evaluation dataset, we expand on the dataset of Kozareva and Hovy (2010) (K&H), which was collected from hyponym-hypernym instances from WordNet (Miller, 1995) spanning three topical domains: animals, plants and vehicles. Because our systems are capable of classifying instances with more than one relation at once, we enhance this dataset with instances of two more relation types: co-hyponymy and meronymy. Co-hyponyms are extracted directly from the K&H dataset: two words are co-hyponyms if they have the same direct ancestor.5 To avoid including generic nouns, such as “migrator” in the “animal” domain, only leaf nodes are considered. The meronym instances are extra"
S15-1021,P14-2050,0,0.201948,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,W14-1618,0,0.410157,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,P14-5010,0,0.00321295,"Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIR C LASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation"
S15-1021,N13-1090,0,0.549126,"i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The go"
S15-1021,W12-4104,0,0.0214934,"nd relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b;"
S15-1021,P09-1113,0,0.136745,"lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can classify. As an alternat"
S15-1021,P10-1134,1,0.846022,"roaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov"
S15-1021,E09-1071,0,0.0480814,"Missing"
S15-1021,S13-2056,0,0.0316207,"al basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our work we focus on the classification of word pairs as instances of lexical-semantic relations out of context. The performance of the GraCE and WECE systems is tested across two datasets, focusing on their ability to classify instances of specific lexical-semantic relations as well as to provide insights into the systems’ generalization capabilities. 4.1 Experimental Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of"
S15-1021,P06-1040,0,0.189331,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,J06-3003,0,0.259422,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,C08-1114,0,0.0743919,"ing the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can cl"
S15-1021,C02-1114,0,0.394996,"preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed"
S15-1021,N13-1120,0,0.0989502,"of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, dependin"
S15-1021,S10-1006,0,\N,Missing
S15-2049,P14-1089,1,0.867636,"Missing"
S15-2049,D11-1072,0,0.0933284,"Missing"
S15-2049,O97-1002,0,0.0279954,"ava class available in the BabelNet API (please refer to Section 2.5 for a detailed explanation). The highest ranked synset among the ones that contain the aligned translation is used to annotate the instance. The system falls back to the BabelNet first sense (BFS) provided by the BabelSynsetComparator for instances with no aligned translation, or in cases where the translation was not found in any of the synsets available for the word in BabelNet. EBL-Hope (Unsupervised + Sense relevance). This approach uses a modified version of the Lesk algorithm and the Jiang & Conrath similarity measure (Jiang and Conrath, 1997). It validates the output from both techniques for enhanced accuracy and exploits semantic relations and corpus (SemCor) inSUDOKU (Unsupervised). This deterministic constraint-based approach relies on a reasonable degree of “document monosemy” (percentage of unique monosemous lemmas in a document) and exploits Personalised PageRank (Agirre et al., 2014) to select the best candidate. The PPR is started with 291 a surfing vector biased towards monosemous words (i.e., their respective sense). Each submission differs by its imposed constraints: Run1 is the plain approach (Manion and Sainudiin, 201"
S15-2049,S10-1003,0,0.0544737,"in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues). Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain. 2 Task Setup The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided. Differently from previous editions (Navigli et al., 2013; Lefever and Hoste, 2013; Manandhar et al., 2010; Lefever and Hoste, 2010; Pradhan et al., 2007; Navigli et al., 2007; Snyder and Palmer, 2004; Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013) and ERD (Carmel et al., 2014). 2.1 Corpora The documents considered in this task are taken from the OPUS project (http://opus.lingfil.uu.se/), 289 more specifically from the EMEA (European"
S15-2049,S14-1005,0,0.107271,"(Jiang and Conrath, 1997). It validates the output from both techniques for enhanced accuracy and exploits semantic relations and corpus (SemCor) inSUDOKU (Unsupervised). This deterministic constraint-based approach relies on a reasonable degree of “document monosemy” (percentage of unique monosemous lemmas in a document) and exploits Personalised PageRank (Agirre et al., 2014) to select the best candidate. The PPR is started with 291 a surfing vector biased towards monosemous words (i.e., their respective sense). Each submission differs by its imposed constraints: Run1 is the plain approach (Manion and Sainudiin, 2014) applied at the document level; Run2 is the iterative version of the previous approach applied at the document level and with words disambiguated in order of increasing polysemy; Run3 is like Run2, but it is first applied to nouns and then to verbs, adjectives, and adverbs. TeamUFAL (Unsupervised). This system exploits Apache Lucene search engine to index Wikipedia documents, Wiktionary entries and WordNet senses. Then, to perform disambiguation, the Lucene ranking method is used to query the index with multiple queries (consisting of the text fragment and context words). Finally, all query re"
S15-2049,P04-1036,0,0.0624024,"rface forms extracted from BabelNet synsets. Moreover, each synset has a prior probability computed over an annotated corpus. For WordNet synsets, SemCor is exploited, while for Wikipedia entities the number of citations in Wikipedia internal links is counted. vua-background (Partially supervised). This approach exploits the Named Entities contained in the test data to generate a background corpus. This is done by finding similar DBpedia entities for the entities in the input documents. Using this background corpus, the system tries to find the predominant sense of the words in the test data (McCarthy et al., 2004). If a predominant sense is recognized for a specific lemma, then it is used, otherwise the system falls back to the “It Makes Sense” WSD system (Zhong and Ng, 2010). 3 During the evaluation period the system did not return any annotation for adjectives due to a misinterpretation of the POS tag set. For full evaluations see the system paper. 292 WSD-games (Unsupervised). This approach is formulated in terms of Evolutionary Game Theory, where each word to be disambiguated is represented as a node in a graph and each sense as a class. The proposed algorithm performs a consistent class assignment"
S15-2049,C14-3003,1,0.919672,"language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns. More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become - together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) - the main reference inventory for EL systems. Over the years, the research community has typically focused on each of these tasks separately. Recently, however, joint approaches have been proposed (Moro et al., 2014b). One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013). A case in point here is BabelNet1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012). Resources like BabelNet provide a common ground for the tasks of WSD and EL. 1 http://babelnet.org 288 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 288–297, c D"
S15-2049,Q14-1019,1,0.767693,"language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns. More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become - together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) - the main reference inventory for EL systems. Over the years, the research community has typically focused on each of these tasks separately. Recently, however, joint approaches have been proposed (Moro et al., 2014b). One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013). A case in point here is BabelNet1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012). Resources like BabelNet provide a common ground for the tasks of WSD and EL. 1 http://babelnet.org 288 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 288–297, c D"
S15-2049,S13-2040,1,0.76202,"ght meaning in context. However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags. Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers. In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation (Navigli et al., 2013), our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues). Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain. 2 Task Setup The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have"
S15-2049,S01-1005,0,0.724729,"ain about social issues). Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain. 2 Task Setup The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided. Differently from previous editions (Navigli et al., 2013; Lefever and Hoste, 2013; Manandhar et al., 2010; Lefever and Hoste, 2010; Pradhan et al., 2007; Navigli et al., 2007; Snyder and Palmer, 2004; Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013) and ERD (Carmel et al., 2014). 2.1 Corpora The documents considered in this task are taken from the OPUS project (http://opus.lingfil.uu.se/), 289 more specifically from the EMEA (European Medicines Agency documents), KDEdoc (the KDE manual corpus) and “The EU bookshop corpus”,"
S15-2049,W04-0811,0,0.933653,"domain, and a broader domain about social issues). Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain. 2 Task Setup The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided. Differently from previous editions (Navigli et al., 2013; Lefever and Hoste, 2013; Manandhar et al., 2010; Lefever and Hoste, 2010; Pradhan et al., 2007; Navigli et al., 2007; Snyder and Palmer, 2004; Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013) and ERD (Carmel et al., 2014). 2.1 Corpora The documents considered in this task are taken from the OPUS project (http://opus.lingfil.uu.se/), 289 more specifically from the EMEA (European Medicines Agency documents), KDEdoc (the KDE manual corpus) and “The"
S15-2049,P10-4014,0,0.667912,"while for Wikipedia entities the number of citations in Wikipedia internal links is counted. vua-background (Partially supervised). This approach exploits the Named Entities contained in the test data to generate a background corpus. This is done by finding similar DBpedia entities for the entities in the input documents. Using this background corpus, the system tries to find the predominant sense of the words in the test data (McCarthy et al., 2004). If a predominant sense is recognized for a specific lemma, then it is used, otherwise the system falls back to the “It Makes Sense” WSD system (Zhong and Ng, 2010). 3 During the evaluation period the system did not return any annotation for adjectives due to a misinterpretation of the POS tag set. For full evaluations see the system paper. 292 WSD-games (Unsupervised). This approach is formulated in terms of Evolutionary Game Theory, where each word to be disambiguated is represented as a node in a graph and each sense as a class. The proposed algorithm performs a consistent class assignment of senses according to the similarity information of each word with the others, so that similar words are constrained to similar classes. The propagation of the inf"
S15-2049,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
S15-2049,S07-1006,1,\N,Missing
S15-2049,S07-1016,0,\N,Missing
S15-2049,W09-2413,0,\N,Missing
S15-2049,J14-1003,0,\N,Missing
S15-2049,S10-1011,0,\N,Missing
S15-2151,baroni-bernardini-2004-bootcat,0,0.157355,"not submit a system for the Chemical domain and the QASSIT team, which submitted only one run for the WordNet Chemical taxonomy. Next, we will provide a short description of each approach in alphabetical order, discussing corpora collection and the approaches adopted for relation discovery and taxonomy construction. INRIASAC (supervised) Corpus: Wikipedia search using terms; Relation discovery: substring inclusion, lexico-syntactic patterns, co-occurrence information based on sentences and documents; Taxonomy construction: none. LT3 (unsupervised) Corpus: web corpus constructed using BootCat (Baroni and Bernardini, 2004) using the provided terms as seed terms; Re906 lation discovery: lexico-syntactic patterns, morphological structure of compound terms, WordNet lookup (Lefever et al., 2014); Taxonomy construction: none. ntnu (unsupervised) Corpus: Wikipedia and WordNet definitions; Relation discovery: hypernym extraction from definitions, WordNet lookup, Wikipedia categories, similarity between keywords; Taxonomy construction: none. QASIT (semi-supervised) Corpus: Wikipedia, DBpedia; Relation discovery: lexico-syntactic patterns, co-occurrence information; Taxonomy construction: Learning Pretopological Spaces"
S15-2151,P14-1089,1,0.666588,"produced two kinds of gold standard taxonomies. WordNet taxonomy Concepts and relationships in the WordNet hypernym-hyponym hierarchy rooted on the corresponding root concept. Combined taxonomy Domain-specific terms and relations from well-known, publicly available, taxonomies other than WordNet: CheBI1 for Chemicals, “The Google product taxonomy”2 for Foods, the “Material Handling Equipment”3 taxonomy for Equipment, and the “Taxonomy of Fields and their Subfields”4 for Science. Hypernym-hyponym relationships were also gathered from a general purpose resource, the Wikipedia Bitaxonomy (WiBi) (Flati et al., 2014), using a semi-automatic approach. For each domain we first manually identified domain sub-hierarchies from WiBi (W ); Second we automatically searched for the terms of W in common with the corresponding gold standard G. For each common term t we added in G the taxonomy rooted on t from W . Table 1 shows the resulting number of vertices |V |, i.e., the number of terms given to the participants, and the number of edges |E |of the produced gold standard taxonomies for the four target domains. Finally, test data consists of eight lists of domain concepts, for which participants were asked to outp"
S15-2151,P05-1014,0,0.027733,"Missing"
S15-2151,C92-2082,0,0.513933,"ethods for taxonomy enrichment and construction. Recently, the task of taxonomy learning from text, also called taxonomy induction, has received an increased interest in the natural language processing Taxonomy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large"
S15-2151,D10-1108,0,0.307974,"Missing"
S15-2151,P08-1119,0,0.413658,"onomy enrichment and construction. Recently, the task of taxonomy learning from text, also called taxonomy induction, has received an increased interest in the natural language processing Taxonomy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large number of noisy, inconsi"
S15-2151,P10-1134,1,0.252601,"omy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large number of noisy, inconsistent relations, that assign multiple parents to a node and that contain cycles, i.e., sequences of vertices that start and end at the same vertex. Hence, the third stage of taxonomy learning, taxo"
S15-2151,velardi-etal-2012-new,1,0.815328,"t output taxonomies 6 7 Domain Root concept Chemicals Equipment Food Science chemical equipment food science Combined taxonomies WordNet taxonomies |V| |E| |V| |E| 17584 612 1156 452 24817 615 1587 465 1351 475 1486 429 1387 485 1533 441 comparative evaluation manual quality assessment Figure 1: The task workflow. 2010; Navigli et al., 2011; Wang et al., 2013). To address the inherent complexity of evaluating taxonomy quality, several methods have been considered in the past including manual evaluation by domain experts, structural evaluation, and automatic evaluation against a gold standard (Velardi et al., 2012). In this task, all these existing evaluation approaches are considered, using a voting scheme to aggregate the results for the final ranking of the systems. We introduce four new domains that have not previously been considered for this task, covering general knowledge domains such as food and equipment and technical domains such as chemicals and science. For each domain, we provide a gold standard taxonomy gathered exclusively from WordNet (Fellbaum, 2005), as well as a gold standard taxonomy that combines terms and relations gathered from other domain-specific sources. 2 Task workflow In th"
S15-2151,J13-3007,1,0.671564,"irected cycles (self loop included). We then use an approach based on the Tarjan algorithm (Tarjan, 1972) to calculate the number of connected components in S. Finally, we compute the number of intermediate nodes as the number of nodes |VS |− |LS |where LS is the set of leaf nodes in S. A leaf node is a node with out-degree = 0. 2.2.3 Comparison against Gold Standard Previous datasets for evaluating taxonomy extraction (Kozareva et al., 2008) mainly rely on WordNet to gather gold standards from several general knowledge domains, such as animals, plants, and vehicles. The datasets proposed in (Velardi et al., 2013) enrich this experimental setting by including two specialized domains, Virus and Artificial Intelligence, that have low coverage in WordNet. A limitation of these datasets is that currently there is no gold standard taxonomy for these domains, therefore only a manual evaluation is possible. The dataset introduced here, instead, covers four new domains, providing two separate gold standards for each domain: one collected from WordNet, a general purpose resource, and a second one that combines relations from domain-specific resources and from a collaborative resource, Wikipedia, for a higher co"
S17-2002,N09-1003,0,0.413176,"Missing"
S17-2002,W13-3520,0,0.0139707,"eline system we included the results of the concept and entity embeddings of NASARI (Camacho-Collados et al., 2016). These embeddings were obtained by exploiting knowledge from Wikipedia and WordNet coupled with general domain corpus-based Word2Vec embeddings (Mikolov et al., 2013). We performed the evaluation with the 300-dimensional English embedded vectors (version 3.0)9 and used them for all languages. For the comparison within and • Subtask 1. The common corpus for subtask 1 was the Wikipedia corpus of the target language. Specifically, systems made use of the Wikipedia dumps released by Al-Rfou et al. (2013).6 • Subtask 2. The common corpus for subtask 2 was the Europarl parallel corpus7 . This corpus is available for all languages except 6 https://sites.google.com/site/rmyeid/ projects/polyglot 7 http://opus.lingfil.uu.se/Europarl. php 8 http://opus.lingfil.uu.se/ OpenSubtitles2016.php 9 http://lcl.uniroma1.it/nasari/ 20 System English r Luminoso run2 0.78 Luminoso run1 0.78 0.78 QLUT run1∗ hhu run1∗ 0.71 HCCL run1∗ 0.68 NASARI (baseline) 0.68 hhu run2∗ 0.66 QLUT run2∗ 0.67 RUFINO run1∗ 0.65 0.60 Citius run2 l2f run2 (a.d.) 0.64 l2f run1 (a.d.) 0.64 Citius run1∗ 0.57 MERALI run1∗ 0.59 Amateur ru"
S17-2002,J06-1003,0,0.0958441,"0 0.60 0.48 0.53 0.44 0.44 0.57 0.61 0.50 0.31 0.40 0.57 0.61 0.05 -0.06 - ρ Final 0.75 0.75 0.72 0.60 0.57 0.64 0.63 0.62 0.41 0.62 -0.06 - 0.74 0.74 0.70 0.60 0.55 0.52 0.51 0.62 0.41 0.62 0.00 - Table 6: Pearson (r), Spearman (ρ) and official (Final) results of participating systems on the five monolingual word similarity datasets (subtask 1). across languages NASARI relies on the lexicalizations provided by BabelNet (Navigli and Ponzetto, 2012) for the concepts and entities in each language. Then, the final score was computed through the conventional closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006), using cosine similarity as the comparison measure. 3.2 Results We present the results of subtask 1 in Section 3.2.1 and subtask 2 in Section 3.2.2. 3.2.1 System Score Official Rank Luminoso run2 Luminoso run1 HCCL run1∗ NASARI (baseline) RUFINO run1∗ SEW run2 (a.d.) SEW run1 RUFINO run2∗ hjpwhuer run1 0.743 0.740 0.658 0.598 0.555 0.552 0.506 0.369 0.018 1 2 3 4 5 6 7 Table 7: Global results of participating systems on subtask 1 (multilingual word similarity). Subtask 1 Table 6 lists the results on all monolingual datasets.10 The systems which made use of the shared Wikipedia corpus are mark"
S17-2002,S17-2034,0,0.0204818,"inds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obta"
S17-2002,W16-2508,1,0.276959,"reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of system"
S17-2002,D16-1235,0,0.0758158,"Missing"
S17-2002,E17-2036,1,0.804232,"Missing"
S17-2002,P15-2001,1,0.838039,"s 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approach"
S17-2002,I05-1067,0,0.020408,"t contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from various issues: 1. The similarity scale used for the annotation of WordSim-353 and MEN (Bruni et al., 2014) does not distinguish between similarity and relatedness, and hence conflates these two. As a result, the datasets contain pairs that are judged to be highly similar even if they are not of similar type or nature. F"
S17-2002,2015.mtsummit-papers.27,0,0.0742157,"Missing"
S17-2002,D09-1124,0,0.0561085,"nce Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets,"
S17-2002,S17-2041,0,0.0352303,"Missing"
S17-2002,S17-2033,0,0.0191197,"arks for evaluation. All kinds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have"
S17-2002,N15-1184,0,0.0720481,"Missing"
S17-2002,J15-4004,0,0.047794,". Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-"
S17-2002,S17-2032,0,0.0235773,"ic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair com"
S17-2002,S17-2037,0,0.0372334,"Missing"
S17-2002,C12-1109,0,0.034733,"24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages ha"
S17-2002,S14-2003,1,0.88311,"Missing"
S17-2002,P11-1076,0,0.00934146,"lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked wit"
S17-2002,P16-2074,0,0.0185427,"ex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from"
S17-2002,S17-2035,0,0.0441697,"Missing"
S17-2002,D14-1162,0,0.118611,"c Word Similarity Jose Camacho-Collados*1 , Mohammad Taher Pilehvar*2 , Nigel Collier2 and Roberto Navigli1 1 2 Department of Computer Science, Sapienza University of Rome Department of Theoretical and Applied Linguistics, University of Cambridge 1 {collados,navigli}@di.uniroma1.it 2 {mp792,nhc30}@cam.ac.uk Abstract word representation, a research field that has recently received massive research attention mainly as a result of the advancements in the use of neural networks for learning dense low-dimensional semantic representations, often referred to as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Almost any application in NLP that deals with semantics can benefit from efficient semantic representation of words (Turney and Pantel, 2010). However, research in semantic representation has in the main focused on the English language only. This is partly due to the limited availability of word similarity benchmarks in languages other than English. Given the central role of similarity datasets in lexical semantics, and given the importance of moving beyond the barriers of the English language and developing languageindependent and multilingual techniques, we felt that this was an appropriat"
S17-2002,S17-2036,0,0.0222596,"ncouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3."
S17-2002,P15-2004,0,0.014103,"oned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarit"
S17-2002,S17-2038,0,0.0206795,"iety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results"
S17-2002,S17-2039,0,0.044791,"Missing"
S17-2002,P14-1044,1,0.822702,"task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations ("
S17-2002,S17-2040,0,0.0437279,"Missing"
S17-2002,S17-2042,0,0.0464227,"Missing"
S17-2002,D13-1141,0,0.039439,"such as RG-65, MC30 (Miller and Charles, 1991), and WS-Sim (Agirre et al., 2009) (the similarity portion of WordSim-353) are relatively small, containing 65, 30, and 200 word pairs, respectively. Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming"
S17-2002,K15-1026,0,0.0981438,"erit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages. 2 Table 1: The set of thirty-four domains. wide range of domains (Section 2.1.1), (2) through translation of these pairs, we obtained word pairs for the other four languages (Section 2.1.2) and, (3) all word pairs of each dataset were manually scored by multiple annotators (Section 2.1.3). 2.1.1 English dataset creation Seed set selection. The dataset creation started with the selection of 500 English words. One of the main objectives o"
S17-2002,S17-2008,0,0.123538,"distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results of the concept and entity embeddings of"
S17-2002,D14-1034,0,\N,Missing
S17-2002,P16-1157,0,\N,Missing
S17-2002,W16-2502,0,\N,Missing
S18-1115,S13-1005,0,0.16065,"Missing"
S18-1115,S18-1116,0,0.0792436,"Missing"
S18-1115,S18-1149,0,0.0363408,"Missing"
S18-1115,E17-2013,0,0.185703,"Missing"
S18-1115,C92-2082,0,0.323886,"systems for any individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification"
S18-1115,S15-2151,1,0.928124,"i♥ Luis Espinosa-Anke♣ Sergio Oramas♦ Tommaso Pasini♥ Enrico Santus♥ Vered Shwartz♠ Roberto Navigli♥ Horacio Saggion♦ ♣ School of Computer Science and Informatics, Cardiff University, United Kingdom ♥ Computer Science Department, Sapienza University of Rome, Italy ♦ Pompeu Fabra University, Barcelona, Spain ♥ MIT, United States ♠ Bar-Ilan University, Ramat Gan, Israel ♣ {camachocolladosj,espinosa-ankel}@cardiff.ac.uk, ♥ {dellibovi,pasini,navigli}@di.uniroma1.it, ♦ {name.surname}@upf.edu, ♥ esantus@mit.edu, ♠ vered1986@gmail.com Abstract web retrieval, website navigation or records management (Bordea et al., 2015). This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were al"
S18-1115,S16-1168,0,0.263061,"s either a concept or a 4 As an example, the term apple could either refer to a fruit (if labeled as concept) or to a company (if labeled as named entity). 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://dbpubs.stanford.edu:8091/ testbed/doc2/WebBase/ ˜ 3 In fact, WordNet encodes hypernym and instance as two separate semantic relations. Instances are always leaf (terminal) nodes in their hierarchies. 714 sources of information with respect to the corpora used in previous tasks, such as Wikipedia in the SemEval 2016 task on taxonomy extraction (Bordea et al., 2016). In fact, the encyclopedic nature of Wikipedia has been exploited in a wide variety of works (Ponzetto and Strube, 2007; Flati et al., 2016; Gupta et al., 2016), and differs substantially from the web-based corpus we put forward here. As source corpus for the Italian subtask (1B) we instead used the 1.3-billion-word itWac corpus7 (Baroni et al., 2009), extracted from different sources of the web within the .it domain. Finally, as source corpus for the Spanish subtask (1C) we considered the 1.8-billion-word Spanish corpus8 (Cardellino, 2016), which also contains heterogeneous documents from di"
S18-1115,S18-1150,0,0.0250373,"Missing"
S18-1115,E17-2036,1,0.898611,"Missing"
S18-1115,N15-1098,0,0.0369984,"ttps://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within do"
S18-1115,D16-1041,1,0.888795,"Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Collados, 2017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The ma"
S18-1115,S18-1151,0,0.0726421,"14 We used the open-source code available at https:// bitbucket.org/luisespinosa/taxoembed 15 https://github.com/vered1986/ UnsupervisedHypernymy 16 Following the conclusions from Shwartz et al. (2017), we set the hyper-parameters to: SLQS: median, PLMI, N = 100 and APSyn: N = 500. 13 Although only P@5 is displayed in the tables due to lack of space, the other thresholds were used in the official evaluation as well. 717 5.2 Participant Systems in general they were outperformed by supervised systems, in some cases their performance came close, especially for concepts. For instance, the ADAPT (Maldonado and Klubika, 2018) system, which is based on a simple similarity measure applied to word embeddings, achieved a very decent 8.13 MAP percentage performance on the medical dataset, using neither supervision nor external resources. Supervised systems produced a larger gap for entities, probably due, as mentioned above, to the lower diversity of possible hypernyms. Table 3 shows a summary of all participant systems, displaying their main features with respect to supervison and external resources used, if any. 5.3 Results A summary of the results is provided in tables 3 to 7, respectively describing results for Eng"
S18-1115,P14-1113,0,0.377885,"t al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathe"
S18-1115,P16-2081,1,0.844897,"Missing"
S18-1115,P10-1134,1,0.838694,"y individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni"
S18-1115,D16-1234,0,0.0290792,"rrent research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 1A: English 1B: Italian 1C: Spanish Term sorrow Nina Simone guacamole 2A: Medical pulmonary embolism 2B: Music Green Day Hypernym(s) sadness, unhappiness musicista, pianista, persona salsa para mojar, salsa, alimento"
S18-1115,D17-1022,0,0.0580762,"Missing"
S18-1115,C14-1097,0,0.0705885,"Missing"
S18-1115,S18-1146,0,0.019286,"Missing"
S18-1115,E17-2064,0,0.0114909,"stributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different and heterogeneous sources. A system operating in this setting r"
S18-1115,L16-1722,1,0.929363,". codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld ap"
S18-1115,L16-1528,1,0.817501,"t appeared too vague or general, as well as terms with mis-attributed domains. Domain-specific corpora. As source corpus for the medical domain (subtask 2A) we provided a combination of texts drawn from the MEDLINE9 (Medical Literature Analysis and Retrieval System) repository, which contains academic documents such as scientific publications and paper abstracts. This corpus contains 130 million words. As regards the music domain (subtask 2B), instead, the source corpus we compiled is a concatenation of several music-specific corpora, i.e. music biographies from Last.fm contained in ELMD 2.0 (Oramas et al., 2016), articles from the music branch of Wikipedia, and a corpus of album customer reviews from Amazon (Oramas et al., 2017). The resulting corpus reaches 100 million words in total. 4.1.2 Term Collection Vocabulary Creation With the aim of simplifying the task for participants by providing a unified hypernym search space, we built a series of vocabulary files including all the possible hypernyms on each dataset. Each vocabulary was constructed by considering all the words occurring at least N times across the source corpus of the corresponding subtask. We set N to five and three in the general-pur"
S18-1115,E14-4008,1,0.929727,"i Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose te"
S18-1115,P17-1192,0,0.0129572,"es its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al."
S18-1115,P16-1226,1,0.918452,"017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 201"
S18-1115,E17-1007,1,0.68931,"ions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Coll"
S18-1115,S18-1148,0,0.0202622,"Missing"
S18-1115,J17-4004,0,0.0330605,"Missing"
S18-1115,S17-1004,0,0.0128295,"ish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2"
S18-1115,D17-1123,0,0.0255651,"Missing"
S18-1115,C14-1212,0,0.0535272,"a et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different a"
S18-1115,P10-2021,0,0.026139,"aining set, separately for each subtask and measure. where Q is a sample of experiment runs, AP(·) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space. Mean Reciprocal Rank (MRR). MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: |Q| 1 X 1 MRR = |Q| ranki i=1 where ranki refers to the rank position of the first relevant outcome for the ith run. While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (Wu et al., 2010; Rodr´ıguezFern´andez et al., 2016). In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k ∈ {1, 3, 5, 15}.13 5.1 Baselines We compared the participating systems with both supervised and unsupervised baselines for each subtask, inspired by recent work on hypernym detection and discovery. In this section we briefly describe each of them. 5.1.1 Unsupervised Baselines Supervised Baselines We first used a na¨ıve most frequent hypernym (MFH) baseline, which simply returns, for each input"
S18-1115,S18-1147,0,0.0304331,"Missing"
velardi-etal-2012-new,C92-2082,0,\N,Missing
velardi-etal-2012-new,P09-1031,0,\N,Missing
velardi-etal-2012-new,P10-1134,1,\N,Missing
velardi-etal-2012-new,D10-1108,0,\N,Missing
velardi-etal-2012-new,J13-3007,1,\N,Missing
W04-0844,magnini-cavaglia-2000-integrating,0,0.0130992,"sociation for Computational Linguistics Structural Semantic Interconnection: a knowledge-based approach to Word Sense Disambiguation export#1 ss food#1 goods#1 clothing#1 kin ss d -o f glo consumer consumption#1 goods#1 to p ic ss k in d glo -of market#1 g lo ss gloss merchandise #1 d-o s f s glo f business activity#1 industry#2 gloss d -o service#1 k in monopoly#1 kind-o f production#1 g ha s lo ss -k i nd k in d -o f LDC http://www.ldc.upenn.edu/ nd 1 enterprise#1 g lo -k i We build a structural representation of word senses using a variety of knowledge sources, i.e. WordNet, Domain Labels (Magnini and Cavaglia, 2000), annotated corpora like SemCor and LDCDSO1. We use this information to automatically commerce#1 activity#1 rt -p a ha s express#1 has-part f kind-o commercial enterprise#2 transportation#5 kind-of artifact#1 trading#1 h as Building structural representations of word senses f -o f 2 kind-o k in d Our approach to WSD lies in the structural pattern recognition framework. Structural or syntactic pattern recognition (Bunke and Sanfeliu, 1990) has proven to be effective when the objects to be classified contain an inherent, identifiable organization, such as image data and time-series data. For the"
W06-0501,W02-1028,0,0.0418531,"Missing"
W06-0501,J04-2002,1,\N,Missing
W14-4711,P14-1122,1,0.921323,"tate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Moreover, we show that video games with a purpose produce higher-quality annotations than crowdsourcing. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 75 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 75–76, Dublin, Ireland, August 23, 2014. Then I will introduce the Wikipedia Bitaxonomy (Flati et al., 2014, WiBi), available at http://wibitaxonomy.org and now integrated into BabelNet. WiBi is the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. WiBi is created in three steps: we first create a taxonomy for the Wikipedia pages by parsing textual definitions, extracting the hypernym(s) and disambiguating them according to the page inventory; next, we leverage the hypernyms in the page taxonomy, together with their links to the corresponding categories, so as to induce a taxonomy over Wikipedia categories while at the same"
W14-4711,Q14-1019,1,0.786904,"een synsets (as relations); from Wikipedia, all the Wikipages (i.e., Wikipages, as concepts) and semantically unspecified relations from their hyperlinks. WordNet and Wikipedia overlap both in terms of concepts and relations: this overlap makes the merging between the two resources possible, enabling the creation of a unified knowledge resource. In order to enable multilinguality, we collect the lexical realizations of the available concepts in different languages. Finally, we connect the multilingual Babel synsets by establishing semantic relations between them. Next, I will present Babelfy (Moro et al., 2014), available at http://babelfy.org, a unified approach that leverages BabelNet to perform Word Sense Disambiguation (WSD) and Entity Linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Babelfy works in three steps: first, given a lexicalized semantic network, we associate with each vertex, i.e., either concept or named entity, a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text. Second, given"
W14-4711,Q14-1035,1,\N,Missing
W14-4711,D08-1027,0,\N,Missing
W14-4711,J14-4005,1,\N,Missing
W14-4711,P10-1023,1,\N,Missing
W14-4711,P13-1133,0,\N,Missing
W14-4711,P14-1044,1,\N,Missing
W14-4711,P13-3000,0,\N,Missing
W14-4711,P13-4000,0,\N,Missing
W14-4711,P13-5000,0,\N,Missing
W14-4711,P13-1000,0,\N,Missing
W14-4711,ehrmann-etal-2014-representing,1,\N,Missing
W14-4711,P14-1089,1,\N,Missing
W15-4205,calzolari-etal-2012-lre,0,0.098711,"ar}@insight-centre.org Abstract statistical taggers, statistical parsers, and statistical machine translation systems) or they require lexico-semantic resources as background knowledge to perform some task (e.g. word sense disambiguation). As the number of language resources available keeps growing, the task of discovering and finding resources that are pertinent to a particular task becomes increasingly difficult. While there are a number of repositories that collect and index metadata of language resources, such as META-SHARE (Federmann et al., 2012), CLARIN (Broeder et al., 2010), LRE-Map (Calzolari et al., 2012), Datahub.io1 and OLAC (Simons and Bird, 2003), they do not provide a complete solution to the discovery problem for two reasons. First, integrated search over all these different repositories is not possible, as they use different data models, different vocabularies and expose different interfaces and APIs. Second, these repositories must strike a balance between quality and coverage, either opting for coverage at the expense of quality of metadata, or vice versa. When collecting metadata from multiple resources, we understand that there are two principal challenges: property harmonization an"
W15-4205,choukri-etal-2012-using,0,0.0195543,"x XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want to submit the resources accompanying papers submitted to conferences. Most fields in LRE-Map consist of a text field with some prespecified options to select and a thorough analysis of the results has been conducted (Mariani et al., 2014). Similarly, the Open Linguistics Working Group (Chiarcos et al., 2012) has been collecting language resources published as linked data in a Related Work Interoperability of metadata is an important pr"
W15-4205,de-marneffe-etal-2006-generating,0,0.0158775,"Missing"
W15-4205,federmann-etal-2012-meta,0,0.277944,"Missing"
W15-4205,broeder-etal-2010-data,0,0.0785251,"Missing"
W15-4205,mariani-etal-2014-facing,0,0.0288608,"Missing"
W15-4205,piperidis-2012-meta,0,0.0544122,"oaches have been pursued to collect metadata of resources. Large consortium-led projects and initiatives such as the CLARIN projects and METANET have attempted to create metadata standards for representing linguistic data. Interoperability of the data stemming from these two repositories is however severely limited due to incompatibilities in their data models. META-SHARE favors a qualitative approach in which a relatively complex XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want t"
W15-4205,Q14-1019,1,\N,Missing
W15-4205,gavrilidou-etal-2012-meta,0,\N,Missing
W16-2508,P14-1023,0,0.0173613,"Missing"
W16-2508,W16-2502,0,0.145044,"uation of these word vector representations (Baroni et al., 2014; Levy et al., 2015). Given a gold standard of human-assigned scores, the usual evaluation procedure consists of calculating the correlation between these human similarity scores and scores calculated by the system. While word similarity has been shown to be an interesting task for measuring the semantic coherence of a vector space model, it suffers from various problems. First, the human inter-annotator agreement of standard datasets has been shown to be relatively too low for it to be considered a reliable evaluation benchmark (Batchkarov et al., 2016). In fact, many systems have already surpassed the human inter-annotator agreement upper bound in most of the standard word similarity datasets (Hill et al., 2015). Another drawback of the word similarity evaluation benchmark is its simplicity, as words are simply viewed as points in the vector space. Other interesting properties of vector space models are not directly addressed in the task. We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create sema"
W16-2508,P15-1144,0,0.0075124,"is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD measures given a cluster and an outlier is (n"
W16-2508,P14-1113,0,0.00941061,"tion (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we"
W16-2508,D14-1067,0,0.0312982,"otball teams FC Barcelona Bayern Munich Real Madrid AC Milan Juventus Atletico Madrid Chelsea Borussia Dortmund Miami Dolphins McLaren Los Angeles Lakers Bundesliga football goal couch fridge Solar System planets Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune Sun Moon Triton Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, whi"
W16-2508,N15-1004,0,0.013363,"tection henceforth, is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD measures given a clust"
W16-2508,N15-1059,1,0.794675,"ftmax for CBOW and negative sampling for Skip-Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performa"
W16-2508,S13-1005,0,0.0919158,"Missing"
W16-2508,P15-1072,1,0.703748,"ftmax for CBOW and negative sampling for Skip-Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performa"
W16-2508,J15-4004,0,0.0358605,"ists of calculating the correlation between these human similarity scores and scores calculated by the system. While word similarity has been shown to be an interesting task for measuring the semantic coherence of a vector space model, it suffers from various problems. First, the human inter-annotator agreement of standard datasets has been shown to be relatively too low for it to be considered a reliable evaluation benchmark (Batchkarov et al., 2016). In fact, many systems have already surpassed the human inter-annotator agreement upper bound in most of the standard word similarity datasets (Hill et al., 2015). Another drawback of the word similarity evaluation benchmark is its simplicity, as words are simply viewed as points in the vector space. Other interesting properties of vector space models are not directly addressed in the task. We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create semantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the"
W16-2508,W16-2501,0,0.0792341,"Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, which is essential for this type of statistical study (Patton, 2005). All sample vectors came from corpus-based models1 trained on the same corpus and all perform well on the considered intrinsic tasks, which constitute a highly homogeneous and not representative sample set. Moreover,"
W16-2508,P12-1092,0,0.090314,"ina Peru Venezuela Chile Ecuador Bolivia Bogot´a Rio de Janeiro New York Madrid town government bottle telephone Table 2: Last four clusters (including outliers) from the 8-8-8 outlier detection dataset. human-assigned scores with a relatively low interannotator agreement. For example, the interannotator agreements in the standard WordSim353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) word similarity datasets were, respectively, 0.61 and 0.67 according to average pair-wise Spearman correlation. In fact, both upper-bound values have already been surpassed by automatic models (Huang et al., 2012; Wieting et al., 2015). expertise. The time spent for the actual creation of a cluster (including outliers) was in all cases less than ten minutes. 3.1 Human performance We assessed the human performance of eight annotators in the task via accuracy. To this end, each annotator was given eight different groups of words, one for each of the topics of the 8-88 dataset. Each group of words was made up of the set of eight words comprising the cluster, plus one additional outlier. All the words were shuffled and given to the annotator without any additional information (e.g. annotators did not know"
W16-2508,W02-0908,0,0.0349216,"ability of vector space models to create semantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the task is extremely high compared to the standard word similarity task, and stateof-the-art word embedding models, whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement. 1 Introduction Vector Space Models have been successfully used on many NLP tasks (Turney and Pantel, 2010) such as automatic thesaurus generation (Crouch, 1988; Curran and Moens, 2002), word similarity (Deerwester et al., 1990; Turney et al., 2003; Radinsky et al., 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going be"
W16-2508,P15-1010,1,0.0430078,"Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performance. As opposed to the word similarity"
W16-2508,A97-1025,0,0.0475522,", whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement. 1 Introduction Vector Space Models have been successfully used on many NLP tasks (Turney and Pantel, 2010) such as automatic thesaurus generation (Crouch, 1988; Curran and Moens, 2002), word similarity (Deerwester et al., 1990; Turney et al., 2003; Radinsky et al., 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the"
W16-2508,D14-1162,0,0.106051,", 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting langu"
W16-2508,Q15-1016,0,0.0456006,"Missing"
W16-2508,W02-1111,0,0.0265731,"arning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we will assume that w1 , ... , wn belong to the same cluster and wn+1 is the outlier. In what follows we explain a procedure for detecting outliers based on semantic similarity. We define the compactness score c(w) of a word w ∈ W as the compactness of the cluster W  {w}, calculated by averaging all pair-wise semantic similarities of the words in W  {w}: 1 k |D| P Formalization c(w) ="
W16-2508,D15-1243,0,0.0190147,"eptune Sun Moon Triton Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, which is essential for this type of statistical study (Patton, 2005). All sample vectors came from corpus-based models1 trained on the same corpus and all perform well on the considered intrinsic tasks, which constitute a highly homogeneous and not representative s"
W16-2508,P11-1076,0,0.038418,"e proof are included in Appendix A. 2.2 Potential applications In this work we focus on the intrinsic semantic properties of vector space models which can be inferred from the outlier detection task. In addition, since it is a task based partially on semantic similarity, high-performing models in the outlier detection task are expected to contribute to applications in which semantic similarity has already shown its potential: Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on a"
W16-2508,J13-3007,1,0.11014,"Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we will assume that w1 , ... , wn belong to the same cluster and wn+1 is the outlier. In what follows we explain a procedure for detecting outliers based on semantic"
W16-2508,C12-1118,0,0.00950283,"rred to as outlier detection henceforth, is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD me"
W16-2508,P15-1032,0,0.0170111,"tion (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting language understanding property of vector space models not fully addressed to date, and this is their a"
W16-2508,Q15-1025,0,0.00584692,"hile Ecuador Bolivia Bogot´a Rio de Janeiro New York Madrid town government bottle telephone Table 2: Last four clusters (including outliers) from the 8-8-8 outlier detection dataset. human-assigned scores with a relatively low interannotator agreement. For example, the interannotator agreements in the standard WordSim353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) word similarity datasets were, respectively, 0.61 and 0.67 according to average pair-wise Spearman correlation. In fact, both upper-bound values have already been surpassed by automatic models (Huang et al., 2012; Wieting et al., 2015). expertise. The time spent for the actual creation of a cluster (including outliers) was in all cases less than ten minutes. 3.1 Human performance We assessed the human performance of eight annotators in the task via accuracy. To this end, each annotator was given eight different groups of words, one for each of the topics of the 8-88 dataset. Each group of words was made up of the set of eight words comprising the cluster, plus one additional outlier. All the words were shuffled and given to the annotator without any additional information (e.g. annotators did not know the topic of the clust"
W16-2508,D13-1141,0,0.0337486,"labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting language understanding property of vector space models not fully addressed to date, and this is their ability to create semantic clusters in th"
W16-2508,D08-1048,0,\N,Missing
W16-2508,P07-1028,0,\N,Missing
