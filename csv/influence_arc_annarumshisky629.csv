2020.bionlp-1.6,N19-1423,0,0.0188955,"reached with (a) randomly initialized word embeddings trained jointly with the rest of the models, (b) domain-independent GloVe Common Crawl embeddings (Pennington et al., 2014), and (c) domain-specific fastText embeddings trained by (Romanov and Shivade, 2018). The latter are initialized with GloVe embeddings trained on Common Crawl, followed by training on 12M PubMed abstracts, and finally on 2M clinical notes from MIMIC-III database (Johnson et al., 2016). In all the experiments, we use 300-dimensional word vectors. We also experimented with transformerbased contextual vectors using BERT (Devlin et al., 2019). More specifically, instead of using LSTM representations of the textual data, we extracted the last layer vectors from ClinicalBERT (Alsentzer et al., 2019) pre-trained on MIMIC notes, and averaged them over input sequence tokens. 5.6 Question order In a visual dialog setting, a model is conditioned on the image vector, the image caption, and the dialog history to predict the answer to a new question. We hypothesized that a model should be able to answer later questions in a dialog better since it has more information from the previous questions and their answers. As described in Section 3.2"
2020.bionlp-1.6,W19-1909,0,0.0418574,"Missing"
2020.bionlp-1.6,D14-1162,0,0.0835616,"the obtained vectors to represent the entire image. Based on the results of the experiment (subsection 6.3), we found that ResNet-101 image vectors yielded the best performance, so we used them in other experiments. 5.4 5.5 Text representations Finally, we investigate the best way for representing the textual data by incorporating different pretrained word vectors. More specifically, we measure the performance of our best-performing SAN model reached with (a) randomly initialized word embeddings trained jointly with the rest of the models, (b) domain-independent GloVe Common Crawl embeddings (Pennington et al., 2014), and (c) domain-specific fastText embeddings trained by (Romanov and Shivade, 2018). The latter are initialized with GloVe embeddings trained on Common Crawl, followed by training on 12M PubMed abstracts, and finally on 2M clinical notes from MIMIC-III database (Johnson et al., 2016). In all the experiments, we use 300-dimensional word vectors. We also experimented with transformerbased contextual vectors using BERT (Devlin et al., 2019). More specifically, instead of using LSTM representations of the textual data, we extracted the last layer vectors from ClinicalBERT (Alsentzer et al., 2019)"
2020.bionlp-1.6,P18-1240,0,0.0288912,"e of four possibilities for each of the 13 abnormalities: {yes, no, maybe, not mentioned in the report}. Related Work Most of the large publicly available datasets (Kaggle, 2017; Rajpurkar et al., 2017) for radiology consist of images associated with a limited amount of structured information. For example, Irvin et al. (2019); Johnson et al. (2019) make images available along with the output of a text extraction module that produces labels for 13 abnormalities in a chest X-ray. Of note recently, the task of generating reports from radiology images has become popular in the research community (Jing et al., 2018; Wang et al., 2018). Two recent shared tasks at ImageCLEF explored the VQA problem with radiology images (Hasan et al., 2018; Abacha et al., 2019). Lau et al. (2018) also released a small dataset VQARAD for the specific task. The first VQA shared task at ImageCLEF (Hasan et al., 2018) used images from articles at PubMed Central. While Abacha et al. (2019) and Lau et al. (2018) use clinical images, the sizes of these datasets are limited. They are a mix of several modalities including 2D modalities such as X-rays, and 3D modalities such as ultrasound, MRI, and CT scans. They also cover several"
2020.bionlp-1.6,D18-1187,1,0.83783,"periment (subsection 6.3), we found that ResNet-101 image vectors yielded the best performance, so we used them in other experiments. 5.4 5.5 Text representations Finally, we investigate the best way for representing the textual data by incorporating different pretrained word vectors. More specifically, we measure the performance of our best-performing SAN model reached with (a) randomly initialized word embeddings trained jointly with the rest of the models, (b) domain-independent GloVe Common Crawl embeddings (Pennington et al., 2014), and (c) domain-specific fastText embeddings trained by (Romanov and Shivade, 2018). The latter are initialized with GloVe embeddings trained on Common Crawl, followed by training on 12M PubMed abstracts, and finally on 2M clinical notes from MIMIC-III database (Johnson et al., 2016). In all the experiments, we use 300-dimensional word vectors. We also experimented with transformerbased contextual vectors using BERT (Devlin et al., 2019). More specifically, instead of using LSTM representations of the textual data, we extracted the last layer vectors from ClinicalBERT (Alsentzer et al., 2019) pre-trained on MIMIC notes, and averaged them over input sequence tokens. 5.6 Quest"
2020.coling-tutorials.5,P19-1334,0,0.0557109,"Missing"
2020.coling-tutorials.5,W17-0906,0,0.030061,"Missing"
2020.coling-tutorials.5,P19-1459,0,0.0419376,"Missing"
2020.coling-tutorials.5,S18-1119,0,0.0340123,"Missing"
2020.coling-tutorials.5,D16-1264,0,0.069522,"Missing"
2020.coling-tutorials.5,P18-2124,0,0.0676152,"Missing"
2020.coling-tutorials.5,2020.acl-main.442,0,0.0203215,"Missing"
2020.coling-tutorials.5,D19-1221,0,0.0431102,"Missing"
2020.coling-tutorials.5,I17-1100,0,0.0672603,"Missing"
2020.coling-tutorials.5,D18-1259,0,0.0505507,"Missing"
2020.coling-tutorials.5,N19-1241,0,0.035186,"Missing"
2020.coling-tutorials.5,D18-1009,0,0.0577859,"Missing"
2020.emnlp-main.259,S17-2001,0,0.0543876,"Missing"
2020.emnlp-main.259,W19-4828,0,0.0339926,"ion patterns, which seems redundant. We conducted the same analysis for the attention patterns in pre-trained vs. fine-tuned BERT for both super-survivors and all heads, and found them to not change considerably after fine-tuning, which is consistent with findings by Kovaleva et al. (2019). Full data is available in Appendix F. Note that this result does not exclude the possibility that linguistic information is encoded in certain combinations of BERT elements. However, to date most BERT analysis studies focused on the functions of individual components (Voita et al., 2019; Htut et al., 2019; Clark et al., 2019; Lin et al., 2019; Vig and Belinkov, 2019; Hewitt and Manning, 2019; Tenney et al., 2019, see also the overview by Rogers et al. (2020b)), and this evidence points to the necessity of looking at their interactions. It also adds to the ongoing discussion of interpretability of self-attention (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020). Once again, heterogenerous pattern counts are only a crude upper bound estimate on potentially interpretable patterns. More sophisticated alternatives should be explored in future work. For instance, the re"
2020.emnlp-main.259,N19-1423,0,0.203569,"and not explained by meaningful self-attention patterns. 1 2 Introduction Much of the recent progress in NLP is due to the transfer learning paradigm in which Transformerbased models first try to learn task-independent linguistic knowledge from large corpora, and then get fine-tuned on small datasets for specific tasks. However, these models are overparametrized: we now know that most Transformer heads and even layers can be pruned without significant loss in performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). One of the most famous Transformer-based models is BERT (Devlin et al., 2019). It became a must-have baseline and inspired dozens of studies probing it for various kinds of linguistic information (Rogers et al., 2020b). We conduct a systematic case study of finetuning BERT on GLUE tasks (Wang et al., 2018) from the perspective of the lottery ticket hypothesis ∗ Equal contribution Related Work Multiple studies of BERT concluded that it is considerably overparametrized. In particular, it is possible to ablate elements of its architecture without loss in performance or even with slight gains (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019). This explains t"
2020.emnlp-main.259,W07-1401,0,0.0262585,"Missing"
2020.emnlp-main.259,2020.repl4nlp-1.18,0,0.126111,"aset sizes and the metrics reported in this study However, so far LTH work focused on the “winning” random initializations. In case of BERT, there is a large pre-trained language model, used in conjunction with a randomly initialized taskspecific classifier; this paper and concurrent work by Chen et al. (2020) are the first to explore LTH in this context. The two papers provide complementary results for magnitude pruning, but we also study structured pruning, posing the question of whether “good” subnetworks can be used as an tool to understand how BERT works. Another contemporaneous study by Gordon et al. (2020) also explores magnitude pruning, showing that BERT pruned before fine-tuning still reaches performance similar to the full model. Ideally, the pre-trained weights would provide transferable linguistic knowledge, fine-tuned only to learn a given task. But we do not know what knowledge actually gets used for inference, except that BERT is as prone as other models to rely on dataset biases (McCoy et al., 2019b; Rogers et al., 2020a; Jin et al., 2020; Niven and Kao, 2019; Zellers et al., 2019). At the same time, there is vast literature on probing BERT architecture blocks for different linguistic"
2020.emnlp-main.259,D19-1424,0,0.0332335,"ot survive pruning are not just “inactive” (Zhang et al., 2019). An obvious, but very difficult question that arises from this finding is whether the “bad” subnetworks Furthermore, should we perhaps be asking the same question with respect to not only subnetworks, but also full models, such as BERT itself and all the follow-up Transformers? There is a trend to automatically credit any new state-of-the-art model with with better knowledge of language. However, what if that is not the case, and the success of pretraining is rather due to the flatter and wider optima in the optimization surface (Hao et al., 2019)? Can similar loss landscapes be obtained from other, non-linguistic pre-training tasks? There are initial results pointing in that direction: Papadimitriou and Jurafsky (2020) report that even training on MIDI music is helpful for transfer learning for LM task with LSTMs. 7 Conclusion This study systematically tested the lottery ticket hypothesis in BERT fine-tuning with two pruning methods: magnitude-based weight pruning and importance-based pruning of BERT self-attention heads and MLPs. For both methods, we find that the pruned “good” subnetworks alone reach the performance comparable with"
2020.emnlp-main.259,2020.findings-emnlp.372,0,0.136343,"Missing"
2020.emnlp-main.259,2020.emnlp-main.574,0,0.0204649,"ous attention can be used as an upper bound estimate on non-trivial linguistic information. In other words, these patterns do not guarantee that a given head has some interpretable function – only that it could have it. This analysis is performed by image classification on generated attention maps from individual heads (100 for each GLUE task), for which we use a small CNN classifier with six layers. The classifier was trained on the dataset of 400 annotated attention maps by Kovaleva et al. (2019). Note that attention heads can be seen as a weighted sum of linearly transformed input vectors. Kobayashi et al. (2020) recently showed that the input vector norms vary considerably, and the inputs to the self-attention mechanism can have a disproportionate impact relative to their self-attention weight. So we consider both the raw attention maps, and, to assess the true impact of the input in the weighted sum, the L2-norm of the transformed input multiplied by the attention weight (for which we annotated 600 more attention maps with the same pattern types as Kovaleva et al. (2019)). The weighted average of F1 scores of the classifier on annotated data was 0.81 for the raw attention maps, and 0.74 for the norm"
2020.emnlp-main.259,D19-1445,1,0.689487,"ks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns. 1 2 Introduction Much of the recent progress in NLP is due to the transfer learning paradigm in which Transformerbased models first try to learn task-independent linguistic knowledge from large corpora, and then get fine-tuned on small datasets for specific tasks. However, these models are overparametrized: we now know that most Transformer heads and even layers can be pruned without significant loss in performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). One of the most famous Transformer-based models is BERT (Devlin et al., 2019). It became a must-have baseline and inspired dozens of studies probing it for various kinds of linguistic information (Rogers et al., 2020b). We conduct a systematic case study of finetuning BERT on GLUE tasks (Wang et al., 2018) from the perspective of the lottery ticket hypothesis ∗ Equal contribution Related Work Multiple studies of BERT concluded that it is considerably overparametrized. In particular, it is possible to ablate elements of its architecture without loss in performance or eve"
2020.emnlp-main.259,W19-4825,0,0.0455725,"Missing"
2020.emnlp-main.259,N19-1112,0,0.0436806,"Missing"
2020.emnlp-main.259,P19-1459,0,0.0290377,"e question of whether “good” subnetworks can be used as an tool to understand how BERT works. Another contemporaneous study by Gordon et al. (2020) also explores magnitude pruning, showing that BERT pruned before fine-tuning still reaches performance similar to the full model. Ideally, the pre-trained weights would provide transferable linguistic knowledge, fine-tuned only to learn a given task. But we do not know what knowledge actually gets used for inference, except that BERT is as prone as other models to rely on dataset biases (McCoy et al., 2019b; Rogers et al., 2020a; Jin et al., 2020; Niven and Kao, 2019; Zellers et al., 2019). At the same time, there is vast literature on probing BERT architecture blocks for different linguistic properties (Rogers et al., 2020b). If there are “good” subnetworks, then studying their properties might explain how BERT works. 3 Methodology All experiments in this study are done on the “BERT-base lowercase” model from the Transformers library (Wolf et al., 2020). It is fine-tuned2 on 9 GLUE tasks, and evaluated with the metrics shown in Table 1. All evaluation is done on the dev sets, as the test sets are not publicly distributed. For each experiment we test 5 ra"
2020.emnlp-main.259,2020.emnlp-main.554,0,0.0193896,"etworks Furthermore, should we perhaps be asking the same question with respect to not only subnetworks, but also full models, such as BERT itself and all the follow-up Transformers? There is a trend to automatically credit any new state-of-the-art model with with better knowledge of language. However, what if that is not the case, and the success of pretraining is rather due to the flatter and wider optima in the optimization surface (Hao et al., 2019)? Can similar loss landscapes be obtained from other, non-linguistic pre-training tasks? There are initial results pointing in that direction: Papadimitriou and Jurafsky (2020) report that even training on MIDI music is helpful for transfer learning for LM task with LSTMs. 7 Conclusion This study systematically tested the lottery ticket hypothesis in BERT fine-tuning with two pruning methods: magnitude-based weight pruning and importance-based pruning of BERT self-attention heads and MLPs. For both methods, we find that the pruned “good” subnetworks alone reach the performance comparable with the full model, while the “bad” ones do not. However, for structured pruning, even the “bad” subnetworks can be finetuned separately to reach fairly strong performance. The “go"
2020.emnlp-main.259,D16-1264,0,0.0530828,"Missing"
2020.emnlp-main.259,2020.tacl-1.54,1,0.867977,"g paradigm in which Transformerbased models first try to learn task-independent linguistic knowledge from large corpora, and then get fine-tuned on small datasets for specific tasks. However, these models are overparametrized: we now know that most Transformer heads and even layers can be pruned without significant loss in performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). One of the most famous Transformer-based models is BERT (Devlin et al., 2019). It became a must-have baseline and inspired dozens of studies probing it for various kinds of linguistic information (Rogers et al., 2020b). We conduct a systematic case study of finetuning BERT on GLUE tasks (Wang et al., 2018) from the perspective of the lottery ticket hypothesis ∗ Equal contribution Related Work Multiple studies of BERT concluded that it is considerably overparametrized. In particular, it is possible to ablate elements of its architecture without loss in performance or even with slight gains (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019). This explains the success of multiple BERT compression studies (Sanh et al., 2019; Jiao et al., 2019; McCarley, 2019; Lan et al., 2020). While NLP focused"
2020.emnlp-main.259,P19-1282,0,0.0316365,"hat this result does not exclude the possibility that linguistic information is encoded in certain combinations of BERT elements. However, to date most BERT analysis studies focused on the functions of individual components (Voita et al., 2019; Htut et al., 2019; Clark et al., 2019; Lin et al., 2019; Vig and Belinkov, 2019; Hewitt and Manning, 2019; Tenney et al., 2019, see also the overview by Rogers et al. (2020b)), and this evidence points to the necessity of looking at their interactions. It also adds to the ongoing discussion of interpretability of self-attention (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020). Once again, heterogenerous pattern counts are only a crude upper bound estimate on potentially interpretable patterns. More sophisticated alternatives should be explored in future work. For instance, the recent information-theoretic probing by minimum description length (Voita and Titov, 2020) avoids the problem of false positives with traditional probing classifiers. 5.3 Information Shared Between Tasks While the “good” subnetworks are not stable, the overlaps between the “good” subnetworks may still be used to characterize the tasks themse"
2020.emnlp-main.259,D13-1170,0,0.0129359,"Missing"
2020.emnlp-main.259,P19-1452,0,0.197805,"portance score, and could all be pruned with about equal success. 25 Frequency 20 15 10 5 0 0.0 0.2 0.4 0.6 0.8 Importance score 1.0 Figure 3: Head importance scores distribution (this example shows CoLA, pruning iteration 1) 5.2 How Linguistic are the “Good” Subnetworks? A popular method of studying functions of BERT architecture blocks is to use probing classifiers for specific linguistic functions. However, “the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used” (Tenney et al., 2019). In this study we use a cruder, but more reliable alternative: the types of self-attention patterns, which Kovaleva et al. (2019) classified as diagonal (attention to previous/next word), block (uniform attention over a sentence), vertical (attention to punctuation and special tokens), vertical+diagonal, and heterogeneous (everything else) (see Figure 4a). The fraction of heterogeneous attention can be used as an upper bound estimate on non-trivial linguistic information. In other words, these patterns do not guarantee that a given head has some interpretable function – only that it could hav"
2020.emnlp-main.259,W19-4808,0,0.0195593,"e conducted the same analysis for the attention patterns in pre-trained vs. fine-tuned BERT for both super-survivors and all heads, and found them to not change considerably after fine-tuning, which is consistent with findings by Kovaleva et al. (2019). Full data is available in Appendix F. Note that this result does not exclude the possibility that linguistic information is encoded in certain combinations of BERT elements. However, to date most BERT analysis studies focused on the functions of individual components (Voita et al., 2019; Htut et al., 2019; Clark et al., 2019; Lin et al., 2019; Vig and Belinkov, 2019; Hewitt and Manning, 2019; Tenney et al., 2019, see also the overview by Rogers et al. (2020b)), and this evidence points to the necessity of looking at their interactions. It also adds to the ongoing discussion of interpretability of self-attention (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020). Once again, heterogenerous pattern counts are only a crude upper bound estimate on potentially interpretable patterns. More sophisticated alternatives should be explored in future work. For instance, the recent information-theoretic probing by mini"
2020.emnlp-main.259,P19-1580,0,0.0892473,"the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns. 1 2 Introduction Much of the recent progress in NLP is due to the transfer learning paradigm in which Transformerbased models first try to learn task-independent linguistic knowledge from large corpora, and then get fine-tuned on small datasets for specific tasks. However, these models are overparametrized: we now know that most Transformer heads and even layers can be pruned without significant loss in performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). One of the most famous Transformer-based models is BERT (Devlin et al., 2019). It became a must-have baseline and inspired dozens of studies probing it for various kinds of linguistic information (Rogers et al., 2020b). We conduct a systematic case study of finetuning BERT on GLUE tasks (Wang et al., 2018) from the perspective of the lottery ticket hypothesis ∗ Equal contribution Related Work Multiple studies of BERT concluded that it is considerably overparametrized. In particular, it is possible to ablate elements of its architecture without los"
2020.emnlp-main.259,D19-1002,0,0.0220554,"exclude the possibility that linguistic information is encoded in certain combinations of BERT elements. However, to date most BERT analysis studies focused on the functions of individual components (Voita et al., 2019; Htut et al., 2019; Clark et al., 2019; Lin et al., 2019; Vig and Belinkov, 2019; Hewitt and Manning, 2019; Tenney et al., 2019, see also the overview by Rogers et al. (2020b)), and this evidence points to the necessity of looking at their interactions. It also adds to the ongoing discussion of interpretability of self-attention (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020). Once again, heterogenerous pattern counts are only a crude upper bound estimate on potentially interpretable patterns. More sophisticated alternatives should be explored in future work. For instance, the recent information-theoretic probing by minimum description length (Voita and Titov, 2020) avoids the problem of false positives with traditional probing classifiers. 5.3 Information Shared Between Tasks While the “good” subnetworks are not stable, the overlaps between the “good” subnetworks may still be used to characterize the tasks themselves. We leave detailed expl"
2020.emnlp-main.259,P19-1472,0,0.0339452,"“good” subnetworks can be used as an tool to understand how BERT works. Another contemporaneous study by Gordon et al. (2020) also explores magnitude pruning, showing that BERT pruned before fine-tuning still reaches performance similar to the full model. Ideally, the pre-trained weights would provide transferable linguistic knowledge, fine-tuned only to learn a given task. But we do not know what knowledge actually gets used for inference, except that BERT is as prone as other models to rely on dataset biases (McCoy et al., 2019b; Rogers et al., 2020a; Jin et al., 2020; Niven and Kao, 2019; Zellers et al., 2019). At the same time, there is vast literature on probing BERT architecture blocks for different linguistic properties (Rogers et al., 2020b). If there are “good” subnetworks, then studying their properties might explain how BERT works. 3 Methodology All experiments in this study are done on the “BERT-base lowercase” model from the Transformers library (Wolf et al., 2020). It is fine-tuned2 on 9 GLUE tasks, and evaluated with the metrics shown in Table 1. All evaluation is done on the dev sets, as the test sets are not publicly distributed. For each experiment we test 5 random seeds. 2 All exper"
2020.emnlp-main.259,2020.emnlp-main.14,0,0.110719,"Missing"
2020.emnlp-main.259,W18-5446,0,0.0501285,"Missing"
2020.emnlp-main.259,I05-5002,0,\N,Missing
2020.emnlp-main.259,N19-1419,0,\N,Missing
2020.emnlp-main.259,N18-1101,0,\N,Missing
2020.emnlp-main.259,Q19-1040,0,\N,Missing
2020.tacl-1.54,N19-1078,0,0.0265318,"Missing"
2020.tacl-1.54,2020.acl-main.431,0,0.079615,"Missing"
2020.tacl-1.54,W19-4828,0,0.0758508,"Missing"
2020.tacl-1.54,D19-6001,0,0.0760365,"Missing"
2020.tacl-1.54,D19-1109,0,0.0644178,"Missing"
2020.tacl-1.54,2020.repl4nlp-1.18,0,0.0483506,"Missing"
2020.tacl-1.54,W19-4825,0,0.111389,"Missing"
2020.tacl-1.54,2020.lifelongnlp-1.3,0,0.0613642,"Missing"
2020.tacl-1.54,N19-1112,0,0.152397,"Missing"
2020.tacl-1.54,N19-1063,0,0.0506039,"Missing"
2020.tacl-1.54,P19-1459,0,0.0614942,"Missing"
2020.tacl-1.54,D19-1005,0,0.0590525,"Missing"
2020.tacl-1.54,2020.findings-emnlp.49,0,0.0370225,"Missing"
2020.tacl-1.54,W18-5431,0,0.0579299,"Missing"
2020.tacl-1.54,2020.acl-main.442,0,0.0321175,"Missing"
2020.tacl-1.54,P19-1282,0,0.0800263,"Missing"
2020.tacl-1.54,P19-1355,0,0.121022,"Missing"
2020.tacl-1.54,2020.acl-main.368,0,0.047507,"Missing"
2020.tacl-1.54,2020.repl4nlp-1.20,0,0.067771,"Missing"
2020.tacl-1.54,D19-1374,0,0.0519364,"Missing"
2020.tacl-1.54,D19-1592,0,0.0807287,"Missing"
2020.tacl-1.54,D19-1534,0,0.0577584,"Missing"
2020.tacl-1.54,W18-5446,0,0.121941,"Missing"
2020.tacl-1.54,P19-1580,0,0.0991377,"Missing"
2020.tacl-1.54,2020.emnlp-main.14,0,0.0949641,"Missing"
2020.tacl-1.54,D19-1286,0,0.0727376,"Missing"
2020.tacl-1.54,2020.emnlp-main.633,0,0.043015,"Missing"
2020.tacl-1.54,D19-1002,0,0.0939086,"Missing"
2020.tacl-1.54,2020.acl-main.745,0,0.0487362,"Missing"
2020.tacl-1.54,2020.emnlp-demos.6,0,0.117351,"Missing"
2020.tacl-1.54,P19-1472,0,0.038261,"Missing"
2020.tacl-1.54,P19-1139,0,0.0621759,"Missing"
2020.tacl-1.54,P98-1013,0,\N,Missing
2020.tacl-1.54,C98-1013,0,\N,Missing
2020.tacl-1.54,N10-1003,0,\N,Missing
2020.tacl-1.54,D14-1162,0,\N,Missing
2020.tacl-1.54,Q18-1018,0,\N,Missing
2020.tacl-1.54,Q19-1004,0,\N,Missing
2020.tacl-1.54,P19-1334,0,\N,Missing
2020.tacl-1.54,N19-1357,0,\N,Missing
2020.tacl-1.54,D19-1077,0,\N,Missing
2020.tacl-1.54,N19-1419,0,\N,Missing
2020.tacl-1.54,P19-1493,0,\N,Missing
2020.tacl-1.54,P19-1356,0,\N,Missing
2020.tacl-1.54,N19-1423,0,\N,Missing
2020.tacl-1.54,D19-1445,1,\N,Missing
2020.tacl-1.54,D19-1250,0,\N,Missing
2020.tacl-1.54,D19-1424,0,\N,Missing
2020.tacl-1.54,D19-1006,0,\N,Missing
2020.tacl-1.54,W19-6204,0,\N,Missing
2020.tacl-1.54,D19-6106,0,\N,Missing
2020.tacl-1.54,D19-5611,0,\N,Missing
2020.tacl-1.54,2020.tacl-1.5,0,\N,Missing
2021.findings-acl.300,N19-1423,0,0.0611036,"Missing"
2021.findings-acl.300,2020.repl4nlp-1.18,0,0.0193427,"ion which involves setting some of its weights to zeros with minimal performance loss. Much pruning work focuses on compression for the sake of efficiency, but it is also used for model analysis, and that is our goal as well. The most common approach is selecting the weights to be pruned by magnitude (Han et al., 2015). Some of the recent findings are that the lottery ticket hypothesis (Frankle and Carbin, 2019) holds for BERT: its largest weights do form subnetworks that can be retrained alone to reach the performance close to that of the full model (Prasanna et al., 2020; Chen et al., 2020; Gordon et al., 2020). In structured pruning, the best subnets of BERT’s heads and MLPs (selected by importance scores) 3393 3 Outlier weights in BERT models In this section, we introduce the methodology for identifying outlier dimensions and describe our method for disabling these outlier features. 3.1 Identification To identify the outlier weights in BERT-like models, we consider all the output components in each encoder layer. We compute the mean and standard deviation of the bias and scaling factors of the output LayerNorm. We identify the dimensions where both of these weights are at least 3σ from the mean. F"
2021.findings-acl.300,2020.findings-emnlp.372,0,0.0369357,"20) show that the location of LayerNorm in Transformer affects the gradient flow and demonstrate the need of the warmup stage. Nguyen and Salazar (2019) inject multiple normalization blocks in specific network submodules to improve model performance. More recently, LayerNorm alternatives have been proposed and shown to have better gradient propagation through the network (Xu et al., 2019; Shen et al., 2020). Overparametrization. After the initial reports of redundancy in the BERT model (Michel et al., 2019; Kovaleva et al., 2019), compressing Transformers quickly became a subfield of its own (Jiao et al., 2020; Zafrir et al., 2019; Fan et al., 2019; Guo et al., 2020). See overviews by Ganesh et al. (2020) and Rogers et al. (2020). Pruning is a class of methods for model compression which involves setting some of its weights to zeros with minimal performance loss. Much pruning work focuses on compression for the sake of efficiency, but it is also used for model analysis, and that is our goal as well. The most common approach is selecting the weights to be pruned by magnitude (Han et al., 2015). Some of the recent findings are that the lottery ticket hypothesis (Frankle and Carbin, 2019) holds for BE"
2021.findings-acl.300,D19-1445,1,0.746228,"es and they focused mostly on the description of the training process. Xiong et al. (2020) show that the location of LayerNorm in Transformer affects the gradient flow and demonstrate the need of the warmup stage. Nguyen and Salazar (2019) inject multiple normalization blocks in specific network submodules to improve model performance. More recently, LayerNorm alternatives have been proposed and shown to have better gradient propagation through the network (Xu et al., 2019; Shen et al., 2020). Overparametrization. After the initial reports of redundancy in the BERT model (Michel et al., 2019; Kovaleva et al., 2019), compressing Transformers quickly became a subfield of its own (Jiao et al., 2020; Zafrir et al., 2019; Fan et al., 2019; Guo et al., 2020). See overviews by Ganesh et al. (2020) and Rogers et al. (2020). Pruning is a class of methods for model compression which involves setting some of its weights to zeros with minimal performance loss. Much pruning work focuses on compression for the sake of efficiency, but it is also used for model analysis, and that is our goal as well. The most common approach is selecting the weights to be pruned by magnitude (Han et al., 2015). Some of the recent findi"
2021.findings-acl.300,2020.acl-main.703,0,0.0623552,"Missing"
2021.findings-acl.300,2021.ccl-1.108,0,0.0693145,"Missing"
2021.findings-acl.300,2021.acl-long.413,0,0.039521,"se of BERT, the layer output component is LayerNorm. We introduce a method to isolate these outlier dimensions for BERT, and we show that the phenomenon is present in six models of BERT family that we examine. It is also present in four other Transformer-based models (ELECTRA, XLNet, BART, and GPT-2), although the effect of their disabling varies. 8 Acknowledgments We would like to thank the anonymous reviewers for their insightful comments. This work is funded in part by the NSF award number IIS-1844740 to Anna Rumshisky. We would like to note that in an independent study concurrent to ours, Luo et al. (2021) report a similar effect of outlier values in the embedding space of BERT and RoBERTa. We invite the reader to look into their study for more details on the outlier features from the perspective of word embeddings. 9 Impact statement This work analyzes the behavior of layer normalization in the popular BERT family of models, using standard benchmarks. No new models are presented, and no data was collected. We estimate that experiments presented in this paper consumed 436.8 kWh of energy which resulted in a 108 Kilograms of CO2 emissions computed using the regional average of power generation t"
2021.findings-acl.300,2020.emnlp-main.259,1,0.845494,"g is a class of methods for model compression which involves setting some of its weights to zeros with minimal performance loss. Much pruning work focuses on compression for the sake of efficiency, but it is also used for model analysis, and that is our goal as well. The most common approach is selecting the weights to be pruned by magnitude (Han et al., 2015). Some of the recent findings are that the lottery ticket hypothesis (Frankle and Carbin, 2019) holds for BERT: its largest weights do form subnetworks that can be retrained alone to reach the performance close to that of the full model (Prasanna et al., 2020; Chen et al., 2020; Gordon et al., 2020). In structured pruning, the best subnets of BERT’s heads and MLPs (selected by importance scores) 3393 3 Outlier weights in BERT models In this section, we introduce the methodology for identifying outlier dimensions and describe our method for disabling these outlier features. 3.1 Identification To identify the outlier weights in BERT-like models, we consider all the output components in each encoder layer. We compute the mean and standard deviation of the bias and scaling factors of the output LayerNorm. We identify the dimensions where both of these"
2021.findings-acl.300,W18-5446,0,0.0885797,"Missing"
C04-1133,W99-0901,0,0.0456944,"ora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly"
C04-1133,W01-0703,0,0.0127387,"icates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disam"
C04-1133,C02-1112,0,0.0114425,", Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays"
C04-1133,A97-1052,0,0.0608604,"sambiguation, selectional preference acquisition, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as t"
C04-1133,briscoe-carroll-2002-robust,0,0.0343565,"isting semantic classes. For instance, checking if, for a certain percentage of pairs in the candidate set, there already exists a set of which both elements are members. 3 Current Implementation The CPA patterns are developed using the British National Corpus (BNC). The sorted instances are used as a training set for the supervised disambiguation. For the disambiguation task, each pattern is translated into into a set of preprocessing-specific features. The BNC is preprocessed using the Robust Accurate Statistical Parsing system (RASP) and semantically tagged with BSO types. The RASP system (Briscoe and Carroll (2002)) tokenizes, POS-tags, and lemmatizes text, generating a forest of full parse trees for each sentence and associating a probability with each parse. For each parse, RASP produces a set of grammatical relations, specifying the relation type, the headword, and the dependent element. All our computations are performed over the single topranked tree for the sentences where a full parse was successfully obtained. Some of the grammatical relations identified by RASP are shown in (10). (10) subjects: ncsubj, clausal (csubj, xsubj) objects: dobj, iobj, clausal complement modifiers: adverbs, modifiers"
C04-1133,C00-1028,0,0.0127386,"ence in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the def"
C04-1133,N01-1013,0,0.0119248,"hms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current Stat"
C04-1133,P03-1007,0,0.0327247,"over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Overwhelmingly, WordNet is chosen as the default resource for dealing with the sparse data problem (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Agirre and Martinez (2001); Clark and Weir (2001); Carroll and McCarthy (2000); Korhonen and Preiss (2003)). Much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (Resnik (1996); Abney and Light (1999); Ciaramita and Johnson (2000); Rooth et al. (1999)). Those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy"
C04-1133,W97-0808,0,0.0223121,"eference acquisition, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierar"
C04-1133,P99-1014,0,0.010384,"ion, as well as anaphora resolution and inference in specialized domains. We apply this methodology to a selected set of verbs, including a subset of the verbs in the Senseval 3 word sense discrimination task and report our initial results. 1.1 Selectional Preference Acquisition: Current State of the Art Predicate subcategorization information constitutes an essential part of the computational lexicon entry. In recent years, a number of approaches have been proposed for dealing computationally with selectional preference acquisition (Resnik (1996); Briscoe and Carroll (1997); McCarthy (1997); Rooth et al. (1999); Abney and Light (1999); Ciaramita and Johnson (2000); Korhonen (2002)). The currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as a distribution over words (cf. Abney and Light (1999)). Semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (Rooth et al. (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic"
C04-1133,W98-0701,0,0.0265348,"work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what"
C04-1133,J01-3001,0,0.0156747,"cal features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns with senses, however. In this paper we modify the notion of word sense, and at the same time revise the manner in which senses are encoded. The notion of word sense that has been generally adopted in the literature is an artifact of several factors in the status quo,"
C04-1133,P03-2029,0,0.0121971,"y supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (2002); Yamashita et al. (2003)) attempts to change this situation and presents a directed effort to investigate the impact of using syntactic features for WSD learning algorithms. Agirre et al (2002) and Yamashita et al. (2003) report resulting improvement in precision. Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns w"
C04-1133,C92-2070,0,0.104375,"do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages. However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998);"
C04-1133,P95-1026,0,0.0211395,"etween predicate senses or complementation patterns in acquisition of selectional constraints (Korhonen (2002); Korhonen and Preiss (2003)) do not use corpus analysis for verb sense classification. 1.2 Word Sense Disambiguation: Current State of the Art Previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages. However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. Topical features track open-class words that appear within a certain window around a target word, and local features track small N-grams associated with the target word. Disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities. That remains the case for most systems that participated in Senseval-2 (Preiss and Yarowsky (2001)). Some recent work (Stetina et al. (1998); Agirre et al. (20"
C04-1133,L02-1000,0,\N,Missing
C18-1004,D16-1245,0,0.0524584,"e more common in literature, and the third one is somewhat less studied. Pairwise models a.k.a. mention pair models build a binary classifier over pairs of mentions (Soon et al., 2001; McCallum and Wellner, 2003). If all the pairs are classified correctly, then all coreferent mentions are identified. The mention ranking models do not rely on the full pairwise classification, but rather compare each mention to its possible antecedents in order to determine whether the mention might refer to an existing antecedent or starts a new coreference chain (Durrett and Klein, 2013; Wiseman et al., 2016; Clark and Manning, 2016). The entity-mention models try constructing representations of discourse entities, and associating different mentions with the entity representations (Luo et al., 2004). However, none of these model types consider more than two mentions together at the low level. By low level here, we mean the processing of input mention features, as opposed to processing of constructed representations. Pairwise models and mention ranking models make low-level decisions on mention pairs only. Some further processing may be applied to reconcile global scope conflicts, but this process no longer relies directly"
C18-1004,D13-1203,0,0.0220917,"ls, and entity-mention models. The first two are more common in literature, and the third one is somewhat less studied. Pairwise models a.k.a. mention pair models build a binary classifier over pairs of mentions (Soon et al., 2001; McCallum and Wellner, 2003). If all the pairs are classified correctly, then all coreferent mentions are identified. The mention ranking models do not rely on the full pairwise classification, but rather compare each mention to its possible antecedents in order to determine whether the mention might refer to an existing antecedent or starts a new coreference chain (Durrett and Klein, 2013; Wiseman et al., 2016; Clark and Manning, 2016). The entity-mention models try constructing representations of discourse entities, and associating different mentions with the entity representations (Luo et al., 2004). However, none of these model types consider more than two mentions together at the low level. By low level here, we mean the processing of input mention features, as opposed to processing of constructed representations. Pairwise models and mention ranking models make low-level decisions on mention pairs only. Some further processing may be applied to reconcile global scope confl"
C18-1004,P04-1018,0,0.0917602,"., 2001; McCallum and Wellner, 2003). If all the pairs are classified correctly, then all coreferent mentions are identified. The mention ranking models do not rely on the full pairwise classification, but rather compare each mention to its possible antecedents in order to determine whether the mention might refer to an existing antecedent or starts a new coreference chain (Durrett and Klein, 2013; Wiseman et al., 2016; Clark and Manning, 2016). The entity-mention models try constructing representations of discourse entities, and associating different mentions with the entity representations (Luo et al., 2004). However, none of these model types consider more than two mentions together at the low level. By low level here, we mean the processing of input mention features, as opposed to processing of constructed representations. Pairwise models and mention ranking models make low-level decisions on mention pairs only. Some further processing may be applied to reconcile global scope conflicts, but this process no longer relies directly on mention features. This paper proposes a neural network model which works on triads of mentions directly. Each time, the system takes three mentions as input, and dec"
C18-1004,H05-1004,0,0.163338,"Missing"
C18-1004,P16-1060,0,0.0453115,"Missing"
C18-1004,J01-4004,0,0.362404,"aims to identify mentions that refer to the same entity. A mention is a piece of text, usually a noun, a pronoun, or a nominal phrase. Resolving coreference often requires understanding the full context, and sometimes also world knowledge not provided in the text. Generally speaking, three types of models have been used for coreference resolution: pairwise models, mention ranking models, and entity-mention models. The first two are more common in literature, and the third one is somewhat less studied. Pairwise models a.k.a. mention pair models build a binary classifier over pairs of mentions (Soon et al., 2001; McCallum and Wellner, 2003). If all the pairs are classified correctly, then all coreferent mentions are identified. The mention ranking models do not rely on the full pairwise classification, but rather compare each mention to its possible antecedents in order to determine whether the mention might refer to an existing antecedent or starts a new coreference chain (Durrett and Klein, 2013; Wiseman et al., 2016; Clark and Manning, 2016). The entity-mention models try constructing representations of discourse entities, and associating different mentions with the entity representations (Luo et"
C18-1004,M95-1005,0,0.747748,"Missing"
C18-1004,N16-1114,0,0.0226143,"dels. The first two are more common in literature, and the third one is somewhat less studied. Pairwise models a.k.a. mention pair models build a binary classifier over pairs of mentions (Soon et al., 2001; McCallum and Wellner, 2003). If all the pairs are classified correctly, then all coreferent mentions are identified. The mention ranking models do not rely on the full pairwise classification, but rather compare each mention to its possible antecedents in order to determine whether the mention might refer to an existing antecedent or starts a new coreference chain (Durrett and Klein, 2013; Wiseman et al., 2016; Clark and Manning, 2016). The entity-mention models try constructing representations of discourse entities, and associating different mentions with the entity representations (Luo et al., 2004). However, none of these model types consider more than two mentions together at the low level. By low level here, we mean the processing of input mention features, as opposed to processing of constructed representations. Pairwise models and mention ranking models make low-level decisions on mention pairs only. Some further processing may be applied to reconcile global scope conflicts, but this process"
C18-1064,P17-1067,0,0.056223,"Missing"
C18-1064,baccianella-etal-2010-sentiwordnet,0,0.0276728,"cts/rusentiment/ Licence details: http: 755 Proceedings of the 27th International Conference on Computational Linguistics, pages 755–763 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 Related Work Russian is generally not as well resourced as English, and that includes the sentiment analysis data. RuSentiLex, the largest sentiment lexicon for Russian3 (Loukachevitch and Levchik, 2016), currently contains 16,057 words, which exceeds the size of such manually constructed English resources as, for example, SentiStrength (Thelwall and Buckley, 2013). However, there is nothing like SentiWordNet (Baccianella et al., 2010), SentiWords, (Gatti et al., 2016), or SenticNet (Cambria et al., 2018) for Russian. There are also few annotated datasets. The datasets from the SentiRuEval 2015 and 2016 competitions are the largest resource that has been available to date (Loukachevitch and Rubtsova, 2016). The SentiRuEval 2016 dataset is comprised by 10,890 tweets from the telecom domain and 12,705 from the banking domain. The Linis project (Koltsova et al., 2016) reports to have crowdsourced annotation for 19,831 blog excerpts, but only 3,327 are currently available on the project website. The choice of VK social network"
C18-1064,I13-1041,0,0.0191122,"nt potentially ambiguous cases and would be easy to apply consistently. Most of the categories we have used have been defined and used before (Liu, 2015; Toprak et al., 2010; Wiebe et al., 2005; Thelwall et al., 2010), and our contribution is mainly their combination that enabled the right balance between coverage and ease of application. 3 There are at least two more projects that attempt to crowdsource sentiment lexicons: SentiBase (http:// web-corpora.net/wsgi/senti_game.wsgi/rules), and Sentimeter (http://sentimeter.ru/assess/ instruction/). At the moment, both appear to be unfinished. 4 Baldwin et al. (2013) did not find significant grammatical or spelling differences between Twitter, Youtube comments, or blogs, but domain (telecom in SentiRuEval) could impact the ratio of professionally edited commercial or news-like texts. 5 The list comprised 169 keywords, including political entities (such as Moscow or Putin) and words coined and used during the Maidan conflict (such as ukrop “dill”, a Russian derogatory term for Ukrainians). 6 This work is covered by an IRB protocol at the authors’ institution. 756 Figure 1: Annotation web-interface. Finding this point of balance required multiple pilots and"
C18-1064,2014.lilt-9.5,0,0.0201147,"Missing"
C18-1064,Q17-1010,0,0.0842305,"Missing"
C18-1064,C10-2028,0,0.0596788,"ness of the speaker while also implying their high opinion of the addressee, and is annotated as negative. Hashtags such as #epicentr (company name) were considered neutral, but those that could agree or disagree with the general sentiment of the post were treated as sentiment markers. This concerned the hashtags that expressed the sentiment explicitly (e.g. #ihate, #sad) or implicitly, via experiences that most people would consider positive or negative (e.g. #beach, #party). The annotators were instructed to not treat the emoticons as automatic sentiment labels, as done by Go et al. (2009), Davidov et al. (2010), Sahni et al. (2017), and others. Some emoticons do indeed strengthen the message (Derks et al., 2008), but others serve to soften its illocutionary force without changing its content (Ernst and Huschens, 2018).8 A user may end a post with a “hedging” emoticon just to express friendliness or politeness, and we found that often to be the case for VK data. Therefore, emoticons were not taken into account when no sentiment was expressed verbally or when they aligned with the content of the message. Emoticons were considered relevant only when they contradicted the verbal clues of sentiment. In t"
C18-1064,W17-1409,0,0.0509533,"Missing"
C18-1064,N16-1162,0,0.028347,"Missing"
C18-1064,W17-4205,0,0.0146747,"ion guidelines that are extensible to other languages. RuSentiment is currently the largest in its class for Russian, with 31,185 posts annotated with Fleiss’ kappa of 0.58 (3 annotations per post). To diversify the dataset, 6,950 posts were pre-selected with an active learning-style strategy. We report baseline classification results, and we also release the best-performing word embeddings trained on 3.2B corpus of Russian social media posts. 1 Introduction Over the past several years sentiment analysis has been increasingly important in political science (Ceron et al., 2015) and journalism (Jiang et al., 2017). Such applications necessitate resources for languages spoken in the conflict zones. Our study focuses on Russian, which to date has little annotated data (Loukachevitch and Rubtsova, 2016; Koltsova et al., 2016), and no openly available sentiment detection systems beyond the dictionary-based ones. However, lexical features have time and again shown inferior performance compared to the supervised learning approaches using annotated data (Gombar et al., 2017), and lack of such a resource severely limits sentiment analysis applications for Russian. We present RuSentiment, a dataset of public po"
C18-1064,L16-1186,0,0.0218913,"word embeddings are available at the project website2 . This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. 1 https://vk.com/about 2 http://text-machine.cs.uml.edu/projects/rusentiment/ Licence details: http: 755 Proceedings of the 27th International Conference on Computational Linguistics, pages 755–763 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 Related Work Russian is generally not as well resourced as English, and that includes the sentiment analysis data. RuSentiLex, the largest sentiment lexicon for Russian3 (Loukachevitch and Levchik, 2016), currently contains 16,057 words, which exceeds the size of such manually constructed English resources as, for example, SentiStrength (Thelwall and Buckley, 2013). However, there is nothing like SentiWordNet (Baccianella et al., 2010), SentiWords, (Gatti et al., 2016), or SenticNet (Cambria et al., 2018) for Russian. There are also few annotated datasets. The datasets from the SentiRuEval 2015 and 2016 competitions are the largest resource that has been available to date (Loukachevitch and Rubtsova, 2016). The SentiRuEval 2016 dataset is comprised by 10,890 tweets from the telecom domain and"
C18-1064,S17-2088,0,0.0361322,"and extensive linguistic analysis. This difficulty is likely the reason why many sentiment annotation projects, including the Russian crowdsourced initiatives mentioned in Section 2, provide only minimal annotation instructions. Such instructions often yield very inconsistent data. Our guidelines are available at the project website in English, with Russian examples.7 We hope that they would be useful for subsequent work in other languages and domains. We prioritized the speed of annotation over detail, opting for a 3-point scale rather than e.g., the 5point scale in SemEval Twitter datasets (Rosenthal et al., 2017). Thus, the task was to rate the prevailing sentiment in complete posts from VK on a three-point scale (“negative"", “neutral”, and “positive”). We also defined the “skip” class for excluding the posts that were too noisy, unclear, or not in Russian (e.g., in Ukrainian). We also made the decision to exclude jokes, poems, song lyrics, and other such content that was not generated by the users themselves. It could be argued that posting jokes on social media should be interpreted as an expression of positive mood, but such data is easy to import from existing web collections. Although the only se"
C18-1064,D12-1110,0,0.0697679,"Missing"
C18-1064,P10-1059,0,0.0306114,"” guidelines that would ensure high enough agreement is far from being solved. Sentiment is an extremely multi-faceted phenomenon, and each research team in the end has to make its own choices about how it would prefer to treat implicit vs. explicit sentiment, subjective feeling and emotion vs. evaluation, and also irony, sarcasm, and other types of mixed sentiment. We aimed to develop comprehensive guidelines that would cover the most frequent potentially ambiguous cases and would be easy to apply consistently. Most of the categories we have used have been defined and used before (Liu, 2015; Toprak et al., 2010; Wiebe et al., 2005; Thelwall et al., 2010), and our contribution is mainly their combination that enabled the right balance between coverage and ease of application. 3 There are at least two more projects that attempt to crowdsource sentiment lexicons: SentiBase (http:// web-corpora.net/wsgi/senti_game.wsgi/rules), and Sentimeter (http://sentimeter.ru/assess/ instruction/). At the moment, both appear to be unfinished. 4 Baldwin et al. (2013) did not find significant grammatical or spelling differences between Twitter, Youtube comments, or blogs, but domain (telecom in SentiRuEval) could imp"
C18-1064,P13-2090,1,0.839742,"ntent that was not generated by the users themselves. It could be argued that posting jokes on social media should be interpreted as an expression of positive mood, but such data is easy to import from existing web collections. Although the only sentiment classes we annotated are “positive” and “negative”, RuSentiment guidelines address both explicit and implicit forms of expressions for the speaker’s internal emotional state (mood) and external attitude (evaluation), as shown in Table 1. These distinctions are often not accounted for in sentiment data, including many of the English datasets (Volkova et al., 2013; AbdulMageed and Ungar, 2017). The guidelines cover such cases of implicit sentiment as rhetorical questions, (non-)desirability, recommendations, and descriptions or mentions of the experiences that most people would consider positive or negative. Additionally, we defined a subcategory of positive posts that covers frequent speech acts, such as expressions of gratitude, greetings, and congratulations. They are very frequent in VK data, and the sentiment they express is overtly positive, but they are also very formulaic. The separate subcategory enables excluding them from the category of pos"
C18-1228,W16-2514,0,0.0199684,", 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs and Kornai, 2016). Crucially, all these approaches make the same core assumption: that there is one feature of a representation that would make it the “best” (the highest correlation with human judgements, the most regular vector offsets, the closest approximation of a linguistic resource, etc.) However, language is a multifaceted phenomenon, and different NLP tasks may rely on its different aspects – which would doom any one-metric-to-rule-them-all approach. This is the starting point for our solution. 3 LDT: the methodology Consider two published modifications of the word2vec Rank Deps FastText model, both t"
C18-1228,N09-1003,0,0.276266,"Missing"
C18-1228,W17-5304,0,0.0185306,"16). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and analogical reasoning) is ov"
C18-1228,W11-2501,0,0.366215,"nce labeling tasks, but SimLex (Hill et al., 2015) performed better (Chiu et al., 2016). This could be due to its focus on a particular type of semantic relations (synonymy, co-hyponymy), which turned out to be relevant for the labeling tasks. Our solution is based on “linguistic diagnostic” tests, achieved by large-scale automatic annotation of linguistic, psychological and distributional relations between words vectors and their neighbors. The resulting data can then be used to find what features are useful for what extrinsic tasks. This work is inspired by the BLESS categorization dataset (Baroni and Lenci, 2011) and by evaluation via a set of representative extrinsic tasks (Nayak et al., 2016). LD analysis starts with sampling the corpus vocabulary, as will be described in Section 4.2. For each word, top n neighbor vectors are extracted from each embedding. Each neighbor undergoes spelling normalization and is paired with the source word for analysis of possible morphological, semantic, distributional and psychological relations between them, as shown in Figure 1. The annotated data is analyzed to produce direct or statistically derived measures of the degree to which a given embedding is characteriz"
C18-1228,W16-2502,0,0.0389635,"rd embeddings are the semantic relatedness tests (Finkelstein et al., 2002; Bruni et al., 2014; Luong et al., 2013; Radinsky et al., 2011). They rely on the idea that the distance between word vectors should correlate with human judgements of how related the two words are (e.g., cat should be closer to tiger than to hammer). A more sophisticated version of this task is the semantic similarity (Agirre et al., 2009; Hill et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be closer to tiger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations wi"
C18-1228,Q17-1010,0,0.491287,"rrelation with human judgements, the most regular vector offsets, the closest approximation of a linguistic resource, etc.) However, language is a multifaceted phenomenon, and different NLP tasks may rely on its different aspects – which would doom any one-metric-to-rule-them-all approach. This is the starting point for our solution. 3 LDT: the methodology Consider two published modifications of the word2vec Rank Deps FastText model, both trained on Wikipedia: the dependency1 colour 0.93 $color 0.75 based embeddings (DEPS) (Levy and Goldberg, 2 colors 0.72 color. . . 0.69 2014a) and FastText (Bojanowski et al., 2017). 3 coloration 0.69 colour 0.69 4 colouration 0.68 color#ff 0.69 Table 1 lists the first 7 nearest neighbors of color (as 5 colours 0.68 color#d 0.68 measured by cosine similarity). Both models output 6 hue 0.66 @color 0.67 the British spelling of the target word (colour). How7 hues 0.65 barcolor 0.67 ever, DEPS also includes derivatives and synonyms, Table 1: Top 7 neighbors of color in while FastText favors misspellings and compounds, as dependency-based and FastText embeddings. could be expected of a subword-level model. Which of these models is “better”? Without the context of some applica"
C18-1228,D15-1075,0,0.0212746,"t the sentence level, because sentiment is more likely to be mixed. The task is performed with a single layer LSTM with 100 hidden units. The classification of subjectivity and objectivity (Subjectivity class.) is tested on Rotten Tomato user review snippets vs official movie plot summaries (Pang and Lee, 2004). We follow the method by Li et al. (2017), employing a simple logistic regression model for the binary classification task. The input sentences are represented as a sum of their constituent word vectors. Finally, the natural language inference task is represented with the SNLI dataset (Bowman et al., 2015). Similarly to the original proposal, we use two separate LSTMs to get a representation of the premise and the hypothesis using the last hidden state. The two hidden representations are merged and fed into a 50-unit dense layer, over which 3-class classification with softmax is performed. 7 https://github.com/shashwath94/Extrinsic-Evaluation-tasks 2694 4.5 Intrinsic tasks Section 2 mentioned the reported lack of correlation between the performance of word embedding models on relatedness and sequence labeling tasks (Chiu et al., 2016). Ghannay et al. (2016) also report that the best-performing"
C18-1228,D14-1082,0,0.00876774,"and generally – a more principled, hypothesis-driven approach to development of distributional semantic representations. 1 Introduction Dense lexical embeddings are the most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embe"
C18-1228,W16-2501,0,0.578175,"16; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence to the contrary (Chiu et al., 2016). We hypothesize that different extrinsic tasks may rely on different aspects of word representations. In that case, the only way to reliably predict what an embedding can do is to know what aspects of language it captures, and what aspects of language are relevant for different tasks. To that end, we propose Linguistic Diagnostics (LD), a new approach to automated qualitative analysis of vector neighborhoods. To the best of our knowledge, this is the first large-scale attempt to identify and quantify the factors that make word embeddings successful with different tasks. We evaluate 60 models"
C18-1228,W10-0609,0,0.0333871,"iger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes lin"
C18-1228,C16-1332,0,0.226577,"e “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and analogical reasoning) is oversimplified, and performance on word analogies has also been shown to depend on cosine similarity between source word vectors (Rogers et al., 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs and Kornai, 2016). Crucially, all these approaches make the same core assumption: that there is one feature of a representation that would make it the “best” (the highest correlation with huma"
C18-1228,W16-2513,0,0.014207,"ling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and anal"
C18-1228,W16-2506,0,0.0233345,"insic evaluation of word embeddings are the semantic relatedness tests (Finkelstein et al., 2002; Bruni et al., 2014; Luong et al., 2013; Radinsky et al., 2011). They rely on the idea that the distance between word vectors should correlate with human judgements of how related the two words are (e.g., cat should be closer to tiger than to hammer). A more sophisticated version of this task is the semantic similarity (Agirre et al., 2009; Hill et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be closer to tiger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018"
C18-1228,L16-1046,0,0.289279,"fficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and analogical reasoning) is oversimplified, and performance on word analogies has also been shown to depend on cosine similarity between source word vectors (Rogers et al., 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs and Kornai, 2016). Crucially, all these approaches make the same core assumption: that there is one feature of a representation that would make it the “best” (the highest correlation with human judgements, the most regular vector offsets, the closest approximation of a linguistic resource, etc.) However, language is a m"
C18-1228,W16-2507,0,0.338879,"rrelate with human judgements of how related the two words are (e.g., cat should be closer to tiger than to hammer). A more sophisticated version of this task is the semantic similarity (Agirre et al., 2009; Hill et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be closer to tiger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming"
C18-1228,N16-2002,0,0.0571684,"election of word embeddings (amounting to 9 and 5 data points, correspondingly). Crucially, they also focus on the same sequence labeling CoNLL tasks. We explore the problem with our set of 60 embeddings, and a wider selection of extrinsic tasks. The intrinsic task datasets are WordSim353 (Finkelstein et al., 2002), together with its split into similarity and relatedness sections (Agirre et al., 2009), RareWords (Luong et al., 2013), MTurk (Radinsky et al., 2011), MEN (Bruni et al., 2014), and also the SimLex999 (Hill et al., 2015) similarity dataset. For the analogy task we use BATS dataset (Gladkova et al., 2016), which is currently the largest analogy dataset for English. We report separate scores for inflectional and derivational morphology, lexicographic and encyclopedic semantics, and the average of all categories. The evaluation on similarity and relatedness datasets is performed as Spearman’s correlation with the human judgement scores. The evaluation on analogies is performed with the state-of-the-art LRCos method (Drozd et al., 2016). 5 5.1 Results Correlation analysis In this study we experimented with 21 morphological, lexicographic, psychological, and distributional factors of word vector n"
C18-1228,D14-1012,0,0.025459,"n some of them). Our approach enables multi-faceted evaluation, parameter search, and generally – a more principled, hypothesis-driven approach to development of distributional semantic representations. 1 Introduction Dense lexical embeddings are the most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a g"
C18-1228,S10-1006,0,0.135289,"Missing"
C18-1228,J15-4004,0,0.81292,"nal Conference on Computational Linguistics, pages 2690–2703 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 Related Work Perhaps the most popular kind of intrinsic evaluation of word embeddings are the semantic relatedness tests (Finkelstein et al., 2002; Bruni et al., 2014; Luong et al., 2013; Radinsky et al., 2011). They rely on the idea that the distance between word vectors should correlate with human judgements of how related the two words are (e.g., cat should be closer to tiger than to hammer). A more sophisticated version of this task is the semantic similarity (Agirre et al., 2009; Hill et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be closer to tiger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al."
C18-1228,W14-1503,0,0.0245902,"al semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and"
C18-1228,D14-1181,0,0.00447323,"s in the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The model we use is similar to the model by Zeng et al. (2014): a CNN equipped with word and distance embeddings. Next, we have 3 tasks relying on how the word embeddings encode semantic information, and to what degree individual word vectors can be combined into an accurate sentence representation. The sentencelevel sentiment polarity classification (Sentiment (sent.)) task is tested with the MR dataset of short movie reviews (Pang and Lee, 2005). Binary classification is performed by a simplified version of the model proposed by Kim (2014). We also add the document-level polarity classification (Sentiment (text)) with the Stanford IMDB movie review dataset (Maas et al., 2011). Polarity is harder to estimate at the document than at the sentence level, because sentiment is more likely to be mixed. The task is performed with a single layer LSTM with 100 hidden units. The classification of subjectivity and objectivity (Subjectivity class.) is tested on Rotten Tomato user review snippets vs official movie plot summaries (Pang and Lee, 2004). We follow the method by Li et al. (2017), employing a simple logistic regression model for t"
C18-1228,W15-1814,0,0.0173906,"d be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple v"
C18-1228,W13-2608,0,0.0256279,"ormance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of lin"
C18-1228,Q14-1041,0,0.0186872,"most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wi"
C18-1228,E17-2063,0,0.0839134,"y are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence to the contrary (Chiu et al., 2016). We hypothesize that different extrinsic tasks may rely on different aspects of word representations. In that case, the only way to reliably predict what an embedding can do"
C18-1228,P14-2050,0,0.579431,"on et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence to the contrary (Chiu et al., 2016). We hypothesize that different extrinsic tasks may rely on different aspects of word representations. In that case, the only way to"
C18-1228,W14-1618,0,0.536785,"on et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence to the contrary (Chiu et al., 2016). We hypothesize that different extrinsic tasks may rely on different aspects of word representations. In that case, the only way to"
C18-1228,Q15-1016,0,0.0561749,"ions in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence t"
C18-1228,D17-1257,1,0.814048,"t al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2017; Lapesa and Evert, 2017). This amounts to an exponential explosion of options in the quest for the best model for a given task. Ideally, there would be a single intrinsic metric for identifying “good” embeddings – and there are many proposals for such a metric (including word relatedness and analogies). However, none of them have been shown to predict performance on a wide range of tasks, and there is evidence to the contrary (Chiu et al., 2016). We hypothesize that different extrinsic tasks may rely on different aspects of word representations. In that case, the only way to reliably predict"
C18-1228,N15-1142,0,0.0246693,"Missing"
C18-1228,W16-2503,0,0.0442887,"hiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and anal"
C18-1228,W13-3512,0,0.671065,"mbination of the English WordNet, Wiktionary, and BabelNet, and is potentially extensible to many languages. This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. 1 http://ldtoolkit.space Licence details: http: 2690 Proceedings of the 27th International Conference on Computational Linguistics, pages 2690–2703 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 Related Work Perhaps the most popular kind of intrinsic evaluation of word embeddings are the semantic relatedness tests (Finkelstein et al., 2002; Bruni et al., 2014; Luong et al., 2013; Radinsky et al., 2011). They rely on the idea that the distance between word vectors should correlate with human judgements of how related the two words are (e.g., cat should be closer to tiger than to hammer). A more sophisticated version of this task is the semantic similarity (Agirre et al., 2009; Hill et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be c"
C18-1228,P11-1015,0,0.0775661,"N equipped with word and distance embeddings. Next, we have 3 tasks relying on how the word embeddings encode semantic information, and to what degree individual word vectors can be combined into an accurate sentence representation. The sentencelevel sentiment polarity classification (Sentiment (sent.)) task is tested with the MR dataset of short movie reviews (Pang and Lee, 2005). Binary classification is performed by a simplified version of the model proposed by Kim (2014). We also add the document-level polarity classification (Sentiment (text)) with the Stanford IMDB movie review dataset (Maas et al., 2011). Polarity is harder to estimate at the document than at the sentence level, because sentiment is more likely to be mixed. The task is performed with a single layer LSTM with 100 hidden units. The classification of subjectivity and objectivity (Subjectivity class.) is tested on Rotten Tomato user review snippets vs official movie plot summaries (Pang and Lee, 2004). We follow the method by Li et al. (2017), employing a simple logistic regression model for the binary classification task. The input sentences are represented as a sum of their constituent word vectors. Finally, the natural languag"
C18-1228,P14-5010,0,0.0027581,"e corpus is syntactically parsed, has+nsubj at/+2 and only the words that are connected with the target word by some dependency relation are Table 2: Bound and unbound linear/dependency taken into account. Li et al. (2017) extended this contexts for the word program in the sentence “Evidea into the “unbound” DEPS context, where ery non-trivial program has at least one bug”. the labels of syntactic roles are ignored. Adapted from (Li et al., 2017). All embeddings were trained on English Wikipedia (August 2013 dump), with a minimum frequency of 100. After dependency parsing by Stanford CoreNLP (Manning et al., 2014), the corpus was lowercased. Negative sampling was set to 5 for SG and 2 for CBOW, no “dirty” sub-sampling. Distribution smoothing was set to 0.75. SG was trained for 2 epochs, CBOW - for 5, and GloVe - for 30. 4.2 Vocabulary Filtering and Sampling Fair evaluation must take into account the amount of information that was available during training. It is possible to run LDT on any embeddings, but it yields the most information when the source corpus is available, and it is possible to estimate raw frequencies and cooccurence counts. The source Wikipedia dump from which the embeddings were produ"
C18-1228,K16-1006,0,0.0197475,"ies to reach any definitive conclusions about the relative (de)merits of different models. At the moment, most work on word embeddings still report experiments with less than ten models, each trained just once. Important directions for future research also include going beyond simple word-level word embeddings. Some of the questions to investigate include the balance between semantic and morphological information in subword-level models (Bojanowski et al., 2017) and their ensembles with word-level models (Yang et al., 2017). It would also be interesting to expand LD to sense-aware embeddings (Melamud et al., 2016), particularly for contextualized representations (Peters et al., 2018). 7 Conclusion We presented LD, a methodology for quantitative/qualitative intrinsic evaluation of word embeddings implemented in an open-source Python library. Moving away from unrealistic single-number evaluations, LD identifies precisely what kinds of information a given word embedding encodes in its vector neighborhoods. Unlike traditional intrinsic tasks, LD can also be used to explain the correlation between performance on different tasks, or the lack thereof. The effectiveness of LD was shown in a large-scale experim"
C18-1228,N13-1090,0,0.486335,"ghbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally – a more principled, hypothesis-driven approach to development of distributional semantic representations. 1 Introduction Dense lexical embeddings are the most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lap"
C18-1228,W16-2504,0,0.114579,". This could be due to its focus on a particular type of semantic relations (synonymy, co-hyponymy), which turned out to be relevant for the labeling tasks. Our solution is based on “linguistic diagnostic” tests, achieved by large-scale automatic annotation of linguistic, psychological and distributional relations between words vectors and their neighbors. The resulting data can then be used to find what features are useful for what extrinsic tasks. This work is inspired by the BLESS categorization dataset (Baroni and Lenci, 2011) and by evaluation via a set of representative extrinsic tasks (Nayak et al., 2016). LD analysis starts with sampling the corpus vocabulary, as will be described in Section 4.2. For each word, top n neighbor vectors are extracted from each embedding. Each neighbor undergoes spelling normalization and is paired with the source word for analysis of possible morphological, semantic, distributional and psychological relations between them, as shown in Figure 1. The annotated data is analyzed to produce direct or statistically derived measures of the degree to which a given embedding is characterized by a given factor (e.g. how many synonyms or morphologically related words are n"
C18-1228,J07-2002,0,0.182959,"Missing"
C18-1228,P04-1035,0,0.00975844,"ng and Lee, 2005). Binary classification is performed by a simplified version of the model proposed by Kim (2014). We also add the document-level polarity classification (Sentiment (text)) with the Stanford IMDB movie review dataset (Maas et al., 2011). Polarity is harder to estimate at the document than at the sentence level, because sentiment is more likely to be mixed. The task is performed with a single layer LSTM with 100 hidden units. The classification of subjectivity and objectivity (Subjectivity class.) is tested on Rotten Tomato user review snippets vs official movie plot summaries (Pang and Lee, 2004). We follow the method by Li et al. (2017), employing a simple logistic regression model for the binary classification task. The input sentences are represented as a sum of their constituent word vectors. Finally, the natural language inference task is represented with the SNLI dataset (Bowman et al., 2015). Similarly to the original proposal, we use two separate LSTMs to get a representation of the premise and the hypothesis using the last hidden state. The two hidden representations are merged and fed into a 50-unit dense layer, over which 3-class classification with softmax is performed. 7"
C18-1228,P05-1015,0,0.332579,"der the task of multi-way classification of semantic relations (Relation class.) between pairs of nominals in the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The model we use is similar to the model by Zeng et al. (2014): a CNN equipped with word and distance embeddings. Next, we have 3 tasks relying on how the word embeddings encode semantic information, and to what degree individual word vectors can be combined into an accurate sentence representation. The sentencelevel sentiment polarity classification (Sentiment (sent.)) task is tested with the MR dataset of short movie reviews (Pang and Lee, 2005). Binary classification is performed by a simplified version of the model proposed by Kim (2014). We also add the document-level polarity classification (Sentiment (text)) with the Stanford IMDB movie review dataset (Maas et al., 2011). Polarity is harder to estimate at the document than at the sentence level, because sentiment is more likely to be mixed. The task is performed with a single layer LSTM with 100 hidden units. The classification of subjectivity and objectivity (Subjectivity class.) is tested on Rotten Tomato user review snippets vs official movie plot summaries (Pang and Lee, 200"
C18-1228,D14-1162,0,0.095544,"such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally – a more principled, hypothesis-driven approach to development of distributional semantic representations. 1 Introduction Dense lexical embeddings are the most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldbe"
C18-1228,N18-1202,0,0.0378705,"different models. At the moment, most work on word embeddings still report experiments with less than ten models, each trained just once. Important directions for future research also include going beyond simple word-level word embeddings. Some of the questions to investigate include the balance between semantic and morphological information in subword-level models (Bojanowski et al., 2017) and their ensembles with word-level models (Yang et al., 2017). It would also be interesting to expand LD to sense-aware embeddings (Melamud et al., 2016), particularly for contextualized representations (Peters et al., 2018). 7 Conclusion We presented LD, a methodology for quantitative/qualitative intrinsic evaluation of word embeddings implemented in an open-source Python library. Moving away from unrealistic single-number evaluations, LD identifies precisely what kinds of information a given word embedding encodes in its vector neighborhoods. Unlike traditional intrinsic tasks, LD can also be used to explain the correlation between performance on different tasks, or the lack thereof. The effectiveness of LD was shown in a large-scale experiment with 60 GloVe and Word2Vec models on 14 intrinsic and extrinsic tas"
C18-1228,S17-1017,1,0.925101,"ce. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relations in the most regular way: simple vector offset should be sufficient to capture semantic shifts such as F rance : P aris to Japan : T okyo. However, this view of linguistic relations (and analogical reasoning) is oversimplified, and performance on word analogies has also been shown to depend on cosine similarity between source word vectors (Rogers et al., 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs an"
C18-1228,P15-4018,0,0.0160663,"they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally – a more principled, hypothesis-driven approach to development of distributional semantic representations. 1 Introduction Dense lexical embeddings are the most common distributional semantic representations in both industrial and academic natural language processing (NLP) systems (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Ruppert et al., 2015). They are used in task-specific neural network models, solving such tasks as named entity recognition (Guo et al., 2014), semantic role labeling (Chen et al., 2014), syntactic parsing (Chen and Manning, 2014), and more. Each year dozens of new models are proposed, each of them with multiple hyper-parameters that may dramatically influence performance (Lapesa and Evert, 2014; Kiela and Clark, 2014; Levy et al., 2015; Lai et al., 2016; Melis et al., 2017). Equally important are the source corpus, its domain, and the type of context (Pad´o and Lapata, 2007; Levy and Goldberg, 2014a; Li et al., 2"
C18-1228,W16-5309,0,0.0392411,"Missing"
C18-1228,D15-1036,0,0.0583344,"l et al., 2015), which basically restricts relatedness to synonymy and co-hyponymy. This evaluation paradigm has come under fire for methodological reasons (Faruqui et al., 2016; Batchkarov et al., 2016), in particular, due to the unreliability of the “middle” judgments: while cat should be closer to tiger than to hammer, it is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the uti"
C18-1228,W16-2521,0,0.0585503,"is not clear whether it should be closer to lion or to tiger (Gladkova and Drozd, 2016). Furthermore, only 1 out of 10 datasets was a good predictor of performance on sequence labeling tasks (Chiu et al., 2016). The proposal for evaluation via coherence of semantic space (Schnabel et al., 2015) inherits all the problems with relatedness (Gladkova and Drozd, 2016). There are multiple proposals for “subconscious intrinsic evaluation” (Bakarov, 2018) based on correlations with psycholinguistic data such as N400 effect (Van Petten, 2014; Ettinger et al., 2016), fMRI scans (Devereux et al., 2010; Søgaard, 2016), eye-tracking (Klerke et al., 2015; Søgaard, 2016), and semantic priming data (Lund et al., 1995; Lund and Burgess, 1996; Jones et al., 2006; Lapesa and Evert, 2013; Ettinger and Linzen, 2016; Auguste et al., 2017). However, there are no large-scale studies that would show the utility of these methods in predicting downstream task performance. It is also possible that any psychological measure would share the subjectivity problem of relatedness judgments. The idea behind the word analogy task (Mikolov et al., 2013b) is that the “best” word embedding is the one that encodes linguistic relation"
C18-1228,W03-0419,0,0.648026,"Missing"
C18-1228,D15-1243,0,0.0217657,"ity between source word vectors (Rogers et al., 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs and Kornai, 2016). Crucially, all these approaches make the same core assumption: that there is one feature of a representation that would make it the “best” (the highest correlation with human judgements, the most regular vector offsets, the closest approximation of a linguistic resource, etc.) However, language is a multifaceted phenomenon, and different NLP tasks may rely on its different aspects – which would doom any one-metric-to-rule-them-all approach. This is the starting point for our solution. 3 LDT: the methodology Consider two published modifications of"
C18-1228,W16-2520,0,0.0455061,"vectors (Rogers et al., 2017; Linzen, 2016; Levy and Goldberg, 2014b). Furthermore, the original vector offset method is underestimating the amount of semantic information captured by the embedding (Drozd et al., 2016). Last but not the least, analogies also fail to yield results consistent with downstream task performance (Ghannay et al., 2016). One more line of research could be called linguistically motivated evaluation. The idea is that a “good” embedding would be somehow similar to a representation that could be constructed from a goldstandard linguistic resource (Tsvetkov et al., 2015; Tsvetkov et al., 2016; Acs and Kornai, 2016). Crucially, all these approaches make the same core assumption: that there is one feature of a representation that would make it the “best” (the highest correlation with human judgements, the most regular vector offsets, the closest approximation of a linguistic resource, etc.) However, language is a multifaceted phenomenon, and different NLP tasks may rely on its different aspects – which would doom any one-metric-to-rule-them-all approach. This is the starting point for our solution. 3 LDT: the methodology Consider two published modifications of the word2vec Rank Deps"
C18-1228,N18-1190,0,0.0344241,"r nouns. For example, the co-hyponymy relation between names of composers would be covered with the current implementation, but giving a higher score to a model that places violin closer to Bach than to Beatles would require evaluating frame-semantic, or at least topical relations, going beyond the traditional dictionaries. A major caveat is the instability of word embeddings: different runs of the same model may yield word vector neighborhoods with significantly different lexical content. Some models are more stable than others (in particular, GloVe was found to be more stable than word2vec) Wendlandt et al. (2018), but most models published in the recent years do not explore their stability. This fact does not disqualify evaluations based on vector neighborhoods (not only LD, but also the traditional relatedness and analogy tasks), but it does highlight the absolute necessity of large-scale studies to reach any definitive conclusions about the relative (de)merits of different models. At the moment, most work on word embeddings still report experiments with less than ten models, each trained just once. Important directions for future research also include going beyond simple word-level word embeddings."
C18-1228,C14-1220,0,0.0564139,", 2003), following the method by Li et al. (2017). The model is a softmax classifier on the window-based concatenation of word embeddings of every training example (window size 3, 20 training epochs). Semantic information at the word level is targeted by one more CoNLL 2003 shared task: named entity recognition (NER), evaluated in the same way as POS-tagging and chunking. We also consider the task of multi-way classification of semantic relations (Relation class.) between pairs of nominals in the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The model we use is similar to the model by Zeng et al. (2014): a CNN equipped with word and distance embeddings. Next, we have 3 tasks relying on how the word embeddings encode semantic information, and to what degree individual word vectors can be combined into an accurate sentence representation. The sentencelevel sentiment polarity classification (Sentiment (sent.)) task is tested with the MR dataset of short movie reviews (Pang and Lee, 2005). Binary classification is performed by a simplified version of the model proposed by Kim (2014). We also add the document-level polarity classification (Sentiment (text)) with the Stanford IMDB movie review dat"
D15-1221,D13-1011,0,0.198081,"Missing"
D15-1221,P11-2014,0,0.0267003,"This allows us to analyze non-rhyming features from (Hirjee and Brown, 2010a), such as number of syllables per line and number of lines per verse. We also desire that, by using the “&lt;endLine&gt;” token, the system has a better chance of understanding rhyme schemes used by an artist. For example, the LSTM can capture the pattern of “came &lt;endLine&gt;” followed shortly by “name &lt;endLine&gt;” to understand that “came” and “name” are a rhyming pair. To do this effectively, the system would need sufficient training data where rhyming pairs occur frequently enough to actually dictate a pattern, similar to (Reddy and Knight, 2011; Addanki and Wu, 2013). 4 Experimental Design 4.1 Dataset We collected songs from 14 artists from the site The Original Hip-Hop (Rap) Lyrics Archive OHHLA.com - Hip-Hop Since 19922 . In the present lyrics generation experiments, we used the lyrics from the rapper Fabolous. For training, we used 219 verses with at least 175 words in each verse. We selected Fabolous because his lyrics produced the highest accuracy in the artist recognition experiments in (Hirjee and Brown, 2010a). We conjecture that because of this, he had the most consistent style, making him a good choice for initial experime"
D15-1221,addanki-wu-2014-evaluating,0,\N,Missing
D17-1092,S15-2135,0,0.120938,"rens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al., 2014; Mirza, 2016). The best methods used by TemporalIE systems to date tend to rely on highly engineered taskspecific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014). For example, in a recent QA-TempEval shared task, the participants routinely used a series of classifiers (such as support vector machine (SVM) or hidden Markov chain SVM) or hybrid methods combining hand crafted rules and SVM, as was used by the top system in that challenge (Mirza and Minard, 2015). While our method also relies on decomposing the temporal relation extraction task into subtasks, we use essentially the same simple LSTM-based architecture for different components, that consume a highly simplified representation of the input. Although there has not been much work applying deep learning techniques to TemporalIE, some relevant work has been done on a similar (but typically more local) task of relation extraction. Convolutional neural networks (Zeng et al., 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and"
D17-1092,agerri-etal-2014-ixa,0,0.0272096,"y them. Feature is main verb is predicate is verb is noun Dataset Explanation whether the token is the main verb of a sentence whether the token is the predicate of a phrase whether the token is a verb whether the token is a noun Table 1: Token features for event extraction We used QA-TempEval (SemEval 2015 Task 5)2 data and evaluation methods in our experiments. The training set contains 276 annotated TimeML files, mostly news articles from major agencies or Wikinews from late 1990s to early 2000s. This We perform tokenization, part-of-speech tagging, and dependency parsing using NewsReader (Agerri et al., 2014). Every token is represented with a set of features derived from preprocessing. Syntactic dependencies are not used for event extraction, but are used later in the pipeline for 2 http://alt.qcri.org/semeval2015/ task5/ 888 Plain Documents HeidelTime Timex Annotator NewsReader Pipeline TEA Timex TLINK Model TEA LSTM Event Annotator TEA LSTM TLINK Models WithinSentence Doublecheck CrossSentence Doublecheck TimeML Annotated Document Pruning System DCT TLINK Figure 1: System overview for our temporal extraction annotator (TEA) system TLINK classification. The features used to identify events are l"
D17-1092,S15-2136,0,0.0323717,"in QATempEval. In TimeBank-Dense, all entity pairs in the same sentence or in consecutive sentences are labeled. If there is no information about the relation between two entities, it is labeled as “vague”. We follow the experimental setup in (Chambers et al., 2014), which splits the corpus into training/validation/test sets of 22, 5, and 9 documents, respectively. Related Work A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al., 2014; Mirza, 2016). The best methods used by TemporalIE systems to date tend to rely on highly engineered taskspecific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014). For example, in a recent QA-TempEval shared task, the participants routinely used a series of classifiers (such as support vector machine (SVM) or hidden Markov chain SVM) or hybrid methods combining hand crafted rules and SVM, as was used by the top system in that challe"
D17-1092,Q14-1022,0,0.620974,"expressions). These components are fairly uniform in their architecture, relying on dependency relations recovered with a very small number of mature, widely available processing tools, and require minimal engineering otherwise. To our knowledge, this is the first attempt to apply such simplified techniques to the TemporalIE task, and we demonstrate this streamlined architecture is able to outperform state-of-the-art results on a temporal QA task with a large margin. In order to demonstrate generalizability of our proposed architecture, we also evaluate it intrinsically using TimeBank-Dense1 (Chambers et al., 2014). TimeBank-Dense annotation aims to approximate a complete temporal relation graph by including all intra-sentential relations, all relations between adjacent sentences, and all relations with document creation time. Although our system In this paper, we propose to use a set of simple, uniform in architecture LSTMbased models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is implemented to extract intra-sentence, crosssentence, and document creation time relations. A “double-checking” technique rev"
D17-1092,P82-1020,0,0.791738,"Missing"
D17-1092,S15-2134,0,0.383651,"processing tasks that require deep language understanding, such as answering questions about the timeline of events or automatically producing text summaries. This work presents intermediate results of an effort to build a temporal reasoning framework with contemporary deep learning techniques. Until recently, there has been remarkably few attempts to evaluate temporal information extraction (TemporalIE) methods in context of downstream applications that require reasoning over the temporal representation. One recent effort to conduct such evaluation was SemEval2015 Task 5, a.k.a. QA-TempEval (Llorens et al., 2015a), which used question answering (QA) as the target application. QA-TempEval evaluated systems 1 https://www.usna.edu/Users/cs/ nchamber/caevo/#corpus 887 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 887–896 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics was not optimized for such a paradigm, and this data is quite different in terms of both the annotation scheme and the evaluation method, we obtain state-of-the-art results on this corpus as well. 2 data contains annotations for events, temporal expr"
D17-1092,S07-1014,0,0.026069,"use TimeBank-Dense dataset, which contains a subset of the documents in QATempEval. In TimeBank-Dense, all entity pairs in the same sentence or in consecutive sentences are labeled. If there is no information about the relation between two entities, it is labeled as “vague”. We follow the experimental setup in (Chambers et al., 2014), which splits the corpus into training/validation/test sets of 22, 5, and 9 documents, respectively. Related Work A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al., 2014; Mirza, 2016). The best methods used by TemporalIE systems to date tend to rely on highly engineered taskspecific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014). For example, in a recent QA-TempEval shared task, the participants routinely used a series of classifiers (such as support vector machine (SVM) or hidden Markov chain SVM) or hybrid methods combining hand"
D17-1092,N16-1065,0,0.0233972,"Missing"
D17-1092,D15-1206,0,0.285941,"ethod also relies on decomposing the temporal relation extraction task into subtasks, we use essentially the same simple LSTM-based architecture for different components, that consume a highly simplified representation of the input. Although there has not been much work applying deep learning techniques to TemporalIE, some relevant work has been done on a similar (but typically more local) task of relation extraction. Convolutional neural networks (Zeng et al., 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016). We take inspiration from some of this work, including specifically the approach proposed by Xu et al. (2015) which uses syntactic dependencies. 3 4 T IMEX and Event Extraction The first task in our TemporalIE pipeline (TEA) is to identify time expressions (TIMEXes) and events in text. We utilized the HeidelTime package (Str¨otgen and Gertz, 2013) to identify TIMEXes. We trained a neural network model to identify event mentions. Contrary to common practice in TemporalIE, our models do not rely on event attributes, and thus we did not attempt to identify them. Feature is main"
D17-1092,C14-1220,0,0.00586287,"SVM) or hybrid methods combining hand crafted rules and SVM, as was used by the top system in that challenge (Mirza and Minard, 2015). While our method also relies on decomposing the temporal relation extraction task into subtasks, we use essentially the same simple LSTM-based architecture for different components, that consume a highly simplified representation of the input. Although there has not been much work applying deep learning techniques to TemporalIE, some relevant work has been done on a similar (but typically more local) task of relation extraction. Convolutional neural networks (Zeng et al., 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016). We take inspiration from some of this work, including specifically the approach proposed by Xu et al. (2015) which uses syntactic dependencies. 3 4 T IMEX and Event Extraction The first task in our TemporalIE pipeline (TEA) is to identify time expressions (TIMEXes) and events in text. We utilized the HeidelTime package (Str¨otgen and Gertz, 2013) to identify TIMEXes. We trained a neural network model to identify event mentions. Cont"
D17-1092,C16-1007,0,0.0335739,"classes and (2) an efficient pruning algorithm to resolve TLINK conflicts. In a QA-based evaluation, our proposed method outperforms state-ofthe-art methods by a large margin. We also obtain state-of-the art results in an intrinsic evaluation on a very different TimeBank-Dense dataset, proving generalizability of the proposed model. TimeBank-Dense Experiments We trained and evaluated the same system on TimeBank-Dense to see how it performs on a similar task with a different set of labels and another method of evaluation. In this experiment, we used the event and TIMEX tags from test data, as Mirza and Tonelli (2016). Since all the NO - LINK (vague) relations are labeled, downsampling was not necessary. We did use double-checking in the final conflict resolution, but without giving positive cases the veto power over NO - LINK. Because NO - LINK relations dominate, especially for cross-sentence pairs, we set class weights to be inversely proportional to the class frequencies during training. We also reduced input batch size to counteract class imbalance. We ran two sets of experiments. One used the uniform configurations for all the neural network models, similar to our experiments with QATempEval. The oth"
D17-1143,W14-2107,0,0.0411363,"a sequence of tokens that represents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; (2) Given an AC, determine the type of AC (claim, premise, etc.); (3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; (4) Given two linked ACs, determine whether the link is a supporting or attacking relation. This can be labeled as a ‘micro’ approach to argument mining (Stab and Gurevych, 2016). In contrast, there have been a number of efforts to identify argument structure at a higher ˇ level (Boltuzic and Snajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012), as well as slightly re-ordering the pipeline with respect to AC types (Rinott et al., 2015)). There are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016). Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ compo"
D17-1143,D16-1238,0,0.0204218,"proposed (Luong et al., 2015), though none of the models used a PN for prediction. In the field of discourse parsing, the work of Li et al. (2016) is the only work, to our knowledge, that incorporates attention into the network architecture. However, the attention is only used in the process of creating representations of the text itself. Attention is not used to predict the overall discourse structure. In fact, the model still relies on a binary classifier to determine if textual components should have a link. Arguably the most similar approach to ours is in the field of dependency parsing (Cheng et al., 2016). The authors propose a model that performs ‘queries’ between word representations in order to determine a distribution over potential headwords. 3 Proposed Approach In this section, we describe our approach to using a sequence-to-sequence model with attention for argument mining, specifically, identifying AC types and extracting the links between them. We begin by giving a brief overview of these models. 3.1 Pointer Network A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding input"
D17-1143,J87-1002,0,0.779882,"abeled as a ‘micro’ approach to argument mining (Stab and Gurevych, 2016). In contrast, there have been a number of efforts to identify argument structure at a higher ˇ level (Boltuzic and Snajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012), as well as slightly re-ordering the pipeline with respect to AC types (Rinott et al., 2015)). There are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016). Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ component that has no outgoing link (the top of the tree). Depending on the corpus (see Section 4), an argument structure can be either a single tree or a forest, consisting of multiple trees. Figure 1 shows an example that we will use throughout the paper to concretely explain how our approach works. First, the left side of the figure presents the raw text of a paragraph in a persuasive essay (Stab and Gurevyc"
D17-1143,W14-2106,0,0.108155,"epresents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; (2) Given an AC, determine the type of AC (claim, premise, etc.); (3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; (4) Given two linked ACs, determine whether the link is a supporting or attacking relation. This can be labeled as a ‘micro’ approach to argument mining (Stab and Gurevych, 2016). In contrast, there have been a number of efforts to identify argument structure at a higher ˇ level (Boltuzic and Snajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012), as well as slightly re-ordering the pipeline with respect to AC types (Rinott et al., 2015)). There are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016). Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ component that has no out"
D17-1143,J81-4005,0,0.746471,"Missing"
D17-1143,C16-1260,0,0.0201587,"ne the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015). The first neural network-based model for argumentation mining was proposed by Laha and Raykar (2016), who use two recurrent networks in end-to-end fashion to classify AC types. Various authors have also proposed to jointly model link extraction with other subtasks from the argument mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing and Ng, 2016; Stab and Gurevych, 2016) or directly feeding previous subtask predictions into a tree-based parser. The former joint approaches are evaluated on an annotated corpus of persuasive essays (Stab and Gurevych, 2014a, 2016), and the latter on a corpus of microtexts (Peldszus, 2014). The ILP framework is effective in enfor"
D17-1143,W14-2111,0,0.0454567,"and Peldszus (2014), and compare our results with the results of the aformentioned authors. Our results show that (1) joint modeling is imperative for competitive performance on the link extraction task, (2) the presence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints. 1365 2 Related Work Palau and Moens (2009) is an early work in argument mining, using a hand-crafted Context-Free Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015). The first neural network-based model for argumentation mining was proposed by Laha and Raykar (2016), who use two recurrent networks in end-to-end fashi"
D17-1143,D16-1035,0,0.0205786,"/graph structures in a linear manner. Vinyals et al. (2015c) use a sequence-tosequence model for the task of syntactic parsing. Bowman et al. (2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014). Standard recurrent neural networks can take in complete sentence sequences and perform competitively with a recursive neural network. Multitask learning for sequence-to-sequence has also been proposed (Luong et al., 2015), though none of the models used a PN for prediction. In the field of discourse parsing, the work of Li et al. (2016) is the only work, to our knowledge, that incorporates attention into the network architecture. However, the attention is only used in the process of creating representations of the text itself. Attention is not used to predict the overall discourse structure. In fact, the model still relies on a binary classifier to determine if textual components should have a link. Arguably the most similar approach to ours is in the field of dependency parsing (Cheng et al., 2016). The authors propose a model that performs ‘queries’ between word representations in order to determine a distribution over pot"
D17-1143,P16-1107,0,0.229385,"e to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both tasks is crucial for high performance. 1 Introduction An important goal in argument mining is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016). One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims (though some corpora have further AC types, such as major claim (Stab and Gurevych, 2016, 2014b)). The task of processing argument structure encapsulates four distinct subtasks (our work focuses on subtasks 2 and 3): (1) Given a sequence of tokens that represents an entire argumentative text, determine the token subsequences that"
D17-1143,W14-2112,0,0.490491,"ACs and determines their type. Our joint model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). While PNs were originally proposed to allow a variable length decoding sequence, our model differs in that it decodes for the same number of timesteps as there are inputs. This is a key insight that allows for a sequence-to-sequence model to be used for structural prediction. Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). Lastly, in respect to the broad task of parsing, our model is flexible because it can easily handle nonprojective, multi-root dependencies. We evaluate our models on the corpora of Stab and Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors. Our results show that (1) joint modeling is imperative for competitive performance on the link extraction task, (2) the presence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus"
D17-1143,D15-1110,0,0.455195,"develop a joint model that extends this architecture to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both tasks is crucial for high performance. 1 Introduction An important goal in argument mining is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016). One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims (though some corpora have further AC types, such as major claim (Stab and Gurevych, 2016, 2014b)). The task of processing argument structure encapsulates four distinct subtasks (our work focuses on subtasks 2 and 3): (1) Given a sequence of tokens that represents an entire argumen"
D17-1143,D14-1162,0,0.0805257,"directional LSTM Encoder LSTM D2 FC3 FC1 FC1 FC1 FC1 Component 1 Component 2 Component 3 Component 4 LSTM D3 FC3 LSTM D4 FC3 Figure 3: Architecture of the joint model applied to the example in Figure 1. Note that D1 points to itself to denote that it has not outgoing link and is therefore the head of a tree. a sequence of tokens, similar to the QuestionAnswering dataset from Weston et al. (2015). We follow the work of Stab and Gurevych (2016) and focus on three different types of features to represent our ACs: (1) Bag-of-Words of the AC; (2) Embedding representation based on GloVe embeddings (Pennington et al., 2014), which uses average, max, and min pooling across the token embeddings; (3) Structural features: Whether or not the AC is the first AC in a paragraph, and whether the AC is in an opening, body, or closing paragraph. See Section 6 for an ablation study of the proposed features. 3.4 Joint Neural Model Up to this point, we focused on the task of extracting links between ACs. However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing and Ng, 2016; Stab and Gurevych, 2014b; P"
D17-1143,S16-1074,0,0.0660828,"Missing"
D17-1143,N16-1164,0,0.572002,"arsing tasks. We then develop a joint model that extends this architecture to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both tasks is crucial for high performance. 1 Introduction An important goal in argument mining is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016). One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims (though some corpora have further AC types, such as major claim (Stab and Gurevych, 2016, 2014b)). The task of processing argument structure encapsulates four distinct subtasks (our work focuses on subtasks 2 and 3): (1) Given a sequence of tokens that re"
D17-1143,D15-1050,0,0.0454326,"en an AC, determine the type of AC (claim, premise, etc.); (3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; (4) Given two linked ACs, determine whether the link is a supporting or attacking relation. This can be labeled as a ‘micro’ approach to argument mining (Stab and Gurevych, 2016). In contrast, there have been a number of efforts to identify argument structure at a higher ˇ level (Boltuzic and Snajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012), as well as slightly re-ordering the pipeline with respect to AC types (Rinott et al., 2015)). There are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016). Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ component that has no outgoing link (the top of the tree). Depending on the corpus (see Section 4), an argument structure can be either a single"
D17-1143,C14-1142,0,0.384167,"ence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints. 1365 2 Related Work Palau and Moens (2009) is an early work in argument mining, using a hand-crafted Context-Free Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015). The first neural network-based model for argumentation mining was proposed by Laha and Raykar (2016), who use two recurrent networks in end-to-end fashion to classify AC types. Various authors have also proposed to jointly model link extraction with other subtasks from the argument mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing an"
D17-1143,D14-1006,0,0.282792,"ence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints. 1365 2 Related Work Palau and Moens (2009) is an early work in argument mining, using a hand-crafted Context-Free Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015). The first neural network-based model for argumentation mining was proposed by Laha and Raykar (2016), who use two recurrent networks in end-to-end fashion to classify AC types. Various authors have also proposed to jointly model link extraction with other subtasks from the argument mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing an"
D17-1261,D14-1124,0,0.0720789,"ow that our proposed regularization technique is imperative for the RNN-based model to perform competitively with the models previously proposed by Zhang et al.. The attention mechanism also contributes to the best performing system. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long se"
D17-1261,W12-1631,0,0.0288353,"ng its prediction. Our model achieves state-of-the-art accuracy on a dataset of debate transcripts annotated with audience favorability of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn. 1 Introduction Conversational agents are a well-researched area of natural language generation (Pilato et al., 2007; Bigham et al., 2008; Augello et al., 2008; Agostaro et al., 2005; Bessho et al., 2012). Elsewhere in the field of natural language generation, there is work that seeks to generate persuasive text (Carenini and Moore, 2006; Reiter et al., 2003; Rosenfeld and Kraus, 2016), which is a logical first step towards creating an automated debate agent. One major deficiency of existing work in this area is its assessment of how convincing (or compelling) a piece of text is; the approaches use theory-driven models of persuasion, rather than being empirically motivated. Furthermore, none of these works provide a model that can optimize persuasiveness at an arbitrary point in a conversation"
D17-1261,D16-1129,0,0.226009,"nd Lebanon, 2007). Niculae and Danescu-Niculescu-Mizil (2016) use several novel features that capture the flow of ideas in the data, as well as team dynamics. Ultimately, however, all these models apply manually derived, preprocessed features and use a basic classifier, like Random Forest or Logistic Regression. In contrast, an RNN model is able to learn which interactions and overall sequences of rhetoric are important for predictive power. There is much less work that approaches the problem of predicting persuasiveness of text. This is due primarily to the lack applicable datasets. However, Habernal and Gurevych (2016b) have recently presented a dataset where argument pairs are annotated for argument convincingness, as well as finer-grained annotations related to the effectiveness of arguments (Habernal and Gurevych, 2016a). The authors experimented with featurebased classifiers, as well as various RNN architectures to construct predictive models for the dataset. The most relevant work for this paper is of course Zhang et al. (2016). The authors use a set of features derived from the notion of idea flow in the debate. More specifically, they follow the method of Monroe et al. (2008) to identify talking poi"
D17-1261,P16-1150,0,0.0794271,"nd Lebanon, 2007). Niculae and Danescu-Niculescu-Mizil (2016) use several novel features that capture the flow of ideas in the data, as well as team dynamics. Ultimately, however, all these models apply manually derived, preprocessed features and use a basic classifier, like Random Forest or Logistic Regression. In contrast, an RNN model is able to learn which interactions and overall sequences of rhetoric are important for predictive power. There is much less work that approaches the problem of predicting persuasiveness of text. This is due primarily to the lack applicable datasets. However, Habernal and Gurevych (2016b) have recently presented a dataset where argument pairs are annotated for argument convincingness, as well as finer-grained annotations related to the effectiveness of arguments (Habernal and Gurevych, 2016a). The authors experimented with featurebased classifiers, as well as various RNN architectures to construct predictive models for the dataset. The most relevant work for this paper is of course Zhang et al. (2016). The authors use a set of features derived from the notion of idea flow in the debate. More specifically, they follow the method of Monroe et al. (2008) to identify talking poi"
D17-1261,N16-1070,0,0.0575414,"006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Random Field (Mao and Lebanon, 2007). Niculae and Danescu-Niculescu-Mizil (2016) use several novel features that capture the flow of ideas in the data, as well as team dynamics. Ultimately, however, all these models apply manually derived, preprocessed features and use a basic classifier, like Random Forest or Logistic Regression. In contrast, an RNN model is able to learn which interactions and overall sequences of rhetoric are important for predictive power. There is much less work that approaches the problem of predicting persuasiveness of text. This is due primarily to the lack applicable datasets. However, Habernal and Gurevych (2016b) have recently presented a datas"
D17-1261,D14-1162,0,0.0803276,"e talking points, we chose the 10 highest ranked talking points from each side and include them in the input representation. Moreover, we believe we can use a simpler talking point metric than that proposed by Monroe et al. (2008) (and used by Zhang et al.) because the recurrent nature of the model will naturally capture the interaction, coverage, and ignorance of the two team’s (and overall) talking points. Aside from talking point-based features, we include the following linguistic features: 1) bag-ofwords for tokens that have been used in at least 50 debates; 2) GloVe embeddings of tokens (Pennington et al., 2014). We use max pooling over all the tokens’ embeddings to create the embedding features. We also use the following nonlinguistic features: 1) whether the turn occurs during the opening, discussion, or conclusion phase of the debate; 2) whether the turn is from the ‘for’ or ‘against’ team, as well as moderator or other speakers, such as show host etc; 3) the initial audience poll is provided at each timestep. This is similar in spirit to Cho et al. (2014)’s decoder model that accesses the final encoder hidden state at each timestep. We acknowledge that it would be possible to model individual tur"
D17-1261,W15-4625,0,0.0899869,"ibutes to the best performing system. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Random Field (Mao and Lebanon, 2007). Niculae and Danescu-Niculescu-Miz"
D17-1261,W16-4318,0,0.0152793,"lassifiers, as well as various RNN architectures to construct predictive models for the dataset. The most relevant work for this paper is of course Zhang et al. (2016). The authors use a set of features derived from the notion of idea flow in the debate. More specifically, they follow the method of Monroe et al. (2008) to identify talking points used by the sides present in a debate. The authors then create features based on the coverage of talking points during the debate. Finally, a Logistic Regression model uses these features to predict which team wins the debate. We also note the work of Santos et al. (2016), which also makes 2466 predictions on a dataset derived from the IQ2 debates. In contrast, their work analyses speech signals, as opposed to textual data. 3 Predictive Model In this section we explain how we apply an RNN to the task of predicting debate winners. We start by addressing the fact that for IQ2 dataset, each timestep involves a text span, as opposed to single tokens, and explaining how we convert this text span into a vector representation for RNN input. Secondly, we explain our RNN model architecture, including our use of an attention mechanism to create a weighted sum over all h"
D17-1261,P13-1048,0,0.0228632,", using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Random Field (Mao and Lebanon, 2007). Niculae and Danescu-Niculescu-Mizil (2016) use several novel features that capture the flow of ideas in the data, as well as team dynamics. Ultimately, however, all these models apply manually derived, prepro"
D17-1261,W10-0214,0,0.0352758,"by Zhang et al.. The attention mechanism also contributes to the best performing system. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Random Field (Mao an"
D17-1261,P15-1012,0,0.053547,"s previously proposed by Zhang et al.. The attention mechanism also contributes to the best performing system. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Co"
D17-1261,W06-1639,0,0.065081,"mechanism also contributes to the best performing system. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn. 2 Related Work Previous work that focuses on conversational language seeks to predict such qualities as disagreements (Allen et al., 2014; Wang and Cardie, 2016), divergence (Niculae and DanescuNiculescu-Mizil, 2016), and participant stance (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Thomas et al., 2006; Rosenthal and McKeown, 2015). What is most relevant for our purposes are the methods these models use for dealing with conversational data. Allen et al. (2014) apply discourse parsing (Joty et al., 2013) and fragment quotation graph (Carenini et al., 2007) tools to detect disagreement in online discussion threads. Wang and Cardie (2016) believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Random Field (Mao and Lebanon, 2007). Nic"
D17-1261,N16-1017,0,0.220502,"g an automated debate agent. One major deficiency of existing work in this area is its assessment of how convincing (or compelling) a piece of text is; the approaches use theory-driven models of persuasion, rather than being empirically motivated. Furthermore, none of these works provide a model that can optimize persuasiveness at an arbitrary point in a conversation. One of the main reasons for a lack of empirically-driven persuasive generation systems is the absence of labeled data. In order to alleviate this problem (though not directly for the sake of producing an automated debate agent), Zhang et al. (2016) have introduced a dataset of debate transcripts from the “Intelligence Squared” (IQ2)1 debates. In these debates, two teams are present, arguing either for or against a given topic. For each debate, an audience poll is conducted both prior to and after the debate. Whichever team has the largest gain in audience support between the pre/post debate polls is the winner. This is a natural way to account for the fact that some sides of a debate may be harder to argue than others, and that audience members may be initially biased given a debate topic. Because of the sequential nature of debating, a"
D18-1525,D15-1075,0,0.0529619,"between 0.001 and 0.0001. We test our learned representations using the SentEval toolkit (Conneau et al., 2017). SentEval is an open-source Python library for evaluating sentence embeddings on a diverse set of language tasks. This toolkit provides a cluster of downstream tasks taken from various competitions such as SemEval as well as a set of probing tasks. In current paper, we focus on the paraphrase detection task using the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004), as well as the inference/entailment tasks using the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) and the SICK-Entailment dataset from SemEval-2014 (Marelli et al., 2014). We selected these tasks because they seem to be likely to benefit from capturing word-level semantic similarity. Table 1 shows the scores averaged over three (3) runs. 4 Discussion We find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see Table 1). The only exception is the weighted similarity loss function. Compared to the logarithm-based losses, this loss applies softer penalties when the groundtruth tokens are predicted to have lower p"
D18-1525,D18-2029,0,0.0166705,"y as good or even better from the point of view of model performance. Consider a situation when a decoder model generates a word by sampling from a softmax over the vocabularysized final layer to produce an output. Since crossentropy loss forces a model to generate the exact Related Work The encoder-decoder setting was first used in deep learning by Sutskever et al. (2014) and has been successfully adapted to a problem of representation learning since then. To date, numerous approaches based on the encoder-decoder idea have been suggested for unsupervised feature extraction from textual data. Cer et al. (2018) modify the Transformer architecture (Vaswani et al., 2017) originally suggested for machine translation to produce sentence embeddings that target transfer learning to other NLP tasks. Arora et al. (2016) claim that sentence representation as simple weighted averaging of word vectors beats more sophisticated recurrent network-based models. McCann et al. (2017) show that adding machine translation-learned vectors to models designed for other NLP tasks improves their performance. Nangia et al. (2017) in RepEval-2017 report that in-sentence attention and biLSTM-based models extract represen4875"
D18-1525,D17-1070,0,0.0227107,"dings. Our autoencoder model is implemented using the PyTorch deep learning framework (Paszke et al., 2017). In our architecture, both the encoder and the decoder are implemented as single layer LSTMs, each with the hidden size of 256 units. We divide our dataset into train/dev/test splits in 70/10/20 ratio. The resulting vocabulary size of the training dataset is 9.5K tokens. For our training, we use the Adam optimizer (Kingma and Ba, 2014) with the learning rate that varies depending on the tested loss between 0.001 and 0.0001. We test our learned representations using the SentEval toolkit (Conneau et al., 2017). SentEval is an open-source Python library for evaluating sentence embeddings on a diverse set of language tasks. This toolkit provides a cluster of downstream tasks taken from various competitions such as SemEval as well as a set of probing tasks. In current paper, we focus on the paraphrase detection task using the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004), as well as the inference/entailment tasks using the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) and the SICK-Entailment dataset from SemEval-2014 (Marelli et al., 2014). We selected thes"
D18-1525,C04-1051,0,0.101203,"r training, we use the Adam optimizer (Kingma and Ba, 2014) with the learning rate that varies depending on the tested loss between 0.001 and 0.0001. We test our learned representations using the SentEval toolkit (Conneau et al., 2017). SentEval is an open-source Python library for evaluating sentence embeddings on a diverse set of language tasks. This toolkit provides a cluster of downstream tasks taken from various competitions such as SemEval as well as a set of probing tasks. In current paper, we focus on the paraphrase detection task using the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004), as well as the inference/entailment tasks using the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) and the SICK-Entailment dataset from SemEval-2014 (Marelli et al., 2014). We selected these tasks because they seem to be likely to benefit from capturing word-level semantic similarity. Table 1 shows the scores averaged over three (3) runs. 4 Discussion We find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see Table 1). The only exception is the weighted similarity loss function. Compare"
D18-1525,marelli-etal-2014-sick,0,0.0326157,"SentEval toolkit (Conneau et al., 2017). SentEval is an open-source Python library for evaluating sentence embeddings on a diverse set of language tasks. This toolkit provides a cluster of downstream tasks taken from various competitions such as SemEval as well as a set of probing tasks. In current paper, we focus on the paraphrase detection task using the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004), as well as the inference/entailment tasks using the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) and the SICK-Entailment dataset from SemEval-2014 (Marelli et al., 2014). We selected these tasks because they seem to be likely to benefit from capturing word-level semantic similarity. Table 1 shows the scores averaged over three (3) runs. 4 Discussion We find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see Table 1). The only exception is the weighted similarity loss function. Compared to the logarithm-based losses, this loss applies softer penalties when the groundtruth tokens are predicted to have lower probabilities. We conclude that the non-linearity introduced by a logarith"
D18-1525,W17-5301,0,0.0142808,"he encoder-decoder idea have been suggested for unsupervised feature extraction from textual data. Cer et al. (2018) modify the Transformer architecture (Vaswani et al., 2017) originally suggested for machine translation to produce sentence embeddings that target transfer learning to other NLP tasks. Arora et al. (2016) claim that sentence representation as simple weighted averaging of word vectors beats more sophisticated recurrent network-based models. McCann et al. (2017) show that adding machine translation-learned vectors to models designed for other NLP tasks improves their performance. Nangia et al. (2017) in RepEval-2017 report that in-sentence attention and biLSTM-based models extract represen4875 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4875–4880 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Here the optimization function can be seen as the “weighted” cross-entropy, meaning that every ground-truth token is represented with similarities to other words in the vocabulary rather than with a traditional one-hotencoding scheme. The schematic illustration of the true label encoding for the weig"
D18-1525,N18-1202,0,0.0479168,"ommon English stop-words from soft target encoding, i.e. we apply a regular cross entropy loss for reconstructing of these words. The schematic illustration is given in Figure 1 (center).   PNsim(yt ,yi ) , yi ∈ top N j=1 sim(yt ,yj ) yi∗ = (5) 0, yi 6∈ top N The objective of a classic autoencoder is to minimize the difference between the given input X and ˆ the reconstructed output X. V X yi∗ logpi i=1 Experiments L(X, g(f (X)) V X We use pre-trained fastText (Bojanowski et al., 2016) word vectors to compute similarities between words. Note that the more recently proposed ELMo embeddings (Peters et al., 2018), for example, can not be used in our case, since they are context-dependent, which means that similarities between individual words can not be precomputed. To find out how the proposed loss functions affect the quality of the derived representations, we trained several autoencoder models using the regular cross-entropy, as well as the three variants of the similarity-based reconstruction loss described above. In these experiments, we use the Yelp restaurant reviews dataset (Shen et al., 2017). This dataset was originally introduced for a sentiment classification task and consists of 600K sent"
D19-1445,N19-1408,0,0.0290264,"her than examining representations extracted from different layers, we focus on the understanding of the self-attention mechanism itself, since it is the key feature of Transformer-based models. Another research direction that is relevant to our work is neural network pruning. Frankle and Carbin (2018) showed that widely used complex architectures suffer from overparameterization, and can be significantly reduced in size without a loss in performance. Goldberg (2019) observed that the smaller version of BERT achieves better scores on a number of syntax-testing experiments than the larger one. Adhikari et al. (2019) questioned the necessity of computationheavy neural networks, proving that a simple yet carefully tuned BiLSTM without attention achieves the best or at least competitive results compared to more complex architectures on the document classification task. Wu et al. (2019) presented more evidence of unnecessary complexity of the self-attention mechanism, and proposed a more lightweight and scalable dynamic convolution-based architecture that outperforms the self-attention baseline. Michel et al. (2019) demonstrated that some layers in Transformer can be reduced down to a single head without sig"
D19-1445,P98-1013,0,0.637967,"types are most likely associated with language model pre-training, while the last two potentially encode semantic and syntactic information. Figure 2: Estimated percentages of the identified selfattention classes for each of the selected GLUE tasks. large number of such relations could be investigated, we chose to examine semantic role relations defined in frame semantics, since they can be viewed as being at the intersection of syntax and semantics. Specifically, we focused on whether BERT captures FrameNet’s relations between frame-evoking lexical units (predicates) and core frame elements (Baker et al., 1998), and whether the links between them produce higher attention weights in certain specific heads. We used pre-trained BERT in these experiments. The data for this experiment comes from FrameNet (Baker et al., 1998), a database that contains frame annotations for example sentences for different lexical units. Frame elements correspond to semantic roles for a given frame, for example, “buyer”, “seller”, and “goods” for the “Commercial transaction” frame evoked by the words “sell” and “spend” or “topic” and “text” for the “Scrutiny” semantic frame evoked by the verb “address”. Figure 4 shows an ex"
D19-1445,S17-2001,0,0.0162073,"as will be discussed below. All the experiments with the pretrained BERT were conducted using the model provided with the PyTorch implementation of BERT (bert-base-uncased, 12-layer, 768-hidden, 12-heads, 110M parameters)3 . We chose this smaller version of BERT because it shows competitive, if not better, performance while having fewer layers and heads, which makes it more interpretable. We use the following subset of GLUE tasks (Wang et al., 2018) for fine-tuning: • MRPC: the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) • STS-B: the Semantic Textual Similarity Benchmark (Cer et al., 2017) • SST-2: the Stanford Sentiment Treebank, two-way classification (Socher et al., 2013) • QQP: the Quora Question Pairs dataset • RTE: the Recognizing Textual Entailment datasets • QNLI: Question-answering NLI based on the Stanford Question Answering Dataset (Rajpurkar et al., 2016) • MNLI: the Multi-Genre Natural Language Inference Corpus, matched section (Williams et al., 2018) Please refer to the original GLUE paper for details on the QQP and RTE datasets (Wang et al., 2018). We excluded two tasks: CoLa and the Winograd Schema Challenge. The latter is excluded due to the small size of the d"
D19-1445,I05-5002,0,0.122612,"eries of experiments with the basic pre-trained or the fine-tuned BERT models, as will be discussed below. All the experiments with the pretrained BERT were conducted using the model provided with the PyTorch implementation of BERT (bert-base-uncased, 12-layer, 768-hidden, 12-heads, 110M parameters)3 . We chose this smaller version of BERT because it shows competitive, if not better, performance while having fewer layers and heads, which makes it more interpretable. We use the following subset of GLUE tasks (Wang et al., 2018) for fine-tuning: • MRPC: the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) • STS-B: the Semantic Textual Similarity Benchmark (Cer et al., 2017) • SST-2: the Stanford Sentiment Treebank, two-way classification (Socher et al., 2013) • QQP: the Quora Question Pairs dataset • RTE: the Recognizing Textual Entailment datasets • QNLI: Question-answering NLI based on the Stanford Question Answering Dataset (Rajpurkar et al., 2016) • MNLI: the Multi-Genre Natural Language Inference Corpus, matched section (Williams et al., 2018) Please refer to the original GLUE paper for details on the QQP and RTE datasets (Wang et al., 2018). We excluded two tasks: CoLa and the Winograd S"
D19-1445,P19-1356,0,0.0569953,"odeling task (Devlin et al., 2018). BERT-based architectures have produced new state-of-the-art performance on a range of NLP tasks of different nature, domain, and complexity, including question answering, sequence tagging, sentiment analysis, and 2 Related work There have been several recent attempts to assess BERT’s ability to capture structural properties of language. Goldberg (2019) demonstrated that BERT consistently assigns higher scores to the correct verb forms as opposed to the incorrect one in a masked language modeling task, suggesting some ability to model subject-verb agreement. Jawahar et al. (2019) extended this work to using multiple layers and tasks, supporting the claim that BERT’s intermediate layers capture rich linguistic information. On the other hand, Tran et al. (2018) concluded that LSTMs generalize to longer 1 https://rajpurkar.github.io/ SQuAD-explorer/ 2 https://gluebenchmark.com/leaderboard 4365 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4365–4374, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sequences better"
D19-1445,N19-1112,0,0.0428786,"s intermediate layers capture rich linguistic information. On the other hand, Tran et al. (2018) concluded that LSTMs generalize to longer 1 https://rajpurkar.github.io/ SQuAD-explorer/ 2 https://gluebenchmark.com/leaderboard 4365 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4365–4374, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sequences better, and are more robust with respect to agreement distractors, compared to Transformers. Liu et al. (2019) investigated the transferability of contextualized word representations to a number of probing tasks requiring linguistic knowledge. Their findings suggest that (a) the middle layers of Transformer-based architectures are the most transferable to other tasks, and (b) higher layers of Transformers are not as task specific as the ones of RNNs. Tang et al. (2018) argued that models using self-attention outperform CNN- and RNN-based models on a word sense disambiguation task due to their ability to extract semantic features from text. Voita et al. (2019) analyzed the original Transformer model on"
D19-1445,A94-1016,0,0.27286,"Missing"
D19-1445,D16-1264,0,0.0610549,"ows competitive, if not better, performance while having fewer layers and heads, which makes it more interpretable. We use the following subset of GLUE tasks (Wang et al., 2018) for fine-tuning: • MRPC: the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) • STS-B: the Semantic Textual Similarity Benchmark (Cer et al., 2017) • SST-2: the Stanford Sentiment Treebank, two-way classification (Socher et al., 2013) • QQP: the Quora Question Pairs dataset • RTE: the Recognizing Textual Entailment datasets • QNLI: Question-answering NLI based on the Stanford Question Answering Dataset (Rajpurkar et al., 2016) • MNLI: the Multi-Genre Natural Language Inference Corpus, matched section (Williams et al., 2018) Please refer to the original GLUE paper for details on the QQP and RTE datasets (Wang et al., 2018). We excluded two tasks: CoLa and the Winograd Schema Challenge. The latter is excluded due to the small size of the dataset. As for CoLa (the task of predicting linguistic acceptability judgments), GLUE authors report that the hu3 https://github.com/huggingface/ pytorch-pretrained-BERT 4366 man performance is only 66.4, which is explained by the problems with the underlying methodology (Schutze, 1"
D19-1445,D18-1187,1,0.890442,"Missing"
D19-1445,D13-1170,0,0.0141992,"cted using the model provided with the PyTorch implementation of BERT (bert-base-uncased, 12-layer, 768-hidden, 12-heads, 110M parameters)3 . We chose this smaller version of BERT because it shows competitive, if not better, performance while having fewer layers and heads, which makes it more interpretable. We use the following subset of GLUE tasks (Wang et al., 2018) for fine-tuning: • MRPC: the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) • STS-B: the Semantic Textual Similarity Benchmark (Cer et al., 2017) • SST-2: the Stanford Sentiment Treebank, two-way classification (Socher et al., 2013) • QQP: the Quora Question Pairs dataset • RTE: the Recognizing Textual Entailment datasets • QNLI: Question-answering NLI based on the Stanford Question Answering Dataset (Rajpurkar et al., 2016) • MNLI: the Multi-Genre Natural Language Inference Corpus, matched section (Williams et al., 2018) Please refer to the original GLUE paper for details on the QQP and RTE datasets (Wang et al., 2018). We excluded two tasks: CoLa and the Winograd Schema Challenge. The latter is excluded due to the small size of the dataset. As for CoLa (the task of predicting linguistic acceptability judgments), GLUE a"
D19-1445,D18-1458,0,0.0437797,"Missing"
D19-1445,D18-1503,0,0.0724223,"Missing"
D19-1445,P19-1580,0,0.0609136,"ement distractors, compared to Transformers. Liu et al. (2019) investigated the transferability of contextualized word representations to a number of probing tasks requiring linguistic knowledge. Their findings suggest that (a) the middle layers of Transformer-based architectures are the most transferable to other tasks, and (b) higher layers of Transformers are not as task specific as the ones of RNNs. Tang et al. (2018) argued that models using self-attention outperform CNN- and RNN-based models on a word sense disambiguation task due to their ability to extract semantic features from text. Voita et al. (2019) analyzed the original Transformer model on a translation task and found out that only a small subset of heads is important for the given task, but these heads have interpretable linguistic functions. Our work contributes to the above discussion, but rather than examining representations extracted from different layers, we focus on the understanding of the self-attention mechanism itself, since it is the key feature of Transformer-based models. Another research direction that is relevant to our work is neural network pruning. Frankle and Carbin (2018) showed that widely used complex architectu"
D19-1445,W18-5446,0,0.120471,"Missing"
D19-1445,N18-1101,0,0.0676559,"Missing"
D19-5005,N19-1423,0,0.00758185,"hday party on Saturday! Figure 2: Types of calls to actions in Russian social networks, with examples 39 scikit-learn2 library. In both cases we used TF-IDF representations of both original posts and posts lemmatized with pymorphy3 library (Korobov, 2015). We picked the best regularization hyperparameters for each model through crossvalidation based on the average F1 score over 5 folds. The current state-of-the-art deep learning approaches rely on large Transformer-based models pre-trained on large text corpora and then finetuned for a given task. In particular, we tried two versions of BERT (Devlin et al., 2019): the multilingual model released in the PyTorch repository of BERT 4 , and the Russian version (RuBERT) released by DeepPavlov5 . The latter model is initialized as multilingual BERT and further fine-tuned on Russian Wikipedia and news corpora (Kuratov and Arkhipov, 2019). Both models have 12 layers and 180M parameters. We trained both models for 40 epochs with the batch size of 32 and the learning rate of 5e−5 . Classifier Acc. F1 LR (no lemmatization) LR (lemmatization) SVC (no lemmatization) SVC (lemmatization) BERT multilingual RuBERT LSTM on ELMo 0.78 0.82 0.80 0.78 0.8 0.86 0.83 0.67 0."
D19-5005,N10-1142,0,0.0371747,"as non-CTAs. This was used as the training dataset for the work to be described in subsequent sections. Prototypical CTAs are imperatives prompting the addressee to perform some action, such as “Don’t let the government tell you what to think!”. This seems like a straightforward category to annotate, but in reality CTAs may be expressed in various ways, including both direct and indirect speech acts. There are many borderline cases that would in the absence of clear guidelines decrease interannotator agreement (IAA). There is relevant work on the task of identification of requests in emails (Lampert et al., 2010) and intention classification for dialogue agents (Quinn and Zaiane, 2014), but, to the best of our knowledge, this work is the first to create a detailed schema for CTA annotation in the context of a political protest. The current work on censorship is concerned not so much with CTAs in particular, but with a broader category of “material with collective action potential”. King et al. (2013) defines such materials as those that ’(a) involve protest or organized crowd formation outside the Internet; (b) related to individuals who have organized or incited collective action on the ground in the"
D19-5005,N18-1202,0,0.0189656,"were 0 protest events. The two green lines correspond to upper and lower attendance estimates. The blue line shows the detected CTAs. Despite the noisiness and incompleteness of the available protest data (see subsection 9.1), the Pearson’s correlation between attendance estimates and the number of detected CTAs is about 0.4, which is considered to be “moderate”. This could make CTAs a useful additional factor to systems based on spatiotemporal, demographic, and/or network activity features. Additionally to BERT representations, we experimented with the contextual embedder of the ELMo model (Peters et al., 2018) pre-trained for Russian and released by DeepPavlov6 . The posts were split into sentences using the NLTK library7 and each sentence token was encoded by the ELMo embedder into a 1024-dimensional vector. The classification was performed by a standard LSTM network (Hochreiter and Schmidhuber, 1997) with a hidden size of 256 units followed by a linear layer. We trained the network for 25 epochs and with the learning rate of 0.001. 8 Russian Wikipedia, Protest movement in Russia (20112013): https://tinyurl.com/y46qyb9w. The results of all the classification experiments are shown in Table 2. The b"
I17-1035,S16-1081,0,0.0157169,"g stances on a topic, whereas Habernal and Gurevych’s dataset has labels for arguments with the same stance towards a topic. Persing and Ng (2015) constructed a corpus of persuasive essays annotated for the essays’ argument strength, which is slightly different to other annotated persuasive essay corpora, which have more of a focus on overall writing quality. NLP datasets involving the processing of text pairs have become more prevalent. Examples include predicting textual entailment (Marelli et al., 2014; Bowman et al., 2015), predicting semantic relatedness/similarity (Marelli et al., 2014; Agirre et al., 2016), and predicting humor (Potash et al., 2016b; Shahaf et al., 2015). These tasks present interesting challenges from a modeling perspective, as methods must allow for semantic comparison between the texts. 3.1 Metric Entropy The Shannon Entropy of a text T containing a set of characters C is defined as: H(T ) = − X c∈C where Although relatively rare in the argument mining community, leveraging external knowledge sources is ubiquitous for the task of questionanswering (Kolomiyets and Moens, 2011), using information retrieval techniques to mine the available documents for answers. Work such as Be"
I17-1035,D13-1160,0,0.0196419,"6), and predicting humor (Potash et al., 2016b; Shahaf et al., 2015). These tasks present interesting challenges from a modeling perspective, as methods must allow for semantic comparison between the texts. 3.1 Metric Entropy The Shannon Entropy of a text T containing a set of characters C is defined as: H(T ) = − X c∈C where Although relatively rare in the argument mining community, leveraging external knowledge sources is ubiquitous for the task of questionanswering (Kolomiyets and Moens, 2011), using information retrieval techniques to mine the available documents for answers. Work such as Berant et al. (2013) forms a knowledge base from external documents, and maps queries to knowledgebase entries. Weston et al. (2014) have proposed a neural network-based approach for large-scale question-answering. In the argument mining community, Rinott et al. (2015) created a dataset for predicting potential support clauses for argumentative topics, while Braunstain et al. (2016) rank Wikipedia sentences for supporting answers made by online user answers. Conversely, Wachsmuth et al. (2017) approach the problem of measuring relevance amongst arguments themselves, proposing a methodology based on PageRank (Page"
I17-1035,D15-1075,0,0.145564,"Language Processing, pages 342–351, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP more likely to exhibit the qualities that make an argument convincing. For all methods that use the presence of Wikipedia articles, we use several variations of a corpus to determine how well the methods leverage topic-specific articles, as opposed to randomly selected articles. In terms of supervised techniques, we first follow previous approaches to classifying paired data that create separate learned representations of elements in a pair that are then concatenated for the final predictive model (Bowman et al., 2015; Mueller and Thyagarajan, 2016; Potash et al., 2016b). Specifically, we experiment with creating separate representations using either a BLSTM or summing individual token embeddings. We then propose modifications of the supervised models to leverage external data. The models grow with increasing complexity, approaching a form of Memory Network (Sukhbaatar et al., 2015) that computes a weighted sum of representations of Wikipedia articles. Our experimental results reveal several important insights into how to approach predicting convincingness. We summarize our findings as follows: 1) Unsuperv"
I17-1035,E17-4008,0,0.0887597,"model to data; (2) we explore modifications of the initial ‘deep’ model used by Habernal and Gurevych (2016a), which was a Bidirectional Long Short-Term Memory (BLSTM) network; (3) we test the feasibility of offering factually relevant knowledge in the form of Wikipedia articles related to the argument topics. 2 Related Work Habernal and Gurevych (2016b) present two methods in their dataset paper: (1) an SVM with numerous hand-crafted features; (2) a BLSTM that only uses word embeddings as input. Aside from the original corpus authors, only one other work has tested on the UKPConvArg dataset. Chalaguine and Schulz (2017) use a feature-selection method to determine the raw feature representation that serves as input into a feed-forward neural network. The authors conduct a thorough ablation study of the performance of individual feature types. The authors’ best model records an accuracy of .766, compared to .781 and .757 of Habernal and Gurevych’s SVM and BLSTM, reIn terms of heuristics, we examine the effectiveness of Metric Entropy (ME) of text to predict convincingness, which is inspired by the notion that written English language is well-formed, as opposed to random. Specifically, high ME corresponds to hi"
I17-1035,D16-1129,0,0.0651004,"and Ng, 2015; Wachsmuth et al., 2016). Zhang et al. (2016) have also attempted to predict argument convincingness, in the form of predicting debate winners. Unfortunately, these are very rare argumentative formats that are seldom encountered in everyday life. In practice, at least at the moment, we tend to digest a large quantity of our information from social media and engage in a tremendous amount of interpersonal communication using it. Since, in social media, communications are roughly a single paragraph, analyzing Table 1: Example of an argument pair where Argument 1 is more convincing. Habernal and Gurevych (2016b) have recently released a dataset of short, single-paragraph arguments annotated for convincingness, which we will refer to as UKPConvArg. For 16 issues, arguments with the same stance are compared with each other to determine, given a pair of arguments, which one is more convincing. Table 1 provides an example of an argument pair with arguments from the prompt ‘Is it better to have a lousy father or to be fatherless’; and the stance: ‘It is better to have a lousy father’. In this pair Argument 1 is chosen to be more convincing. Other such issues include: ‘Does India have the potential to 34"
I17-1035,D15-1050,0,0.0772195,"a text T containing a set of characters C is defined as: H(T ) = − X c∈C where Although relatively rare in the argument mining community, leveraging external knowledge sources is ubiquitous for the task of questionanswering (Kolomiyets and Moens, 2011), using information retrieval techniques to mine the available documents for answers. Work such as Berant et al. (2013) forms a knowledge base from external documents, and maps queries to knowledgebase entries. Weston et al. (2014) have proposed a neural network-based approach for large-scale question-answering. In the argument mining community, Rinott et al. (2015) created a dataset for predicting potential support clauses for argumentative topics, while Braunstain et al. (2016) rank Wikipedia sentences for supporting answers made by online user answers. Conversely, Wachsmuth et al. (2017) approach the problem of measuring relevance amongst arguments themselves, proposing a methodology based on PageRank (Page et al., 1999). P (c) = P (c) log2 P (c) (1) f req(c) len(T ) (2) and f req(c) is the number of times c appears in T . Consequently, ME is the Shannon entropy divided by the text length, len(T ). Since ME produces a continuous output, it is sensible"
I17-1035,P16-1150,0,0.0668964,"and Ng, 2015; Wachsmuth et al., 2016). Zhang et al. (2016) have also attempted to predict argument convincingness, in the form of predicting debate winners. Unfortunately, these are very rare argumentative formats that are seldom encountered in everyday life. In practice, at least at the moment, we tend to digest a large quantity of our information from social media and engage in a tremendous amount of interpersonal communication using it. Since, in social media, communications are roughly a single paragraph, analyzing Table 1: Example of an argument pair where Argument 1 is more convincing. Habernal and Gurevych (2016b) have recently released a dataset of short, single-paragraph arguments annotated for convincingness, which we will refer to as UKPConvArg. For 16 issues, arguments with the same stance are compared with each other to determine, given a pair of arguments, which one is more convincing. Table 1 provides an example of an argument pair with arguments from the prompt ‘Is it better to have a lousy father or to be fatherless’; and the stance: ‘It is better to have a lousy father’. In this pair Argument 1 is chosen to be more convincing. Other such issues include: ‘Does India have the potential to 34"
I17-1035,marelli-etal-2014-sick,0,0.0134677,"dataset and that of Habernal and Gurevych (2016b) is that in the debate dataset, the debate teams have opposing stances on a topic, whereas Habernal and Gurevych’s dataset has labels for arguments with the same stance towards a topic. Persing and Ng (2015) constructed a corpus of persuasive essays annotated for the essays’ argument strength, which is slightly different to other annotated persuasive essay corpora, which have more of a focus on overall writing quality. NLP datasets involving the processing of text pairs have become more prevalent. Examples include predicting textual entailment (Marelli et al., 2014; Bowman et al., 2015), predicting semantic relatedness/similarity (Marelli et al., 2014; Agirre et al., 2016), and predicting humor (Potash et al., 2016b; Shahaf et al., 2015). These tasks present interesting challenges from a modeling perspective, as methods must allow for semantic comparison between the texts. 3.1 Metric Entropy The Shannon Entropy of a text T containing a set of characters C is defined as: H(T ) = − X c∈C where Although relatively rare in the argument mining community, leveraging external knowledge sources is ubiquitous for the task of questionanswering (Kolomiyets and Moe"
I17-1035,C16-1158,0,0.092081,"ther, it causes have warned that him/her to look for a growing up without father figure. Dura father can per- ing such searches, a manently change child may end up getthe structure of a ting sexual harassed child’s brain and or being emotionally make him/her more exploited to various aggressive and angry. degrees. Introduction Predicting argument convincingness has mostly been studied in relation to the overall quality of a persuasive essay (Attali and Burstein, 2004; Landauer, 2003; Shermis et al., 2010), with a recent focus specifically on predicting argument strength (Persing and Ng, 2015; Wachsmuth et al., 2016). Zhang et al. (2016) have also attempted to predict argument convincingness, in the form of predicting debate winners. Unfortunately, these are very rare argumentative formats that are seldom encountered in everyday life. In practice, at least at the moment, we tend to digest a large quantity of our information from social media and engage in a tremendous amount of interpersonal communication using it. Since, in social media, communications are roughly a single paragraph, analyzing Table 1: Example of an argument pair where Argument 1 is more convincing. Habernal and Gurevych (2016b) have rec"
I17-1035,E17-1105,0,0.178795,"olomiyets and Moens, 2011), using information retrieval techniques to mine the available documents for answers. Work such as Berant et al. (2013) forms a knowledge base from external documents, and maps queries to knowledgebase entries. Weston et al. (2014) have proposed a neural network-based approach for large-scale question-answering. In the argument mining community, Rinott et al. (2015) created a dataset for predicting potential support clauses for argumentative topics, while Braunstain et al. (2016) rank Wikipedia sentences for supporting answers made by online user answers. Conversely, Wachsmuth et al. (2017) approach the problem of measuring relevance amongst arguments themselves, proposing a methodology based on PageRank (Page et al., 1999). P (c) = P (c) log2 P (c) (1) f req(c) len(T ) (2) and f req(c) is the number of times c appears in T . Consequently, ME is the Shannon entropy divided by the text length, len(T ). Since ME produces a continuous output, it is sensible to evaluate it using the regression task from Habernal and Gurevych (2016b). Because ME is a combination of Shannon Entropy and text length, we also evaluate their effectiveness separately as well. We admit, however, that our in"
I17-1035,N16-1017,0,0.108961,"ed that him/her to look for a growing up without father figure. Dura father can per- ing such searches, a manently change child may end up getthe structure of a ting sexual harassed child’s brain and or being emotionally make him/her more exploited to various aggressive and angry. degrees. Introduction Predicting argument convincingness has mostly been studied in relation to the overall quality of a persuasive essay (Attali and Burstein, 2004; Landauer, 2003; Shermis et al., 2010), with a recent focus specifically on predicting argument strength (Persing and Ng, 2015; Wachsmuth et al., 2016). Zhang et al. (2016) have also attempted to predict argument convincingness, in the form of predicting debate winners. Unfortunately, these are very rare argumentative formats that are seldom encountered in everyday life. In practice, at least at the moment, we tend to digest a large quantity of our information from social media and engage in a tremendous amount of interpersonal communication using it. Since, in social media, communications are roughly a single paragraph, analyzing Table 1: Example of an argument pair where Argument 1 is more convincing. Habernal and Gurevych (2016b) have recently released a data"
I17-1035,D14-1162,0,0.0984635,"ikipedia articles W, we define the Wikipedia Similarity Score, W SS of an argument a as: X W SS(a) = awT (3) Model SVM BLSTM SE LEN ME w∈W For pairwise prediction, we predict the argument with the higher score as the more convincing argument. We consider two possible representations for texts: 1) term-frequency (TF) count, and 2) Summing the embeddings of all the tokens in the text. For the TF representation, we use the CountVectorizer class from Scikit-learn (Pedregosa et al., 2011) to process the text and create the appropriate representation. For the embedding representation, we use GloVe (Pennington et al., 2014) 300 dimensions learned from the Common Crawl corpus with 840 billion tokens. Our Wikipedia data is from the May 20th, 2017 dump1 . We clean the raw Wikipedia data using ˇ uˇrek and Sojka, 2010). We experigensim (Reh˚ ment with three different Wikipedia corpora. The first corpus has a set of 30 hand-picked Wikipedia articles, chosen to be of the same subject matter of the various topics in the argument convincingness corpora. We refer to this corpus as Wiki hand-picked (hp). The second corpus contains 38k random Wikipedia articles, chosen to be approximately the length of the hand-picked artic"
I17-1035,P15-1053,0,0.306028,"l University have a father, it causes have warned that him/her to look for a growing up without father figure. Dura father can per- ing such searches, a manently change child may end up getthe structure of a ting sexual harassed child’s brain and or being emotionally make him/her more exploited to various aggressive and angry. degrees. Introduction Predicting argument convincingness has mostly been studied in relation to the overall quality of a persuasive essay (Attali and Burstein, 2004; Landauer, 2003; Shermis et al., 2010), with a recent focus specifically on predicting argument strength (Persing and Ng, 2015; Wachsmuth et al., 2016). Zhang et al. (2016) have also attempted to predict argument convincingness, in the form of predicting debate winners. Unfortunately, these are very rare argumentative formats that are seldom encountered in everyday life. In practice, at least at the moment, we tend to digest a large quantity of our information from social media and engage in a tremendous amount of interpersonal communication using it. Since, in social media, communications are roughly a single paragraph, analyzing Table 1: Example of an argument pair where Argument 1 is more convincing. Habernal and"
L18-1639,D15-1075,0,0.0242813,". In order to use RNNs to process sequences of sentences, we need to train a sentence encoder. Given the small dataset, we also tried external resources. Conneau et al. (2017) proposed a model to produce “universal sentence representations”. They train sentence encoders on Natural Language Inference (NLI) task and claim such a process involves high-level understanding of text, and thus generates better sentence embeddings than other supervised or unsupervised learning methods. In our experiments, we use their pre-trained sentence encoder to represent sentences. The encoder is trained on SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017). 3.1. Computational Thinking Coding Scheme The dialogue of each team is segmented into 20-minute conversation chunks. Eventually we have 10 chunks for Team A and 9 for Team B. Then the dialogue turns of the 19 files are fully annotated. We developed a computational thinking coding scheme, as shown in Table 1. Code A ATO ATV D DO NSTO QB QS QR O Category Analysis Algorithmic thinking -operation Algorithmic thinking -variable Design Debugging observations Non-specific test outcome Query building Query software Query robot Other Description Planning, developi"
L18-1639,D17-1070,0,0.118264,"re complex meanings, and thus sequential sentence labeling is a more difficult task. 3. Dataset The dataset consists of dialogues of middle school students solving a robotics challenge together. There are two collaborative teams. Team A has 1 boy and 2 girls, and Team B has 2 boys. All students wore wireless microphones, and their interactions were videotaped. In this study, we obtained the transcribed dialogues and use text data for analysis. In order to use RNNs to process sequences of sentences, we need to train a sentence encoder. Given the small dataset, we also tried external resources. Conneau et al. (2017) proposed a model to produce “universal sentence representations”. They train sentence encoders on Natural Language Inference (NLI) task and claim such a process involves high-level understanding of text, and thus generates better sentence embeddings than other supervised or unsupervised learning methods. In our experiments, we use their pre-trained sentence encoder to represent sentences. The encoder is trained on SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017). 3.1. Computational Thinking Coding Scheme The dialogue of each team is segmented into 20-minute conversation chunks."
L18-1639,I08-1050,0,0.0283456,"ision trees (Moldovan et al., ; Samei et al., 2014). These methods require handcrafted features, and their performance depends on the application domain. Few if any works to date have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are mostly based on traditional statistical machine learning methods too, such as Na¨ıve Bayes (Huang et al., 2013), SVM (McKnight and Srinivasan, 2003; Hirohata et al., 2008), HMM (Lin et al., 2006), and CRF (Hirohata et al., 2008; Hassanzadeh et al., 2014). Recently, popular approaches use neural networks with word embeddings (Socher et al., 2013; Kim, 2014) and/or character embeddings (Zhang et al., 2015; Conneau et al., 2016) to train sentence encoders, and then perform classification. Those neural networks use either convolutional layers or recurrent layers to learn deep representations, and often produce better results than older systems. However, one drawback of most of the models is that they do not make use of context, but focus on representing each senten"
L18-1639,D10-1084,0,0.0368012,"ls, known as the “computational thinking codes (CT codes)”. We need to build computational models to assign the codes to dialogue turns automatically. In a broader sense, this is a dialogue action (DA) annotation task, with multiple agents in the dialogue. It is also related to the sequential sentence labeling task, in which a series of sentences need to be labeled. 2. Related Work Many different classification algorithms have been used for dialogue act annotation. The most popular ones use support vector machines (SVM) (Margolis et al., 2010; Tavafi et al., 2013), Hidden Markov models (HMM) (Kim et al., 2010; Tavafi et al., 2013), conditional random fields (CRF) (Kim et al., 2010; Tavafi et al., 2013), and decision trees (Moldovan et al., ; Samei et al., 2014). These methods require handcrafted features, and their performance depends on the application domain. Few if any works to date have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are mostly based on traditional statistical machi"
L18-1639,D14-1181,0,0.00393362,"ly popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are mostly based on traditional statistical machine learning methods too, such as Na¨ıve Bayes (Huang et al., 2013), SVM (McKnight and Srinivasan, 2003; Hirohata et al., 2008), HMM (Lin et al., 2006), and CRF (Hirohata et al., 2008; Hassanzadeh et al., 2014). Recently, popular approaches use neural networks with word embeddings (Socher et al., 2013; Kim, 2014) and/or character embeddings (Zhang et al., 2015; Conneau et al., 2016) to train sentence encoders, and then perform classification. Those neural networks use either convolutional layers or recurrent layers to learn deep representations, and often produce better results than older systems. However, one drawback of most of the models is that they do not make use of context, but focus on representing each sentence independently (Dernoncourt et al., 2016). Moreover, the sequence of labels are not directly modeled like in CRF, although RNNs can capture that indirectly. Dernoncourt et al. (2016) tr"
L18-1639,W06-3309,0,0.0135253,"; Samei et al., 2014). These methods require handcrafted features, and their performance depends on the application domain. Few if any works to date have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are mostly based on traditional statistical machine learning methods too, such as Na¨ıve Bayes (Huang et al., 2013), SVM (McKnight and Srinivasan, 2003; Hirohata et al., 2008), HMM (Lin et al., 2006), and CRF (Hirohata et al., 2008; Hassanzadeh et al., 2014). Recently, popular approaches use neural networks with word embeddings (Socher et al., 2013; Kim, 2014) and/or character embeddings (Zhang et al., 2015; Conneau et al., 2016) to train sentence encoders, and then perform classification. Those neural networks use either convolutional layers or recurrent layers to learn deep representations, and often produce better results than older systems. However, one drawback of most of the models is that they do not make use of context, but focus on representing each sentence independently (Dernon"
L18-1639,W10-2607,0,0.0330554,"). Dialogue turns (utterances) are annotated with carefully designed labels, known as the “computational thinking codes (CT codes)”. We need to build computational models to assign the codes to dialogue turns automatically. In a broader sense, this is a dialogue action (DA) annotation task, with multiple agents in the dialogue. It is also related to the sequential sentence labeling task, in which a series of sentences need to be labeled. 2. Related Work Many different classification algorithms have been used for dialogue act annotation. The most popular ones use support vector machines (SVM) (Margolis et al., 2010; Tavafi et al., 2013), Hidden Markov models (HMM) (Kim et al., 2010; Tavafi et al., 2013), conditional random fields (CRF) (Kim et al., 2010; Tavafi et al., 2013), and decision trees (Moldovan et al., ; Samei et al., 2014). These methods require handcrafted features, and their performance depends on the application domain. Few if any works to date have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential"
L18-1639,D13-1170,0,0.00265986,"have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are mostly based on traditional statistical machine learning methods too, such as Na¨ıve Bayes (Huang et al., 2013), SVM (McKnight and Srinivasan, 2003; Hirohata et al., 2008), HMM (Lin et al., 2006), and CRF (Hirohata et al., 2008; Hassanzadeh et al., 2014). Recently, popular approaches use neural networks with word embeddings (Socher et al., 2013; Kim, 2014) and/or character embeddings (Zhang et al., 2015; Conneau et al., 2016) to train sentence encoders, and then perform classification. Those neural networks use either convolutional layers or recurrent layers to learn deep representations, and often produce better results than older systems. However, one drawback of most of the models is that they do not make use of context, but focus on representing each sentence independently (Dernoncourt et al., 2016). Moreover, the sequence of labels are not directly modeled like in CRF, although RNNs can capture that indirectly. Dernoncourt et a"
L18-1639,W13-4017,0,0.0239381,"rances) are annotated with carefully designed labels, known as the “computational thinking codes (CT codes)”. We need to build computational models to assign the codes to dialogue turns automatically. In a broader sense, this is a dialogue action (DA) annotation task, with multiple agents in the dialogue. It is also related to the sequential sentence labeling task, in which a series of sentences need to be labeled. 2. Related Work Many different classification algorithms have been used for dialogue act annotation. The most popular ones use support vector machines (SVM) (Margolis et al., 2010; Tavafi et al., 2013), Hidden Markov models (HMM) (Kim et al., 2010; Tavafi et al., 2013), conditional random fields (CRF) (Kim et al., 2010; Tavafi et al., 2013), and decision trees (Moldovan et al., ; Samei et al., 2014). These methods require handcrafted features, and their performance depends on the application domain. Few if any works to date have used the recently popular neural network-based approaches for dialogue segment labeling. We can also consider DA annotation a special case of sequential sentence labeling, in which every sentence is an utterance. Existing systems for sequential sentence labeling are"
L18-1639,N18-1101,0,0.0304747,"Missing"
N19-1088,D17-1070,0,0.226193,"better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of paraphrase detection and show that they significantly outperform the embeddings of a regular autoencoder. 1 Introduction Despite the recent successes in using neural models for representation learning for natural language text, learning a meaningful representation of input sentences remains an open research problem. A variety of approaches, from sequence-to-sequence models that followed the work of Sutskever et al. (2014) to the more recent proposals (Arora et al., 2017; Nangia et al., 2017; Conneau et al., 2017; Logeswaran and Lee, 2018; Subramanian et al., 2018; Cer et al., 2018) share one common drawback. Namely, all of them encode the input sentence into just one single vector of a fixed size. One way to bypass the limitations of a single vector representation is to use an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017). We propose to approach this problem differently 1 https://github.com/text-machine-lab/ adversarial_decomposition 815 Proceedings of NAACL-HLT 2019, pages 815–825 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We e"
N19-1088,C04-1051,0,0.0543212,"DNet on two tasks: the shift of language register and diachronic language change. Our solution achieves superior results, and t-SNE visualizations of the learned meaning and form embeddings illustrate that the proposed motivational loss leads to significantly better separation of the form embeddings. that require understanding of the meaning of the sentences regardless of their form. We evaluated embeddings produced by the ADNet, trained in the Headlines dataset, on the paraphrase detection task. We used the SentEval toolkit (Conneau et al., 2017) and the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The F1 scores on this task for different models are presented in Table 3. Note that all models, except InferSent, are unsupervised. The InferSent model was trained on a big SNLI dataset, consisting of more than 500,000 manually annotated pairs. ADNet achieves the the highest score among the unsupervised systems and far outperforms the regular sequence-to-sequence autoencoder. 6.4 References Multiple forms and stylistic similarities Samuel Albanie, S´ebastien Ehrhardt, and Jo˜ao F Henriques. 2017. Stopping gan violence: Generative unadversarial networks. arXiv preprint arXiv:1703.02528. In or"
N19-1088,P16-1094,0,0.0341956,"vector of a fixed size. One way to bypass the limitations of a single vector representation is to use an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017). We propose to approach this problem differently 1 https://github.com/text-machine-lab/ adversarial_decomposition 815 Proceedings of NAACL-HLT 2019, pages 815–825 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We evaluate the proposed methods for learning separate aspects of input representation in the following case studies: to pay to that vector and the current hidden state. Li et al. (2016) used “speaker” vectors as an additional input to a conversational model, improving consistency of dialog responses. Finally, Ficler and Goldberg (2017) performed an extensive evaluation of conditioned language models based on “content” (theme and sentiment) and “style” (professional, personal, length, descriptiveness). Importantly, they showed that it is possible to control both “content” and “style” simultaneously. 1. Diachronic language change. Specifically, we consider the Early Modern English (e.g. What would she have?) and the contemporary English ( What does she want?). 2. Social regist"
N19-1088,W17-4912,0,0.0618386,"2014; Vaswani et al., 2017). We propose to approach this problem differently 1 https://github.com/text-machine-lab/ adversarial_decomposition 815 Proceedings of NAACL-HLT 2019, pages 815–825 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We evaluate the proposed methods for learning separate aspects of input representation in the following case studies: to pay to that vector and the current hidden state. Li et al. (2016) used “speaker” vectors as an additional input to a conversational model, improving consistency of dialog responses. Finally, Ficler and Goldberg (2017) performed an extensive evaluation of conditioned language models based on “content” (theme and sentiment) and “style” (professional, personal, length, descriptiveness). Importantly, they showed that it is possible to control both “content” and “style” simultaneously. 1. Diachronic language change. Specifically, we consider the Early Modern English (e.g. What would she have?) and the contemporary English ( What does she want?). 2. Social register (Halliday et al., 1968), i.e. subsets of language appropriate in a given context or characteristic of a certain group of speakers. Social registers i"
N19-1088,P16-1014,0,0.0184074,"wo clusters by the nature of the training data: parallel aligned corpora, or non-aligned datasets. The aligned corpora enable approaching the problem of form shift as a paraphrasing or machine translation problem. Xu et al. (2012) used statistical and dictionary-based systems on a dataset of original plays by Shakespeare and their contemporary translations. Carlson et al. (2017) trained an LSTM network on 33 versions of the Bible. Jhamtani et al. (2017) used a Pointer Network (Vinyals et al., 2015), an architecture that was successfully applied to a wide variety of tasks (Merity et al., 2016; Gulcehre et al., 2016; Potash et al., 2017), to enable direct copying of the input tokens to the output. All these works use BLEU (Papineni et al., 2002) as the main, or even the only evaluation measure. This is only possible in cases where a parallel corpus is available. Related work As mentioned above, the most relevant previous work comes from research on style transfer2 . It can be divided into two groups: 1. Approaches that aim to generate text in a given form. For example, the task may be to produce just any verse as long as it is in the “style” of the target poet. 2. Approaches that aim to induce a change i"
N19-1088,W17-5301,0,0.0221154,"tor and encourages a better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of paraphrase detection and show that they significantly outperform the embeddings of a regular autoencoder. 1 Introduction Despite the recent successes in using neural models for representation learning for natural language text, learning a meaningful representation of input sentences remains an open research problem. A variety of approaches, from sequence-to-sequence models that followed the work of Sutskever et al. (2014) to the more recent proposals (Arora et al., 2017; Nangia et al., 2017; Conneau et al., 2017; Logeswaran and Lee, 2018; Subramanian et al., 2018; Cer et al., 2018) share one common drawback. Namely, all of them encode the input sentence into just one single vector of a fixed size. One way to bypass the limitations of a single vector representation is to use an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017). We propose to approach this problem differently 1 https://github.com/text-machine-lab/ adversarial_decomposition 815 Proceedings of NAACL-HLT 2019, pages 815–825 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computat"
N19-1088,P02-1040,0,0.103565,"aching the problem of form shift as a paraphrasing or machine translation problem. Xu et al. (2012) used statistical and dictionary-based systems on a dataset of original plays by Shakespeare and their contemporary translations. Carlson et al. (2017) trained an LSTM network on 33 versions of the Bible. Jhamtani et al. (2017) used a Pointer Network (Vinyals et al., 2015), an architecture that was successfully applied to a wide variety of tasks (Merity et al., 2016; Gulcehre et al., 2016; Potash et al., 2017), to enable direct copying of the input tokens to the output. All these works use BLEU (Papineni et al., 2002) as the main, or even the only evaluation measure. This is only possible in cases where a parallel corpus is available. Related work As mentioned above, the most relevant previous work comes from research on style transfer2 . It can be divided into two groups: 1. Approaches that aim to generate text in a given form. For example, the task may be to produce just any verse as long as it is in the “style” of the target poet. 2. Approaches that aim to induce a change in either the “form” or the “meaning” of an utterance. For example, “Good bye, Mr. Anderson.” can be transformed to “Fare you well, g"
N19-1088,C12-1177,0,0.542377,"Massachusetts Lowell Lowell, MA 01854 {aromanov,arum,arogers}@cs.uml.edu david donahue@student.uml.edu Abstract and design a method for adversarial decomposition of the learned input representation into multiple components. Our method encodes the input sentence into several vectors, where each vector is responsible for a specific aspect of the sentence. In terms of learning different separable components of input representation, our work most closely relates to the style transfer work, which has been applied to a variety of different aspects of language, from diachronic language differences (Xu et al., 2012) to authors’ personalities (Lipton et al., 2015) and even sentiment (Hu et al., 2017; Fu et al., 2018). The style transfer work effectively relies on the more classical distinction between meaning and form (de Saussure, 1959), which accounts for the fact that multiple surface realizations are possible for the same meaning. For simplicity, we will use this terminology throughout the rest of the paper. Consider encoding an input sentence into a meaning vector and a form vector. This enables a controllable change of meaning or form by a simple change applied to these vectors. For example, we can"
N19-1088,D15-1221,1,0.866033,"den representation of the encoded sentence to not have any information about the target sentence attributes. Mueller et al. (2017) used a VAE to produce a hidden representation of a sentence, and then modify it to match the desired form. Unlike Hu et al. (2017), they do not separate the form and meaning embeddings. Shen et al. (2017) applied a GAN to align the hidden representation of sentences from two corpora and forced them not to have any information about the form an via adversarial loss. During the decoding, similarly to Lipton et al. (2015), An example of the first group is the work of Potash et al. (2015), who trained several separate networks on verses by different hip-hop artists. An LSTM network successfully generated verses that were stylistically similar to the verses of the target artist (as measured by cosine distance on tf-idf vectors). More complicated approaches use language models that are conditioned in some way. For example, Lipton et al. (2015) produced product reviews with a target rating by passing the rating as an additional input at each timestep of an LSTM model. Tang et al. (2016) generated reviews not only with a given rating but also for a specific product. At each timest"
N19-1088,D17-1143,1,0.815895,"re of the training data: parallel aligned corpora, or non-aligned datasets. The aligned corpora enable approaching the problem of form shift as a paraphrasing or machine translation problem. Xu et al. (2012) used statistical and dictionary-based systems on a dataset of original plays by Shakespeare and their contemporary translations. Carlson et al. (2017) trained an LSTM network on 33 versions of the Bible. Jhamtani et al. (2017) used a Pointer Network (Vinyals et al., 2015), an architecture that was successfully applied to a wide variety of tasks (Merity et al., 2016; Gulcehre et al., 2016; Potash et al., 2017), to enable direct copying of the input tokens to the output. All these works use BLEU (Papineni et al., 2002) as the main, or even the only evaluation measure. This is only possible in cases where a parallel corpus is available. Related work As mentioned above, the most relevant previous work comes from research on style transfer2 . It can be divided into two groups: 1. Approaches that aim to generate text in a given form. For example, the task may be to produce just any verse as long as it is in the “style” of the target poet. 2. Approaches that aim to induce a change in either the “form” or"
N19-1088,E17-1107,0,0.0378829,"Missing"
P05-3021,W97-0810,0,0.041464,"Missing"
P05-3021,W01-1315,0,0.0329764,"(Sang and D´ej´ean, 2001); they are currently being implemented as sequences of finite-state transducers along the lines of (A¨ıtMokhtar and Chanod, 1997). Evaluation results are not yet available. 6 SputLink SputLink is a temporal closure component that takes known temporal relations in a text and derives new 83 implied relations from them, in effect making explicit what was implicit. A temporal closure component helps to find those global links that are not necessarily derived by other means. SputLink is based on James Allen’s interval algebra (1983) and was inspired by (Setzer, 2001) and (Katz and Arosio, 2001) who both added a closure component to an annotation environment. Allen reduces all events and time expressions to intervals and identifies 13 basic relations between the intervals. The temporal information in a document is represented as a graph where events and time expressions form the nodes and temporal relations label the edges. The SputLink algorithm, like Allen’s, is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations. For example, if A precedes B and B precedes C, then we can compose the two relations"
P05-3021,N03-2019,1,0.852311,"each event. For example, a past tense non-stative verb followed by a past perfect non-stative verb, with grammatical aspect maintained, suggests that the second event precedes the first. GUTenLINK uses default rules for ordering events; its handling of successive past tense nonstative verbs in case (iii) will not correctly order sequences like Max fell. John pushed him. GUTenLINK is intended as one component in a larger machine-learning based framework for ordering events. Another component which will be developed will leverage document-level inference, as in the machine learning approach of (Mani et al., 2003), which required annotation of a reference time (Reichenbach, 1947; Kamp and Reyle, 1993) for the event in each finite clause. 1 TimeBank is a 200-document news corpus manually annotated with TimeML tags. It contains about 8000 events, 2100 time expressions, 5700 TLINKs and 2600 SLINKs. See (Day et al., 2003) and www.timeml.org for more details. An early version of GUTenLINK was scored at .75 precision on 10 documents. More formal Precision and Recall scoring is underway, but it compares favorably with an earlier approach developed at Georgetown. That approach converted eventevent TLINKs from"
P05-3021,W01-0708,0,0.0278935,"Missing"
P05-3021,P00-1010,1,\N,Missing
P16-1182,N12-1019,0,0.0188925,"rate. Furthermore, correlation of system output with human-derived scores typically provides an overall score but fails to isolate specific errors that metrics tend to miss. This makes it difficult to discover system-specific weaknesses to improve their performance. For instance, an ngram-based metric might effectively detect non-fluent, syntactic errors, but could also be fooled by legitimate paraphrases whose ngrams simply did not appear in the training set. Although there has been some recent work on paraphrasing that provided detailed error analysis of system outputs (Socher et al., 2011; Madnani et al., 2012), more often than not such investigations are seen as above-and-beyond when assessing metrics. The goal of this paper is to propose a process for consistent and informative automated analysis of evaluation metrics. This method is demonstrably more consistent and interpretable than correlation with human annotations. In addition, we extend the SICK dataset to include un-scored fluency-focused sentence comparisons and we propose a toy metric for evaluation. The rest of the paper is as follows: Section 2 introduces the corruption-based metric unit testing process, Section 3 lists the existing met"
P16-1182,2001.mtsummit-papers.68,0,0.335857,"f these entries follow the same “Negated Subject” transformation between sentence 1 and sentence 2, yet humans annotated them with an inconsistently wide range of scores (from 1 to 5). Regardless of whether the gold labels for this particular transformation should score this high or low, they should score be scored consistently. Introduction The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics. Efforts have been made to create standard metrics (Papineni et al., 2001; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2014) to help advance the state-of-the-art. However, most such popular metrics, despite their wide use, have serious deficiencies. Many rely on ngram matching and assume that annotators generate all reasonable reference sentences, which is infeasible for many tasks. Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video captioning. To design better metrics, we need a principled approach to evaluating their performance. Historically, MT metrics have been evaluated by how well they correlate wi"
P16-1182,W10-1703,0,0.0901768,"Missing"
P16-1182,W14-3348,0,0.152753,"egated Subject” transformation between sentence 1 and sentence 2, yet humans annotated them with an inconsistently wide range of scores (from 1 to 5). Regardless of whether the gold labels for this particular transformation should score this high or low, they should score be scored consistently. Introduction The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics. Efforts have been made to create standard metrics (Papineni et al., 2001; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2014) to help advance the state-of-the-art. However, most such popular metrics, despite their wide use, have serious deficiencies. Many rely on ngram matching and assume that annotators generate all reasonable reference sentences, which is infeasible for many tasks. Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video captioning. To design better metrics, we need a principled approach to evaluating their performance. Historically, MT metrics have been evaluated by how well they correlate with human annotations (CallisonBurch et"
P16-1182,W04-1013,0,0.101388,"the same “Negated Subject” transformation between sentence 1 and sentence 2, yet humans annotated them with an inconsistently wide range of scores (from 1 to 5). Regardless of whether the gold labels for this particular transformation should score this high or low, they should score be scored consistently. Introduction The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics. Efforts have been made to create standard metrics (Papineni et al., 2001; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2014) to help advance the state-of-the-art. However, most such popular metrics, despite their wide use, have serious deficiencies. Many rely on ngram matching and assume that annotators generate all reasonable reference sentences, which is infeasible for many tasks. Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video captioning. To design better metrics, we need a principled approach to evaluating their performance. Historically, MT metrics have been evaluated by how well they correlate with human an"
P16-1182,W14-3336,0,0.0151886,"t al., 2014) to help advance the state-of-the-art. However, most such popular metrics, despite their wide use, have serious deficiencies. Many rely on ngram matching and assume that annotators generate all reasonable reference sentences, which is infeasible for many tasks. Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video captioning. To design better metrics, we need a principled approach to evaluating their performance. Historically, MT metrics have been evaluated by how well they correlate with human annotations (CallisonBurch et al., 2010; Machacek and Bojar, 2014). However, as we demonstrate in Sec. 5, human judgment can result in inconsistent scoring. This presents a serious problem for determining whether 1935 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1935–1943, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics a metric is ”good” based on correlation with inconsistent human scores. When ”gold” target data is unreliable, even good metrics can appear to be inaccurate. Furthermore, correlation of system output with human-derived scores typically provides an overall s"
P16-1182,2006.amta-papers.25,0,0.0607231,"e candidate and reference sentences. Commonly-used metrics include BLEU, CIDEr, and TER: • BLEU, an early MT metric, is a precisionbased metric that rewards candidates whose words can be found in the reference but penalizes short sentences and ones which overuse popular n-grams (Papineni et al., 2001). • CIDEr, an image captioning metric, uses a consensus-based voting of tf-idf weighted ngrams to emphasize the most unique segments of a sentence in comparison (Vedantam et al., 2014). • TER (Translation Edit Rate) counts the changes needed so the surface forms of the output and reference match (Snover et al., 2006). Other metrics have attempted to capture similarity beyond surface-level pattern matching: • METEOR, rather than strictly measuring ngram matches, accounts for soft similarities between sentences by computing synonym and paraphrase scores between sentence alignments (Denkowski and Lavie, 2014). We evaluate the strengths and weaknesses of these existing metrics in Section 7. 3.2 Toy Metric: W2V-AVG To demonstrate how this paper’s techniques can also be applied to measure a new evaluation metric, we create a toy metric, W2V-AVG, using the cosine of the centroid of a sentence’s word2vec embeddin"
P16-1182,P02-1040,0,\N,Missing
P16-1182,marelli-etal-2014-sick,0,\N,Missing
P18-1049,Q14-1022,0,0.534122,"e resulting timegraph. Common ways of ameliorating the conflicts is to apply some ad hoc constraints to account for basic properties of relations (e.g. transitivity), often without considering the content of the text per se. For example, Ling and Weld (2010) designed transitivity formulae, used with local features. Sun (2014) proposed a strategy that “prefers the edges that can be inferred by other edges in the graph and remove the ones that are least so”. Another approach is to use the results from separate classifiers to rank results according to their general confidence (Mani et al., 2007; Chambers et al., 2014). High-ranking results overwrite low-ranking ones. Meng et al. (2017) used a greedy pruning algorithm to remove weak edges from the timegraph until it is coherent. When humans read text, we certainly do not follow the procedure of interpreting interpret relations only locally first, and later come up with a compromise solution that involves all the entities. Instead, if local information is insufficient, we consider the relevant information from the wider context, and resolve the ambiguity as soon as possible. The resolved relations are stored in our We propose a context-aware neural network m"
P18-1049,D17-1092,1,0.668045,"ply some ad hoc constraints to account for basic properties of relations (e.g. transitivity), often without considering the content of the text per se. For example, Ling and Weld (2010) designed transitivity formulae, used with local features. Sun (2014) proposed a strategy that “prefers the edges that can be inferred by other edges in the graph and remove the ones that are least so”. Another approach is to use the results from separate classifiers to rank results according to their general confidence (Mani et al., 2007; Chambers et al., 2014). High-ranking results overwrite low-ranking ones. Meng et al. (2017) used a greedy pruning algorithm to remove weak edges from the timegraph until it is coherent. When humans read text, we certainly do not follow the procedure of interpreting interpret relations only locally first, and later come up with a compromise solution that involves all the entities. Instead, if local information is insufficient, we consider the relevant information from the wider context, and resolve the ambiguity as soon as possible. The resolved relations are stored in our We propose a context-aware neural network model for temporal information extraction, with a uniform architecture"
P18-1049,P17-2001,0,0.180738,"Missing"
P18-1049,S16-1198,0,0.0272273,"versity of Massachusetts Lowell arum@cs.uml.edu Yuanliang Meng Text Machine Lab for NLP Department of Computer Science University of Massachusetts Lowell ymeng@cs.uml.edu Abstract (RNN) models (Meng et al., 2017; Cheng and Miyao, 2017; Tourille et al., 2017) and convolutional neural networks (CNN) (Lin et al., 2017). These models predominantly use token embeddings as input, avoiding handcrafted features for each task. Typically, neural network models outperform traditional statistical models. Some studies also try to combine neural network models with rule-based information retrieval methods (Fries, 2016). These systems require different models for different pair types, so several models must be combined to fully process text. A common disadvantage of all these models is that they build relations from isolated pairs of entities (events or temporal expressions). This context-blind, pairwise classification often generates conflicts in the resulting timegraph. Common ways of ameliorating the conflicts is to apply some ad hoc constraints to account for basic properties of relations (e.g. transitivity), often without considering the content of the text per se. For example, Ling and Weld (2010) desi"
P18-1049,C16-1007,0,0.159246,"Missing"
P18-1049,S15-2135,0,0.0168836,"tion about global context into discourse-scale processing of natural text. 1 Introduction Extracting information about the order and timing of events from text is crucial to any system that attempts an in-depth natural language understanding, whether related to question answering, temporal inference, or other related tasks. Earlier temporal information extraction (TemporalIE) systems tended to rely on traditional statistical learning with feature-engineered task-specific models, typically used in succession (Yoshikawa et al., 2009; Ling and Weld, 2010; Sun et al., 2013; Chambers et al., 2014; Mirza and Minard, 2015). Recently, there have been some attempts to extract temporal relations with neural network models, particularly with recurrent neural networks 527 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 527–536 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics memory as “context” for further processing. If the later evidence suggests our early interpretation was wrong, we can correct it. This paper proposes a model to simulate such mechanisms. Our model introduces a Global Context Layer (GCL), inspir"
P18-1049,P17-2035,0,0.0635568,"Missing"
P18-1049,P82-1020,0,0.810306,"Missing"
P18-1049,J81-4005,0,0.714904,"Missing"
P18-1049,P09-1046,0,0.416675,"GCL is also the first model to use an NTM-like architecture to incorporate the information about global context into discourse-scale processing of natural text. 1 Introduction Extracting information about the order and timing of events from text is crucial to any system that attempts an in-depth natural language understanding, whether related to question answering, temporal inference, or other related tasks. Earlier temporal information extraction (TemporalIE) systems tended to rely on traditional statistical learning with feature-engineered task-specific models, typically used in succession (Yoshikawa et al., 2009; Ling and Weld, 2010; Sun et al., 2013; Chambers et al., 2014; Mirza and Minard, 2015). Recently, there have been some attempts to extract temporal relations with neural network models, particularly with recurrent neural networks 527 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 527–536 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics memory as “context” for further processing. If the later evidence suggests our early interpretation was wrong, we can correct it. This paper proposes a model"
P18-1049,W17-2341,0,0.13958,"Missing"
pustejovsky-etal-2006-towards,W91-0217,0,\N,Missing
pustejovsky-etal-2006-towards,bel-etal-2000-simple,0,\N,Missing
pustejovsky-etal-2006-towards,havasi-etal-2006-bulb,1,\N,Missing
rumshisky-etal-2012-word,D08-1027,0,\N,Missing
rumshisky-etal-2012-word,S07-1001,0,\N,Missing
rumshisky-etal-2012-word,N06-2015,0,\N,Missing
rumshisky-etal-2012-word,P06-1014,0,\N,Missing
rumshisky-etal-2012-word,W10-0731,0,\N,Missing
rumshisky-etal-2012-word,W11-0409,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0807,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0813,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-1908,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,C04-1133,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,briscoe-carroll-2002-robust,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,pustejovsky-etal-2006-towards,1,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0834,0,\N,Missing
rumshisky-pustejovsky-2006-inducing,W04-0828,0,\N,Missing
S10-1005,J05-1004,0,0.277012,"in argument selection. For example, in (2) below, the sense annotation for the verb enjoy should arguably assign similar values to both (2a) and (2b). Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then 27 Proceedings of"
S10-1005,burchardt-etal-2006-salsa,0,0.0282901,"nnotation for the verb enjoy should arguably assign similar values to both (2a) and (2b). Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then 27 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010"
S10-1005,W04-1908,1,0.864145,"Missing"
S10-1005,W09-3716,1,0.701906,". Train: Algorithm is trained over a corpus annotated with the target feature set; Test: Algorithm is tested against held-out data; Evaluate: Standardized evaluation of results; Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), and TimeBank (Pustejovsky et al., 2005). 1 This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 28 e. Hear, sense perceive physical sound : HUMAN hear SOUND We used a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). Types were selected for their prevalence in manually identified selection context patterns developed for several hundred English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The types used for annotation were: ABSTRACT ENTITY, ANIMATE , ARTIFACT, A"
S10-1005,J91-4003,1,0.586536,"whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Valeria Quochi ILC-CNR Pisa, Italy (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selectio"
S10-1005,W08-1206,1,0.854565,"rchitecture 4.1 Data Set Construction Phase: English For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: This set of types is purposefully shallow and non-hierarchical. For example, HUMAN is a subtype of both ANIMATE and PHYSICAL OB JECT , but annotators and system developers were instructed to choose the most relevant type (e.g., HUMAN) and to ignore inheritance. 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 3. A set of sentences was randomly extracted for each target verb from the BNC (Burnard, 1995). The extracted sentences were parsed automatically, and the sentences organized according to the grammatical relation the target verb was involved in. Sentences were excluded from the set if the target argument was expressed as anaphor, or was not present in the sentence. The semantic he"
S10-1005,bel-etal-2000-simple,0,0.23492,"nformare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. HUMAN ARTIFACT 7. The Italian training data contained 1466 instances, 381 of which are coercions; the test data had 1463 instances, with 384 coercions. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates were adapted from the Italian Pattern Dictionary (Hanks and Jezek, 2007) and mapped to their SIMPLE equivalents (Lenci et al., 2000). a. arrivare, sense reach a location: [prep] LOCATION → HUMAN (accusare, annunciare) → HUMAN (annunciare, avvisare) EVENT → LOCATION (arrivare, raggiungere) ARTIFACT → EVENT (cominciare, completare) EVENT → DOCUMENT (leggere, divorare) HUMAN → DOCUMENT (leggere, divorare) EVENT → SOUND (ascoltare, echeggiare) ARTIFACT → SOUND (ascoltare, echeggiare) LOCATION 5 Data Format The test and training data were provided in XML. The relation between the predicate (viewed as a function) and its argument were represented by composition link elements (CompLink), as arriva 30 shown below. The test data di"
S10-1005,verhagen-2010-brandeis,0,0.0257747,"en argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: 4. Word sense disambiguation of the target predicate was performed manually on each extracted sentence, matching the target against the sense inventory and the corresponding type templates as described above. The appropriate senses were then saved into the database along with the associated type template. 5. The sentences containing coercive senses of the target verbs were loaded into the Brandeis Annotation Tool (Verhagen, 2010). Annotators were presented with a list of sentences and asked to determine whether the argument in the specified grammatical relation to the target belongs to the type associated with that sense in the corresponding template. Disagreements were resolved by adjudication. a. Arrive (at), sense reach a destination or goal : HU MAN arrive at LOCATION b. Cancel, sense call off : HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PROPOSITION d. Finish, sense complete an activity: HUMAN finish EVENT 29 Coerion Type EVENT → LOCATION ARTIFACT → EVENT EVENT → PROPO"
S10-1005,S07-1007,0,0.0214918,"in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Valeria Quochi ILC-CNR Pisa, Italy (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly. In fact, the metonymy example in (1) is an instance of a much more pervasive phen"
S10-1005,W04-2705,0,0.0983243,"Missing"
S10-1005,miltsakaki-etal-2004-penn,0,\N,Missing
S10-1005,S07-1016,0,\N,Missing
S10-1005,E06-2001,0,\N,Missing
S10-1005,C04-1133,1,\N,Missing
S10-1005,ide-suderman-2004-american,0,\N,Missing
S10-1005,ohara-2008-lexicon,0,\N,Missing
S15-2107,P11-2008,0,0.130697,"Missing"
S15-2107,S14-2086,0,0.0449516,"Missing"
S15-2107,S14-2111,0,0.0118957,"over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The top system for Subtask A in both 2013 and 2014 from NRC Canada (Mohammad et al., 2013; Zhu et al., 2014) used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics setup to learn sentiment-specific word embedding features along with state-of-the-art hand-crafted features (Tang et al., 2014). Our goal in developing TwitterHawk was to build on the success of feature-driven approaches established as state-of-the-art in the two previous years of SemEval Twitter Sentiment Analysis competi"
S15-2107,S13-2053,0,0.0729708,"time communication of sentiment, thus providing unprecedented insight into how well-received products, events, and people are in the public’s eye. But working with this new genre is challenging. Twitter imposes a 140-character limit on messages, which causes users to use novel abbreviations and often disregard standard sentence structures. Most systems that participated in this task over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The top system for Subtask A in both 2013 and 2014 from NRC Canada (Mohammad et al., 2013; Zhu et al., 2014) used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, c Denver, Colorado, June"
S15-2107,S12-1033,0,0.0271454,"ature value from 1 to -1. Lexicon Features We used several Twitterspecific and general-purpose lexicons. The lexicons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numer"
S15-2107,S13-2052,0,0.0421851,"provided categories for words and phrases included the number of words that belonged to each category. For phrase-level analysis, the text span used for these features was the target phrase itself. For the tweet-level analysis, the text span covered the whole tweet. Table 1 shows which lexicons we used when building each classifier. 3 Results In this section, we describe the experiments we conducted during system development, as well as the official SemEval Task 10 results. The scores reported throughout this section are calculated as the average of the positive and negative class F-measure (Nakov et al., 2013); the neutral label classification does not directly affect the score. 3.1 System Development Experiments Both phrase-level and tweet-level systems were tuned in 10-fold cross-validation using the 2013 training, dev, and test data (Nakov et al., 2013). We 644 used fixed data folds in order to compare different runs. Feature ablation studies, parameter tuning, and comparison of different pre-processing steps were performed using this setup. We conducted ablation studies for lexicon features using tweet-level evaluation. Table 2 shows ablation results obtained in 10-fold cross-validation. The fi"
S15-2107,N13-1039,0,0.0603426,"Missing"
S15-2107,W02-1011,0,0.0198582,"ith a regex. 642 For hashtags that are in the manually segmented list, we use the segmentation that we identified as correct. If h is an acronym, we do not segment it. Finally, for CamelCase, we treat the capitalization as indicating the segment boundaries. Normalization and Negation During the normalization phrase, all tokens are lowercased. Next, we replace URLs, user mentions, and numbers with generic URL, USER, and NUMBER tokens, respectively. The remaining tokens are stemmed using NLTKs Snowball stemmer (Bird et al., 2009). We also process negation contexts following the strategy used by Pang et al. (2002). We define a negation context to be a text span that begins with a negation word (such as ‘no’) and ends with a punctuation mark, hashtag, user mention, or URL. The suffix neg is appended to all words inside of a negation context. We use the list of negation words from Potts (2011). 2.2 Machine Learning For the phrase-level sentiment classification, we trained a linear Support Vector Machine (SVM) using scikit-learn’s LinearSVC (Pedregosa et al., 2011) on the Subtask A training data, which contained 4832 positive examples, 2549 negative, and 384 neutral. The regularization parameter was set t"
S15-2107,S15-2078,0,0.305078,"achusetts Lowell 198 Riverside St, Lowell, MA 01854, USA {wboag,ppotash,arum}@cs.uml.edu Abstract For the past three years, the International Workshop on Semantic Evaluation (SemEval) has been hosting a task dedicated to sentiment analysis of Twitter data. This year, our team participated in four subtasks of the challenge: Contextual Polarity Disambiguation (phrase-level), B: Message Polarity Classification (tweet-level), C: Topic-Based Message Polarity Classification (topic-based), and D: Detecting Trends Towards a Topic (trending sentiment). For a more thorough description of the tasks, see Rosenthal et al. (2015). Our system placed 1st out of 7 submissions for topic-based sentiment prediction (Subtask C), 3rd out of 6 submissions for detecting trends toward a topic (Subtask D), 10th out of 40 submissions for tweet-level sentiment prediction (Subtask B), and 5th out of 11 for phrase-level prediction (Subtask A). Our system also ranked 4th out of 40 submissions in identifying the sentiment of sarcastic tweets. This paper describes TwitterHawk, a system for sentiment analysis of tweets which participated in the SemEval-2015 Task 10, Subtasks A through D. The system performed competitively, most notably p"
S15-2107,S14-2033,0,0.0366416,"ts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics setup to learn sentiment-specific word embedding features along with state-of-the-art hand-crafted features (Tang et al., 2014). Our goal in developing TwitterHawk was to build on the success of feature-driven approaches established as state-of-the-art in the two previous years of SemEval Twitter Sentiment Analysis competitions. We therefore focused on identifying and incorporating the strongest features used by the best systems, most notably, sentiment lexicons that showed good performance in ablation studies. We also performed multiple rounds of pre-processing which included tokenization, spelling correction, hashtag segmentation, wordshape replacement of URLs, as well as handling negated contexts. Our main insight"
S15-2107,H05-1044,0,0.0545935,"icons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numeric score included: • the average sentiment score for the text span; • the total number of positively scored words in"
S15-2107,S14-2077,0,\N,Missing
S16-1115,S12-1051,0,0.19148,"d systems used either a simple LSTM architecture or a Tree-LSTM structure. We found that of the three base systems, the feature-based model obtained the best results, outperforming each LSTM-based model’s correlation by .06. Ultimately, the ensemble system was able to outperform the base systems substantially, obtaining a weighted Pearson correlation of 0.738, and placing 7th out of 115 participating systems. We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches."
S16-1115,S13-1004,0,0.114202,"a simple LSTM architecture or a Tree-LSTM structure. We found that of the three base systems, the feature-based model obtained the best results, outperforming each LSTM-based model’s correlation by .06. Ultimately, the ensemble system was able to outperform the base systems substantially, obtaining a weighted Pearson correlation of 0.738, and placing 7th out of 115 participating systems. We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task"
S16-1115,S14-2010,0,0.0813226,"ecture or a Tree-LSTM structure. We found that of the three base systems, the feature-based model obtained the best results, outperforming each LSTM-based model’s correlation by .06. Ultimately, the ensemble system was able to outperform the base systems substantially, obtaining a weighted Pearson correlation of 0.738, and placing 7th out of 115 participating systems. We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual"
S16-1115,S15-2045,0,0.0911027,"structure. We found that of the three base systems, the feature-based model obtained the best results, outperforming each LSTM-based model’s correlation by .06. Ultimately, the ensemble system was able to outperform the base systems substantially, obtaining a weighted Pearson correlation of 0.738, and placing 7th out of 115 participating systems. We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has be"
S16-1115,P14-1023,0,0.0123884,"15), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic relatedness (Marelli et al., 2014a). The ∗ These three authors contributed equally to this work. Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output a real number in the range [0,5] where 0 means lowest and 5 means highest similarity. The top-performing system from last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015; H¨anig et al., 741 Proceedings of SemEval-2016, pages 741–748, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2015) follow this trend of exploiting word alignment and embedding similarity across textual pairs. Recent work (Bowman et al., 2014; Bowman et al., 2015b; Tai et al., 2015) has shown the effectiveness of deep learning end-to-end architectures using recurrent and recursive neural networks on tasks similar to semantic similarity, such as semantic relatedne"
S16-1115,D15-1075,0,0.154775,"om last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015; H¨anig et al., 741 Proceedings of SemEval-2016, pages 741–748, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2015) follow this trend of exploiting word alignment and embedding similarity across textual pairs. Recent work (Bowman et al., 2014; Bowman et al., 2015b; Tai et al., 2015) has shown the effectiveness of deep learning end-to-end architectures using recurrent and recursive neural networks on tasks similar to semantic similarity, such as semantic relatedness (Tai et al., 2015), natural language inference (Bowman et al., 2015a), and textual entailment (Marelli et al., 2014b), providing state-of-theart performance. Since all of these tasks evaluate the relationship between the semantics of two input sentences, it stands to reason that systems with such architecture may perform well on the more general task of semantic similarity. In our submissio"
S16-1115,D14-1082,0,0.0311438,"ally equations (14) and (15)), the network produces a probability distribution over scores and is trained using categorical cross-entropy loss. To build the network, we use the Keras library (Chollet, 2015), a Python neural network library written on top of Theano (Bergstra et al., 2010; Bastien et al., 2012). states of an arbitrary number of child units. As previously mentioned, the Tree-LSTM architecture has been used for the semantic relatedness task. For this task, the specific tree structure the authors use is a dependency tree, obtained via the Stanford Neural Network Dependency Parser (Chen and Manning, 2014). For the semantic relatedness task, the system takes as input two pieces of text, and outputs a real number in the range [1,5]. Therefore, one only needs to modify the original Tree-LSTM system to output a number in the range [0,5] in order to apply it to the task of semantic similarity. The specific architecture we use is referred to by the original authors as Child-Sum Tree-LSTM (Tai et al., 2015). Using the authors’ original notation, and using C(j) to denote the set of children of node j, the hidden state at a given node is computed as follows: h˜j = X hk (4) k∈C(j) 2.2 Deep End-to-End LS"
S16-1115,W05-1203,0,0.0435012,"base systems substantially, obtaining a weighted Pearson correlation of 0.738, and placing 7th out of 115 participating systems. We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as"
S16-1115,S15-2031,0,0.0815777,"mble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic rel"
S16-1115,S15-2046,0,0.0239386,"Missing"
S16-1115,W05-0909,0,0.169127,"Missing"
S16-1115,N12-1019,0,0.0591732,"Missing"
S16-1115,S14-2001,0,0.128233,"t al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic relatedness (Marelli et al., 2014a). The ∗ These three authors contributed equally to this work. Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output a real number in the range [0,5] where 0 means lowest and 5 means highest similarity. The top-performing system from last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015;"
S16-1115,marelli-etal-2014-sick,0,0.119459,"t al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic relatedness (Marelli et al., 2014a). The ∗ These three authors contributed equally to this work. Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output a real number in the range [0,5] where 0 means lowest and 5 means highest similarity. The top-performing system from last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015;"
S16-1115,S12-1053,0,0.0612245,"Missing"
S16-1115,P02-1040,0,0.0979865,"Missing"
S16-1115,D14-1162,0,0.0768753,"r in range [a,b] by setting r equal to a vector of integer values [a ... b ]. For our system, we simply set r equal to the vector [0 1 2 3 4 5], as opposed to the original values of [1 2 3 4 5] for the semantic relatedness task. The inclusion of zero will better facilitate the output of scores less than one. This is because distributions that have their mass highly concentrated in the first index will have little remaining mass in the other indices that can then multiply by the non-zero integers. For our implementation we used the code from the original authors9 , which uses Glove embeddings (Pennington et al., 2014) as input vectors xj , hidden states (hi ) of size 200, and a similarity module of size 50 (hs (13)). For training, we use a regularization strength of 0.0004, a minibatch size of 25, and a learning rate of 0.05. The model trained for 10 epochs. We also decided to disregard the use of a validation set, since our intermediate results showed that, if anything, the use of a validation set hurt performance. 2.2.2 LSTM Despite the potential of the Tree-LSTM model for this task, (Bowman et al., 2015b) shows that even for text that is highly syntactically dependent, a standard LSTM architecture can p"
S16-1115,2006.amta-papers.25,0,0.0409596,"Missing"
S16-1115,Q14-1018,0,0.0136772,"n that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic relatedness (Marelli et al., 2014a). The ∗ These three authors contributed equally to this work. Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output a real number in the range [0,5] where 0 means lowest and 5 means highest similarity. The top-performing system from last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015; H¨anig et al., 741 Proceedings of SemEval-2016, pages 741–748, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2015) follow this trend of exploiting word alignment and embedding similarity across textual pairs. Recent work (Bowman et al., 2014; Bowman et al., 2015b; Tai et al., 2015) has shown the effectiveness of deep learning end-to-end architectures using rec"
S16-1115,S15-2027,0,0.0484691,"We find that the ensemble system’s success comes largely from its ability to form a consensus and eliminate complementary noise from its base systems’ predictions. 1 The STS task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015) has aggregated a sizable dataset of sentence pairs annotated with numeric similarity scores. The presence of this dataset allows for a shift from earlier work that mostly used unsupervised learning (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006), to the supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012)"
S16-1115,P15-1150,0,0.693517,"ultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Baroni et al., 2014) to create a two-feature regression model. Other topperforming systems (Han et al., 2015; H¨anig et al., 741 Proceedings of SemEval-2016, pages 741–748, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2015) follow this trend of exploiting word alignment and embedding similarity across textual pairs. Recent work (Bowman et al., 2014; Bowman et al., 2015b; Tai et al., 2015) has shown the effectiveness of deep learning end-to-end architectures using recurrent and recursive neural networks on tasks similar to semantic similarity, such as semantic relatedness (Tai et al., 2015), natural language inference (Bowman et al., 2015a), and textual entailment (Marelli et al., 2014b), providing state-of-theart performance. Since all of these tasks evaluate the relationship between the semantics of two input sentences, it stands to reason that systems with such architecture may perform well on the more general task of semantic similarity. In our submission, we are interested"
S16-1115,S15-2001,0,0.0263804,"e supervised approaches that leverage the labeled data (Sultan et al., 2015; Han et al., 2015; H¨anig et al., 2015). The availability of labeled data from different genres also allows for a clearer evaluation and a better comparison across different approaches. Introduction The task of semantic textual similarity (STS) has been developed over the past several SemEval competitions with the idea of capturing the degree of similarity in the meaning conveyed by two snippets of text (usually two sentences). In that respect, the STS task can be seen as similar to such tasks as paraphrase detection (Xu et al., 2015), recognizing textual entailment (RTE) (Negri et al., 2012), and semantic relatedness (Marelli et al., 2014a). The ∗ These three authors contributed equally to this work. Specifically, the task of semantic similarity is defined as follows: given an input of two sentences, output a real number in the range [0,5] where 0 means lowest and 5 means highest similarity. The top-performing system from last year’s task (Sultan et al., 2015) relied heavily on a hand-engineered word alignment tool (Sultan et al., 2014) (achieving 5th place with the aligner alone), combined with dense word embeddings (Bar"
S17-2004,H05-1067,0,0.775554,"5). Whereas for the HW segment, viewers submit a tweet in response to a hashtag, for the NYCC readers submit humorous captions in response to a cartoon. It is important to note this key distinction between the two datasets, because we believe that the presence of the hashtag allows for further innovative NLP methodologies aside from solely analyzing the tweets themselves. In Radev et al. (2015), the authors developed more than 15 unsupervised methods for ranking submissions for the NYCC. The methods can be categorized into broader categories such as originality and content-based. Related Work Mihalcea and Strapparava (2005) developed a humor dataset of puns and humorous one-liners intended for supervised learning. In order to generate negative examples for their experimental design, the authors used news titles from Reuters and the British National Corpus, as well as proverbs. Recently, Yang et al. (2015) used the same dataset for experimental purposes, taking text from AP News, New York Times, Yahoo! Answers, and proverbs as their negative examples. To further reduce the bias of their negative examples, the authors selected negative examples with a vocabulary that is in the dictionary created from the positive"
S17-2004,S17-2065,0,0.0265591,"n the dataset. The approaches used by teams varied from n-gram language models, word association, to semantic relatedness features. In addition, the TakeLab team used cultural reference features, such as movie and song references, and Google Trends features for named entities. During the performed analysis, the team found these features most useful for the model. Considering neural network-based systems, LSTMs were used the most, which is expected given the sequential nature of text data. Plain LSTM models alone, using pretrained word embeddings, achieved competitive results, and DataStories (Baziotis et al., 2017) ranked third using a siamese bidirectional LSTM model with attention. One key difference between the dataset used in this task and the datasets based on the NYCC (Radev et al., 2015; Shahaf et al., 2015) is the presence of the hashtag. Some teams used additional hashtag-based features in their systems. For example, humor patterns, defined by the hashtag, were one of the most important features for the TakeLab team. Other teams used semantic distances between the hashtag and tweets as features. Table 1 also includes the standard deviation of system scores across the hashtags. Looking at the nu"
S17-2004,W06-1625,0,0.582341,"Missing"
S17-2004,S17-2067,0,0.0532323,"entations as external knowledge for the neural network systems. This is in opposition to other systems that relied on the output of separate tools, or looking up terms in corpora. Some teams, such as HumorHawk8 (Donahue et al., 2017) and #WarTeam, used a combination of these two types of systems, and notably, the system that was ranked first in Subtask A (HumorHawk) was an ensemble system that utilized prediction from both feature-based and neural networks-based models. As for the feature-based systems, one trend we observed is that many teams tried to capture the incongruity aspect of humor (Cattle and Ma, 2017) , often present in the dataset. The approaches used by teams varied from n-gram language models, word association, to semantic relatedness features. In addition, the TakeLab team used cultural reference features, such as movie and song references, and Google Trends features for named entities. During the performed analysis, the team found these features most useful for the model. Considering neural network-based systems, LSTMs were used the most, which is expected given the sequential nature of text data. Plain LSTM models alone, using pretrained word embeddings, achieved competitive results,"
S17-2004,S17-2010,1,0.826133,"Missing"
S17-2004,N12-2012,0,0.076002,"Missing"
S17-2004,S17-2063,0,0.013638,"n the training data had the word ‘Song’. Likewise, five hashtags had the word ‘Celeb’, and there was one more hashtag with the word ‘Broadway’. Alternatively, The two hashtags with the best performance followed the ‘X in X words’ format, for which there were 16 such hashtags in the training data. Regarding the hashtag BadJobIn5Words, there are six hashtags in the training data beginning with the word ‘Bad’. System Analysis For the teams that participated in both subtasks, they used the output of a single system to predict for both subtasks. Two teams, SVNIT (Mahajan and Zaveri, 2017) and QUB (Han and Toner, 2017) , initially predicted the labels of each tweet based on the output of a supervised classifier, and then used these labels to both rank the tweets and make pairwise predictions for the subtasks. Duluth took a similar approach, but used the output of a language model to rank the tweets, as opposed to labels provided by a classifier. Conversely, TakeLab sought to solve subtask A first, then used the frequencies of a tweet being chosen as funnier in a pair to provide a single, ordered metric to make predictions for subtask B. The team that only participated in subtask B, #WarTeam, also used the o"
S17-2004,P11-2016,0,0.671837,"Missing"
S17-2004,S17-2064,0,0.133337,"Missing"
S17-2004,S17-2066,0,0.0697993,"Missing"
S17-2004,D15-1284,0,0.403369,"ative NLP methodologies aside from solely analyzing the tweets themselves. In Radev et al. (2015), the authors developed more than 15 unsupervised methods for ranking submissions for the NYCC. The methods can be categorized into broader categories such as originality and content-based. Related Work Mihalcea and Strapparava (2005) developed a humor dataset of puns and humorous one-liners intended for supervised learning. In order to generate negative examples for their experimental design, the authors used news titles from Reuters and the British National Corpus, as well as proverbs. Recently, Yang et al. (2015) used the same dataset for experimental purposes, taking text from AP News, New York Times, Yahoo! Answers, and proverbs as their negative examples. To further reduce the bias of their negative examples, the authors selected negative examples with a vocabulary that is in the dictionary created from the positive examples. Also, the authors forced the negative examples to have a similar text length compared to the positive examples. Zhang and Liu (2014) constructed a dataset for recognizing humor in Twitter in two parts. First, 2 Alternatively, Shahaf et al.(2015) approach the NYCC dataset with"
S17-2004,S17-2069,0,0.0285118,"ng data. For example, 12 hashtags in the training data had the word ‘Song’. Likewise, five hashtags had the word ‘Celeb’, and there was one more hashtag with the word ‘Broadway’. Alternatively, The two hashtags with the best performance followed the ‘X in X words’ format, for which there were 16 such hashtags in the training data. Regarding the hashtag BadJobIn5Words, there are six hashtags in the training data beginning with the word ‘Bad’. System Analysis For the teams that participated in both subtasks, they used the output of a single system to predict for both subtasks. Two teams, SVNIT (Mahajan and Zaveri, 2017) and QUB (Han and Toner, 2017) , initially predicted the labels of each tweet based on the output of a supervised classifier, and then used these labels to both rank the tweets and make pairwise predictions for the subtasks. Duluth took a similar approach, but used the output of a language model to rank the tweets, as opposed to labels provided by a classifier. Conversely, TakeLab sought to solve subtask A first, then used the frequencies of a tweet being chosen as funnier in a pair to provide a single, ordered metric to make predictions for subtask B. The team that only participated in subtas"
S17-2010,S17-2004,1,0.828235,"Missing"
S17-2010,L16-1079,0,0.0625718,"Missing"
S17-2010,S15-2107,1,0.836838,"neme Model corresponding phonemes. We use a 0.6/0.4 traintest split. Once the model is trained, we extract the intermediate embedding state (200 dim) between the encoder and decoder; this acts as a phonetic embedding, containing all information needed to pronounce the word. The resulting phonetic embedding for each word is concatenated with a semantic embedding to serve as the input for the embedding humor model (see below). Table 3.1 shows sample output of the model. 3.2 1. Sentiment of each tweet in a pair, obtained with TwitterHawk, a state-of-the-art sentiment analysis system for Twitter (Boag et al., 2015). 2. Sentiment of the tokenized hashtag. 3. Length of each tweet in both tokens and characters (a very long tweet might not be funny) 4. Distance of the average GloVe embeddings of the tokens of the tweets to the global centroid of the embeddings of all tweets for the given hashtag. 5. Minimum, maximum and average distance from each token in a tweet to the hashtag. 6. Number of tokens belonging to the top-10 most frequent POS tags on the training data. Embedding Humor Model For both tweets in a tweet pair, a concatenation of a GloVe word embedding (Pennington et al., 2014) and phonetic embeddi"
S17-2010,S16-1091,0,0.0218662,"encoders are fed into dense layers. For encoder input N , the three dense layers are of size (3/4)N , (1/2)N , and 1. Each layer gradually reduces dimensionality to final binary decision. Character Humor Model 3.6 The character-based humor model processes each tweet as a sequence of characters with a CNN (Koushik, 2016). 30-dimensional embeddings are learned per character as input. The output of the CNN for both tweets in the pair are inserted into dense layers. 3.4 Embedding/Character Joint Model Ensemble Model Inspired by the success of ensemble models in other tasks (Potash et al., 2016a; Rychalska et al., 2016) we built an ensemble model that combines the predictions of the character-based model, embedding-based model, the character/embedding joint humor model, and the feature-based XGBoost model to make the final prediction which incorporates different views of the input data. For the ensemble model itself, we use an XGBoost model again. Input predictions are obtained by using 5-fold cross-validation on the training data. XGBoost Feature-Based Model In order to approach the problem from a different prospective, in addition to the neural networkbased systems described above, we constructed a feature"
S17-2010,N16-1079,0,0.140584,"an ensemble model which achieved 0.675 accuracy in the official task evaluation. 1 Introduction Computational approaches to how humour is expressed in language have received relatively limited attention up until very recently. With few exceptions, they have used feature-based machine learning techniques (Zhang and Liu, 2014; Radev et al., 2015) drawing on hand-engineered features such as sentence length, the number of nouns, number of adjectives, and tf-idf-based LexRank (Erkan and Radev, 2004). Among the recent proposals, puns have been emphasized as a crucial component of humor expression (Jaech et al., 2016). Others have proposed that text is perceived as humorous when it deviates in some way from what is expected (Radev et al., 2015). One of the reasons for such dominant position of the feature-based approaches is the fact that the datasets have been relatively small, rendering deep learning methods ineffective. Furthermore, exist98 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 98–102, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics riety of syntactic and semantic features to detect irony in tweets. To summari"
S17-2010,D14-1162,0,0.0830173,"alysis system for Twitter (Boag et al., 2015). 2. Sentiment of the tokenized hashtag. 3. Length of each tweet in both tokens and characters (a very long tweet might not be funny) 4. Distance of the average GloVe embeddings of the tokens of the tweets to the global centroid of the embeddings of all tweets for the given hashtag. 5. Minimum, maximum and average distance from each token in a tweet to the hashtag. 6. Number of tokens belonging to the top-10 most frequent POS tags on the training data. Embedding Humor Model For both tweets in a tweet pair, a concatenation of a GloVe word embedding (Pennington et al., 2014) and phonetic embedding is processed by an LSTM encoder at each time-step (per word). We use word embeddings pre-trained on a Twitter corpus, available on the GloVe website1 . Zero padding is added to the end of each tweet for a maximum length of 20 words/tweet. The output of each LSTM encoder (800 dim) is inserted into dense layers, and a binary classification decision is generated. 3.3 3.5 The output of the embedding model LSTM encoders and the character model CNN encoders are fed into dense layers. For encoder input N , the three dense layers are of size (3/4)N , (1/2)N , and 1. Each layer"
S17-2010,S16-1115,1,0.844319,"he character model CNN encoders are fed into dense layers. For encoder input N , the three dense layers are of size (3/4)N , (1/2)N , and 1. Each layer gradually reduces dimensionality to final binary decision. Character Humor Model 3.6 The character-based humor model processes each tweet as a sequence of characters with a CNN (Koushik, 2016). 30-dimensional embeddings are learned per character as input. The output of the CNN for both tweets in the pair are inserted into dense layers. 3.4 Embedding/Character Joint Model Ensemble Model Inspired by the success of ensemble models in other tasks (Potash et al., 2016a; Rychalska et al., 2016) we built an ensemble model that combines the predictions of the character-based model, embedding-based model, the character/embedding joint humor model, and the feature-based XGBoost model to make the final prediction which incorporates different views of the input data. For the ensemble model itself, we use an XGBoost model again. Input predictions are obtained by using 5-fold cross-validation on the training data. XGBoost Feature-Based Model In order to approach the problem from a different prospective, in addition to the neural networkbased systems described above"
W04-1908,briscoe-carroll-2002-robust,0,0.0330228,"lements are members. Each feature may be realized by a number of RASP relations. For instance, a feature dealing with objects would take into account RASP relations ’dobj’, ’obj2’, and ’ncsubj’ (for passives). 3 Current Implementation The CPA patterns are developed using the British National Corpus (BNC). The sorted instances are used as a training set for the supervised disambiguation. For the disambiguation task, each pattern is translated into into a set of preprocessing-specific features. The BNC is preprocessed with the RASP parser and semantically tagged with BSO types. The RASP system (Briscoe and Carroll (2002)) generates full parse trees for each sentence, assigning a probability to each parse. It also produces a set of grammatical relations for each parse, specifying the relation type, the headword, and the dependent element. All our computations are performed over the single top-ranked tree for the sentences where a full parse was successfully obtained. Some of the RASP grammatical relations are shown in (10). (10) subjects: ncsubj, clausal (csubj, xsubj) objects: dobj, iobj, clausal complement modifiers: adverbs, modifiers of event nominals We use endocentric semantic typing, i.e., the headword"
W04-1908,W97-0808,0,\N,Missing
W04-1908,C02-1112,0,\N,Missing
W04-1908,C00-1028,0,\N,Missing
W04-1908,C92-2070,0,\N,Missing
W04-1908,N01-1013,0,\N,Missing
W04-1908,A97-1052,0,\N,Missing
W04-1908,P95-1026,0,\N,Missing
W04-1908,J01-3001,0,\N,Missing
W04-1908,P03-1007,0,\N,Missing
W04-1908,P99-1014,0,\N,Missing
W04-1908,P03-2029,0,\N,Missing
W04-1908,W01-0703,0,\N,Missing
W04-1908,W98-0701,0,\N,Missing
W04-1908,L02-1000,0,\N,Missing
W08-1206,P07-1005,0,0.0364904,"qualitative analysis of the effects they have on decisions made by the annotators and annotator error. We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame. 1 Introduction Lexical ambiguity is pervasive in natural language, and its resolution has been used to improve performance of a number of natural language processing (NLP) applications, such as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), cross-language information retrieval and question answering (Resnik, 2006). Sense differentiation for the predicates depends on a number of factors, including syntactic frame, semantics of the arguments and adjuncts, contextual clues from the wider context, text domain identification, etc. Preparing sense-tagged data for training and evaluation of word sense disambiguation (WSD) systems involves two stages: (1) creating a sense inventory and (2) applying it in annotation. Creating sense inventories for polysemous words is a task that is notoriously difficult to formali"
W08-1206,N06-2015,0,0.141177,", 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBa"
W08-1206,rumshisky-pustejovsky-2006-inducing,1,0.836895,"Missing"
W08-1206,W04-0807,0,0.0798072,"xt selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between differen"
W08-1206,W04-0811,0,0.0219762,"ense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous"
W08-1206,P06-1014,0,0.0211512,"number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 2005), OntoNotes ("
W08-1206,J05-1004,0,0.0847045,"lmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 2005), OntoNotes (Hovy et al., 2006)) typically provides no way to differentiate between sense distinctions motivated by different factors. Treating different disambiguation factors separately would allow one to examine the contribution of each factor, as well as the success of a given algorithm in identifying the corresponding senses. Within the scope of a sentence, syntactic frame and semantics of the arguments are most prominent in sense Sense inventories for polysemous predicates are often comprised by a number of related senses. In this paper, we examine different types of relations within sen"
W08-1206,W04-1908,1,0.792932,"Missing"
W08-1206,C04-1133,1,\N,Missing
W08-1206,briscoe-carroll-2002-robust,0,\N,Missing
W08-1206,D07-1007,0,\N,Missing
W09-2414,burchardt-etal-2006-salsa,0,0.0272909,"the task. 1 (1) a. John reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this cas"
W09-2414,ide-suderman-2004-american,0,0.0149133,"l be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) H UMAN deny E NTITY to H UMAN H UMAN deny H UMAN E NTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. Semantic head for the target grammatical relation will be identified in each case. is saved into the database, along with the associated type template. In the second subtask, the"
W09-2414,W04-2705,0,0.0985323,"Missing"
W09-2414,miltsakaki-etal-2004-penn,0,0.0594706,"Missing"
W09-2414,ohara-2008-lexicon,0,0.0191172,"reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial p"
W09-2414,J05-1004,0,0.34099,"the problem in detail and describe the data preparation for the task. 1 (1) a. John reported in late from Washington. b. Washington reported in late. Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semant"
W09-2414,S07-1016,0,0.0504028,"Missing"
W09-2414,W04-1908,1,0.906371,"identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying semantic type of the target argument. Sense inventories for the verbs and the type templates associated with different syntactic frames will be provided to the participants. Figure 2: Corpus Development Architecture 3.1 Semantic Types 4 In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). The BSO types were selected for their prevalence in manually identified selection context patterns developed for several hundreds English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The following list of types is currently being used for annotation: Preparing the data for this task will be done in two phases: the data set construction phase and the annotation phase. The first phase consists of (1) selecting the target verbs to be annotated and compiling a sense inventory for each target, and (2)"
W09-2414,W09-3716,1,0.551497,"e House (LOCATION → HUMAN) denied this statement. c. The Boston office called with an update (EVENT → INFO ). b. Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; 89 PropBank (Palmer et al., 2005) NomBank (Meyers et al., 2004) TimeBank (Pustejovsky et al., 2005) Opinion Corpus (Wiebe et al., 2005) Penn Discourse TreeBank (Miltsakaki et al., 2004) 1 This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). The definition of coercion will be extended to include instances of type-shifting due to what we term the qua-relation. (7) a. You can crush the pill (P HYSICAL O BJECT) between two spoons. (Selection) b. It is always possible to crush imagination (A BSTRACT E NTITY qua P HYSICAL O BJECT) under the weight of numbers. (Coercion/qua-relation) In order to determine whether type-shifting has taken place, the classification task must then involve the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on"
W09-2414,J91-4003,1,0.527846,"the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the"
W09-2414,W08-1206,1,0.84921,"E and PHYSICAL OBJECT, the system should simply choose the most relevant type (i.e. HUMAN) and not be concerned with type inheritance. The present set of types may be revised if necessary as the annotation proceeds. 90 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) H UMAN deny E NTITY to H UMAN H UMAN deny H UMAN E NTITY The set of type templates for each verb will be built using"
W09-2414,S07-1007,0,\N,Missing
W09-2414,C04-1133,1,\N,Missing
W09-3716,burchardt-etal-2006-salsa,0,0.0604023,"xplain what each task includes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or P"
W09-3716,W03-1901,0,0.0247673,"from the database2 : Sir Nicholas Lyell, Attorney General, denies a cover-up. <SELECTOR sid=&quot;s1&quot;&gt;denies</SELECTOR&gt; a <NOUN nid=&quot;n1&quot;&gt;cover-up</NOUN&gt; . <CompLink cid=&quot;cid1&quot; sID=&quot;s1&quot; relatedToNoun=&quot;n1&quot; gramRel=&quot;dobj&quot; compType=&quot;COERCION&quot; sourceType=&quot;EVENT&quot; targetType=&quot;PROPOSITION&quot;/&gt; 3.2 Qualia Selection in Modification Constructions For this task, the relevant semantic relations are defined in terms of the qualia structure. We examine two kinds of constructions in this task: adjectival modification of nouns and nominal compounds3 . 2 While we present these examples as an inline annotation, a LAF (Ide and Romary, 2003) compliant offset annotation is fully compatible with GLML. 3 Since target nouns have already been selected for these two tasks, it is also possible to annotate qualia selection in verb-noun contexts such as Can you shine the lamp over here? (TELIC). However, here we focus solely on the modification contexts mentioned here. 175 3.2.1 Adjectival Modification of Nouns This task involves annotating how particular noun qualia values are bound by the adjectives. Following Pustejovsky (2000), we assume that the properties grammatically realized as adjectives ”bind into the qualia structure of nouns,"
W09-3716,ohara-2008-lexicon,0,0.118901,"cludes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or PropBank (Palm"
W09-3716,J05-1004,0,0.0619913,"ns; (iii) Type selection in modification of dot objects. We explain what each task includes and provide a description of the annotation interface. We also include the XML format for GLML including examples of annotated sentences. 1 Introduction 1.1 Motivation In this paper, we introduce a methodology for annotating compositional operations in natural language text. Most annotation schemes encoding “propositional” or predicative content have focused on the identification of the predicate type, the argument extent, and the semantic role (or label) assigned to that argument by the predicate (see Palmer et al., 2005, Ruppenhofer et al., 2006, Kipper, 2005, Burchardt et al., 2006, Ohara, 2008, Subirats, 2004). The emphasis here will be on identifying the nature of the compositional operation rather than merely annotating the surface types of the entities involved in argument selection. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any se"
W09-3716,J91-4003,1,0.832111,"election. 169 Proceedings of the 8th International Conference on Computational Semantics, pages 169–180, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Consider the well-known example below. The distinction in semantic types appearing as subject in (1) is captured by entity typing, but not by any sense tagging from, e.g., FrameNet (Ruppenhofer et al., 2006) or PropBank (Palmer et al., 2005). (1) a. Mary called yesterday. b. The Boston office called yesterday. While this has been treated as type coercion or metonymy in the literature (cf. Hobbs et al., 1993 , Pustejovsky, 1991, Nunberg, 1979, Egg, 2005), the point here is that an annotation using frames associated with verb senses should treat the sentences on par with one another. Yet this is not possible if the entity typing given to the subject in (1a) is HUMAN and that given for (1b) is ORGANIZATION. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organiza"
W09-3716,W08-1206,1,\N,Missing
W09-3716,S07-1007,0,\N,Missing
W09-3716,W04-1908,1,\N,Missing
W09-3716,C04-1133,1,\N,Missing
W11-0409,W10-0701,0,0.0992173,"le. The methodology we propose explicitly addresses the question of related word senses and fuzzy boundaries between them, without trying to establish hard divisions where empirically there are none. The proposed method uses Amazon’s Mechanical Turk for sense annotation. Over the last several of years, Mechanical Turk, introduced by Amazon as “artificial artificial intelligence”, has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al., 2008) and even to modify sense inventories. But the original sense inventory construction has always been left to the experts. In contrast, in the annotation method we describe, the expert is eliminated from the annotation process. As has been the case with using Mechanical Turk for other NLP tasks, the proposed annotation is quite inexpensive and can be done very quickly, while maintaining expert-level annotation quality. The resulting resource will produce several ways to empiricall"
W11-0409,D09-1030,0,0.0364037,"semous word, simultaneously with the corresponding senseannotated lexical sample. The methodology we propose explicitly addresses the question of related word senses and fuzzy boundaries between them, without trying to establish hard divisions where empirically there are none. The proposed method uses Amazon’s Mechanical Turk for sense annotation. Over the last several of years, Mechanical Turk, introduced by Amazon as “artificial artificial intelligence”, has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al., 2008) and even to modify sense inventories. But the original sense inventory construction has always been left to the experts. In contrast, in the annotation method we describe, the expert is eliminated from the annotation process. As has been the case with using Mechanical Turk for other NLP tasks, the proposed annotation is quite inexpensive and can be done very quickly, while maintaining expert-"
W11-0409,N06-2015,0,0.246558,"Missing"
W11-0409,2005.mtsummit-papers.11,0,0.0119983,"nt thresholds for eliminating annotators, in each case evaluating the resulting improvement in cluster quality. • Using prototype-quality control step. We would need to re-annotate a subset of words using an additional step, during which poor quality prototype sentences will be eliminated. This step would be integrated with the main annotation as follows. For each candidate prototype sentence, we would collect the first few similarity judgments from the selected number of annotators. If a certain percentage of judgments are logged as unclear, the sentence is elimicorpus, such as the Europarl (Koehn, 2005); 2 • Use the best MTurk annotation procedure as established in Sec 5.2 to cluster the extracted instances; • Obtain translation equivalents for each instance of the target word using word-alignment produced with Giza++ (Och and Ney, 2000); • Compute the distances between the obtained clusters by estimating the probability of different lexicalization of the two senses from the word-aligned parallel corpus. nated from the set, and another prototype sentence is selected. We would evaluate the results of this modification, using different thresholds for the number of judgments collected and the p"
W11-0409,P06-1014,0,0.10129,"Missing"
W11-0409,P00-1056,0,0.0756579,"hich poor quality prototype sentences will be eliminated. This step would be integrated with the main annotation as follows. For each candidate prototype sentence, we would collect the first few similarity judgments from the selected number of annotators. If a certain percentage of judgments are logged as unclear, the sentence is elimicorpus, such as the Europarl (Koehn, 2005); 2 • Use the best MTurk annotation procedure as established in Sec 5.2 to cluster the extracted instances; • Obtain translation equivalents for each instance of the target word using word-alignment produced with Giza++ (Och and Ney, 2000); • Compute the distances between the obtained clusters by estimating the probability of different lexicalization of the two senses from the word-aligned parallel corpus. nated from the set, and another prototype sentence is selected. We would evaluate the results of this modification, using different thresholds for the number of judgments collected and the percentage of “unclear” ratings. 5.3 Using translation equivalents to compute distances between senses The goal of this set of studies is to investigate the viability of computing distances between the sense clusters obtained for a given wo"
W11-0409,J05-1004,0,0.0463988,"Missing"
W11-0409,D08-1027,0,0.100657,"Missing"
W11-0409,W04-0811,0,0.0300248,"Missing"
W11-0409,steinberger-etal-2006-jrc,0,0.0670173,"Missing"
W11-0409,W04-0807,0,\N,Missing
W11-0409,W10-0700,0,\N,Missing
W13-5412,S07-1002,0,0.0259215,"chy for the word “visa”, given in Figure 1. If the noun “visa” is found in direct object position of the target verb, traversing the tree to the root would produce features such as noun-approval dobj, etc. 5 BOW baseline models, and (3) the best-performing system from SemEval2010. Since HDP performs better overall, we chose the HDP model to experiment with syntactic and ontological features. For completeness, we include results for the WordNetpopulated syntactic features with the LDA model. 6.1 Evaluation Measures Following the established practice in SemEval competitions and subsequent work (Agirre and Soroa, 2007; Manandhar et al., 2010; Brody and Lapata, 2009; Yao and Durme, 2011), we conduct supervised evaluation. A small amount of labeled data is used to map the induced topics to real-world senses; for a description of the method see (Agirre and Soroa, 2007). The resulting mapping is probabilistic; for topics 1, . . . , K and senses 1, . . . , S, we compute the KS values P (s|k) = count(instances predicted k, labeled s) . count(instances predicted k) Then given θj ∗ , we can make a better prediction for instance j ∗ than just assigning the most likely sense to its most likely topic. Instead, we com"
W13-5412,D07-1109,0,0.0268659,"Missing"
W13-5412,E09-1013,0,0.115035,"guating the verb ’deny’, unless our model can select the appropriate nodes of the subtree rooted at the synset ’message, content, subject matter, substance’. Our model should also infer the associations between such nodes and other context relevant features that select the sense ’refuse to grant’ (such as the presence of ditransitive constructions, etc.) In this paper, we use the topic modeling approach to identify ontology-derived features that can prove useful for sense induction. Bayesian approaches to sense induction have recently been shown to perform well in the WSI task. In particular, Brody and Lapata (2009) have adapted the Latent Dirichlet Allocation (LDA) generative topic model to WSI by treating each occurrence context of an ambiguous word as a document, and the derived topics as sense-selecting context patterns represented as collections of features. They applied their model to the SemEval2007 set of ambiguous nouns, beating the best-performing system in its WSI task. Yao and Van Durme (2011) used a non-parametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the same task and showed that following the same basic assumptions, it performs comparably, with the advantage of avoi"
W13-5412,D07-1007,0,0.0115958,"ic and knowledge-based features and show that both parametric and non-parametric models consistently benefit from additional feature types. We perform evaluation on the SemEval2010 WSI verb data and show statistically significant improvement in accuracy (p &lt; 0.001) both over the bag-of-words baselines and over the best system that competed in the SemEval2010 WSI task. 1 Introduction The resolution of lexical ambiguity in language is essential to true language understanding. It has been shown to improve the performance of such applications as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), and crosslanguage information retrieval and question answering (Resnik, 2006). Word sense induction (WSI) is the task of automatically grouping the target word’s contexts of occurrence into clusters corresponding to different senses. Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses. Much of the classic bottom-up WSI and thesaurus construction work – as well as many successful systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have explicitly avoided the us"
W13-5412,P07-1005,0,0.0218641,"integrating syntactic and knowledge-based features and show that both parametric and non-parametric models consistently benefit from additional feature types. We perform evaluation on the SemEval2010 WSI verb data and show statistically significant improvement in accuracy (p &lt; 0.001) both over the bag-of-words baselines and over the best system that competed in the SemEval2010 WSI task. 1 Introduction The resolution of lexical ambiguity in language is essential to true language understanding. It has been shown to improve the performance of such applications as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), and crosslanguage information retrieval and question answering (Resnik, 2006). Word sense induction (WSI) is the task of automatically grouping the target word’s contexts of occurrence into clusters corresponding to different senses. Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses. Much of the classic bottom-up WSI and thesaurus construction work – as well as many successful systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have ex"
W13-5412,E03-1020,0,0.118466,"texts of occurrence into clusters corresponding to different senses. Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses. Much of the classic bottom-up WSI and thesaurus construction work – as well as many successful systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have explicitly avoided the use of existing knowledge sources, instead representing the disambiguating context using bag-of-words (BOW) or syntactic features (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Pedersen, 2010; Kern et al., 2010). This particularly concerns the attempts to integrate the information about semantic classes of words present in the sense-selecting contexts. Semantic roles (such as those found in PropBank (Palmer et al., 2005) or FrameNet (Ruppenhofer et al., 2006)) tend to generalize poorly across the vocabulary. Lexical ontologies (and WordNet (Fellbaum, 2010) in particular) are not always empirically grounded in language use and often do not represent the relevant semantic distinctions. Very often, some parts of the ontology are better suited for a particular disambig"
W13-5412,S10-1080,0,0.0133106,"ir distributional similarity (Hindle, 1990; Pereira et al., 1993; Sch¨utze, 1998; Grefenstette, 1994; Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Agirre et al., 2006). One of the recent evaluations of the state of the art in word sense induction was conducted at SemEval2010 (Manandhar et al., 2010). The participant systems focused on a variety of WSI improvements including feature selection/dimensionality reduction techniques (Pedersen, 2010), experiments with bigram and cooccurrence features (Pedersen, 2010) and syntactic features (Kern et al., 2010), and increased scalability (Jurgens and Stevens, 2010). Following the success of topic modeling in information retrieval, Boyd-Graber et al. (2007) developed an extension of the LDA model for word sense disambiguation that used WordNet walks to generate sense assignments for lexical items. Their model treated synset paths as hidden variables, with the assumption that words within the same topic will share synset paths within WordNet, i.e. each topic will be associated with walks that prefer different “neighborhoods”of WordNet. One problem with their approach is that it relies fully on the integrity of WordNet’s organization, and has no way to dis"
W13-5412,S10-1078,0,0.0669265,"onding to different senses. Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses. Much of the classic bottom-up WSI and thesaurus construction work – as well as many successful systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have explicitly avoided the use of existing knowledge sources, instead representing the disambiguating context using bag-of-words (BOW) or syntactic features (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Pedersen, 2010; Kern et al., 2010). This particularly concerns the attempts to integrate the information about semantic classes of words present in the sense-selecting contexts. Semantic roles (such as those found in PropBank (Palmer et al., 2005) or FrameNet (Ruppenhofer et al., 2006)) tend to generalize poorly across the vocabulary. Lexical ontologies (and WordNet (Fellbaum, 2010) in particular) are not always empirically grounded in language use and often do not represent the relevant semantic distinctions. Very often, some parts of the ontology are better suited for a particular disambiguation task than others. In this wor"
W13-5412,P03-1054,0,0.00358976,"o the bag-of-words features. We describe these classes in more detail below. Preprocessing done on the data includes: (1) tokenization, (2) identifying stopwords, (3) stemming tokens, (4) detecting sentence boundaries, (5) tagging tokens with their parts of speech, and (6) obtaining collapsed dependencies within sentences including the target words. For tokenization, sentence boundary detection, and part-of-speech tagging, we use OpenNLP (OpenSource, 2010). We remove the stop words and stem using the Snowball stemmer. For collapsed syntactic dependencies we use the Stanford Dependency Parser (Klein and Manning, 2003). Bag of Words Following previous literature (Brody and Lapata, 2009), we use a 20 word window (excluding stopwords) for BOW features. In our experiments, a smaller window size failed to produce better performance. Ontology-Based Populated Syntactic Features To capture syntactic information, we use populated dependency relations. We populate these relations with semantic information from WordNet (Miller et al., 1990) as follows. For each syntactic dependency between the target word and the context word, we locate all synsets for the context word. We then traverse the WordNet hierarchy upwards"
W13-5412,S10-1011,0,0.354709,"sk. Yao and Van Durme (2011) used a non-parametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the same task and showed that following the same basic assumptions, it performs comparably, with the advantage of avoiding the extra tuning for the number of senses. We investigate the question of how well such models would perform when some knowledge of syntactic structure and semantics is added into the system, in particular, when bag-of-words features are supplemented by the knowledge-enriched syntactic features. We use the SemEval2010 WSI task data for the verbs for evaluation (Manandhar et al., 2010). This data set choice is motivated by the fact that (1) for verbs, sense-selecting context patterns often most directly depend on the nouns that occur in syntactic dependencies with them, and (2) the nominal parts of WordNet tend to have much cleaner ontological distinctions and property inheritance than, say, the verb synsets, where the subsumption hierarchy is organized according how specific the verb’s manner of action is. The choice of the SemEval2010 verb data set was motivated by the fact that SemEval2007 verb data is dominated by the most frequent sense for many target verbs, with 11 o"
W13-5412,J05-1004,0,0.0116483,"l systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have explicitly avoided the use of existing knowledge sources, instead representing the disambiguating context using bag-of-words (BOW) or syntactic features (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Pedersen, 2010; Kern et al., 2010). This particularly concerns the attempts to integrate the information about semantic classes of words present in the sense-selecting contexts. Semantic roles (such as those found in PropBank (Palmer et al., 2005) or FrameNet (Ruppenhofer et al., 2006)) tend to generalize poorly across the vocabulary. Lexical ontologies (and WordNet (Fellbaum, 2010) in particular) are not always empirically grounded in language use and often do not represent the relevant semantic distinctions. Very often, some parts of the ontology are better suited for a particular disambiguation task than others. In this work, we assume that features based on such ontology segments would correlate well with other context features. Consider, for example, the expression ”to deny the visa”. When choosing between two senses of ’deny’ (’r"
W13-5412,S10-1081,0,0.0611524,"clusters corresponding to different senses. Unlike word sense disambiguation (WSD), it does not rely on a pre-existing set of senses. Much of the classic bottom-up WSI and thesaurus construction work – as well as many successful systems from the recent SemEval competitions – Anna Rumshisky Department of Computer Science University of Massachusetts, Lowell, MA arum@cs.uml.edu have explicitly avoided the use of existing knowledge sources, instead representing the disambiguating context using bag-of-words (BOW) or syntactic features (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Pedersen, 2010; Kern et al., 2010). This particularly concerns the attempts to integrate the information about semantic classes of words present in the sense-selecting contexts. Semantic roles (such as those found in PropBank (Palmer et al., 2005) or FrameNet (Ruppenhofer et al., 2006)) tend to generalize poorly across the vocabulary. Lexical ontologies (and WordNet (Fellbaum, 2010) in particular) are not always empirically grounded in language use and often do not represent the relevant semantic distinctions. Very often, some parts of the ontology are better suited for a particular disambiguation task than"
W13-5412,J98-1004,0,0.373286,"Missing"
W13-5412,W11-1102,0,0.0233379,"Missing"
W17-4203,D14-1162,0,0.0801841,"Missing"
W17-4203,P13-1162,0,0.0336449,"pressed sentiment toward each article. Zhou et al (2011) detected and classified the political bias of news stories using the users’ votes at such collaborative news curation sites as diggs.com. Relatedly, Conover et al (2011) used Twitter political tags to show that retweet patterns induce homogeneous, clearly defined user communities with extremely sparse retweets between the communities. Most state-of-the-art work on bias detection deals with known pre-defined biases and relies either strictly on text or strictly on user reactions in order to determine the bias of a statement. For example, Recasens et al. (2013) developed a system for identifying the bias-carrying term in the sentence, using a dataset of Wikipedia edits that were meant to remove bias. The model uses a logistic regression classifier with several types of linguistic features including word token, word lemma, partof-speech tags, and several lexicons. The classifier also looks at the edits that have previously been made on the article. Using the same dataset, Kuang and Davison (2016) build upon previous 17 8 Conclusion crises. In International Conference on Social Informatics. Springer, pages 257–272. In this paper we address the issue o"
W17-4203,P14-1105,0,0.0491326,"Missing"
W18-1604,D15-1221,1,0.868754,"stwriting is therefore to create a system that can take as input a given artist’s work and generate similar yet unique lyrics. Our objective in this work is to provide a quantifiable direction and foundation for the task of rap lyric generation and similar tasks through (1) developing an evaluation methodology for such models and (2) illustrating how such evaluation can be used to analyze system performance, including advantages and limitations of a specific language model developed for this task. As an illustration case, we use the ghostwriter model previously proposed in exploratory work by Potash et al. (2015), which uses a recurrent neural network (RNN) with Long Short-Term Memory (LSTM) for rap lyric generation. The following are the main contributions of this paper. We present a comprehensive manual evaluation methodology of the generated verses along three key aspects: fluency, coherence, and style matching. We introduce an improvement to the semi-automatic methodology used by Potash et al. (2015) that automatically penalizes repetitive text, removing the need for manual intervention. Finally, we build a corpus of lyrics for 13 rap artists, each with his own unique style, and conduct a comprehe"
W18-1604,D13-1011,0,0.0264698,"s discussed below implement models that generate complete verses from scratch (including verse structure), which is the goal of the models we aim to evaluate. In terms of manual evaluation, Barbieri et al. (2012) have a set of annotators evaluate generated lyrics along two separate dimensions: grammar and semantic relatedness to song title. The annotators rate the dimensions with scores 1-3. A similar strategy is used by Gerv´as (2000), where the author has annotators evaluate generated verses with regard to syntactic correctness and overall aesthetic value, providing scores in the range 1-5. Wu et al. (2013) have annotators determine the effectiveness of various systems based on fluency as well as rhyming. Some heuristic-based automated approaches have also been used, e.g., by Oliveira et al. (2014) who use a simple automatic heuristic that awards lines for ending in a termination previously used in the generated stanza. Malmi et al. (2015) evaluate their generated lyrics based on the verses’ rhyme density, on the assumption that a higher rhyme density means better lyrics. 3 Dataset For our evaluation experiments, we selected the following list of artists in four different categories: • Three top"
