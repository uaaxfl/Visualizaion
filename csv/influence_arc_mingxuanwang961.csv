2020.acl-demos.1,P16-1129,0,0.0348835,"Missing"
2020.acl-demos.1,2020.wmt-1.63,0,0.0143566,"he historical records of each team. • In-match Description. It describes most important events in the game such as “some3 Table 1: Examples of Sports News Generation Time 23’ Category Score Player Didac Team Espanyol Generated Text 第23分 钟 ， 西 班 牙 人 迪达克打入一球。 35’ Yellow Card Mubarak Alav´es 第35分 钟 ， 阿 拉 维 斯 穆巴拉克吃到一张黄 牌。 approach 1 . Furthermore, our machine translation system leverages named-entity (NE) replacement for glossaries including team name, player name and so on to improve the translation accuracy. It can be further improved by recent machine translation techniques (Yang et al., 2020; Zheng et al., 2020). 阿拉维斯 与 ⻄班⽛⼈ 的 ⽐赛 打 成 了 input text sequence (Wang et al., 2017). The architecture is illustrated in Figure 4, we made the following augmentations on the base Tacotron 2 model: • We applied an additional speaker as well as language embedding to support multi-speaker and multilingual input. 平⼿ • We introduced a variational autoencoder-style residual encoder to encode the variational length mel into a fix length latent representation, and then conditioned the representation to the decoder. Transformer Encoder Named Entity Replacement Translated Text In the 23rd minute, Espanyol Didac scored a go"
2020.emnlp-main.210,2020.acl-main.747,0,0.0715994,"Missing"
2020.emnlp-main.210,N19-1423,0,0.245367,". Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several li"
2020.emnlp-main.210,P15-1166,0,0.022539,"tialization fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.210,2020.findings-emnlp.283,0,0.0187955,"013a) first introduces dictionaries to align word representations from different languages. A series of followup studies focus on aligning the word representation across languages (Xing et al., 2015; Ammar et al., 2016; Smith et al., 2017; Lample et al., 2018b). Inspired by the success of BERT, Conneau and Lample (2019) introduced XLM - masked language models trained on multiple languages, as a way to leverage parallel data and obtain impressive empirical results on the cross-lingual natural language inference (XNLI) benchmark and unsupervised NMT(Sennrich et al., 2016a; Lample et al., 2018a; Garcia et al., 2020). Huang et al. (2019) extended XLM with multi-task learning and proposed a universal language encoder. Different from these works, a) mRASP is actually a multilingual sequence to sequence model which is more desirable for NMT pre-training; b) mRASP introduces alignment regularization to bridge the sentence representation across languages. 6 Conclusion In this paper, we propose a multilingual neural machine translation pre-training model (mRASP). To bridge the semantic space between different languages, we incorporate word alignment into the pre-training model. Extensive experiments are conduct"
2020.emnlp-main.210,D19-1252,0,0.34639,"f parallel corpus to simulate different scenarios. Most of the En-X parallel datasets are from the pre-training phase to avoid introducing new information. Most pairs for fine-tuning are from previous years of WMT and IWSLT. Specifically, we use WMT14 for EnDe and En-Fr, WMT16 for En-Ro. For pairs like Nl(Dutch)-Pt(Portuguese) that are not available in WMT or IWSLT, we use news-commentary instead. For a detailed description, please refer to the Appendix. 2652 8 CTNMT only reports the Transformer-base setting. Lang-Pairs Size En→De 4.5M Zh→En 20M En→Fr 40M Direct CTNMT8 (2020) mBART (2020) XLM (2019) MASS (2019) mBERT (2019) 29.3 30.1 28.8 28.9 28.6 24.1 - 43.2 42.3 41.0 - mRASP 30.3 24.7 44.3 Table 2: Fine-tuning performance for popular medium and rich resource MT tasks. For fair comparison, we report detokenized BLEU on WMT newstest18 for Zh→En and tokenized BLEU on WMT newstest14 for En→Fr and En→De. Notice unlike previous methods (except CTNMT) which do not improve in the rich resource settings, mRASP is again able to consistently improve the downstream MT performance. It is the first time to verify that low-resource language pairs can be utilized to improve rich resource MT. Based on"
2020.emnlp-main.210,Q17-1024,0,0.0398988,"Missing"
2020.emnlp-main.210,2020.acl-main.703,0,0.118192,"Missing"
2020.emnlp-main.210,2020.tacl-1.47,0,0.324,"following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several limitations for machine translation tasks. First, pre-trained language models such as BERT are not easy to directly fine-tune unless using some sophisticated techniques (Yang et al., 2020). Second, there is a discrepancy between existing pre-training objective and down-stream ones in MT. Existing pre-training approaches such as MASS (Song et al., 2019) and mBART (Liu et al., 2020) rely on auto-encoding objectives to pre-train the models, which are different from translation. Therefore, their fine-tuned MT models still do not achieve adequate improvement. Third, existing MT pre-training approaches focus on using multilingual models to improve MT for low resource or medium resource languages. There has not been one pre-trained MT model that can improve for any pairs of languages, even for rich resource settings such as English-French. In this paper, we propose multilingual Random Aligned Substitution Pre-training (mRASP), a method to pre-train a MT model for many languag"
2020.emnlp-main.210,2021.ccl-1.108,0,0.128783,"Missing"
2020.emnlp-main.210,W18-6309,0,0.0189222,"ource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually ac"
2020.emnlp-main.210,P19-1015,0,0.0284706,"M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually achieves inferior accura"
2020.emnlp-main.210,P16-1009,0,0.537502,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,W18-6301,0,0.161617,"ts cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the scale of the dataset increasing, the gap between the randomly initialized baseline and pre-training model is becoming closer. It is worth noting that, for En→De benchmark, we obtain 1.0 BLEU points gains9 . To verify mRASP can further boost performance on rich resource datasets, we also conduct experiments on En→Zh and En→Fr. We compare our results with two strong baselines reported by Ott et al. (2018); Li et al. (2019). As shown in Table 2, surprisingly, when large parallel datasets are provided, it still benefits from pre-training models. In En→Fr, we obtain 1.1 BLEU points gains. Comparing to other Pre-training Approaches We compare our mRASP to recently proposed multilingual pre-training models. Following Liu et al. (2020), we conduct experiments on En-Ro, the only pairs with established results. To make a fair comparison, we report de-tokenized BLEU. As illustrated in Table 4 , Our model reaches comparable performance on both En→Ro and Ro→En. We also combine Back Translation (Sennrich"
2020.emnlp-main.210,P16-1162,0,0.792767,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,D14-1162,0,0.0844587,"Missing"
2020.emnlp-main.210,N18-1202,0,0.293103,"exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there"
2020.emnlp-main.210,W18-6319,0,0.0204161,"ly initialized models directly on downstream bilingual parallel corpus as a comparison with pre-training models. Fine-tuning We fine-tune our obtained mRASP model on the target language pairs. We apply a dropout rate of 0.3 for all pairs except for rich resource such as En-Zh and En-Fr with 0.1. We carefully tune the model, setting different learning rates and learning scheduler warm-up steps for different data scale. For inference, we use beam-search with beam size 5 for all directions. For most cases, We measure case-sensitive tokenized BLEU. We also report de-tokenized BLEU with SacreBLEU (Post, 2018) for a fair comparison with previous works. 3.2 Main Results We first conduct experiments on the (extremely) low-resource and medium-resource datasets, where multilingual translation usually obtains significant improvements. As illustrated in Table 1, we obtain significant gains in all datasets. For extremely low resources setting such as En-Be (Belarusian) where the amount of datasets cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the sca"
2020.emnlp-main.210,N18-2084,0,0.109204,"Missing"
2020.emnlp-main.210,N15-1104,0,0.0311338,"fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.733,S14-2010,0,0.269265,"Missing"
2020.emnlp-main.733,S16-1081,0,0.234494,"Missing"
2020.emnlp-main.733,S12-1051,0,0.0820126,"trate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhari"
2020.emnlp-main.733,S13-1004,0,0.067209,"Missing"
2020.emnlp-main.733,D15-1075,0,0.171507,"an latent variable, is then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this proble"
2020.emnlp-main.733,S17-2001,0,0.0610254,"entence),sentence∼D log pZ (fφ−1 (u)) + log |det ∂fφ−1 (u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) fo"
2020.emnlp-main.733,D18-2029,0,0.0389952,".91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible."
2020.emnlp-main.733,marelli-etal-2014-sick,0,0.0539398,"u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formul"
2020.emnlp-main.733,L18-1269,0,0.080249,"or various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhariwal (2018) 9123 Dataset STS-B SICK-R STS-12 Avg. GloVe embeddings Avg. BERT embeddings BERT CLS-vector Pub"
2020.emnlp-main.733,D17-1070,0,0.058629,") 68.95 (↑) 78.48 (↑) 72.15 73.98 75.53 (↑) 77.62 (↑) 77.35 79.15 80.63 (↑) 81.95 (↑) 73.91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpu"
2020.emnlp-main.733,D14-1162,0,0.109243,"://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highlycosting or even intractable. In this paper, we aim to answer two major questions: (1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly? (2) If the BERT embeddings capture enough semantic information that"
2020.emnlp-main.733,N19-1423,0,0.0613683,"ance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence"
2020.emnlp-main.733,D19-1006,0,0.105365,"on that is hard to be directly utilized, how can we make it easier without external supervision? Towards this end, we first study the connection between the BERT pretraining objective and the semantic similarity task. Our analysis reveals that the sentence embeddings of BERT should be able to intuitively reflect the semantic similarity between sentences, which contradicts with experimental observations. Inspired by Gao et al. (2019) who find that the language modeling performance can be limited by the learned anisotropic word embedding space where the word embeddings occupy a narrow cone, and Ethayarajh (2019) who find that BERT word embeddings also suffer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistentl"
2020.emnlp-main.733,P19-1315,0,0.02216,", h&gt; c wx can be approximately decomposed as follows, ∗ h&gt; c wx ≈ log p (x|c) + λc = PMI(x, c) + log p(x) + λc . (2) (3) p(x,c) where PMI(x, c) = log p(x)p(c) denotes the pointwise mutual information between x and c, log p(x) is a word-specific term, and λc is a context-specific term. PMI captures how frequently two events cooccur more than if they independently occur. Note that co-occurrence statistics is a typical tool to deal with “semantics” in a computational way — specifically, PMI is a common mathematical surrogate to approximate word-level semantic similarity (Levy and Goldberg, 2014; Ethayarajh et al., 2019). Therefore, roughly speaking, it is semantically meaningful to compute the dot product between a context embedding and a word embedding. Higher-Order Co-Occurrence Statistics as Context-Context Semantic Similarity. During pretraining, the semantic relationship between two contexts c and c0 could be inferred and reinforced with their connections to words. To be specific, if both the contexts c and c0 co-occur with the same word w, the two contexts are likely to share similar semantic meaning. During the training dynamics, when c and w occur at the same time, the embeddings hc and xw are encour"
2020.emnlp-main.733,D19-1370,1,0.909392,"her to their kNN neighbors compared to the embeddings of high-frequency words. This demonstrates that lowfrequency words tends to disperse sparsely. Due to the sparsity, many “holes” could be formed around the low-frequency word embeddings in the embedding space, where the semantic meaning can be poorly defined. Note that BERT sentence embeddings are produced by averaging the context embeddings, which is a convexitypreserving operation. However, the holes violate the convexity of the embedding space. This is a common problem in the context of representation learining (Rezende and Viola, 2018; Li et al., 2019; Ghosh et al., 2020). Therefore, the resulted sentence embeddings can locate in the poorly-defined areas, and the induced similarity can be problematic. &quot; Invertible mapping The BERT sentence embedding space Standard Gaussian latent space (isotropic) Figure 1: An illustration of our proposed flow-based calibration over the original sentence embedding space of BERT. 3 Proposed Method: BERT-flow To verify the hypotheses proposed in Section 2.2, and to circumvent the incompetence of the BERT sentence embeddings, we proposed a calibration method called BERT-flow in which we take advantage of an i"
2020.emnlp-main.733,2021.ccl-1.108,0,0.220472,"Missing"
2020.emnlp-main.733,D16-1264,0,0.0232629,"T baselines in most cases, and outperforms the state-of-the-art SBERT/SRoBERTa results by a large margin. Robustness analysis with respect to random seeds are provided in Appendix C. 4.2 Unsupervised Question-Answer Entailment In addition to the semantic textual similarity tasks, we examine the effectiveness of our method on unsupervised question-answer entailment. We use Question Natural Language Inference (QNLI, Wang et al. (2019)), a dataset comprising 110K question-answer pairs (with 5K+ for testing). QNLI extracts the questions as well as their corresponding context sentences from SQUAD (Rajpurkar et al., 2016), and annotates each pair as either entailment or no entailment. In this paper, we further adapt QNLI as an unsupervised task. The similarity between a question and an answer can be predicted by computing the cosine similarity of their sentence embeddings. Then we regard entailment as 1 and no entailment as 0, and evaluate the performance of the methods with AUC. As shown in Table 4, our method consistently improves the AUC on the validation set of QNLI. Also, learning flow on the target dataset can produce superior results compared to learning flows on NLI. 9125 Method AUC BERTbase -NLI-last2"
2020.emnlp-main.733,D19-1410,0,0.237997,"fer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistently better than the [CLS] vector as shown in (Reimers and Gurevych, 2019). 9119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9119–9130, c November 16–20, 2020. 2020 Association for Computational Linguistics product or cosine similarity. To address these issues, we propose to transform the BERT sentence embedding distribution into a smooth and isotropic Gaussian distribution through normalizing flows (Dinh et al., 2015), which is an invertible function parameterized by neural networks. Concretely, we learn a flow-based generative model to maximize the likelihood of generating BERT sentence embeddings from a standard G"
2020.emnlp-main.733,N18-1101,0,0.234674,"s then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this problem. 2 Understanding the S"
2020.findings-emnlp.441,J96-1002,0,0.111295,"ing as the evaluation metric. 2. For the other circumstances where no such special token can be used, a mean-pooling operation P is applied to the encoder output, i.e. 1 x = n nt=1 ht , where ht denotes the contextual word representation of the tth token produced by the encoder. The latent space H is spanned by all the latent states. Baseline Approaches. We use two common baseline approaches in NLP active learning to compare with our framework, namely random sampling (RM) and entropy-based uncertainty sampling (US). For sequence classification tasks, we adopt the widely used Max Entropy (ME) (Berger et al., 1996) as uncertainty measurement: H ME (x) = − c X P (y = m|x) log P (y = m|x) (3) m=1 where c is the number of classes. For sequence labeling tasks, we use total token entropy (TTE) (Settles and Craven, 2008) as uncertainty measurement: H T T E (x) = − l N X X P (yi = m|x) log P (yi = m|x) i=1 m=1 (4) where N is the sequence length and l is the number of labels. Latent Space Definition We use the adversarial attack in our AUSDS learning framework to find informative samples, which rely on a well-defined latent space. Two types of latent spaces are defined here based on the encoder architectures an"
2020.findings-emnlp.441,C04-1051,0,0.0605365,"which requires renewal of the sampler mapper M . The algorithm terminates until the unlabeled text corpus Ti is used up. 4 Experiments We evaluate the AUSDS learning framework on sequence classification and sequence labeling tasks. For the oracle labeler O, we directly use the labels provided by the datasets. In all the experiments, we take average results of 5 runs with different random seeds to alleviate the influence of randomness. 4.1 Set-up Dataset. We use five datasets, namely Stanford Sentiment Treebank (SST-2 / SST-5) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), AG’s News Corpus (AG News) (Zhang et al., 2015) and CoNLL 2003 Named Entity Recognition dataset 4912 Dataset SST-2 (Socher et al., 2013) SST-5 (Socher et al., 2013) MRPC (Dolan et al., 2004) AG News (Zhang et al., 2015) CoNLL’03 (Sang and De Meulder, 2003) Task sequence classification sequence classification sequence classification sequence classification sequence labeling Sample Size 11.8k sentences, 215k phrases 11.8k sentences, 215k phrases 5,801 sentence pairs 12k sentences 22k sentences, 300k tokens Table 1: 5 datasets we used for sentence learning experiments, across sequence classific"
2020.findings-emnlp.441,N18-1202,0,0.150104,"dversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popular pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness. 1 Introduction Deep neural models become popular in natural language processing (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Neural models usually consume massive labeled data, which requires a huge quantity of human labors. But data are not born equal, where informative data with high uncertainty are decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs t"
2020.findings-emnlp.441,W03-0419,0,0.140818,"Missing"
2020.findings-emnlp.441,D08-1112,0,0.883398,"decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs to traverse all unlabeled data to find informative unlabeled samples, which are always near the decision boundary with large entropy. However, the traverse process is very time-consuming, thus cannot be executed frequently (Settles and Craven, 2008). A common choice is to perform the sampling process after every specific period, and it samples and labels informative unlabeled data then trains the model until convergence (Deng et al., 2018). We argue that infrequently performing uncertainty sampling may lead to the “ineffective sampling” problem. Because in the early phase of training, the decision boundary changes quickly, which makes previously collected samples less effective after several updates of the model. Ideally, uncertainty sampling should be performed frequently in the early phase of model training. In this paper, we propose t"
2020.findings-emnlp.441,D13-1170,0,\N,Missing
2020.wmt-1.112,Q19-1038,0,0.0136828,"he hyper-parameters for the best filtering performance, and four systems are ensembled to achieve the final results. 2 System Architecture 2.1 Data Introduction In detail, as is shown in Table 1, the WMT20 shared task provides: • Document pairs, including 391, 250 KhmerEnglish and 45, 312 Pashto-English document pairs; • Sentence-aligned corpora extracted from the 985 983 Proceedings of the 5th Conference on Machine Translation (WMT), pages 983–988 985–990 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics above document pairs, using Hunalign (DBL, 2008) and LASER (Artetxe and Schwenk, 2019), including 4, 169, 574 Khmer-English and 1, 022, 883 Pashto-English sentence pairs; • Parallel data which can be used to build filtering and alignment system, including 290, 051 Khmer-English and 123, 198 Pashto-English parallel sentences; • Monolingual data, including approximately 1.9 billion English, 14.0 million Khmer, and 6.6 million Pashto sentences. 2.2.1 We trained the word alignment model on the provided clean parallel corpus by using the fast-align toolkit (Dyer et al., 2013), and get the forward and reverse word translation probability tables. It’s worth mentioning that both of Pas"
2020.wmt-1.112,W19-5404,0,0.044703,"Missing"
2020.wmt-1.112,W19-5358,0,0.221143,"Thus we choose to extract our own set of sentence pairs from the provided document pairs, and design a mining module aiming to gather as many parallel sentence candidates as possible. We then elaborate on our mining procedure and mining module, shown in Figure 1, in detail. merge True Document Pairs Word Alignment Model High Quality? Mining Module Scoring Module LASER Candidates Mining Parallel Sentences This step is operated by our mining module. With the bilingual word translation probability tables, the mining module evaluates the translation quality of bilingual sentence pairs by YiSi-2 (Lo, 2019), which involves both lexical weight and lexical similarity. The Document pairs are first segmented on each language side using Polyglot 2 . This initial segmentation is represented as: en-ps 45K 1.0M 123K Initial Parallel Corpus Word Alignment e = e1 e2 · · · ea = ea1 (1) b f = f 1f 2 · · · f b = f 1 (2) where ek (f k ) is a segment of consecutive words of document e (f ). Then we compute the sentence similarity (translation quality) by iteration from the initial segment (e1 , f 1 ). If the similarity reaches the preset threshold for (ei , f j ), we pick the segment pair as parallel sentence"
2020.wmt-1.112,W03-2205,0,0.090179,"e pick the segment pair as parallel sentence candidate, and continue the computation from (ei+1 , f j+1 ). We notice that the inconsistency of segmentation in the document pairs can lead to the results: a sentence in one language contains information only part of a sentence in the other language, or two sentences (in different languages) both contain part of their information in common. These resulting sentence pairs may have low similarity scores. In order to alleviate this problem, we also incorporate a parallel segmentation method in our mining module. We follow the basic idea proposed in (Nevado et al., 2003) where the parallel segmentation finding problem is treated as an optimization problem and a dynamic programming scheme is 1 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 https://github.com/aboSamoor/polyglot Figure 1: Mining Procedure 984 986 used to search for the best segmentation. We then briefly introduce our method. 3 After obtaining the monolingual initial segmentab tion (ea1 , f 1 ), a parallel segmentation is represented as: j1 j2 s ≡ ([ek11 , f 1 ], [ekk21 +1 , f j1 +1 ], k s| j s| | | · · · , [ek|s|−1 +1 , f j|s|−1 +1 ]) (3) where |s |is"
2020.wmt-1.112,W18-6301,0,0.0197889,"irs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining module adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions. 1 Introduction With the rapid development of machine translation, especially Neural Machine Translation (NMT) (Vaswani et al., 2017; Ott et al., 2018; Zhu et al., 2020), parallel corpus in high quality and large quantity is in urgent demand. These parallel corpora can be used to train and build robust machine translation models. However, for some language pairs on low-resource conditions, few parallel resources are available. Since it is much easier to obtain quantities of monolingual data, it may help if we can extract parallel sentences from monolingual data through alignment and filtering. The WMT19 shared task on parallel corpus filtering for low-resource conditions (Koehn et al., 2019) provides noisy parallel corpora in SinhalaEnglish"
2020.wmt-1.112,W19-5434,0,0.0197698,"ctive. For each objective, we hold out 5K sentences or sentence pairs for validation and 5K for the test. We pre-train the XLM using two different settings on 8 Tesla-V100 GPU: a) Standard: The embedding size is 1024, with 12 layers and 64 batch size. b) Small: The embedding size is 512, with 6 layers and 32 batch size. The other values of hyperparameters are all set to the default values. The two pre-trained XLM model is then fine-tuned in downstream task and further ensembled. To score sentence pairs according to their parallelism, classification models are usually used (Xu and Koehn, 2017; Bernier-Colborne and Lo, 2019). In the training phrase, it is formulated as a binary classification problem, whether the sentence pair is semantically similar to each other or not. In the inference phrase, the probability of the positive class is considered as the score of the sentence pair. Therefore, we use the provided parallel sentence pairs as the positive instances, and construct negative instances taking advantage of such positive instances similar to Bernier-Colborne and Lo (2019). Specifically, we generate negative examples in the following ways: • Shuffle the sentences in source language and target language respe"
2020.wmt-1.112,N19-1423,0,0.00836658,"air. Secondly, different reranking mechanisms are used to adjust the scores. Finally, we ensemble four different models to improve the performance of our systems. 2.3.1 XLM-based Scorer Recently, pre-trained transformer-based models play an important role in a variety of NLP tasks, such as question answering, relation extraction, etc.. Pre-trained models are often trained from scratch with self-supervised objective, and then fine-tuned to adapt to the downstream tasks. In our system, we choose the XLM (Conneau and Lample, 2019) as our main model. The reason are as follows: a) Similar to BERT (Devlin et al., 2019; Yang et al., 2019), XLM has Masked Language Model (MLM) objective, which enables us to make the most use of the provided monolingual corpora; b) XLM also has Translation Language Model (TLM) objective. Taking two parallel sentences as input, it predicts the randomly masked tokens. In this way, crosslingual features can be captured; c) With a large amount of training corpus in different languages, XLM can provide powerful cross-lingual representation for downstream tasks, which is very suitable for parallel corpus filtering situations. We follow the instructions 4 to prepare the training data"
2020.wmt-1.112,N13-1073,0,0.0375979,"ssociation for Computational Linguistics above document pairs, using Hunalign (DBL, 2008) and LASER (Artetxe and Schwenk, 2019), including 4, 169, 574 Khmer-English and 1, 022, 883 Pashto-English sentence pairs; • Parallel data which can be used to build filtering and alignment system, including 290, 051 Khmer-English and 123, 198 Pashto-English parallel sentences; • Monolingual data, including approximately 1.9 billion English, 14.0 million Khmer, and 6.6 million Pashto sentences. 2.2.1 We trained the word alignment model on the provided clean parallel corpus by using the fast-align toolkit (Dyer et al., 2013), and get the forward and reverse word translation probability tables. It’s worth mentioning that both of Pashto and Khmer corpus are tokenized before word alignment model training for accuracy consideration. We separate Pashto words by moses tokenizer1 . For Khmer, we use the character (u200B in Unicode) as separator when it’s available and otherwise use a dictionarybased tokenizer by maximizing the word sequence probability. 2.2.2 Table 1: Statistics of Provided Data Scale Document Pairs Extracted Sentence Pairs Parallel Sentences 2.2 en-km 391K 4.2M 290K Mining Module Besides the given ali"
2020.wmt-1.33,W19-5206,0,0.0142554,"ystem. Apart from splitting the monolingual data into several disjoint parts, we sampled the parallel data so that each model has different deviations on the parallel data. We tested bagging sampling (sample with replacement) and up-sampling(sample with replacement under Tag Back-Translation Recently, back-translation (Edunov et al., 2018) is a standard method to improve the translation quality by leveraging the large scale monolingual data. Starting from WMT19, the source of the test set is the natural text and the of the test set is the translationese text. We find the tag back-translation (Caswell et al., 2019) method can achieve better BLEU compared with previous methods proposed in Edunov et al. (2018). 306 Testset Random w/ mRASP Ps→En En→Km Km→En En→Ta Ta→En 10.2 13.8 39.3 42.8 12.7 14.4 7.4 9.2 14.0 17.9 Table 1: Comparison between randomly initialized baseline model and model initialized from mRASP model Direction Model 1 Testset Model 2 Data Split Ensemble Model Baseline Iter 1 Iter 2 Iter 3 Model 3 Figure 1: Data Diversity Matters for Final System Iterative Joint Training Zhang et al. (2018) proposed an iterative joint training method for better usage of monolingual data from source side and"
2020.wmt-1.33,D18-1045,0,0.0117731,"000ffn and Dynamic Convolution. We report in Table 1 the best score in each setting and direction, and find that mRASP significantly outperforms the baseline. 3.2 Experiment Techniques Parallel Data Up-sampling According to the experiments, data diversity matters for the whole system. Apart from splitting the monolingual data into several disjoint parts, we sampled the parallel data so that each model has different deviations on the parallel data. We tested bagging sampling (sample with replacement) and up-sampling(sample with replacement under Tag Back-Translation Recently, back-translation (Edunov et al., 2018) is a standard method to improve the translation quality by leveraging the large scale monolingual data. Starting from WMT19, the source of the test set is the natural text and the of the test set is the translationese text. We find the tag back-translation (Caswell et al., 2019) method can achieve better BLEU compared with previous methods proposed in Edunov et al. (2018). 306 Testset Random w/ mRASP Ps→En En→Km Km→En En→Ta Ta→En 10.2 13.8 39.3 42.8 12.7 14.4 7.4 9.2 14.0 17.9 Table 1: Comparison between randomly initialized baseline model and model initialized from mRASP model Direction Mode"
2020.wmt-1.33,2020.emnlp-main.210,1,0.742443,"der or deeper Transformers, dynamic convolutions). The final system includes text pre-process, data selection, synthetic data generation, advanced model ensemble, and multilingual pretraining. 1 Introduction We participated in the WMT2020 shared news translation task in 14 directions: English↔Chinese, English↔German, French↔German, English↔Polish, English↔Tamil,English↔Pashto,English↔Khmer, covering language pairs from high to low resources. In this year’s translation task, we mainly focus on exploiting self-supervised and unsupervised methods for NMT to make full use of the monolingual data (Lin et al., 2020; Yang et al., 2019). We aims at building a general training framework which can be well applied to different translation directions. Our models are mainly based on the Transformer (Vaswani et al., 2017). Techniques used in the submitted systems include iterative back-translation, knowledge distillation. We also employed several tricks to improve in-domain BLEU scores, typically in-domain transfer learning. We also experimented with a multilingual pretraining technique which we proposed recently (Lin et al., 2020). 2 We use the implementations in Fairseq(Ott et al., 2019). All models are train"
2020.wmt-1.33,N16-1046,0,0.0303144,"Missing"
2020.wmt-1.33,N19-4009,0,0.014462,"the monolingual data (Lin et al., 2020; Yang et al., 2019). We aims at building a general training framework which can be well applied to different translation directions. Our models are mainly based on the Transformer (Vaswani et al., 2017). Techniques used in the submitted systems include iterative back-translation, knowledge distillation. We also employed several tricks to improve in-domain BLEU scores, typically in-domain transfer learning. We also experimented with a multilingual pretraining technique which we proposed recently (Lin et al., 2020). 2 We use the implementations in Fairseq(Ott et al., 2019). All models are trained with Adam optimizer (Kingma and Ba, 2014). We use the “inverse sqrt lr” scheduler with 4000 warm-up steps and set the max learning rate to 5e-4. The betas are (0.9, 0.98). During training, the batches are made of similar length sequences, so we avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performanc"
2020.wmt-1.33,W18-6301,0,0.0316618,"All models are trained with Adam optimizer (Kingma and Ba, 2014). We use the “inverse sqrt lr” scheduler with 4000 warm-up steps and set the max learning rate to 5e-4. The betas are (0.9, 0.98). During training, the batches are made of similar length sequences, so we avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance"
2020.wmt-1.33,W18-6319,0,0.0125323,"n 3.1, we pretrained three multilingual models with different model architectures (DLCL 25layers, Transformer 15000ffn and Dynamic Convolution 25e6d) on all parallel data available in WMT20 except the English↔Chinese to avoid a large dictionary1 and fine-tuned the pre-trained models on the their own parallel data with different data sampling strategies to get 9 baseline models2 . Then we applied the tag back-translation, joint training, knowledge distillation and random ensemble methods as described in Section 3 to get the final translation system. All BLEU scores were reported with SacreBLEU(Post, 2018). 4.1 Chinese→English Final Submission We submitted our VolcTrans online system (unconstrained). The final submission achieves 36.6 BLEU. You can get access to VolcTrans online system on http://translate. volcengine.cn/. 1 Direction En→Zh Testset wmt19 Baseline iterative BT Ensemble KD Ensemble System 38.5 38.9 41.5 42.0 BLEU on WMT20 testset submission 44.9 Table 3: Results of English→Chinese by sacreBLEU 4.2 English→Chinese For English→Chinese, we train English↔Chinese jointly. We use all parallel data available: News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT Corpus and W"
2020.wmt-1.33,W19-5341,0,0.140709,"avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance • Transformer 15e6d: According to Sun et al. (2019), a transformer with larger encoder layer number can learn better representation of source sentence and get better BLEU scores. We increase the number of encoder layers from 6 to 15 layers in the transformer big arc"
2020.wmt-1.33,W18-6429,1,0.84313,"s where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance • Transformer 15e6d: According to Sun et al. (2019), a transformer with larger encoder layer number can learn better representation of source sentence and get better BLEU scores. We increase the number of encoder layers from 6 to 15 layers in the transformer big architecture which is t"
2021.acl-demo.7,N19-1423,0,0.0127725,"/kaldi-asr.org/ 56 various data formats defined by Dataset, preprocesses data samples according to Task and writes to the disk. Transfer Learning NeurST supports initializing the model variables from well-trained models as long as they have the same variable names. As for ST, we can initialize the ST encoder with a well-trained ASR encoder and initialize the ST decoder with a well-trained MT decoder, which facilitates to achieve promising improvements. Besides, NeurST also provides scripts for converting released models from other repositories, like wav2vec2.0 (Baevski et al., 2020) and BERT (Devlin et al., 2019). Researchers can conveniently integrate these pre-trained components to the customized models. end scale decay at decay steps MT ASR ST 1.0 3.5 3.5 1.0 2.0 1.5 50k 50k 50k 50k language aligned to text in a target language: libri-trans (Kocabiyikoglu et al., 2018) 5 is a small EN→FR dataset which was originally started from the LibriSpeech corpus, the audiobook recordings for ASR (Panayotov et al., 2015). The English utterances were automatically aligned to the e-books in French, and 236 hours of English speech aligned to French translations at utterance level were finally extracted. It has be"
2021.acl-demo.7,L18-1001,0,0.0284877,"e names. As for ST, we can initialize the ST encoder with a well-trained ASR encoder and initialize the ST decoder with a well-trained MT decoder, which facilitates to achieve promising improvements. Besides, NeurST also provides scripts for converting released models from other repositories, like wav2vec2.0 (Baevski et al., 2020) and BERT (Devlin et al., 2019). Researchers can conveniently integrate these pre-trained components to the customized models. end scale decay at decay steps MT ASR ST 1.0 3.5 3.5 1.0 2.0 1.5 50k 50k 50k 50k language aligned to text in a target language: libri-trans (Kocabiyikoglu et al., 2018) 5 is a small EN→FR dataset which was originally started from the LibriSpeech corpus, the audiobook recordings for ASR (Panayotov et al., 2015). The English utterances were automatically aligned to the e-books in French, and 236 hours of English speech aligned to French translations at utterance level were finally extracted. It has been widely used in previous studies. As such, we use the clean 100-hour portion plus the augmented machine translation from Google Translate as the training data and follow its split of dev and test data. MuST-C (Di Gangi et al., 2019)6 is a multilingual speech tra"
2021.acl-demo.7,N19-1202,0,0.135606,"Missing"
2021.acl-demo.7,D15-1166,0,0.0606516,"s and has widespread applications, like cross-language videoconferencing or customer support chats. Traditionally, researchers build a speech translation system via a cascading manner, including an automatic speech recognition (ASR) and a machine translation (MT) subsystem (Ney, 1999; Casacuberta et al., 2008; Kumar et al., 2014). Cascade systems, however, suffer from error propagation problems, where an inaccurate ASR output would theoretically cause translation errors. Owing to recent progress of sequence-to-sequence modeling for both neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and end-to-end speech recognition (Chan et al., 2016; Chiu et al., 2018; Dong et al., 2018), 55 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extractio"
2021.acl-demo.7,N16-1109,0,0.0615633,"Missing"
2021.acl-demo.7,2020.acl-demos.34,0,0.49433,"the method on TED English-Chinese; and Dong et al. (2021) use libri-trans English-French and IWSLT2018 English-German dataset; and Wu et al. (2020) show the results on CoVoST dataset and the FR/RO portions of MuST-C dataset. Different datasets make it difficult to compare the performance of their approaches. Further, even for the same dataset, the baseline results are not necessarily kept consistent. Take the libri-trans EnglishFrench dataset as an example. Dong et al. (2021) report the pre-trained baseline as 15.3 and the result of Liu et al. (2019) is 14.3 in terms of tokenized BLEU, while Inaguma et al. (2020) report 15.5 (detokenized BLEU). The mismatching baseline results in an unfair comparison on the improvements of their approaches. We think one of the primary reasons is that the preprocessing of audio data is complex, and the ST model training involves many tricks, such as pre-training and data augmentation. Therefore a reproducible and reliable benchmark is required. In this work, we present NeurST , a toolkit for easily building and training end-toend ST models, as well as end-to-end ASR and NeurST is an open-source toolkit for neural speech translation. The toolkit mainly focuses on end-to"
2021.acl-demo.7,2020.acl-main.344,0,0.703106,"he Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extraction, data preprocessing, model training, and inference for researchers to reproduce the benchmarks. Though there exist several counterparts, such as Lingvo (Shen et al., 2019), fairseq-ST (Wang et al., 2020a) and Kaldi 1 style ESPnet-ST (Inaguma et al., 2020), NeurST is specially designed for speech translation tasks, which encapsulates the details of speech processing and frees the developers from data engineering. It is easy to use and extend. The contributions of this work are as follows: • NeurST is designed specifically for end-toend ST, with clean and simple code. It is lightweight and independent of Kaldi, which simplifies installation and usage, and is more compatible for NLP researchers. • We report strong benchmarks with welldesigned hyper-parameters and show best practice on several S"
2021.acl-demo.7,P16-1009,0,0.0473172,"9; Jiang et al., 2020) on large-scale scenarios. Design and Features NeurST is implemented with both TensorFlow2 and PyTorch backends. In this section, we will introduce the design components and features of this toolkit. 2.1 Data Preprocessing NeurST supports on-the-fly data preprocessing via a number of lightweight python packages, like python speech features2 for extracting audio features (e.g. mel-frequency cepstral coefficients and log-mel filterbank coefficients). And for text processing, NeurST integrates some effective tokenizers, including moses tokenizer3 , byte pair encoding (BPE) (Sennrich et al., 2016b) and SentencePiece4 . Alternatively, the training data can be preprocessed and stored in binary files (e.g., TFRecord) beforehand, which is guaranteed to improve the I/O performance during training. Moreover, to simplify such operations, NeurST provides the command-line tool to create such record files, which automatically iterates on Design NeurST divides one running job into four components: Dataset, Model, Task and Executor. Dataset NeurST abstracts out a common interface Dataset for data input. For example, we can train a speech translation model from either a raw dataset tarball or pre-"
2021.acl-demo.7,2021.naacl-industry.15,1,0.764527,"ning. By default, NeurST offers evaluation on development data during training and keeps track of the checkpoints with the best evaluation results. Monitoring NeurST supports TensorBoard for monitoring metrics during training, such as training loss, training speed, and evaluation results. Model Serving There is no gap between the research models and production models under NeurST , while they can be easily served with TensorFlow Serving. Moreover, for higher performance serving of standard transformer models, NeurST is able to integrate with other optimized inference libraries, like lightseq (Wang et al., 2021). 3.2 Data Preprocessing Beyond the officially released version, we performed no other audio to text alignment and data cleaning on libri-trans and MuST-C datasets. For speech features, we extracted 80-channel logmel filterbank coefficients with windows of 25ms and steps of 10ms, resulting in 80-dimensional features per frame. The audio features of each sample were then normalized by the mean and the standard deviation. All texts were segmented into subword level by first applying Moses tokenizer and then BPE. In detail, we removed all punctuations and lowercased the sentences in the source si"
2021.acl-demo.7,P16-1162,0,0.106414,"9; Jiang et al., 2020) on large-scale scenarios. Design and Features NeurST is implemented with both TensorFlow2 and PyTorch backends. In this section, we will introduce the design components and features of this toolkit. 2.1 Data Preprocessing NeurST supports on-the-fly data preprocessing via a number of lightweight python packages, like python speech features2 for extracting audio features (e.g. mel-frequency cepstral coefficients and log-mel filterbank coefficients). And for text processing, NeurST integrates some effective tokenizers, including moses tokenizer3 , byte pair encoding (BPE) (Sennrich et al., 2016b) and SentencePiece4 . Alternatively, the training data can be preprocessed and stored in binary files (e.g., TFRecord) beforehand, which is guaranteed to improve the I/O performance during training. Moreover, to simplify such operations, NeurST provides the command-line tool to create such record files, which automatically iterates on Design NeurST divides one running job into four components: Dataset, Model, Task and Executor. Dataset NeurST abstracts out a common interface Dataset for data input. For example, we can train a speech translation model from either a raw dataset tarball or pre-"
2021.acl-demo.7,2020.findings-emnlp.230,0,0.324467,"Missing"
2021.acl-demo.7,2021.iwslt-1.6,1,0.860965,"Missing"
2021.acl-demo.7,2020.aacl-demo.6,0,0.701062,"he Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extraction, data preprocessing, model training, and inference for researchers to reproduce the benchmarks. Though there exist several counterparts, such as Lingvo (Shen et al., 2019), fairseq-ST (Wang et al., 2020a) and Kaldi 1 style ESPnet-ST (Inaguma et al., 2020), NeurST is specially designed for speech translation tasks, which encapsulates the details of speech processing and frees the developers from data engineering. It is easy to use and extend. The contributions of this work are as follows: • NeurST is designed specifically for end-toend ST, with clean and simple code. It is lightweight and independent of Kaldi, which simplifies installation and usage, and is more compatible for NLP researchers. • We report strong benchmarks with welldesigned hyper-parameters and show best practice on several S"
2021.acl-long.155,2021.naacl-main.458,1,0.735104,"Missing"
2021.acl-long.155,D19-1633,0,0.559953,"ngy2 Hamming h22 Hamming Replace h Replace 0.5 0.5 Replace h0.5 h2h2 y2 y2 h2 yy2 2 y2 y2 y2Hamming 2 2 Inputs Distance Inputs Distance Inputs Distance Inputs 0.7 yy33 y3 N(0.7 0.7y 0.7 3   y h3h3 y3 y3 h3 yy3 3 y3 y3 y3N(Y,yN( 3 Y, Y ) = 33 3  Y Y, ) =Y 3) = 3 y y4 0.6 0.6 hh44 0.6h yy4 4 ysuch y4 yword h4h4Notice y4 y4 h4 that h0.6 sentence. interdependency 4 4 4 4 4 yy55 y5 0.9 0.9 0.9y y0.9 h h y y h5 yy 5 y y y y5 5 is crucial, Transformer explicitly captures 5 5 5 5 5 5 5 5 as5 the that via decoding from left to right (Figure 1a). Several remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding. Their common idea is to decode the target tokens iteratively while each pass of decoding is trained using the masked language model (Figure 1c). Since these methods require multiple passes of decoding, its generation speed is measurably slower than the vanilla NAT. With single-pass generation only, these methods still largely lag behind the autoregressive Transformer. 1993 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natur"
2021.acl-long.155,2020.acl-main.36,0,0.423954,"complex methods to better decide the output lengths: noisy parallel decoding (NPD) and connectionist temporal classification (CTC). For NPD (Gu et al., 2018), we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.7"
2021.acl-long.155,D18-1149,0,0.0780326,"Missing"
2021.acl-long.155,D19-1573,0,0.0461076,"Missing"
2021.acl-long.155,D18-1336,0,0.0366605,"Missing"
2021.acl-long.155,D19-1437,0,0.242673,"ate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.70 / 18.6× 1 1 1 1 1 19.17 24.15 25.20 25.31 26.07 23.20 27.28 29.52 30.68 29.68 29.79 31.45 / 32.20 / 31.44 31.81 / 32.84 / 2.4× 9.7× / / 6.1× 1 1 1 1 1 20.36 25.52 25.21 26.39 26.55 24.81 28.73 29.84 29.54 31.02 28.47 32.60 31.19 32.79 32.87 29.43 33.46 32.04 33.84 33.51 15.3×† 14.6"
2021.acl-long.155,P19-2049,0,0.0193196,"ed language model, and the model iteratively replaces masked tokens with new outputs. (Li et al., 2020) first predict the left token and right token for each position, and decode the final token at the current position conditioned on the left-and-right tokens predicted before. Despite the relatively better accuracy, the multiple decoding iterations reduce the inference efficiency of non-autoregressive models. Scheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019). Although scheduled sampling also modifies decoder inputs in training, there are mainly two differences between our work and scheduled sampling. Firstly, scheduled sampling mixes up the predicted sequence and the gold target sequence, and our method does not mix predicted sequences into decoder inputs. Besides, GLAT aims to learn word interdependency for single-pass parallel generation and scheduled sampling is designed for alleviating exposure bias. 6 Conclusion In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel ge"
2021.acl-long.155,2020.emnlp-main.83,0,0.461609,"Missing"
2021.acl-long.155,P16-1162,0,0.0586813,"ated tokens after generation. 4 Experiments In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines. Ablation studies and further analysis are also included to verify the effects of different components used in GLAT. 4.1 Experimental Settings Datasets We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs). These datasets are tokenized and segmented into subword units using BPE encodings (Sennrich et al., 2016). We preprocess WMT14 EN-DE by following the data preprocessing in Vaswani et al. (2017). For WMT16 EN-RO and IWSLT16 DE-EN, we use the processed data provided in Lee et al. (2018). Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets. We employ the transformer with the base setting in Vaswani et al. (2017) as the teacher for knowledge distillation. Then, we train our GLAT on distilled data. Baselines and Setup We compare our method with the base Transformer and strong represent"
2021.acl-long.155,P19-1125,1,0.913324,"t not selected. The training loss above is calculated against these remaining tokens. 1995 GLAT adopts similar encoder-decoder architecture as the Transformer with some modification (Figure 1d). Its encoder fenc is the same multihead attention layers. Its decoder fdec include multiple layers of multi-head attention where each layer attends to the full sequence of both encoder representation and the previous layer of decoder representation. During the initial prediction, the input to the decoder H = {h1 , h2 , ..., hT } are copied from the encoder output using either uniform copy or soft copy (Wei et al., 2019). The initial tokens Yˆ are predicted using argmax decoding with fdec (fenc (X; θ), H; θ). To calculate the loss LGLM , we compare the initial prediction Yˆ against the ground-truth to select tokens within the target sentence, i.e. GS(Y, Yˆ ). We then replace those sampled indices of h’s with corresponding target word embeddings, H 0 = RP(Embyt ∈GS(Y,Yˆ ) (yt ), H), where RP replaces the corresponding indices. Namely, if a token in the target is sampled, its word embedding replaces the corresponding h. Here the word embeddings are obtained from the softmax embedding matrix of the decoder. The"
2021.acl-long.21,L18-1144,0,0.10998,"Missing"
2021.acl-long.21,N19-1121,0,0.0584207,"Missing"
2021.acl-long.21,D19-5610,0,0.0525408,"Missing"
2021.acl-long.21,Q19-1038,0,0.0653772,"Missing"
2021.acl-long.21,D19-1165,0,0.0371356,"he introduction of noised bilingual and noised monolingual data for multilingual NMT. The above two Settings and Datasets Parallel Dataset PC32 We use the parallel dataset PC32 provided by Lin et al. (2020). It con5 They apply RAS only on parallel data xi is in language Li and xj is in language Lj , where i, j ∈ {L1 , . . . , LM } 7 We will release our synonym dictionary 6 4 Higher temperature increases the difficulty to distinguish positive sample from negative ones. 246 En-Fr wmt14 → ← bilingual Transformer-6(Lin et al., 2020) Transformer-12(Liu et al., 2020) pre-train & fine-tuned Adapter (Bapna and Firat, 2019) mBART(Liu et al., 2020) XLM(Conneau and Lample, 2019) MASS(Song et al., 2019) mRASP(Lin et al., 2020) unified multilingual Multi-Distillation (Tan et al., 2019) m-Transformer mRASP w/o finetune(**) mRASP2 En-Tr wmt17 → ← En-Es wmt13 → ← En-Ro wmt16 →(*) ← En-Fi wmt17 → ← Avg 43.2 41.4 39.8 - 9.5 12.2 33.2 - 34.3 34.3 34.0 36.8 20.2 21.8 - 41.1 44.3 45.4 17.8 20.0 22.5 23.4 35.4 34.0 - 33.7 - 37.7 37.6 38.8 38.5 39.1 38.9 22.4 24.0 28.5 28.0 - 42.0 43.1 43.5 38.1 39.2 39.3 18.8 20.0 21.4 23.1 25.2 25.8 32.8 34.0 34.5 33.7 34.3 35.0 31.6 35.9 37.5 38.0 35.8 37.7 38.8 39.1 22.0 20.0 22.0 23.4 21"
2021.acl-long.21,N19-1423,0,0.0700011,"Missing"
2021.acl-long.21,P17-1176,0,0.0498838,"Missing"
2021.acl-long.21,P15-1166,0,0.0535499,"Missing"
2021.acl-long.21,P19-1121,0,0.016476,"osed mRASP2. It takes a pair of parallel sentences (or augmented pseudo-pair) and computes normal cross entropy loss with a multi-lingual encoder-decoder. In addition, it computes contrastive loss on the representations of the aligned pair (positive example) and randomly selected non-aligned pair (negative example). 2017). Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020). Despite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020). Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pa"
2021.acl-long.21,2020.tacl-1.47,0,0.025868,"0 directions (See Appendix) and in this table we pick the representative ones. Different from our work, final BLEU scores of mBART, XLM, MASS and mRASP are obtained by multilingual pre-training and finetuning on a single direction. Adapter is a trade-off between unified multilingual model and bilingual model (trained on 6 languages on WMT data). Multi-Distillation is improved over Adapter with selective distillation methods. Results for Transformer-6 (6 layers for encoder and decoder) are from Lin et al. (2020). Results for Transformer12 (12 layers for encoder and decoder separately) are from Liu et al. (2020). (*) Note that for En→Ro direction, we follow the previous setting to calculate BLEU score after removing Romanian dialects. (**) For mRASP w/o finetune we report the results implemented by ourselves, with 12 layers encoder and decoder and our data. Both m-Transformer and our mRASP2 have 12 layers for encoder and decoder. data. The total number of sentences in MC24 is 1.01 billion. The detail of data volume is listed in the Appendix. We apply AA on MC24 by randomly replacing words in the source side sentences with synonyms from a multilingual dictionary. Therefore the source side might contai"
2021.acl-long.21,W18-6319,0,0.0157097,"5 Nl(*) X→Nl Nl→X 2.2 6.0 2.3 6.3 5.3 6.1 Fr→X 22.3 4.8 21.7 De X→De De→X 14.4 14.2 4.2 4.8 12.3 15.0 Ru X→Ru Ru→X 16.6 19.9 5.7 4.8 16.4 19.1 Fr Avg of all 15.56 5.05 15.31 Table 3: Zero-Shot: We report de-tokenized BLEU using sacreBLEU in OPUS-100. We observe consistent BLEU gains in zero-shot directions on different evaluation sets, see Appendix for more details. mRASP2 further improves the quality. We also list BLEU of pivot-based model (X→En then En→Y using m-Transformer) as a reference, mRASP2 only lags behind Pivot by -0.25 BLEU. (*) Note that Dutch(Nl) is not included in PC32. 4 BLEU (Post, 2018). For tokenized BLEU, we tokenize both reference and hypothesis using Sacremoses11 toolkit then report BLEU using the multi-bleu.pl script12 . For Chinese (Zh), BLEU score is calculated on character-level. Experiment Results This section shows that mRASP2 provides consistent performance gains for supervised and unsupervised English-centric translation directions as well as for non-English directions. Experiment Details We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as wel"
2021.acl-long.21,P16-1162,0,0.0347259,"ent Details We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as well as a learning rate of 3e-4 with polynomial decay scheduling and a warm-up step of 10000. For optimization, we use Adam optimizer (Kingma and Ba, 2015) with  = 1e-6 and β2 = 0.98. To stabilize training, we set the threshold of gradient norm to be 5.0 and clip all gradients with a larger norm. We set the hyper-parameter λ = 1.0 in Eq.3 during training. For multilingual vocabulary, we follow the shared BPE (Sennrich et al., 2016) vocabulary of Lin et al. (2020), which includes 59 languages. The vocabulary contains 64808 tokens. After adding 59 language tokens, the total size of vocabulary is 64867. 4.1 English-Centric Directions Supervised Directions As shown in Table 1, mRASP2 clearly improves multilingual baselines by a large margin in 10 translation directions. Previously, multilingual machine translation underperforms bilingual translation in rich-resource scenarios. It is worth noting that our multilingual machine translation baseline is already very competitive. It is even on par with the strong mBART bilingual"
2021.acl-long.21,2020.acl-main.252,0,0.0186296,"guages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improvIntroduction Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora. Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual translation models are appealing for two reasons. First, they are model efficient, enabling easier deployment (Johnson et al., 244 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 244–258 August 1–6, 2021. ©2021 Association for Computational Linguistics ing English-centric1 directions (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020). A few recent exceptions are Zhang et al. (2020) and Fan et al. (2020), who trained many-to-many systems with int"
2021.acl-long.21,P19-1120,0,0.0409191,"Missing"
2021.acl-long.21,P19-1117,0,0.0639978,"o translate from one language to another. To distinguish different languages, we add an additional language identification token preceding each sentence, for both source side and target side. The base architecture of mRASP2 is the state-of-theart Transformer (Vaswani et al., 2017). A little different from previous work, we choose a larger setting with a 12-layer encoder and a 12-layer decoder to increase the model capacity. The model dimension is 1024 on 16 heads. To ease the training of the deep model, we apply Layer Normalization for word embedding and pre-norm residual connection following Wang et al. (2019a) for both encoder and decoder. Therefore, our multilingual NMT baseline is much stronger than that of Transformer big model. More formally, we define L = {L1 , . . . , LM } where L is a collection of M languages involving in the training phase. Di,j denotes a parallel dataset of (Li , Lj ), and D denotes all parallel datasets. The training loss is cross entropy defined as: X Lce = − log Pθ (xi |xj ) (1) As such, many-to-many translations can make the most of the knowledge from all supervised directions and the model can perform well for both English-centric and non-English settings. In this"
2021.acl-long.21,2020.acl-main.148,0,0.523329,"ed non-aligned pair (negative example). 2017). Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020). Despite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020). Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improvIntroduction Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora. Recent work on multilingual machine translation aims to create a single unified model to trans"
2021.acl-long.25,P15-1166,0,0.0237718,"of existing language pairs. Besides, LaSS can boost zero-shot translation by up to 26.5 BLEU. 2 Related Work Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages (Johnson et al., 2017). There is a transfer-interference trade-off in this architecture (Arivazhagan et al., 2019): boosting the performance of low resource languages or maintain the performance of high resource languages. To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al., 2018). Sachan and Neubig (2018) compares different sharing methods and finds different sharing methods have a great impact on performance. Recently, Zhang et al. (2021) analyze when and where language specific capacity matters. Li et al. (2020) uses a binary conditional latent variable to decide which language each layer belongs to. Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tun"
2021.acl-long.25,1983.tc-1.13,0,0.275744,"Missing"
2021.acl-long.25,N16-1101,0,0.0195815,"t translation by up to 26.5 BLEU. 2 Related Work Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages (Johnson et al., 2017). There is a transfer-interference trade-off in this architecture (Arivazhagan et al., 2019): boosting the performance of low resource languages or maintain the performance of high resource languages. To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al., 2018). Sachan and Neubig (2018) compares different sharing methods and finds different sharing methods have a great impact on performance. Recently, Zhang et al. (2021) analyze when and where language specific capacity matters. Li et al. (2020) uses a binary conditional latent variable to decide which language each layer belongs to. Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tuning (Frankle and Carbin, 2019; Liu et al., 2019). Frankle and"
2021.acl-long.25,D19-1074,1,0.833073,"ailable at https: //github.com/NLP-Playground/LaSS. 1 (a) Full network En En En Zh Fr De (b) LaSS Figure 1: Illustration of a full network and languagespecific ones (LaSS). — represents shared weights. — , — and — represents weights for En→Zh, En→Fr and En→De, respectively. Compared to the full multilingual model, each LaSS learned model has language universal and language specific weights. Introduction Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019). Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual NMT enjoys the advantage of deployment. Further, the parameter sharing of multilingual NMT encourages transfer learning of different languages. An extreme case is zero-shot translation, where direct translation between a language pair never seen in training is possible (Johnson et al., 2017). ∗ Zh Fr"
2021.acl-long.25,P19-1176,0,0.0512669,"Missing"
2021.acl-long.25,D18-1326,0,0.073559,"019). Therefore, multilingual NMT models often suffer from performance degradation compared with their corresponding bilingual baseline, especially for rich-resource translation directions. The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters (Aharoni et al., 2019; Zhang et al., 2020). However, it is not parameter or computation efficient and needs larger multilingual training datasets to avoid over-fitting. An alternative solution is to design language-aware components, such as division of the hidden cells into shared and language-dependent ones (Wang et al., 2018), adaptation layers (Bapna and Firat, 2019; Philip et al., 2020), language-aware layer normalization 293 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 293–305 August 1–6, 2021. ©2021 Association for Computational Linguistics and linear transformation (Zhang et al., 2020), and latent layers (Li et al., 2020). In this work, we propose LaSS, a method to dynamically find and learn Language Specific Subnetwork for multilingual NMT. LaSS accommodates one sub-network for each la"
2021.acl-long.25,2020.acl-main.148,0,0.425914,"or En→Zh, En→Fr and En→De, respectively. Compared to the full multilingual model, each LaSS learned model has language universal and language specific weights. Introduction Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019). Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual NMT enjoys the advantage of deployment. Further, the parameter sharing of multilingual NMT encourages transfer learning of different languages. An extreme case is zero-shot translation, where direct translation between a language pair never seen in training is possible (Johnson et al., 2017). ∗ Zh Fr De Equal contribution. While very promising, several challenges remain in multilingual NMT. The most challenging one is related to the insufficient model capacity. Since multiple languages are accommodated in a single model, the modeling cap"
2021.acl-tutorials.4,N19-1006,0,0.148651,"ource NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategies, such as knowledge distillation and adapter (Bapna and Firat, 2019; Liang et al., 2021). The next topic is multi-lingual pre-training for NMT. In this context, we aims at mitigating the English-centric bias and suggest that it is possible Pre-training is a dominant paradigm in Nature Language Processing (NLP) (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019a), Computer Vision (CV) (He et al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then applied to the downstream tasks by either providing context-aware representation of the input, or initializing the parameters of the downstream model for fine-tuning. Recently, the trend of self-supervised pre-training and task-specific fine-tuning finally fully hits neural machine translation (NMT) (Zhu et al., 2020; Yang et al., 2020; Chen et al., 2020). Despite its success, introducing a universal pretrained model to"
2021.acl-tutorials.4,D19-1165,0,0.0281768,"ver, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategies, such as knowledge distillation and adapter (Bapna and Firat, 2019; Liang et al., 2021). The next topic is multi-lingual pre-training for NMT. In this context, we aims at mitigating the English-centric bias and suggest that it is possible Pre-training is a dominant paradigm in Nature Language Processing (NLP) (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019a), Computer Vision (CV) (He et al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then appl"
2021.acl-tutorials.4,2020.emnlp-main.210,1,0.899005,"e on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al.,"
2021.acl-tutorials.4,2020.acl-main.705,0,0.019998,"t al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then applied to the downstream tasks by either providing context-aware representation of the input, or initializing the parameters of the downstream model for fine-tuning. Recently, the trend of self-supervised pre-training and task-specific fine-tuning finally fully hits neural machine translation (NMT) (Zhu et al., 2020; Yang et al., 2020; Chen et al., 2020). Despite its success, introducing a universal pretrained model to NMT is non-trivial and not necessarily yields promising results, especially for the resource-rich setup. Unique challenges remain in several aspects. First, the objective of most pretraining methods are different from the downstream NMT tasks. For example, BERT (Devlin et al., 2019), a popular pre-trained model, is designed for language understanding with only a transformer encoder, while an NMT model usually consists of an encoder and a decoder to perform cross-lingual generation. This gap makes it not feasible enough to apply"
2021.acl-tutorials.4,2021.acl-long.25,1,0.648401,"pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclu"
2021.acl-tutorials.4,2020.tacl-1.47,0,0.085368,"age Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et a"
2021.acl-tutorials.4,2021.ccl-1.108,0,0.0585324,"Missing"
2021.acl-tutorials.4,N19-1423,0,0.491374,"n enhancing the performance of NMT, how to design a better pretraining model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training. The first topic is the monolingual pre-training for NMT, which is one of the most well-studied field. Monolingual text representations like ELMo, GPT, MASS and BERT have superiorities, which significantly boost the performances of various natural language processing tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Song et al., 2019). However, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategi"
2021.acl-tutorials.4,2021.naacl-main.457,1,0.722729,"et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclude the tutorial by pointing out the best practice when applying pre-training for NMT. The topics cover various of pre-training methods for different NMT scenarios. After this tutorial, the audience will understand why pre-training for NMT is different from other tasks and how to make the most of pre-training for NMT. Importantly, we will give deep analyze about how and why pre-training works in NMT, which will inspire future work on designing pre-training paradigm specific for NMT. 2 • Unified sequence-to-sequence pre-trainin"
2021.acl-tutorials.4,2021.acl-long.21,1,0.718701,"torial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al"
2021.acl-tutorials.4,N18-1202,0,0.012014,"ole of pre-training in enhancing the performance of NMT, how to design a better pretraining model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training. The first topic is the monolingual pre-training for NMT, which is one of the most well-studied field. Monolingual text representations like ELMo, GPT, MASS and BERT have superiorities, which significantly boost the performances of various natural language processing tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Song et al., 2019). However, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models"
2021.acl-tutorials.4,P19-1493,0,0.012108,"d the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al.,"
2021.acl-tutorials.4,N18-2084,0,0.0178786,"ssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al."
2021.acl-tutorials.4,2020.acl-main.344,0,0.0134936,"t al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclude the tutorial by pointing out the best practice when applying pre-training for NMT. The topics cover various of pre-training methods for different NMT scenarios. After this tutorial, the audience will understand why pre-training for NMT is different from other tasks and how to make the most of pre-training for NMT. Importantly, we will give deep analyze about how and why pre-training works in NMT, which will inspire future work on designing pre-training paradig"
2021.emnlp-main.579,2020.acl-main.692,0,0.057278,"ys and [W1 ; b1 ] are trainable parameters. The bandwidth of learnable Laplacian kernel kq −k k Kl (qi , kj ; σ) = exp(− i σ j ) is modeled in the 1 For two d-dimension vectors same way as the bandwidth of learnable Gaussian qxPand y, we compute the d 2 2. L distance between x and y as (x − y ) kernel. i i i=1 7282 Train Dev Test Law 467k 2k 2k Medical 248k 2k 2k Koran 18k 2k 2k IT 223k 2k 2k Subtitles 500k 2k 2k Table 1: The number of training, development and test examples of 5 domain-specific datasets. The training data of Subtitles domain is sampled from the full Subtitles training set by Aharoni and Goldberg (2020). 3.3 Adaptive Mixing of Base Prediction and Retrieved Examples To mix the model-based distribution and examplebased distribution adaptively, we model the mixing weight λ with a learnable neural network. The mixing weight λ is computed by a multilayer perceptron with query qi and weighted sum e as inputs, where [W2 ; b2 ; W3 ; b3 ] are of keys k trainable parameters. ei ] + b2 ) + b3 ) λ = sigmoid(W3 ReLU(W2 [qi ; k (5) ei = k k X wj k j (6) j=1 wj ∝ K(qi , kj ; θ) (7) In this way, kNN-MT (Khandelwal et al., 2021) could be seen as a specific case of KSTER, with fixed Gaussian kernel and mixing"
2021.emnlp-main.579,D19-1165,0,0.0152561,"tperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parame"
2021.emnlp-main.579,N19-1191,0,0.222739,"success (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn K"
2021.emnlp-main.579,P19-1175,0,0.0175103,"ates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examples are retrieved based on fuzzy matching (Bulte and Tezcan, 2019; Xu et al., 2020), embedding similarity, or a mixture of the two approaches (Bapna and Firat, 2019). 3 Methodology In this section, we first formulate the kernelsmoothed machine translation (KSTER), which smooths neural machine translation (NMT) output with retrieved token level examples. Then we introduce the modeling and training of the learnable kernel and adaptive mixing weight. The overview of KSTER is shown in Figure 2. 3.1 Kernel-Smoothed Machine Translation Base Model for Neural Machine Translation The state-of-the-art NMT models are based on the encoder-decoder architecture. The enco"
2021.emnlp-main.579,D18-1340,0,0.13778,"Koran IT domain on which kNN-MT and KSTER are adapted Subtitles Figure 1: The domain-specific and general domain translation performance in EN-DE translation. Base is a Transformer model trained on general domain WMT data. kNN-MT and our proposed KSTER are adapted for domain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of t"
2021.emnlp-main.579,W17-4702,0,0.0162509,"Missing"
2021.emnlp-main.579,2021.acl-long.378,0,0.0648744,"Missing"
2021.emnlp-main.579,2021.findings-emnlp.23,0,0.0311758,"the source. NMT systems are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambi"
2021.emnlp-main.579,2021.acl-short.47,1,0.702718,"ain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7280–7290 c November 7–11, 2021. 2021 Association for Computational Linguistics and retrieved examples brings noise to translation (Bapna and Firat, 2019). Second, completely non-parametric methods"
2021.emnlp-main.579,W04-3250,0,0.250837,".24 IT 22.99 34.48 29.55 31.82 29.21 39.57 37.56 36.90 Subtitles 20.65 25.16 21.80 22.63 23.13 27.73 22.86 25.15 Average-specific 23.54 32.92 33.08 34.53 26.75 36.77 36.64 37.74 Table 5: Test set BLEU scores of multi-domain machine translation. Average-specific is the averaged performance in 5 specific domains. For general domain sentence translation, KSTER outperforms kNN-MT for 3 and 6 BLEU scores in EN-DE and DE-EN direction respectively. For domain-specific translation, KSTER outperforms kNNMT for 1.5 and 1.1 BLEU scores in EN-DE and DE-EN direction. Significance test by paired bootstrap (Koehn, 2004) resampling shows that KSTER outperforms kNN-MT significantly in all domains except for Koran domain in EN-DE translation and IT domain in DE-EN translation. 4.3 Multi-Domain Machine Translation In MDMT, since there is no domain label available in test time, examples from all domains are translated with one model. We build a mixed database with training data of general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 A"
2021.emnlp-main.579,C18-1111,0,0.020353,"nce. We therefore drop the most similar examples during training to reduce this discrepancy. With above improvements, KSTER shows the following advantages: • Extensive experiments show that, KSTER outperforms kNN-MT, a strong competitor, in specific domains for 1.1 to 1.5 BLEU scores while keeping the performance in general domain. • KSTER outperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Faraj"
2021.emnlp-main.579,W17-3204,0,0.0259316,"domain labels of examples are available in test time. In MDMT, the domain labels of examples are not available in test time, so examples from all domains are translated with one model, which is a more practical setting. 4.1 Datasets and Implementation Details Datasets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build"
2021.emnlp-main.579,D19-3019,0,0.0471854,"Missing"
2021.emnlp-main.579,W17-4713,0,0.0203522,"2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empir"
2021.emnlp-main.579,2020.emnlp-main.733,1,0.684475,"ms are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambiguation ability of"
2021.emnlp-main.579,2021.acl-long.25,1,0.667655,"k is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examp"
2021.emnlp-main.579,P02-1040,0,0.109427,"Missing"
2021.emnlp-main.579,2021.tacl-1.2,0,0.0122763,"ion (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced"
2021.emnlp-main.579,W18-6319,0,0.0203748,"Missing"
2021.emnlp-main.579,D18-1104,0,0.0205885,"1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search se"
2021.emnlp-main.579,2020.acl-main.144,0,0.0600508,"Missing"
2021.emnlp-main.579,N18-1120,0,0.09194,"l., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Examp"
2021.emnlp-main.579,P16-1162,0,0.0436248,"asets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build the Since the database is built from the training data and KSTER is trained on the training data, similar examples can constantly be retrieved from the database during training. However, in test time, there may be no example in the database that is 2 similar to"
2021.emnlp-main.579,tiedemann-2012-parallel,0,0.0180797,"nels and different weights for different examples. EN-DE General Specific 24.72 33.08 26.06 33.40 27.80 34.02 27.74 34.38 DE-EN General Specific 25.87 36.64 27.89 37.37 31.88 37.19 31.94 37.61 Table 7: Ablation study of learnable kernel and mixing weight in KSTER with Gaussian kernel in MDMT. Both learnable kernel and learnable mixing weight bring improvement. None represents that both kernel and mixing weight are fixed, in which case KSTER degenerates to kNN-MT. KSTER with Laplacian kernel in unseen domains, which is important in real-world MDMT applications. We take Bible and QED from OPUS (Tiedemann, 2012)6 as unseen domains and randomly sample 2k examples from each domain for test. We directly use the MDMT models to translate sentences from unseen domains. The results of ENDE translation are presented in Table 6. KSTER outperforms all baselines, which shows strong generalization ability. 4.4 Inference Speed A common concern about non-parametric methods in MT is that searching similar examples may slow the inference speed. We test the inference speed KSTER in MDMT in EN-DE translation, which is the setting with the largest database. The averaged inference time in general domain and 5 specific d"
2021.emnlp-main.579,D19-1670,0,0.0233319,"general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 Averaged domain-specific performance 34 BLEU 29 BLEU of kNN-MT changes with the size of database. In this work, we study the performance change of kNN-MT and KSTER with low-quality database. Specifically, we test the robustness of these models in DAMT when the database is noisy. We add token-level noise to the English sentences in parallel training data by EDA (Wei and Zou, 2019) 5 . For each word in a sentence, it is modified with a probability of 0.1. The candidate modifications contain synonym replacement, random insertion, random swap and random deletion with equal probability. Then we use the noisy training data to construct the noisy database. We study the effects of source side noise and target side noise on translation performance. The experiment results are presented in Table 4. Target side noise has more negative effect to translation performance than source side noise. The BLEU scores of KSTER drop less apparently in all settings, which indicates that the p"
2021.findings-acl.195,D19-5304,0,0.024143,"s demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce e"
2021.findings-acl.195,P18-1163,0,0.0279739,"Missing"
2021.findings-acl.195,N19-1202,0,0.0291476,"Missing"
2021.findings-acl.195,W19-6603,0,0.02628,"Missing"
2021.findings-acl.195,N16-1109,0,0.159703,"anslation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus t"
2021.findings-acl.195,E09-1030,0,0.0544836,"r MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based los"
2021.findings-acl.195,2020.acl-demos.34,0,0.190402,"erive a novel bi-modal contrastive training task to learn an alignment between semantic memories of two modalities. Finally, Chimera achieves a new state-of-the-art performance on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al."
2021.findings-acl.195,L18-1001,0,0.0250668,"s. MuST-C contains translations from English (EN) to 8 languages: Dutch (NL), French (FR), German (DE), Italian (IT), Portuguese (PT), Romanian (RO), Russian (RU), and Spanish (ES). With each pair consisting of at least 385 hours of audio recordings, to the best of our knowledge, MuST-C is currently the largest speech translation dataset available for each language pair. It includes data from English TED talks with manual transcripts and translations at the sentence level. We use the dev and tst-COMMON sets as our development and test data, respectively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation t"
2021.findings-acl.195,2020.coling-main.314,0,0.288628,"ing modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) are most related to ours, as they also attempt to train mode"
2021.findings-acl.195,W18-6309,0,0.0126882,". On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here"
2021.findings-acl.195,2006.iwslt-papers.1,0,0.0870071,"Liu et al., 2020) are most related to ours, as they also attempt to train models on ASR or MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based a"
2021.findings-acl.195,N19-4009,0,0.0504683,"× X X X X × × × × × × X × × × × × × × × X × × X X 22.7 22.9 22.4 23.6 23.1 22.1 25.2 22.3 25.6 27.1 • 32.9 32.8 31.6 33.5 34.1 34.5 34.3 35.0 35.6 15.3 15.8 14.7 15.2 15.8 16.7 17.4 27.2 28.0 26.9 28.1 28.7 30.2 30.6 22.7 23.8 23.0 24.2 24.2 24.0 25.0 21.9 21.9 21.0 22.9 22.4 23.2 24.0 28.1 28.0 26.3 30.0 29.3 29.7 30.2 27.3 27.4 24.9 27.6 28.2 28.5 29.2 Table 1: Main results on tst-COMMON subset on all 8 languages in MuST-C dataset. “Speech” denotes unlabeled audio data. • : the result uses a mixed WMT14+OpenSubtitles data for MT pre-training. EN-DE Among the baselines, † shows results from Ott et al. (2019), ‡ from Inaguma et al. (2020), ? from Zhang et al. (2020a), ♦ from Le et al. (2020), ] from Liu et al. (2020), [ from Indurthi et al. (2019), and ◦ from Pino et al. (2019). ∗ shows results of a simple baseline model by combining a Wav2Vec2 module (Baevski et al., 2020) and a Transformer model, which could be viewed as the “no external data” version of Chimera. External Data Speech ASR MT Model ∗ EN-FR W2V2-T TCEN † LSTM ‡ AFS ◦ Multilingual ? Transformer ⊥ Curiculum ⊥ COSTT [ LUT ♦ STAST ] X × × × × × × × × × × × X × X X X × X X × × X × × × × X × × 6.4 17.1 17.0 17.2 17.6 17.7 18.0 18.2 18.3"
2021.findings-acl.195,P02-1040,0,0.109641,"plit from words, and normalized. Non-print punctuation is removed. The sentences are then tokenized with Moses tokenizer 5 . We filter out samples whose number of source or target tokens is over 250 and whose ratio of source and target text lengths is outside range [2/3, 3/2]. For sub-wording, we use a unigram sentencepiece6 model with a dictionary size of 10000. On each translation direction, The sentencepiece model is learned on all text data from both ST and MT corpora. The dictionary is shared across MT and ST and across source and target languages. The performance is evaluated with BLEU (Papineni et al., 2002) using sacreBLEU 7 . We average 5 https://github.com/mosessmt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl 6 https://github.com/google/sentencepiece 7 https://github.com/mjpost/sacrebleu, with configuration 2218 S.S. Projection Decoder EN-DE EN-FR MT Contrastive EN-DE EN-FR Fixed Fixed Fixed Fixed 25.6 24.3 24.2 23.8 35.0 34.3 33.4 33.1 X X × × X × X × 25.6 25.0 24.7 25.1 35.0 34.6 34.6 34.6 Table 4: Performance of Mem-16 Chimera when freezing different modules in fine-tuning. S.S. Projection is abbreviation for shared semantic projection. “Fixed” indicates that weights in this mo"
2021.findings-acl.195,2020.autosimtrans-1.2,0,0.0353742,"on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) a"
2021.findings-acl.195,N19-1285,0,0.0281362,"Missing"
2021.findings-acl.195,L16-1147,0,0.0153501,"tively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16"
2021.findings-acl.195,D17-1145,0,0.0950134,"a +1.9 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has"
2021.findings-acl.195,2020.iwslt-1.8,0,0.0534939,"Missing"
2021.findings-acl.195,P19-1115,0,0.0277419,"Missing"
2021.findings-acl.195,W19-4305,0,0.0177039,"-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is"
2021.findings-acl.195,W18-3023,0,0.0162589,"n (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As"
2021.findings-acl.195,2020.findings-emnlp.230,0,0.170183,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,2020.acl-main.148,0,0.0607551,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,P19-1649,0,0.0304663,"Missing"
2021.findings-acl.195,2020.acl-main.150,0,0.0133604,"errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As a benefit of shared semantic projection, Chimera is able to leverage large-scale MT training corpo"
2021.findings-acl.195,W97-0400,0,0.769807,"Missing"
2021.findings-acl.195,2020.acl-main.344,0,0.130173,"ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus tens of words). This issue is further coupled with the rel"
2021.findings-acl.264,C18-1263,0,0.0177442,"ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn,"
2021.findings-acl.264,2004.iwslt-evaluation.1,0,0.0795441,"Missing"
2021.findings-acl.264,W18-6408,1,0.778461,"shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language d"
2021.findings-acl.264,P17-1106,0,0.0298889,"hang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them onl"
2021.findings-acl.264,P15-1166,0,0.0266418,"ng the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et"
2021.findings-acl.264,N16-1101,0,0.0233489,"age tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunt"
2021.findings-acl.264,N18-1032,0,0.0241304,"aswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al."
2021.findings-acl.264,P19-1121,0,0.0207753,"ed by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al., 2017). To further simplify the MNMT models, Johnson et al., (2017) propose to add language tags (LTs) to the beginning of input data to i"
2021.findings-acl.264,Q17-1024,0,0.0606056,"Missing"
2021.findings-acl.264,2005.mtsummit-papers.11,0,0.0777922,", 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn, 2005) datasets. Table 2 shows an overview of the datasets. We choose four different languages (English included) for both IWSLT17 and Europarl, and 20 languages for TED talks. All the training data are English-centric parallel data, which means either the source-side or target-side of the sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, res"
2021.findings-acl.264,D18-2012,0,0.0139251,"s. We choose four different languages (English included) for both IWSLT17 and Europarl, and 20 languages for TED talks. All the training data are English-centric parallel data, which means either the source-side or target-side of the sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, respectively. We learned a joint SentencePiece model (Kudo and Richardson, 2018) for sub-word training on all languages with 40,000 merge operations for each dataset. We limit the size of joint vocabulary to 40,000 for all three datasets. Settings We use the open-source implementation (Ott et al., 2019) of Transformer model (Vaswani et al., 2017). Following the settings of (Liu et al., 2020a), we use a 5-layer encoder and 5-layer decoder variation of Transformer-base model (Vaswani et al., 2017) for TED and IWSLT17. For Europarl v7, we use a standard Transformer-big model (Vaswani et al., 2017). Sentence pairs are batched together by approximate sentence length. Each batc"
2021.findings-acl.264,D19-1167,0,0.0117114,"l., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al., 2017). To further simplify the MNMT models, Johnson et al., (2017) propose to add language tags (LTs) to the beginning of input data to indicate the target language. Then a shared v"
2021.findings-acl.264,2020.tacl-1.47,0,0.0563854,"Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn, 2005) datasets. Table 2 shows an overview o"
2021.findings-acl.264,D15-1166,0,0.0668276,"egies. We demonstrate that a proper LT strategy could enhance the consistency of semantic representations and alleviate the off-target issue in zero-shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource l"
2021.findings-acl.264,N19-4009,0,0.0211238,"sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, respectively. We learned a joint SentencePiece model (Kudo and Richardson, 2018) for sub-word training on all languages with 40,000 merge operations for each dataset. We limit the size of joint vocabulary to 40,000 for all three datasets. Settings We use the open-source implementation (Ott et al., 2019) of Transformer model (Vaswani et al., 2017). Following the settings of (Liu et al., 2020a), we use a 5-layer encoder and 5-layer decoder variation of Transformer-base model (Vaswani et al., 2017) for TED and IWSLT17. For Europarl v7, we use a standard Transformer-big model (Vaswani et al., 2017). Sentence pairs are batched together by approximate sentence length. Each batch has approximately 30,000 source tokens and 30,000 target tokens. We use the Adam (Kingma and Ba, 2015) optimizer to update the parameters and 3002 Dataset languages #zero-shot directions #training sents per direction #sent"
2021.findings-acl.264,P02-1040,0,0.109224,"Missing"
2021.findings-acl.264,W19-5202,0,0.482681,"Missing"
2021.findings-acl.264,W18-6319,0,0.0321457,"Missing"
2021.findings-acl.264,N18-2084,0,0.117507,"Missing"
2021.findings-acl.264,1983.tc-1.13,0,0.442489,"Missing"
2021.findings-acl.264,2020.acl-main.148,0,0.135214,"DEC means placing the TLT on the decoder (target) side of model. S-ENC-T-ENC and S-ENC-T-DEC place the SLT on the encoder side, but the former also places the TLT on encoder side, while the latter on the decoder side. find that the LT strategies are crucial for the zeroshot MNMT translation quality. Ignoring SLTs and placing the TLTs on the encoder side could achieve the best performance during our experiments. (ii) We conduct extensive visualization analysis to demonstrate that the proper LT strategy could enhance the consistency of semantic representation and alleviate the off-target issue (Zhang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017)"
2021.findings-acl.264,2020.acl-main.150,1,0.778499,"o demonstrate that the proper LT strategy could enhance the consistency of semantic representation and alleviate the off-target issue (Zhang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack sy"
2021.findings-acl.264,N16-1004,0,0.0196597,"ding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike"
2021.findings-emnlp.233,2021.acl-long.21,1,0.782419,". Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has to capture tempts (Yang et al., 2020a; Zhu et al., 2020b; Rothe the source context. Specifically, the decoder in et al., 2020). The pre-training and fine-tuning style NMT has a “cross-attention” sub-layer that plays a becomes an important alternative to take advantage transduction role (Bahdanau et al., 2015), while preof monolingual data (Yang et al., 2020c,b; Liu et al., trained models have none, as is shown in Figure 2. 2020; Pan et al., 2021). This mismatch between the generation models and An intuitive question comes as: Can we bridge conditional generation models makes it a challenge BERT-like pre-trained encoders and GPT-like de- for the usage of pre-trained models as translation coders to form a high-quality translation model? decoders. Since they only need monolingual data, we can Therefore, some previous works manually insert reduce the reliance on the large parallel corpus. cross-attention sub-layer or adapters (Rothe et al., 2020; Ma et al., 2020; Guo et al., 2020). However, ∗ Work is done while at ByteDance. 1 the extra i"
2021.findings-emnlp.233,P02-1040,0,0.110894,"rameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98, with warmup_steps = 4000. Without extra statement, we use dropout = 0.3 (Srivastava et al., 2014). Label smoothing (Szegedy et al., 2016) of value = 0.1 is also adopted. Besides, we use fp16 mixed precision training (Micikevicius et al., 2018) with Horovod library with RDMA inter-GPU communication (Sergeev and Del Balso, 2018). • Evaluation: We uniformly conduct beam search with size = 5 and length penalty α = 0.6. For hi, ja, and zh, we use SacreBLEU (Post, 2018). Otherwise, we use tokenized BLEU (Papineni et al., 2002) with the open-source script 6 . 4.3 Main Results As is shown in Table 1 and 2, our methods obtain significant improvements across all language pairs. For x→en and en→x pairs, advances of nearly 6 BLEU and 3 BLEU are achieved. We also compare the results with loading from mBART, a well-known multilingual pre-trained sequence-tosequence model (Liu et al., 2020) 7 . Due to the language difference, we only tune the model on a part of languages. With both 12-layers depth and 1024-dimensions width, our method outperforms mBART on almost all pairs, proving the superiority of Graformer comparing with"
2021.findings-emnlp.233,D18-1039,0,0.0469664,"Missing"
2021.findings-emnlp.233,W18-6319,0,0.0120202,"we go through the total data for five times. Parameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98, with warmup_steps = 4000. Without extra statement, we use dropout = 0.3 (Srivastava et al., 2014). Label smoothing (Szegedy et al., 2016) of value = 0.1 is also adopted. Besides, we use fp16 mixed precision training (Micikevicius et al., 2018) with Horovod library with RDMA inter-GPU communication (Sergeev and Del Balso, 2018). • Evaluation: We uniformly conduct beam search with size = 5 and length penalty α = 0.6. For hi, ja, and zh, we use SacreBLEU (Post, 2018). Otherwise, we use tokenized BLEU (Papineni et al., 2002) with the open-source script 6 . 4.3 Main Results As is shown in Table 1 and 2, our methods obtain significant improvements across all language pairs. For x→en and en→x pairs, advances of nearly 6 BLEU and 3 BLEU are achieved. We also compare the results with loading from mBART, a well-known multilingual pre-trained sequence-tosequence model (Liu et al., 2020) 7 . Due to the language difference, we only tune the model on a part of languages. With both 12-layers depth and 1024-dimensions width, our method outperforms mBART on almost all"
2021.findings-emnlp.233,N18-2084,0,0.0204132,"the corpus of “zh_cn” instead of “zh”. 4 https://github.com/neulab/ word-embeddings-for-nmt 3 T denotes the length of sequence. <2lang&gt;, x1 , x2 , ..., xt−1 . x<t = Datasets and Preprocess • Pre-training: We use News-Crawl corpus 2 plus WMT datasets. We conduct deduplication and label the data by language. In the end, we collect 1.4 billion sentences in 45 languages, which is only one-fifth of that of mBART (Liu et al., 2020). The detailed list of languages and corresponding scales is in Appendix A. • Multilingual Translation: We use TED datasets, the most widely used MNMT datasets, following Qi et al. (2018); Aharoni et al. (2019). We extract 30 languages 3 from & to English, with the size of 3.18M sentence pairs in raw data and 10.1M sentence pairs in sampled bidirectional data. The detailed list of language pairs and scales is in Appendix A. We download the data from the open source 4 Pre-train Multilingual GPT (Decoder for Generation) T X Experiments In this paper, we perform many-to-many style multilingual translation (Johnson et al., 2017). The detailed illustrations of the datasets and implementation are as follows. m(x) and m(x) denote the masked words and rest words from x LLM = − (3) hN"
2021.findings-emnlp.233,2020.emnlp-main.208,0,0.0932283,"o! Hello! German English BERT GPT Grafting Chinese French Bonjour! Figure 1: Grafting pre-trained (masked) language models like BERT and GPT for machine translation. Moreover, if the combination of models is univer- Hallo! sal, it can be applied to translation for multiple BERT languages, as is shown in Figure 1. However, though many works successfully gain improvements by loading encoder/decoder parameters from BERT-like pre-trained encoders (Zhu BERT et al., 2020b; Guo et al., 2020), they do not achieve satisfactory results with loading decoder parameters from GPT-like pre-trained decoders (Yang et al., 2020a; Rothe et al., 2020). Theoretically, the well-trained decoder model like GPT should bring 1 Introduction better generation ability to the translation model. In recent years, pre-trained (masked) language We suggest the outcome may be attributed to the models have achieved significant progress in all architecture mismatch. kinds of NLP tasks (Devlin et al., 2019; RadPre-trained (masked) language models predict ford et al., 2019). Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has"
2021.findings-emnlp.233,2020.acl-main.148,0,0.0290241,"019b), learning better representation (Wang et al., their strengths. The primary target is to link the 2019a), massive training (Aharoni et al., 2019; generation model to the source side and maintain the invariability of the architecture in the mean- Arivazhagan et al., 2019), interlingua (Zhu et al., time. Therefore, we propose Graformer, with pre- 2020a), and adpater (Zhu et al., 2021). These works trained models grafted by a connective sub-module. mainly utilize parallel data. There are also some works taking advantage of The structure of the pre-trained parts remains unmonolingual corpus. Zhang et al. (2020); Wang changed, and we train the grafting part to learn to translate. For universality and generalization, et al. (2020) use back-translation (BT) to improve we also extend the model to multilingual NMT, MNMT. However, for MNMT, BT is tremendously costly, reaching O(n), or even O(n2 ). Siddhant achieving mBERT+mGPT. et al. (2020); Wang et al. (2020) adopt multi-task Generally, the translation process can be divided learning (MTL), combining with other tasks such into three parts: representation, transduction, and generation, respectively achieved by the encoder, as masked language model (MLM)"
2021.findings-emnlp.233,2020.acl-main.150,0,0.531267,"the well-trained decoder model like GPT should bring 1 Introduction better generation ability to the translation model. In recent years, pre-trained (masked) language We suggest the outcome may be attributed to the models have achieved significant progress in all architecture mismatch. kinds of NLP tasks (Devlin et al., 2019; RadPre-trained (masked) language models predict ford et al., 2019). Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has to capture tempts (Yang et al., 2020a; Zhu et al., 2020b; Rothe the source context. Specifically, the decoder in et al., 2020). The pre-training and fine-tuning style NMT has a “cross-attention” sub-layer that plays a becomes an important alternative to take advantage transduction role (Bahdanau et al., 2015), while preof monolingual data (Yang et al., 2020c,b; Liu et al., trained models have none, as is shown in Figure 2. 2020; Pan et al., 2021). This mismatch between the generation models and An intuitive question comes as: Can we bridge conditional generation models makes it a challenge BERT-like pre-trained encoders and GPT-like de- for the us"
2021.findings-emnlp.233,1983.tc-1.13,0,0.383958,"Missing"
2021.findings-emnlp.233,P19-1117,0,0.017939,"tempts and conthe pre-training objective is usually a variant of firm its feasibility. The most well-known work auto-encoding (Song et al., 2019; Liu et al., 2020), is from Johnson et al. (2017), who conduct a sewhich is different from the downstream translation ries of interesting experiments. And the usage of objective and may not achieve adequate improve- the language token style is widely accepted. Also, ments (Lin et al., 2020). many subsequent works continuously explore new approaches in MNMT, such as parameter sharIn this paper, we mainly focus on exploring ing (Blackwood et al., 2018; Wang et al., 2019b; the best way to simultaneously take advantage of Tan et al., 2019a), parameter generation (Platanthe pre-trained representation model and generaios et al., 2018), knowledge distillation (Tan et al., tion model (e.g., BERT+GPT) without limiting 2019b), learning better representation (Wang et al., their strengths. The primary target is to link the 2019a), massive training (Aharoni et al., 2019; generation model to the source side and maintain the invariability of the architecture in the mean- Arivazhagan et al., 2019), interlingua (Zhu et al., time. Therefore, we propose Graformer, with pre-"
2021.findings-emnlp.233,2020.emnlp-main.75,0,0.0286409,"2020a), and adpater (Zhu et al., 2021). These works trained models grafted by a connective sub-module. mainly utilize parallel data. There are also some works taking advantage of The structure of the pre-trained parts remains unmonolingual corpus. Zhang et al. (2020); Wang changed, and we train the grafting part to learn to translate. For universality and generalization, et al. (2020) use back-translation (BT) to improve we also extend the model to multilingual NMT, MNMT. However, for MNMT, BT is tremendously costly, reaching O(n), or even O(n2 ). Siddhant achieving mBERT+mGPT. et al. (2020); Wang et al. (2020) adopt multi-task Generally, the translation process can be divided learning (MTL), combining with other tasks such into three parts: representation, transduction, and generation, respectively achieved by the encoder, as masked language model (MLM) (Devlin et al., cross-attention, and decoder. In multilingual NMT, 2019), denoising auto-encoding (DAE) (Vincent et al., 2008), or masked sequence-to-sequence genthe transduction can only be trained with multiple eration (MASS) (Song et al., 2019). However, the parallel data. But the rest two can be pre-trained optimization target is different from"
2021.findings-emnlp.240,W18-1819,0,0.0317786,"imple: they attached a dedicated token given a source sentence s and the target language l, at the beginning of the source sentence to specify the multilingual MT system shall output a sentence the target language, while the rest of the model was that resembles human reference t. shared among all languages. The paper has set a Currently, Transformer (Vaswani et al., 2017) milestone of multilingual MT and has become the gains popularity and becomes the paradigm for basis for most subsequent work. state-of-the-art NMT systems. Here, we follow 2813 the recent implementations (Klein et al., 2017; Vaswani et al., 2018) of the pre-norm transformer, whose layer normalization is applied to the input of each sub-layer. The transformation of i-th sublayer taking xi as input can be formulated as: Output Layer Decoder x N + Feed Forward + Encoder x N Feed Forward xi`1 “ Fθ pxi q “ sub-layerθ pLN pxi qq ` xi (1) Layer Adapter Layer Norm Multilingual Embedding Deficiency CamachoCollados and Pilehvar (2018) addressed the meaning conflation deficiency problem of the word embedding as a single vector is limited for representing polysemy. We extend the meaning conflation deficiency into the multilingual scenario. Genera"
2021.findings-emnlp.240,P19-1117,0,0.0213368,"xternal knowledge from human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They appended small networks (named adapters) to a pre-trained base network and only tuned the adapter on the specific task. Houlsby et al. (2019) adopted the idea into NLP domain adaptation tasks and designed the adapter for the Transformer, as shown in Fig. 2a. Bapna and Firat (2019) further extended the model to MT domain adaptation, and they regarded multilingu"
2021.findings-emnlp.240,P19-1579,0,0.0266709,"strate the efficacy of CIAT through extensive experiments on IWSLT, OPUS-100, and WMT benchmark datasets, surpassing other multilingual models over most of the translation directions. 2 Related Work Recent studies paid more attention to the performance improvement of multilingual models based on Johnson et al. (2017)’s effort. Several improved the model with external knowledge from human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They"
2021.findings-emnlp.240,2020.acl-main.148,0,0.168709,"ings in different languages — bride in English refers to a woman soon to get married 1 Introduction while in French it means horse bridle. When it Machine translation (MT) is a core task in nat- comes to machine translation, the effect goes beural language processing. In recent years, neu- yond word embedding. As a single model has ral machine translation (NMT) approaches have bounded capacity, the multilingual learning may made tremendous progress and takes the lead in the cause negative influences among shared paramefield (Bahdanau et al., 2015; Vaswani et al., 2017; ters (Liu et al., 2017; Zhang et al., 2020). We Johnson et al., 2017). Conventionally, each NMT conjecture that such a performance degradation is model only tackles a single language direction (e.g. due to the interference across languages brought English Ñ German). A commonly used model by joint training on multiple language directions. like Transformer has S “ 240 million parameters. Such interference affects both joint token embedTherefore, Translating N language pairs requires ding and representations from intermediate layers. training models separately for each direction, re- We argue that resolving the interference is critical su"
2021.findings-emnlp.240,2020.acl-main.150,0,0.0410324,"rom human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They appended small networks (named adapters) to a pre-trained base network and only tuned the adapter on the specific task. Houlsby et al. (2019) adopted the idea into NLP domain adaptation tasks and designed the adapter for the Transformer, as shown in Fig. 2a. Bapna and Firat (2019) further extended the model to MT domain adaptation, and they regarded multilingual MT as a domain a"
2021.findings-emnlp.396,P19-1425,0,0.0197578,"E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study tran"
2021.findings-emnlp.396,2020.acl-main.529,0,0.0281867,"use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al."
2021.findings-emnlp.396,C18-1055,0,0.0198205,"ng findings. First, the performance of Secoco-E2E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors."
2021.findings-emnlp.396,W18-6317,0,0.0137347,"real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw"
2021.findings-emnlp.396,W18-1807,0,0.102508,"Missing"
2021.findings-emnlp.396,W18-6453,0,0.0172586,"Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a recon"
2021.findings-emnlp.396,D19-5506,0,0.0151559,"a for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, w"
2021.findings-emnlp.396,P19-1291,0,0.0169515,"l., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and the"
2021.findings-emnlp.396,N19-1314,0,0.0159315,"very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caus"
2021.findings-emnlp.396,N19-4009,0,0.0150201,"hod enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BLEU 4 Methods Speech BLEU 4 WMT En-De BLEU 4 AVG BLEU 4 Latency (ms/sent) BASE BASE +s"
2021.findings-emnlp.396,W18-6319,0,0.0117715,"roduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BL"
2021.findings-emnlp.396,P18-2037,0,0.0182766,"s potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthe"
2021.findings-emnlp.396,P16-1162,0,0.0421259,"et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness"
2021.findings-emnlp.396,N19-1190,0,0.0237898,"et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and then translate it. We present examples in Table 3. We 5 Conclusions can see that m"
2021.findings-emnlp.396,P19-1583,0,0.0203659,"ss of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two deco"
2021.findings-emnlp.396,D17-1319,0,0.0238081,"tion. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2"
2021.findings-emnlp.396,D18-1316,0,0.0167893,"e, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recogn"
2021.findings-emnlp.396,W19-5368,0,0.0796658,"ethod against the following three baseline systems. BASE One widely-used way to achieve NMT robustness is to mix raw clean data with noisy data to train NMT models. We refer to models trained with/without synthetic data as BASE/BASE+synthetic. R EPAIR To deal with noisy inputs, one might train a repair model to transform noisy inputs into clean inputs that a normally trained translation model can deal with. Both the repair and translation model are transformer-based models. As a pipeline model (repairing before translating), R EPAIR may suffer from error propagation. R ECONSTRUCTION We follow Zhou et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al"
2021.findings-emnlp.396,D17-1147,0,0.060923,"Missing"
2021.findings-emnlp.396,P18-2048,0,0.0159752,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.396,2021.naacl-industry.14,1,0.749888,"-Edit), as illustrated in the right part of Figure 2. In general, Secoco-E2E provides better robustness without sacrificing decoding speed. For Secoco-Edit, iterative editing enables better interpretability. Detailed editing operations provide a different perspective on how the model resists noise. 3 Experiments 3.1 We conducted our experiments on three test sets, including Dialogue, Speech, and WMT14 En-De, to examine the effectiveness of Secoco. Dialogue is a real-world Chinese-English dialogue test set constructed based on TV drama subtitles1 , which contains three types of natural noises (Wang et al., 2021). Speech is an in-house ChineseEnglish speech translation test set which contains various noise from ASR. To evaluate Secoco on different language pairs, we also used WMT14 EnDe test sets to build a noisy test set with random deletion and insertion operations. Table 1 shows the details of the three test sets. For Chinese-English translation, we used WMT2020 Chinese-English data2 (48M) for Dialogue, and CCMT3 (9M) for Speech. For WMT En-De, we adopted the widely-used WMT14 training data4 (4.5M). We synthesized corresponding 1 https://github.com/rgwt123/DialogueMT http://www.statmt.org/wmt20/tra"
2021.findings-emnlp.396,P19-1123,0,0.0345329,"Missing"
2021.findings-emnlp.396,W18-6314,0,0.0205705,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.iwslt-1.6,2020.iwslt-1.3,0,0.249258,"Missing"
2021.iwslt-1.6,N19-1006,0,0.0218955,"AI Lab), including cascade and end-to-end speech translation (ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen"
2021.iwslt-1.6,W19-5206,0,0.0143123,"mber of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072, and the attention head is 12 for MT#5 only. We use Adam optimizer with the same schedule algorithm as Vaswani et al. (2017). All models are trained with a global batch size of 65,536. Knowledge Distillation Sequence-level knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) is another useful technique to improve performance. In this way, w"
2021.iwslt-1.6,N19-1202,0,0.16845,"Missing"
2021.iwslt-1.6,2020.emnlp-main.644,0,0.0264713,"(ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen. 250,942 281,241 562,517 157,909 111,600 450 961 899 231 165 Transf"
2021.iwslt-1.6,W18-6319,0,0.0158439,"moved from both hypothesis and reference. We use word error rate (WER) to evaluate the ASR model and report case-sensitive detokenized BLEU10 for MT. No other data segmentation techniques are applied to the dev/test sets. Results on MuST-C dev and tst-COMMON, as well as dev(v1) and tst-COMMON(v1) from MuST-C v1 (Gangi et al., 2019) are listed together, which serve as strong baselines for comparison purpose in the end-to-end speech translation field. When evaluating the simultaneous translation, we use the official SimulEval (Ma et al., 2020) toolkit and report case-sensitive detokenized BLEU (Post, 2018) and Average Lagging (Ma et al., 2019) Inference We explore the look-ahead beam search strategy for inference. Specifically, we apply beam search to generate M (M > 1) tokens at each decoding step and pick the first token in the one with the highest log-probability out of multiple decoding paths. The look-ahead beam search achieves consistent performance improvement when keval is small while its 10 https://github.com/jniehues-kit/ sacrebleu 69 # System Training data composition dev tst-COM dev(v1) tst-COM(v1) Pure MT 1 MT (w/o punc. & lc) 2 MT (w/ punc. & tc) 3 ensemble MT (w/o punc. & lc) 4 e"
2021.iwslt-1.6,D16-1139,0,0.177957,"test set surpasses the end-to-end baseline by 7.9 BLUE scores, while it is still lagging behind our cascade model by 1.5 BLUE scores. It is not surprising since some well-optimized methods for MT can not be easily used on ST, such as back translation. However, our experience shows that the external data can effectively close the gap between end-to-end models and cascade models. In parallel, we also participate in the simultaneous NMT track, which translates in real-time. Our system is based on an efficient wait-k model (Elbayad et al., 2020). We investigate large-scale knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) and back translation methods. Specially, we develop a multi-path training strategy, which enables a unified model serving different wait-k paths. Our target is to obtain the best translation quality at different latency levels. The remaining part of the paper proceeds as follows. Section 2 and section 3 describe our cascade and end-to-end systems respectively. Section 4 presents the implementation of simultaneous NMT models. Each section starts from the training sources and how we synthesize large-scale data. And then, we give details about the model structure and techn"
2021.iwslt-1.6,P16-1009,0,0.0426042,"the model ensemble. The detailed setups are listed in Table 2. We over-sample the in-domain datasets (i.e., MuST-C/iwslt-corpusrelated portions) to improve the in-domain performance. Specifically, to control the ratio of samples from different data sources, we sample a fixed num1 ber of sentences being proportional to ( PNsNs ) T , s where Ns is the number of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072,"
2021.iwslt-1.6,P16-1162,0,0.0831726,"the model ensemble. The detailed setups are listed in Table 2. We over-sample the in-domain datasets (i.e., MuST-C/iwslt-corpusrelated portions) to improve the in-domain performance. Specifically, to control the ratio of samples from different data sources, we sample a fixed num1 ber of sentences being proportional to ( PNsNs ) T , s where Ns is the number of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072,"
2021.iwslt-1.6,2020.emnlp-demos.19,0,0.0351337,"ting the offline ST models, tags such as applause and laughing are removed from both hypothesis and reference. We use word error rate (WER) to evaluate the ASR model and report case-sensitive detokenized BLEU10 for MT. No other data segmentation techniques are applied to the dev/test sets. Results on MuST-C dev and tst-COMMON, as well as dev(v1) and tst-COMMON(v1) from MuST-C v1 (Gangi et al., 2019) are listed together, which serve as strong baselines for comparison purpose in the end-to-end speech translation field. When evaluating the simultaneous translation, we use the official SimulEval (Ma et al., 2020) toolkit and report case-sensitive detokenized BLEU (Post, 2018) and Average Lagging (Ma et al., 2019) Inference We explore the look-ahead beam search strategy for inference. Specifically, we apply beam search to generate M (M > 1) tokens at each decoding step and pick the first token in the one with the highest log-probability out of multiple decoding paths. The look-ahead beam search achieves consistent performance improvement when keval is small while its 10 https://github.com/jniehues-kit/ sacrebleu 69 # System Training data composition dev tst-COM dev(v1) tst-COM(v1) Pure MT 1 MT (w/o pun"
2021.iwslt-1.6,2020.acl-main.344,0,0.0966598,"speech translation (ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen. 250,942 281,241 562,517 157,909 111,60"
2021.iwslt-1.6,2020.acl-main.532,0,0.0133063,"data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072, and the attention head is 12 for MT#5 only. We use Adam optimizer with the same schedule algorithm as Vaswani et al. (2017). All models are trained with a global batch size of 65,536. Knowledge Distillation Sequence-level knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) is another useful technique to improve performance. In this way, we enlarge the trainin"
2021.iwslt-1.6,2021.acl-demo.7,1,0.860965,"Missing"
2021.naacl-industry.12,N19-1121,0,0.0183821,"system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in e"
2021.naacl-industry.12,N18-1032,0,0.021685,"e important than the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state"
2021.naacl-industry.12,P17-1042,0,0.0117438,"n supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the performances of zero-shot directions are greatly improved and the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and"
2021.naacl-industry.12,D18-1399,0,0.0746721,"ural machine translation (NMT) has achieved great success and reached satisfactory translation performance for several language pairs (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Such breakthroughs heavily depend on the availability of colossal amounts of bilingual sentence pairs, such as the some 40 million parallel sentence pairs used in the training of WMT14 English French Task. As bilingual sentence pairs are costly to collect, the success of NMT has not been fully duplicated in the vast majority of language pairs, especially for zero-resource languages. Recently, (Artetxe et al., 2018b; Lample et al., 2018a; ?) tackled this challenge by training unsupervised neural machine translation (UNMT) models using only monolingual data, which achieves considerably high accuracy, but still not on par with that of the state of the art supervised models. 89 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 89–96 June 6–11, 2021. ©2021 Association for Computational Linguistics tion (Figure 1(d)). We introduce cross-lingual supervision which aims at modeling explicit translation probabilities across languages. Taking three languages as an example, suppose the target unsupervised"
2021.naacl-industry.12,P19-1019,0,0.0178482,"the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and parallel data, and combine unsupervised and supervised machine translation through multilingual translation method into a single model C UNMT to ensure better performance for unsupervised language pairs. Strategies of Synthetic Data Generation For the synthetic data generation, the reported results are from greedy decoding for time efficiency. We compared the effec"
2021.naacl-industry.12,J82-2005,0,0.441809,"Missing"
2021.naacl-industry.12,P17-1176,0,0.0217729,", the performance of beam search is slightly inferior. A possible reason is that the beam search makes the synthetic data further biased on the learned pattern. The results suggest that C UNMT is exceedingly robust to the sampling strategies when performing forward and backward cross translation. 5 Related Work 6 Multilingual NMT It has been proven low resource machine translation can adopt methods to utilize other rich resource data in order to develop a better system. These methods include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT inc"
2021.naacl-industry.12,D18-1045,0,0.02103,"the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterpa"
2021.naacl-industry.12,2020.emnlp-main.210,1,0.72213,"baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the perfor"
2021.naacl-industry.12,D16-1026,0,0.0305797,"Missing"
2021.naacl-industry.12,2020.tacl-1.47,0,0.0624579,"man, 20 million sentences from available WMT monolingual News Crawl datasets were randomly selected. For Romanian monolingual data, all of the available Romanian sentences from News Crawl dataset were used and and were supplemented with WMT16 monolingual data to yield a total of in 2.9 million sentences. For parallel data, we use the standard WMT 2014 English-French dataset consisting of about 36M sentence pairs, and the 91 Supervised Transformer Comparison systems of UNMT UNMT (Lample et al., 2018c) EMB (Lample and Conneau, 2019) MLM (Lample and Conneau, 2019) MASS (Song et al., 2019) MBART (Liu et al., 2020) C UNMT C UNMT w/o Para. C UNMT w/ Para. C UNMT + Forward C UNMT + Backward + Forward (Fr, En, De) En-Fr Fr-En 41.0 - (De, En, Fr) En-De De-En 34.0 38.6 (Ro, En, Fr) En-Ro Ro-En 34.3 34.0 25.1 29.4 33.4 37.5 - 24.2 29.4 33.3 34.9 - 17.2 21.3 26.4 28.3 29.8 21.0 27.3 34.3 35.2 34.0 21.2 27.5 33.3 35.2 35.0 19.4 26.6 31.8 33.1 30.5 32.90 34.37 35.88 37.60 31.93 32.77 33.64 35.18 23.03 23.99 26.50 27.60 31.01 31.98 33.11 34.10 33.23 33.95 34.12 35.09 32.34 33.15 33.61 33.95 Table 1: Main results comparisons. MASS uses large scale pre-training and back translation during fine-tuning. MBART employ"
2021.naacl-industry.12,P16-1009,0,0.0259438,"s include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT incorporating distant supervision to tackle the challenge of the unsupervised translation task. By mixing different training schemes into one model and utilizing unrelated bilingual corpus, we greatly improve the performance of the unsupervised NMT direction. By joint training, C UNMT can serve all translation directions in one model. Empirically, C UNMT has been proven to deliver substantial improvements over several strong UNMT competitors and even achieve comparable performance to supe"
2021.naacl-industry.12,N19-1120,0,0.0358227,"Missing"
2021.naacl-industry.14,W19-4822,0,0.012826,"gual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construc"
2021.naacl-industry.14,N19-1311,0,0.0204783,"y WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construction, and the anonymous review"
2021.naacl-industry.14,P18-1163,0,0.0163769,"m dialogue this paper. inputs into forms that an ordinary NMT system We use a special token <sep&gt; as the separa- can deal with. REPAIR DIAL involves training a tor to concatenate sentences into a parallel sub- repair model to transform x0d to xd and a clean translation model that translates xd to yd . As a document {(xd , yd )}, as shown in Figure 1a. Contextual Perturbation We then consider gener- pipeline method, REPAIR DIAL may suffer from ating perturbation example x0d from xd with re- error propagation. spect to sub-document context. For ProDrop, ROBUST DIAL We extend the robust NMT 107 (Cheng et al., 2018) to dialogue-level translation. Specifically, we take both the original (xd , yd ) and the perturbated (x0d , yd ) bilingual pairs as training instances. So the model is more resilient on dialogue translation. During the inference stage, the robust model directly translates raw inputs into the target language. 3.3 MTL DIAL ROBUST DIAL has the potential to handle translation problems caused by noisy dialogue inputs. However, the internal mechanism is rather implicit and in a black box. Therefore, the improvement is limited, and it is not easy to analyze the improvement. To address this issue, w"
2021.naacl-industry.14,N19-1423,0,0.00610164,". As shown in  of Figure 1b, the only difference is that we have a contextual labeling module based on the encoder. We denote the final layer output of the Transformer encoder as H. For each token hi in H = (h1 , h2 , ..., hm ), the probability of contextual labeling is defined as: P (pi = j|X) = sof tmax(W · hi + b)[j] (1) where X = (x1 , x2 , ..., xm ) is the input sequence, P (pi = j|X) is the conditional probability that token xi is labeled as j (j ∈ 0, 1, 2, 3 as defined above). Here we make the labeling module as simple as possible, so that the Transformer encoder can behave like BERT (Devlin et al., 2019), learning more information related to perturbation and guiding the decoder to find desirable translations. During the training phrase, the model takes (xd , x0d , `x , `0x , yd ) as the training data. The learning process is driven by optimizing two objectives, corresponding to sequence labeling as auxiliary loss (LSL ) and machine translation as the primary loss (LM T ) in a multi-task learning framework. LSL = −log(P (`x |xd ) + P (`0x |x0d )) (2) LM T = −log(P (yd |xd ) + P (yd |x0d )) (3) where update_num is the number of updating steps during training. We introduce multi-task learning fo"
2021.naacl-industry.14,2020.wmt-1.3,0,0.0751299,"Missing"
2021.naacl-industry.14,P14-2047,0,0.0298297,"e analyzed challenges. c) We create a Chinese-English test set specifically containing those problems and conduct experiments to evaluate proposed method on this test set. 2 Analysis on Dialogue Translation There were already some manual analyses of translation errors, especially in the field of discourse translation. Voita et al. (2019) study EnglishRussian translation and find three main challenges for discourse translation: deixis, ellipsis, and lexical cohesion. For Chinese-English translation, tense consistency, connective mismatch, and content-heavy sentences are the most common issues (Li et al., 2014). Different from previous works, we mainly analyze the specific phenomena in dialogue translation. We begin with a study on a bilingual dialogue corpus (Wang et al., 2018).1 We translate source sentences into the target language at sentence level and compare translation results with reference at dialogue level. Around 1,000 dialogues are evaluated, and the results are reported in Table 2. From the statistic, we observe two persistent dialogue translation problems: pronoun dropping (ProDrop), punctuation drop1 Types of phenomena Correct ProDrop PunDrop Incorrect segmentation Other translation e"
2021.naacl-industry.14,E17-2004,0,0.0166318,"ose the task of translating Bilingual Multi-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep"
2021.naacl-industry.14,2020.emnlp-main.210,1,0.747598,"slation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve th"
2021.naacl-industry.14,N19-1190,0,0.0681472,"ulti-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with tran"
2021.naacl-industry.14,P19-1116,0,0.11,"Missing"
2021.naacl-industry.14,L16-1436,0,0.199346,"p (2) and DialTypo (3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in"
2021.naacl-industry.14,2020.tacl-1.47,0,0.0223257,". Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve their comfirst analyz"
2021.naacl-industry.14,W18-6311,0,0.339718,"(3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from sati"
2021.naacl-industry.14,2001.mtsummit-papers.68,0,0.0302211,"ages 105–112 June 6–11, 2021. ©2021 Association for Computational Linguistics encoder part automatically learns how to de-noise the noise input via explicit supervisory signals provided by additional contextual labeling. We also propose three strong baselines for dialogue translation, including repair (REPAIR DIAL) and robust (ROBUST DIAL) model. To alleviate the challenges arising from the scarcity of dialogue data, we use sub-documents in the bilingual parallel corpus to enable the model to learn from crosssentence context. Additionally as for evaluation, the most commonly used BLEU metric (Papineni et al., 2001) for NMT is not good enough to provide a deep look into the translation quality in such a scenario. Thus, we build a Chinese-English test set containing sentences with the issues in ProDrop, PunDrop and DialTypo, attached with the human translation and annotation. Finally, we get a test set of 300 dialogues with 1,931 parallel sentences. The main contributions of this paper are as follows: a) We analyze three challenges ProDrop, PunDrop and DialTypo, which greatly impact the understanding and translation of a dialogue. b) We propose a contextual multi-task learning method to tackle the analyze"
2021.naacl-industry.14,W18-6319,0,0.075643,"Missing"
2021.naacl-industry.14,P16-1162,0,0.0565515,"h separately. tences containing missing punctuation or typos according to the annotation information. As for ProDrop, we evaluate the translation quality by the percentage of correctly recovering and translating the dropped pronouns. 4.2 Settings We adopt the Chinese-English corpus from WMT20203 , with about 48M sentence pairs, as our bilingual training data D. We select newstest2019 as the development set. After splicing, we get Ddoc with 1.2M pairs and corresponding pertur0 bated dataset D0 and Ddoc with 48M and 1.2M pairs respectively. We use byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and limit the number of merge operations to a maximum of 30K. In our studies, all translation models are Transformer-big, including 6 layers for both encoders and decoders, 1024 dimensions for model, 4096 dimensions for FFN layers and 16 heads for attention. During training, we use label smoothing = 0.1 (Szegedy et al., 2016), attention dropout = 0.1 and dropout (Hinton et al., 2012) with a rate of 0.3 for all other layers. We use Adam (Kingma and Ba, 2015) to train the NMT models. β1 and β2 of Adam are set to 0.9 and 0.98, the learning rate is set to 0.0005, and gra"
2021.naacl-industry.15,P19-1285,0,0.031224,"esign a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training f"
2021.naacl-industry.15,N19-1423,0,0.0445457,"the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training frameworks, such as TensorFlow and PyTorch, require 1 accommodating flexible model architectures and https://github.com/NVIDIA/ backward propagation, which introduce additional FasterTransformer 113 Proceedings o"
2021.naacl-industry.15,D18-1045,0,0.0119964,"rks show that LightSeq achieves up to 14x speedup compared with TensorFlow and 1.4x compared with FasterTransformer, a concurrent CUDA implementation. The code is available at https://github.com/bytedance/ lightseq. 1 Introduction memory allocation and extra overhead of using fine-grain kernel functions. Therefore, the direct deployment of the training framework is not able to make full use of the hardware resource. Taking an example of machine translation, the Transformer big model currently takes roughly 2 seconds to translate a sentence, which is unacceptable in both academia and industry (Edunov et al., 2018; Hsu et al., 2020). Second, current optimizing compilers for deep learning such as TensorFlow XLA (Abadi et al., 2017), TVM (Chen et al., 2018) and Tensor RT (Vanholder, 2016) are mainly designed for fixed-size inputs. However, most NLP problems enjoy variable-length inputs, which are much more complex and require dynamic memory allocation. Therefore, a high-performance sequence inference library for variable-length inputs is required. There are several concurrent CUDA libraries which share a similar idea with our project, such as FasterTransformer 1 and TurboTransformers (Fang et al., 2021)."
2021.naacl-industry.15,Q17-1010,0,0.0509495,"Missing"
2021.naacl-industry.15,2020.sustainlp-1.7,0,0.0316767,"q achieves up to 14x speedup compared with TensorFlow and 1.4x compared with FasterTransformer, a concurrent CUDA implementation. The code is available at https://github.com/bytedance/ lightseq. 1 Introduction memory allocation and extra overhead of using fine-grain kernel functions. Therefore, the direct deployment of the training framework is not able to make full use of the hardware resource. Taking an example of machine translation, the Transformer big model currently takes roughly 2 seconds to translate a sentence, which is unacceptable in both academia and industry (Edunov et al., 2018; Hsu et al., 2020). Second, current optimizing compilers for deep learning such as TensorFlow XLA (Abadi et al., 2017), TVM (Chen et al., 2018) and Tensor RT (Vanholder, 2016) are mainly designed for fixed-size inputs. However, most NLP problems enjoy variable-length inputs, which are much more complex and require dynamic memory allocation. Therefore, a high-performance sequence inference library for variable-length inputs is required. There are several concurrent CUDA libraries which share a similar idea with our project, such as FasterTransformer 1 and TurboTransformers (Fang et al., 2021). We will highlight"
2021.naacl-industry.15,2020.sustainlp-1.20,0,0.0610973,"Missing"
2021.naacl-industry.15,2020.emnlp-main.210,1,0.711389,"allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training frameworks, such as TensorFlow and PyTorch, require 1 accommodating flexible model architectures and https://github.com/NVIDIA/ backward propagation, which introduce additional FasterTransformer 113 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 113–120 June 6"
2021.naacl-industry.15,D15-1166,0,0.0393122,"ow approaches. Second, we specially design a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence pr"
2021.naacl-industry.15,2020.findings-emnlp.217,0,0.0553695,"d, we specially design a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems"
2021.naacl-main.457,W18-6402,0,0.0749247,"ted work MNMT As a language shared by people worldwide, visual modality may help machines have a more comprehensive perception of the real world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) pr"
2021.naacl-main.457,W17-4746,0,0.0357485,"Missing"
2021.naacl-main.457,P19-1653,0,0.0387452,"ation to a shared space and learn with over, ImagiT is also flexible, accepting external the auxiliary triplet alignment task. The common parallel text data or non-parallel image caption- practice is to use convolutional neural networks to ing data. We evaluate our Imagination modal on extract visual information and then using attention the Multi30K dataset. The experiment results show mechanisms to extract visual contexts (Caglayan that our proposed method significantly outperforms et al., 2016; Calixto et al., 2016; Libovický and the text-only NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer"
2021.naacl-main.457,N19-1422,0,0.0343201,"Missing"
2021.naacl-main.457,P02-1040,0,0.108833,"Missing"
2021.naacl-main.457,W16-2359,0,0.0177404,"ask learning framework to ground viappealing method in low-resource scenario. More- sual representation to a shared space and learn with over, ImagiT is also flexible, accepting external the auxiliary triplet alignment task. The common parallel text data or non-parallel image caption- practice is to use convolutional neural networks to ing data. We evaluate our Imagination modal on extract visual information and then using attention the Multi30K dataset. The experiment results show mechanisms to extract visual contexts (Caglayan that our proposed method significantly outperforms et al., 2016; Calixto et al., 2016; Libovický and the text-only NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or u"
2021.naacl-main.457,D17-1105,0,0.0232848,"slation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) prosual information through imagination, making it an pose a multitask learning framework to ground viappealing method in low-resource scenario. More- sual representation to a shared space and learn with over, ImagiT"
2021.naacl-main.457,P17-1175,0,0.040814,"Missing"
2021.naacl-main.457,P19-1642,0,0.0615196,"Missing"
2021.naacl-main.457,W14-3348,0,0.0506089,"Missing"
2021.naacl-main.457,W17-4718,0,0.27698,"1: The problem setup of our proposed ImagiT is different from existing multimodal NMT. A multimodal NMT model takes both text and paired image as the input, while ImagiT takes only sentence in the source language as the usual NMT task. ImagiT synthesizes an image and utilize the internal visual representation to assist translation. 2015; Vaswani et al., 2017). Such limitations hinder the applicability of visual information in NMT. Visual foundation has been introduced in a novel multimodal Neural Machine Translation (MNMT) To address the bottlenecks mentioned above, task (Specia et al., 2016; Elliott et al., 2017; Bar- Zhang et al. (2020) propose to build a lookup table rault et al., 2018), which uses bilingual (or multi- from an image dataset and then using the searchlingual) parallel corpora annotated by images de- based method to retrieve pictures that match the scribing sentences’ contents (see Figure 1(a)). The source language keywords. However, the lookup superiority of MNMT lies in its ability to use visual table is built from Multi30K, which leads to a relinformation to improve the quality of translation, atively limited coverage of the pictures, and pobut its effectiveness largely depends on"
2021.naacl-main.457,W16-3210,0,0.0474565,"Missing"
2021.naacl-main.457,I17-1014,0,0.216479,"words. However, the lookup superiority of MNMT lies in its ability to use visual table is built from Multi30K, which leads to a relinformation to improve the quality of translation, atively limited coverage of the pictures, and pobut its effectiveness largely depends on the avail- tentially introduces much irrelevant noise. It does ability of data sets, especially the quantity and qual- not always find the exact image corresponding to ity of annotated images. In addition, because the the text, or the image may not even exist in the cost of manual image annotation is relatively high, database. Elliott and Kádár (2017) present a multiat this stage, MNMT is mostly applied on a small task learning framework to ground visual represenand specific dataset, Multi30K (Elliott et al., 2016), tation to a shared space. Their architecture called and is not suitable for large-scale text-only Neu- “imagination” shares an encoder between a primary ral Machine Translation (NMT) (Bahdanau et al., NMT task and an auxiliary task of ranking the vi5738 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5738–5748 June 6–11, 2021."
2021.naacl-main.457,W16-2360,0,0.0194797,"eal world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) prosual information through imagination, making it an pose a multitask learning framework to ground viappealing method in low-resource"
2021.naacl-main.457,W16-2346,0,0.0421506,"Missing"
2021.naacl-main.457,2020.acl-main.400,0,0.274068,"ly NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer to avoid encoding irrelevant information in images, and Yin et al. (2020) propose a graph-based multimodal fusion encoder to capture various relationships. Text-to-image synthesis Traditional Text-toimage (T2I) synthesis mainly uses keywords to search for small image regions, and finally optimizes the entire layout (Zhu et al., 2007). After generative adversarial networks (GANs) (Goodfellow et al., 2014) were proposed, scholars have presented a variety of GAN-based T2I models. Reed et al. (2016) propose DC-GAN and design a direct and straightfo"
2021.naacl-main.457,2020.acl-main.273,0,0.0592254,"del complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer to avoid encoding irrelevant information in images, and Yin et al. (2020) propose a graph-based multimodal fusion encoder to capture various relationships. Text-to-image synthesis Traditional Text-toimage (T2I) synthesis mainly uses keywords to search for small image regions, and finally optimizes the entire layout (Zhu et al., 2007). After generative adversarial networks (GANs) (Goodfellow et al., 2014) were proposed, scholars have presented a variety of GAN-based T2I models. Reed et al. (2016) propose DC-GAN and design a direct and straightforward network and a training strategy for T2I generation. Zhang et al. (2017) propose stackGAN, which contains multiple cas"
2021.naacl-main.457,Q14-1006,0,0.398356,"xperiments to verify and analyze how imagination helps the translation. 2 Related work MNMT As a language shared by people worldwide, visual modality may help machines have a more comprehensive perception of the real world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inf"
2021.naacl-main.457,D18-1400,0,0.0449185,"Missing"
C18-1124,buck-etal-2014-n,0,0.0611415,"Missing"
C18-1124,P15-1001,0,0.0303019,"ranslated text conditioned on the input representation. A potential issue with this encoderdecoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with translating long sentences. A recent successful extension of NMT models is the attention mechanism which conducts a soft search over source tokens and yields an attentive vector to represent the most relevant segments of the source sentence for the current decoding state (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Vaswani et al., 2017). The typical attention mechanism frees the neural translation model from having to squash all the information of the source sentence into a fixed vector, however, it ignores some important information hidden in the target sequence (Cheng et al., 2016). At least three challenges still remains in the translation process. The first issue relates to the memory compression problems in the decoding process. During each translation step, a hidden state vector implicitly maintains at least two types of information, including both the most relevant source con"
C18-1124,P17-1064,1,0.851355,"for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the simil"
C18-1124,D15-1166,0,0.0604492,"ext incrementally from left to right. In the NMT task, Wang et al. (2016) present a decoder enhanced decoder with an external shared memory which extends the capacity of the network and has the potential to read, write, and forget information. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 201"
C18-1124,D16-1096,0,0.0185334,"ion. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be vie"
C18-1124,N18-1124,0,0.0379124,"Missing"
C18-1124,P02-1040,0,0.100662,"mplementation of gate combination where the the gating weights are equal to the attention weights. 4 4.1 Experiments Datasets We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, English-German. The evaluation metric is BLEU. For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from LDC corpora1 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04), 2005 (MT05), and 2006 (MT06) datasets as our test sets. For English-German, to compare with the results reported by previous work, we used the same subset of the WMT 2014 training corpus that contains 4.5M sentence pairs with 91M English words and 87M German words. The concatenation of news-test 2012 and news-test 2013 is used as the"
C18-1124,P16-5005,0,0.360122,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1008,0,0.340085,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1125,0,0.0295194,"Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approac"
C18-1124,D16-1093,0,0.0257529,"the potential to better handle sentences of arbitrary length. Second, we forcus on controlling the information flow between the source side memory and the target side memory and design a gate to balance the contribution of the two-sides. Recurrent Residual Networks Our work is also related to residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2015; Szegedy et al., 2016). Recently, several architectures using residual connections with LSTMs have been proposed (Kim et al., 2017; Wang, 2017; Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016;"
C18-1124,P17-1013,1,0.800529,"Missing"
C18-1124,D16-1160,0,0.0213139,"diction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the similar idea with these work"
C18-1124,P17-1140,1,0.837212,"A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be viewed as a variant of R"
C18-1124,Q16-1027,0,0.0191054,"de attention based NMT. Encoder The goal of the encoder is to build meaningful representations of source sentences. The typical encoder consists of a bidirectional RNN which processes the raw input in backward and forward direction with two separate layers, and then concatenates them together. In this work, we choose another bidirectional approach to process the sequence in order to learn more temporal dependencies. Specifically, an RNN layer processes the input sequence in a forward direction. The output of this layer is taken by an upper RNN layer as input, processed in a reverse direction (Zhou et al., 2016). More formally,The encoder network reads the source input x = {x1 , ..., xT } and processes it into a source side memory M s = {h1 , h2 · · · , hT }. where xi ∈ Rdx . The output on layer ` is  xt , `=1 ` (2) ht = `−1 ` LSTM(ht+d , ht ), ` > 1 where • h`t ∈ Rdh gives the output of layer ` at location t. • The directions are marked by a direction term d = (−1)` . If we fixed d to −1, the input will be processed in forward direction, otherwise backward direction. • We only apply the top-most hidden states as the source side memory which is then fed to the decoder. Decoder The decoder uses anoth"
D16-1027,D16-1053,0,0.025072,"Missing"
D16-1027,P15-2088,1,0.891904,"Missing"
D16-1027,D15-1166,0,0.0416826,"Missing"
D16-1027,P02-1040,0,0.097279,"Missing"
D16-1027,P16-1008,1,0.249335,"Missing"
D16-1027,D11-1020,1,0.66588,"Missing"
D19-1074,N03-1017,0,0.0249257,"T achieves comparable results with the Transformer system. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems.1 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Gehring et al., 2017a; Kalchbrenner et al., 2016; Sennrich et al., 2015; Vaswani et al., 2017). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network 1 The work is partially done when the first author worked at Tencent. 803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 803–812, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Output Probabilities effective (Sabour et al., 2017; Zhao et al., 2018; Gong et al., 2018). Following a similar spirit to use this"
D19-1074,buck-etal-2014-n,0,0.0480794,"Missing"
D19-1074,P05-1033,0,0.131559,"e results with the Transformer system. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems.1 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Gehring et al., 2017a; Kalchbrenner et al., 2016; Sennrich et al., 2015; Vaswani et al., 2017). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network 1 The work is partially done when the first author worked at Tencent. 803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 803–812, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Output Probabilities effective (Sabour et al., 2017; Zhao et al., 2018; Gong et al., 2018). Following a similar spirit to use this technique, we p"
D19-1074,D15-1166,0,0.0748147,"at is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, C AP S NMT achieves comparable results with the Transformer system. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems.1 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Gehring et al., 2017a; Kalchbrenner et al., 2016; Sennrich et al., 2015; Vaswani et al., 2017). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network 1 The work is partially done when the first author worked at Tencent. 803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 803–812,"
D19-1074,P02-1040,0,0.105344,"Vaswani et al., 2017). Their experimental results strongly demonstrate that adding positional information in the text is more effective than in image since there is some sequential information in the sentence. In this work, we follow (Vaswani et al., 2017) to apply sine and cosine functions of different frequencies. 4 4.1 Datasets We mainly evaluated C APS NMT on the widely used WMT English-German and English-French translation task. The evaluation metric is BLEU. We tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work (Papineni et al., 2002). For English-German, to compare with the results reported by previous work, we used the same subset of the WMT 2014 training corpus that contains 4.5M sentence pairs with 91M English words and 87M German words. The concatenation of news-test 2012 and news-test 2013 is used as the validation set and news-test 2014 as the test set. To evaluate at scale, we also report the results of English-French. To compare with the results reported by previous work on end-to-end NMT, we used the same subset of the WMT 2014 training corpus that contains 36M sentence pairs. The concatenation of news-test 2012"
D19-1074,D13-1176,0,0.0610405,"igure 4. In particular, we test the BLEU scores on sentences longer than {0, 10, 20, 30, 40, 50}. We were surprised to discover that the capsule encoder did well on medium-length sentences. There is no degradation on sentences with less than 40 words, however, there is still a gap on the longest sentences. A deeper capsule encoder potentially helps to address the degradation problem and we will leave this in the future work. BLEU 28 5 Linear Time Neural Machine Translation Several papers have proposed to use neural networks to directly learn the conditional distribution from a parallel corpus(Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner et al., 2016). In (Sutskever et al., 2014), an RNN was used to encode a source sentence and starting from the last hidden state, to decode a target sentence. Different the RNN based approach, Kalchbrenner et al., (2016) propose ByteNet which makes use of the convolution networks to build the liner-time NMT system. Unlike the previous work, the C APS NMT encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CapsNMT Transformer 27 26 0 10 20 30 40 Related Work 50"
D19-1074,1983.tc-1.13,0,0.75607,"Missing"
D19-1074,D18-1350,0,0.0694294,"conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network 1 The work is partially done when the first author worked at Tencent. 803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 803–812, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Output Probabilities effective (Sabour et al., 2017; Zhao et al., 2018; Gong et al., 2018). Following a similar spirit to use this technique, we present C APS NMT, which is characterized by capsule encoder to address the drawbacks of the conventional linear-time approaches. The capsule encoder processes the attractive potential to address the aggregation issue, and then introduces an iterative routing policy to decide the credit attribution between nodes from lower (child) and higher (parent) layers. Three strategies are also proposed to stabilize the dynamic routing process. We empirically verify C APS NMT on WMT14 English-German task and a larger WMT14 English"
D19-1074,Q16-1027,0,0.0415139,"Missing"
P15-1003,P14-1062,0,0.207783,"Missing"
P15-1003,J07-2003,0,0.064903,"Missing"
P15-1003,D14-1179,0,0.0842528,"Missing"
P15-1003,P05-1066,0,0.106045,"Missing"
P15-1003,P14-1129,0,0.0614801,"Missing"
P15-1003,N04-1035,0,0.0736712,"Missing"
P15-1003,P07-1019,0,0.0144132,"Missing"
P15-1003,D13-1176,0,0.131209,"Missing"
P15-1003,N03-1017,0,0.00644014,"Missing"
P15-1003,P07-2045,0,0.0132544,"Missing"
P15-1003,D13-1108,1,0.848069,"Missing"
P15-1003,P02-1038,0,0.428481,"Missing"
P15-1003,J03-1002,0,0.00890853,"Missing"
P15-1003,P03-1021,0,0.072223,"Missing"
P15-1003,P08-1066,0,0.0292075,"Missing"
P15-1003,D11-1020,1,0.715388,"Missing"
P15-1003,D13-1106,0,\N,Missing
P15-1151,D13-1106,0,0.0780662,"Missing"
P15-1151,D11-1033,0,0.0341918,"Missing"
P15-1151,J92-1002,0,0.559939,"Missing"
P15-1151,P96-1041,0,0.351522,"Missing"
P15-1151,P14-1129,0,0.0225834,"Missing"
P15-1151,D13-1176,0,0.0950312,"Missing"
P15-1151,P14-1062,0,0.0830922,"Missing"
P15-1151,P02-1038,0,0.155749,"Missing"
P15-1151,D13-1140,0,0.0268192,"Missing"
P17-1013,buck-etal-2014-n,0,0.111414,"Missing"
P17-1013,P05-1033,0,0.0333576,"d the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 201"
P17-1013,D14-1179,0,0.0307604,"Missing"
P17-1013,P15-1001,0,0.26178,"study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014"
P17-1013,N03-1017,0,0.0167194,"EU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivasta"
P17-1013,P06-1077,1,0.570823,"orted results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NM"
P17-1013,D15-1166,0,0.699847,"through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed repres"
P17-1013,P02-1040,0,0.0977629,"arget word embedding at time step t, ct is dynamically obtained follows Equation (10). There are Ldec layers of RNNs armed with LAUs in the decoder. At inference stage, we only utilize the top-most hidden states sLdec to make the final prediction with a softmax layer: p(yi |y&lt;i , x) = softmax(Wo siLdec ) (12) . 4 Experiments 4.1 Setup We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, EnglishGerman. The evaluation metric is BLEU2 (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from 1 github.com/nyu-dl/dl4mt-tutorial/ tree/master/session2 2 For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work. LDC corpora3 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) datasets as our test sets. For English"
P17-1013,D16-1050,0,0.0425113,"Missing"
P17-1013,D16-1160,0,0.0389859,"ective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sut"
P17-1013,Q16-1027,1,0.434023,"nts, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs). Wu et al. (2016) and Zhou et al. (2016) found that deep architectures in both the encoder and decoder are essential for capturing subtle irregularities in the source and target languages. However, training a deep neural network is not as simple as stacking layers. Optimization often becomes increasingly difficult with more layers. One reasonable explanation is the notorious problem of vanishing/exploding gradients which was first studied in the context of vanilla RNNs (Pascanu et al., 2013b). Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connect"
P17-1013,P16-1159,0,0.0575075,"Missing"
P17-1013,P16-1008,1,0.825673,"surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the inp"
P17-1013,P06-1066,1,0.72775,"he same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turne"
P17-1013,P08-1023,1,\N,Missing
P17-1013,P16-5005,0,\N,Missing
P17-1140,P05-1033,0,0.0285008,"mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src re"
P17-1140,P06-1067,0,0.478247,"are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points. Figure 1: The source word “yiju” does not obtain appropriate attention and its word sense is completely neglected. To enhance the attention mechanism, implicit word reordering knowledge needs to be incorporated into attention-based NMT. In this paper, we introduce three distortion models that originated from SMT (Brown et al., 1993; Koehn et al., 2003; Och et al., 2004; Tillmann, 2004; Al-Onaizan and Papineni, 2006), so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word. Our focus is to extend the attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Our models have three merits: 1. Extended word reordering knowledge. Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism. 2. Convenient to be incorporated into attention-based NMT. Our distortion models are d"
P17-1140,W14-4012,0,0.0500402,"Missing"
P17-1140,D14-1179,0,0.0527966,"Missing"
P17-1140,P05-1066,0,0.0969476,"task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are concatenated at the corresponding p"
P17-1140,N04-1035,0,0.0412781,"the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in S-Distortion model is"
P17-1140,P15-1001,0,0.0584537,"dentical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count”"
P17-1140,D13-1176,0,0.052373,"translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the lat"
P17-1140,C16-1205,1,0.702274,"proving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearch∗ (30K) + T-Distortion + S-Distortion + H-Distortion BLEU 20.90 24.33‡ 24.10‡ 24."
P17-1140,J03-1002,0,0.0524536,"Missing"
P17-1140,P02-1040,0,0.0984942,"3 Training We carry the translation task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are c"
P17-1140,P16-1162,0,0.0948834,"performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the trainin"
P17-1140,P16-1159,0,0.0390894,"e state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word col"
P17-1140,N03-1017,0,0.31876,"ls enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al"
P17-1140,D15-1166,0,0.274308,"ur system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequ"
P17-1140,P16-1008,0,0.387371,"ion quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The col"
P17-1140,D16-1027,1,0.702952,"s improvements on BLEU scores proves the eﬀectiveness of proposed approaches in improving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearc"
P17-1140,P01-1067,0,0.216794,"nce k that conditioned on the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in"
P17-1140,Q16-1027,1,0.71397,"troduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The collocation “zuixin yi"
P17-1140,N04-4026,0,\N,Missing
P19-1125,W17-4123,0,0.186965,"al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables"
P19-1125,D16-1009,0,0.0211846,"(T 0 t/T ) (Gu et al., 2017; Lee et al., 2018). As the source and target sentences are often of different lengths, AT model need to predict the target length T 0 during inference stage. The length prediction problem can be viewed as a typical classification problem based on the output of the encoder. we follow Lee et al. (2018) to predict the length of the target sequence. The proposed Round function is unstable and non-differentiable, which make the decoding task difficult. We therefore propose a differentiable and robust method named SoftCopy following the spirit of the attention mechanism (Hahn and Keller, 2016; Bengio, 2009). The weight wi,j depends on the distance relationship between the source position i and the target position j. wij = softmax(−|j − i|/τ ) (10) where xi is usually the source embedding at position i. It is also worth mentioning that we take the top-most hidden states instead of the word embedding as xi in order to cache the global context information. 3.3.2 Learning from AT Experts The conditional independence assumption prevents NAT model from properly capturing the highly multimodal distribution of target translations. AT models takes already generated target tokens as inputs,"
P19-1125,P82-1020,0,0.816089,"Missing"
P19-1125,P07-2045,0,0.0125476,"or method in (Gu et al., 2017), (Lee et al., 2018) and (Kaiser et al., 2018) respectively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We rep"
P19-1125,N03-1017,0,0.0808619,"the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De. 1 (a) Autoregressive NMT (b) Non-Autoregressive NMT Figure 1: Neural architectures for Autoregressive NMT and Non-Autoregressive NMT. Introduction Neural machine translation (NMT) with encoderdecoder architectures (Sutskever et al., 2014; Cho et al., 2014) achieve significantly improved performance compared with traditional statistical methods(Koehn et al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al."
P19-1125,D18-1149,0,0.388239,"2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computat"
P19-1125,D18-1336,0,0.243494,"ss, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving sig"
P19-1125,P16-1162,0,0.149108,"tively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We replace the reference target sentence of each pair of training example (X, Y ) with a new"
P19-1125,D18-1044,0,0.0568991,"perty of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving significantly fast tran"
W18-6429,N03-1017,0,0.0610058,"Missing"
W18-6429,N16-1046,0,0.044018,"Missing"
W18-6429,D15-1166,0,0.0907201,"Missing"
W18-6429,P16-1008,0,0.0354354,"Missing"
W18-6429,Q16-1027,0,0.0426397,"Missing"
