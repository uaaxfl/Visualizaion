2021.naacl-main.51,Preregistering {NLP} research,2021,-1,-1,3,0.813391,3387,emiel miltenburg,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research."
2020.inlg-1.10,"The {CACAPO} Dataset: A Multilingual, Multi-Domain Dataset for Neural Pipeline and End-to-End Data-to-Text Generation",2020,-1,-1,4,1,3388,chris lee,Proceedings of the 13th International Conference on Natural Language Generation,0,"This paper describes the CACAPO dataset, built for training both neural pipeline and end-to-end data-to-text language generation systems. The dataset is multilingual (Dutch and English), and contains almost 10,000 sentences from human-written news texts in the sports, weather, stocks, and incidents domain, together with aligned attribute-value paired data. The dataset is unique in that the linguistic variation and indirect ways of expressing data in these texts reflect the challenges of real world NLG tasks."
2020.inlg-1.45,Gradations of Error Severity in Automatic Image Descriptions,2020,-1,-1,3,0.989438,3387,emiel miltenburg,Proceedings of the 13th International Conference on Natural Language Generation,0,"Earlier research has shown that evaluation metrics based on textual similarity (e.g., BLEU, CIDEr, Meteor) do not correlate well with human evaluation scores for automatically generated text. We carried out an experiment with Chinese speakers, where we systematically manipulated image descriptions to contain different kinds of errors. Because our manipulated descriptions form minimal pairs with the reference descriptions, we are able to assess the impact of different kinds of errors on the perceived quality of the descriptions. Our results show that different kinds of errors elicit significantly different evaluation scores, even though all erroneous descriptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe."
2020.evalnlgeval-1.3,Evaluation rules! On the use of grammars and rule-based systems for {NLG} evaluation,2020,-1,-1,4,0.989438,3387,emiel miltenburg,Proceedings of the 1st Workshop on Evaluating NLG Evaluation,0,"NLG researchers often use uncontrolled corpora to train and evaluate their systems, using textual similarity metrics, such as BLEU. This position paper argues in favour of two alternative evaluation strategies, using grammars or rule-based systems. These strategies are particularly useful to identify the strengths and weaknesses of different systems. We contrast our proposals with the (extended) WebNLG dataset, which is revealed to have a skewed distribution of predicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach."
W19-8643,Best practices for the human evaluation of automatically generated text,2019,0,4,5,1,3388,chris lee,Proceedings of the 12th International Conference on Natural Language Generation,0,"Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG."
W19-8649,On task effects in {NLG} corpus elicitation: a replication study using mixed effects modeling,2019,0,0,5,1,3387,emiel miltenburg,Proceedings of the 12th International Conference on Natural Language Generation,0,"Task effects in NLG corpus elicitation recently started to receive more attention, but are usually not modeled statistically. We present a controlled replication of the study by Van Miltenburg et al. (2018b), contrasting spoken with written descriptions. We collected additional written Dutch descriptions to supplement the spoken data from the DIDEC corpus, and analyzed the descriptions using mixed effects modeling to account for variation between participants and items. Our results show that the effects of modality largely disappear in a controlled setting."
W19-8656,A Personalized Data-to-Text Support Tool for Cancer Patients,2019,0,0,6,0,23352,saar hommes,Proceedings of the 12th International Conference on Natural Language Generation,0,"In this paper, we present a novel data-to-text system for cancer patients, providing information on quality of life implications after treatment, which can be embedded in the context of shared decision making. Currently, information on quality of life implications is often not discussed, partly because (until recently) data has been lacking. In our work, we rely on a newly developed prediction model, which assigns patients to scenarios. Furthermore, we use data-to-text techniques to explain these scenario-based predictions in personalized and understandable language. We highlight the possibilities of NLG for personalization, discuss ethical implications and also present the outcomes of a first evaluation with clinicians."
R19-1070,Question Similarity in Community Question Answering: A Systematic Exploration of Preprocessing Methods and Models,2019,0,0,3,1,24427,florian kunneman,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers have developed methods to automatically detect question-similarity, which was one of the shared tasks in SemEval. The best performing systems for this task made use of Syntactic Tree Kernels or the SoftCosine metric. However, it remains unclear why these methods seem to work, whether their performance can be improved by better preprocessing methods and what kinds of errors they (and other methods) make. In this paper, we therefore systematically combine and compare these two approaches with the more traditional BM25 and translation-based models. Moreover, we analyze the impact of preprocessing steps (lowercasing, suppression of punctuation and stop words removal) and word meaning similarity based on different distributions (word translation probability, Word2Vec, fastText and ELMo) on the performance of the task. We conduct an error analysis to gain insight into the differences in performance between the system set-ups. The implementation is made publicly available from https://github.com/fkunneman/DiscoSumo/tree/master/ranlp."
D19-6307,Surface Realization Shared Task 2019 ({MSR}19): The Team 6 Approach,2019,0,0,2,1,5949,thiago ferreira,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR{'}19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rule-based and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available."
D19-5512,Automatic identification of writers{'} intentions: Comparing different methods for predicting relationship goals in online dating profile texts,2019,0,1,3,1,3388,chris lee,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) LIWC, (2) machine learning, and (3) a human baseline. A newly collected corpus of online dating profile texts (a genre not explored before in the ACL anthology) was used, accompanied by the profile writers{'} self-selected relationship goal (long-term versus date). These three approaches were tested by comparing their performance on identifying both the intended relationship goal and content-related text labels. Results show that LIWC and machine learning models correlate with human evaluations in terms of content-related labels. LIWC{'}s content-related labels corresponded more strongly to humans than those of the classifier. Moreover, all approaches were similarly accurate in predicting the relationship goal."
D19-1052,Neural data-to-text generation: A comparison between pipeline and end-to-end architectures,2019,0,1,4,1,5949,thiago ferreira,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. By contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer, two state-of-the art deep learning methods. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available."
W18-6901,Context-sensitive Natural Language Generation for robot-assisted second language tutoring,2018,0,1,3,0,27614,bram willemsen,Proceedings of the Workshop on {NLG} for Human{--}Robot Interaction,0,"This paper describes the L2TOR intelligent tutoring system (ITS), focusing primarily on its output generation module. The L2TOR ITS is developed for the purpose of investigating the efficacy of robot-assisted second language tutoring in early childhood. We explain the process of generating contextually-relevant utterances, such as task-specific feedback messages, and discuss challenges regarding multimodality and multilingualism for situated natural language generation from a robot tutoring perspective."
W18-6504,"Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods",2018,0,2,2,1,3388,chris lee,Proceedings of the 11th International Conference on Natural Language Generation,0,"The current study investigated novel techniques and methods for trainable approaches to data-to-text generation. Neural Machine Translation was explored for the conversion from data to text as well as the addition of extra templatization steps of the data input and text output in the conversion process. Evaluation using BLEU did not find the Neural Machine Translation technique to perform any better compared to rule-based or Statistical Machine Translation, and the templatization method seemed to perform similarly or sometimes worse compared to direct data-to-text conversion. However, the human evaluation metrics indicated that Neural Machine Translation yielded the highest quality output and that the templatization method was able to increase text quality in multiple situations."
W18-6521,Enriching the {W}eb{NLG} corpus,2018,0,4,3,1,5949,thiago ferreira,Proceedings of the 11th International Conference on Natural Language Generation,0,"This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a,b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, Lexicalization and Referring Expression Generation. We also produce a silver-standard German translation of the corpus to enable the exploitation of NLG approaches to other languages than English. The enriched corpus is publicly available."
W18-3910,Varying image description tasks: spoken versus written descriptions,2018,0,1,3,1,3387,emiel miltenburg,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"Automatic image description systems are commonly trained and evaluated on written image descriptions. At the same time, these systems are often used to provide spoken descriptions (e.g. for visually impaired users) through apps like TapTapSee or Seeing AI. This is not a problem, as long as spoken and written descriptions are very similar. However, linguistic research suggests that spoken language often differs from written language. These differences are not regular, and vary from context to context. Therefore, this paper investigates whether there are differences between written and spoken image descriptions, even if they are elicited through similar tasks. We compare descriptions produced in two languages (English and Dutch), and in both languages observe substantial differences between spoken and written descriptions. Future research should see if users prefer the spoken over the written style and, if so, aim to emulate spoken descriptions."
W18-3604,Surface Realization Shared Task 2018 ({SR}18): The {T}ilburg {U}niversity Approach,2018,0,1,3,1,5949,thiago ferreira,Proceedings of the First Workshop on Multilingual Surface Realisation,0,"This study describes the approach developed by the Tilburg University team to the shallow task of the Multilingual Surface Realization Shared Task 2018 (SR18). Based on (Castro Ferreira et al., 2017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 50 for 5 different languages (English, French, Italian, Portuguese and Spanish) and above 35 for the Dutch language."
P18-1182,{N}eural{REG}: An end-to-end approach to referring expression generation,2018,21,6,5,1,5949,thiago ferreira,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines."
C18-1082,"Evaluating the text quality, human likeness and tailoring component of {PASS}: A {D}utch data-to-text system for soccer",2018,0,0,3,1,3388,chris lee,Proceedings of the 27th International Conference on Computational Linguistics,0,"We present an evaluation of PASS, a data-to-text system that generates Dutch soccer reports from match statistics which are automatically tailored towards fans of one club or the other. The evaluation in this paper consists of two studies. An intrinsic human-based evaluation of the system{'}s output is described in the first study. In this study it was found that compared to human-written texts, computer-generated texts were rated slightly lower on style-related text components (fluency and clarity) and slightly higher in terms of the correctness of given information. Furthermore, results from the first study showed that tailoring was accurately recognized in most cases, and that participants struggled with correctly identifying whether a text was written by a human or computer. The second study investigated if tailoring affects perceived text quality, for which no results were garnered. This lack of results might be due to negative preconceptions about computer-generated texts which were found in the first study."
C18-1188,Aspect-based summarization of pros and cons in unstructured product reviews,2018,0,0,4,1,24427,florian kunneman,Proceedings of the 27th International Conference on Computational Linguistics,0,"We developed three systems for generating pros and cons summaries of product reviews. Automating this task eases the writing of product reviews, and offers readers quick access to the most important information. We compared SynPat, a system based on syntactic phrases selected on the basis of valence scores, against a neural-network-based system trained to map bag-of-words representations of reviews directly to pros and cons, and the same neural system trained on clusters of word-embedding encodings of similar pros and cons. We evaluated the systems in two ways: first on held-out reviews with gold-standard pros and cons, and second by asking human annotators to rate the systems{'} output on relevance and completeness. In the second evaluation, the gold-standard pros and cons were assessed along with the system output. We find that the human-generated summaries are not deemed as significantly more relevant or complete than the SynPat systems; the latter are scored higher than the human-generated summaries on a precision metric. The neural approaches yield a lower performance in the human assessment, and are outperformed by the baseline."
C18-1310,{DIDEC}: The {D}utch Image Description and Eye-tracking Corpus,2018,0,2,4,1,3387,emiel miltenburg,Proceedings of the 27th International Conference on Computational Linguistics,0,"We present a corpus of spoken Dutch image descriptions, paired with two sets of eye-tracking data: Free viewing, where participants look at images without any particular purpose, and Description viewing, where we track eye movements while participants produce spoken descriptions of the images they are viewing. This paper describes the data collection procedure and the corpus itself, and provides an initial analysis of self-corrections in image descriptions. We also present two studies showing the potential of this data. Though these studies mainly serve as an example, we do find two interesting results: (1) the eye-tracking data for the description viewing task is more coherent than for the free-viewing task; (2) variation in image descriptions (also called {`}image specificity{'}; Jas and Parikh, 2015) is only moderately correlated across different languages. Our corpus can be used to gain a deeper understanding of the image description task, particularly how visual attention is correlated with the image description process."
W17-3501,Linguistic realisation as machine translation: Comparing different {MT} models for {AMR}-to-text generation,2017,0,6,4,1,5949,thiago ferreira,Proceedings of the 10th International Conference on Natural Language Generation,0,"In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrase-based and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models."
W17-3513,"{PASS}: A {D}utch data-to-text system for soccer, targeted towards specific audiences",2017,21,6,2,1,3388,chris lee,Proceedings of the 10th International Conference on Natural Language Generation,0,"We present PASS, a data-to-text system that generates Dutch soccer reports from match statistics. One of the novel elements of PASS is the fact that the system produces corpus-based texts tailored towards fans of one club or the other, which can most prominently be observed in the tone of voice used in the reports. Furthermore, the system is open source and uses a modular design, which makes it relatively easy for people to add extensions. Human-based evaluation shows that people are generally positive towards PASS in regards to its clarity and fluency, and that the tailoring is accurately recognized in most cases."
E17-1062,"Generating flexible proper name references in text: Data, models and evaluation",2017,10,4,2,1,5949,thiago ferreira,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"This study introduces a statistical model able to generate variations of a proper name by taking into account the person to be mentioned, the discourse context and variation. The model relies on the REGnames corpus, a dataset with 53,102 proper name references to 1,000 people in different discourse contexts. We evaluate the versions of our model from the perspective of how human writers produce proper names, and also how human readers process them. The corpus and the model are publicly available."
W16-6608,Abstractive Compression of Captions with Attentive Recurrent Neural Networks,2016,32,0,2,1,19006,sander wubben,Proceedings of the 9th International Natural Language Generation conference,0,"INLG 2016 : The 9th International Natural Language Generation conference, Edinburgh, Scotland, September 5-8, 2016"
W16-6612,"The Multilingual Affective Soccer Corpus ({MASC}): Compiling a biased parallel corpus on soccer reportage in {E}nglish, {G}erman and {D}utch",2016,4,2,3,0,33321,nadine braun,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-6636,Towards proper name generation: a corpus analysis,2016,5,1,3,1,5949,thiago ferreira,Proceedings of the 9th International Natural Language Generation conference,0,"We introduce a corpus for the study of proper name generation. The corpus consists of proper name references to people in webpages, extracted from the Wikilinks corpus. In our analyses, we aim to identify the different ways, in terms of length and form, in which a proper names are produced throughout a text."
P16-1054,Towards more variation in text generation: Developing and evaluating variation models for choice of referential form,2016,20,8,2,1,5949,thiago ferreira,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model."
N16-1048,Individual Variation in the Choice of Referential Form,2016,9,3,2,1,5949,thiago ferreira,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This study aims to measure the variation between writers in their choices of referential form by collecting and analysing a new and publicly available corpus of referring expressions. The corpus is composed of referring expressions produced by different participants in identical situations. Results, measured in terms of normalized entropy, reveal substantial individual variation. We discuss the problems and prospects of this finding for automatic text generation applications."
W15-4706,Moving Targets: Human References to Unstable Landmarks,2015,6,0,2,0,33322,adriana baltaretu,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"In the present study, we investigate if speakers refer to moving entities in route directions (RDs) and how listeners evaluate these references. There is a general agreement that landmarks should be perceptually salient and stable objects. Animated movement attracts visual attention, making entities salient. We ask speakers to watch videos of crossroads and give RDs to listeners, who in turn have to choose a street on which to continue (Experiment 1) or choose the best instruction among three RDs (Experiment 2). Our results show that speakers mention moving entities, especially when their movement is informa- tive for the navigation task (Experiment 1). Listeners understand and use moving landmarks (Experiment 1), yet appreciate stable landmarks more (Experiment 2)."
wubben-etal-2014-creating,Creating and using large monolingual parallel corpora for sentential paraphrase generation,2014,42,4,3,1,19006,sander wubben,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we investigate the automatic generation of paraphrases by using machine translation techniques. Three contributions we make are the construction of a large paraphrase corpus for English and Dutch, a re-ranking heuristic to use machine translation for paraphrase generation and a proper evaluation methodology. A large parallel corpus is constructed by aligning clustered headlines that are scraped from a news aggregator site. To generate sentential paraphrases we use a standard phrase-based machine translation (PBMT) framework modified with a re-ranking component (henceforth PBMT-R). We demonstrate this approach for Dutch and English and evaluate by using human judgements collected from 76 participants. The judgments are compared to two automatic machine translation evaluation metrics. We observe that as the paraphrases deviate more from the source sentence, the performance of the PBMT-R system degrades less than that of the word substitution baseline system."
W13-2702,Using character overlap to improve language transformation,2013,21,0,2,1,19006,sander wubben,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,Language transformation can be defined as translating between diachronically distinct language variants. We investigate the transformation of Middle Dutch into Modern Dutch by means of machine translation. We demonstrate that by using character overlap the performance of the machine translation process can be improved for this task.
W13-2108,Graphs and Spatial Relations in the Generation of Referring Expressions,2013,32,9,3,0.288853,40967,jette viethen,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"When they introduced the Graph-Based Algorithm (GBA) for referring expression generation, Krahmer et al. (2003) flaunted the natural way in which it deals with relations between objects; but this feature has never been tested empirically. We fill this gap in this paper, exploring referring expression generation from the perspective of theGBAand focusing in particular on generating human-like expressions in visual scenes with spatial relations. We compare the originalGBAagainst a variant that we introduce to better reflect human reference, and find that although the originalGBAperforms reasonably well, our new algorithm offers an even better match to human data (77.91% Dice). Further, it can be extended to capture speaker variation, reaching an 82.83% Dice overlap with human-produced expressions."
W12-1503,"Learning Preferences for Referring Expression Generation: Effects of Domain, Language and Algorithm",2012,22,8,2,1,23349,ruud koolen,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"One important subtask of Referring Expression Generation (REG) algorithms is to select the attributes in a definite description for a given object. In this paper, we study how much training data is required for algorithms to do this properly. We compare two REG algorithms in terms of their performance: the classic Incremental Algorithm and the more recent Graph algorithm. Both rely on a notion of preferred attributes that can be learned from human descriptions. In our experiments, preferences are learned from training sets that vary in size, in two domains and languages. The results show that depending on the algorithm and the complexity of the domain, training on a handful of descriptions can already lead to a performance that is not significantly different from training on a much larger data set."
P12-1107,Sentence Simplification by Monolingual Machine Translation,2012,33,94,3,1,19006,sander wubben,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems."
J12-1006,Computational Generation of Referring Expressions: A Survey,2012,0,0,1,1,3389,emiel krahmer,Computational Linguistics,0,"This article offers a survey of computational research on referring expression generation (REG). It introduces the REG problem and describes early work in this area, discussing what basic assumptio..."
W11-1604,Comparing Phrase-based and Syntax-based Paraphrase Generation,2011,33,2,4,1,19006,sander wubben,Proceedings of the Workshop on Monolingual Text-To-Text Generation,0,"Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source."
P11-2116,Does Size Matter {--} How Much Data is Required to Train a {REG} Algorithm?,2011,15,6,3,0.916029,16451,mariet theune,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG). To enable comparison between different-sized training sets, a systematic training method was developed. The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance."
W10-4221,Cross-linguistic Attribute Selection for {REG}: Comparing {D}utch and {E}nglish,2010,8,4,3,0.916029,16451,mariet theune,Proceedings of the 6th International Natural Language Generation Conference,0,In this paper we describe a cross-linguistic experiment in attribute selection for referring expression generation. We used a graph-based attribute selection algorithm that was trained and cross-evaluated on English and Dutch data. The results indicate that attribute selection can be done in a largely language independent way.
W10-4223,Paraphrase Generation as Monolingual Translation: Data and Evaluation,2010,22,30,3,1,19006,sander wubben,Proceedings of the 6th International Natural Language Generation Conference,0,In this paper we investigate the automatic generation and evaluation of sentential paraphrases. We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT paraphrasing system over the word substitution system. We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence.
P10-2011,Preferences versus Adaptation during Referring Expression Generation,2010,18,13,2,1,23350,martijn goudbeek,Proceedings of the {ACL} 2010 Conference Short Papers,0,Current Referring Expression Generation algorithms rely on domain dependent preferences for both content selection and linguistic realization. We present two experiments showing that human speakers may opt for dispreferred properties and dispreferred modifier orderings when these were salient in a preceding interaction (without speakers being consciously aware of this). We discuss the impact of these findings for current generation algorithms.
koolen-krahmer-2010-tuna,The {D}-{TUNA} Corpus: A {D}utch Dataset for the Evaluation of Referring Expression Generation Algorithms,2010,21,8,2,1,23349,ruud koolen,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present the D-TUNA corpus, which is the first semantically annotated corpus of referring expressions in Dutch. Its primary function is to evaluate and improve the performance of REG algorithms. Such algorithms are computational models that automatically generate referring expressions by computing how a specific target can be identified to an addressee by distinguishing it from a set of distractor objects. We performed a large-scale production experiment, in which participants were asked to describe furniture items and people, and provided all descriptions with semantic information regarding the target and the distractor objects. Besides being useful for evaluating REG algorithms, the corpus addresses several other research goals. Firstly, the corpus contains both written and spoken referring expressions uttered in the direction of an addressee, which enables systematic analyses of how modality (text or speech) influences the human production of referring expressions. Secondly, due to its comparability with the English TUNA corpus, our Dutch corpus can be used to explore the differences between Dutch and English speakers regarding the production of referring expressions."
ruiter-etal-2010-human,Human Language Technology and Communicative Disabilities: Requirements and Possibilities for the Future,2010,-1,-1,4,0,45969,marina ruiter,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"For some years now, the Nederlandse Taalunie (Dutch Language Union) has been active in promoting the development of human language technology (HLT) applications for users of Dutch with communication disabilities. The reason is that HLT products and services may enable these users to improve their verbal autonomy and communication skills. We sought to identify a minimum common set of HLT resources that is required to develop tools for a wide range of communication disabilities. In order to reach this goal, we investigated the specific HLT needs of communicatively disabled people and related these needs to the underlying HLT software components. By analysing the availability and quality of these essential HLT resources, we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of Dutch. The results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of HLT resources for this target group. In the current study results were obtained for Dutch, but a similar approach can also be used for other languages."
J10-2007,Last Words: What Computational Linguists Can Learn from Psychologists (and Vice Versa),2010,41,14,1,1,3389,emiel krahmer,Computational Linguistics,0,"Sometimes I am amazed by how much the xefxacx81eld of computational linguistics haschanged in the past 15 to 20 years. In the mid-nineties, I was working in a researchinstitute where language and speech technologists worked in relatively close quarters.Speech technology seemed on the verge of a major breakthrough; this was around thetime that Bill Gates was quoted in Business Week as saying that speech was not justthe future of Windows, but the future of computing itself. At the same time, languagetechnology was, well, nowhere. Bill Gates certainly wasnxe2x80x99t championing language tech-nology in those days. And while the possible applications of speech technology seemedendless (who would use a keyboard in 2010, when speech-driven user interfaces wouldhave replaced traditional computers?) the language people were thinking hard aboutpossible applications for their admittedly somewhat immature technologies.Predicting the future is a tricky thing. No major breakthrough came for speechtechnology xe2x80x94 I am still typing this. However, language technology did change almostbeyond recognition. Perhaps one of the main reasons for this has been the explosivegrowth of the internet, which helped language technology in two different ways. Onthe one hand it instigated the development and rexefxacx81nement of techniques needed forsearching in document collections of unprecedented size, on the other it resulted in alarge increase of freely available text data. Recently, language technology has been par-ticularly successful for tasks where huge amounts of textual data is available to whichstatistical machine learning techniques can be applied (Halevy, Norvig, and Pereira2009). As a result of these developments, mainstream computational linguistics is nowa successful, application-oriented discipline which is particularly good at extractinginformation from sequences of words.But there is more to language than that. For speakers, words are the result of acomplex speech production process; for listeners they are what starts off the similarlycomplex comprehension process. However, in many current applications no attentionis given to the processes by which words are produced nor to the processes by whichthey can be understood. Language is treated as a product not as a process, in theterminology of Clark (1996). In addition, we use language not only as a vehicle forfactual information exchange; speakers may have all sorts of other intentions with theirwords; they may want to convince others to do or buy something, they may want toinduce a particular emotion in the addressee etc. These days, most of computationallinguistics (with a few notable exceptions, more about which below) has little to say"
C10-1085,Automatic analysis of semantic similarity in comparable text through syntactic tree matching,2010,18,12,2,1,32380,erwin marsi,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We propose to analyse semantic similarity in comparable text by matching syntactic trees and labeling the alignments according to one of five semantic similarity relations. We present a Memory-based Graph Matcher (MBGM) that performs both tasks simultaneously as a combination of exhaustive pairwise classification using a memory-based learner, followed by global optimization of the alignments using a combinatorial optimization algorithm. The method is evaluated on a monolingual treebank consisting of comparable Dutch news texts. Results show that it performs substantially above the baseline and close to the human reference."
W09-2812,Reducing Redundancy in Multi-document Summarization Using Lexical Semantic Similarity,2009,12,8,4,0,16715,iris hendrickx,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"We present an automatic multi-document summarization system for Dutch based on the MEAD system. We focus on redundancy detection, an essential ingredient of multi-document summarization. We introduce a semantic overlap detection tool, which goes beyond simple string matching. Our results so far do not confirm our expectation that this tool would outperform the other tested methods."
W09-0604,Is Sentence Compression an {NLG} task?,2009,19,3,2,1,32380,erwin marsi,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and grammaticality. We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work."
W09-0621,Clustering and Matching Headlines for Automatic Paraphrase Acquisition,2009,12,26,3,1,19006,sander wubben,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines."
W09-0630,Realizing the Costs: Template-Based Surface Realisation in the {GRAPH} Approach to Referring Expression Generation,2009,4,3,3,0,47101,ivo brugman,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"We describe a new realiser developed for the TUNA 2009 Challenge, and present its evaluation scores on the development set, showing a clear increase in performance compared to last year's simple realiser."
W08-1138,{GRAPH}: The Costs of Redundancy in Referring Expressions,2008,6,13,1,1,3389,emiel krahmer,Proceedings of the Fifth International Natural Language Generation Conference,0,"We describe a graph-based generation system that participated in the TUNA attribute selection and realisation task of the REG 2008 Challenge. Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 DICE and .54 MASI scores for attribute selection on the development set. For realisation, it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions."
P08-2049,Query-based Sentence Fusion is Better Defined and Leads to More Preferred Results than Generic Sentence Fusion,2008,4,25,1,1,3389,emiel krahmer,"Proceedings of ACL-08: HLT, Short Papers",0,"We show that question-based sentence fusion is a better defined task than generic sentence fusion (Q-based fusions are shorter, display less variety in length, yield more identical results and have higher normalized Rouge scores). Moreover, we show that in a QA setting, participants strongly prefer Q-based fusions over generic ones, and have a preference for union over intersection fusions."
viethen-etal-2008-controlling,Controlling Redundancy in Referring Expressions,2008,16,16,3,0.444444,40967,jette viethen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Krahmer et al.Âs (2003) graph-based framework provides an elegant and flexible approach to the generation of referring expressions. In this paper, we present the first reported study that systematically investigates how to tune the parameters of the graph-based framework on the basis of a corpus of human-generated descriptions. We focus in particular on replicating the redundant nature of human referring expressions, whereby properties not strictly necessary for identifying a referent are nonetheless included in descriptions. We show how statistics derived from the corpus data can be integrated to boost the frameworkÂs performance over a non-stochastic baseline."
W07-1414,Dependency-based paraphrasing for recognizing textual entailment,2007,9,27,2,1,32380,erwin marsi,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper addresses syntax-based paraphrasing methods for Recognizing Textual Entailment (RTE). In particular, we describe a dependency-based paraphrasing algorithm, using the DIRT data set, and its application in the context of a straightforward RTE system based on aligning dependency trees. We find a small positive effect of dependency-based paraphrasing on both the RTE3 development and test sets, but the added value of this type of paraphrasing deserves further analysis."
2007.mtsummit-ucnlg.19,Cost-based attribute selection for {GRE} ({GRAPH}-{SC}/{GRAPH}-{FP}),2007,6,2,4,0.764567,16451,mariet theune,Proceedings of the Workshop on Using corpora for natural language generation,0,In this paper we discuss several approaches to the problem of content determination for the generation of referring expressions (GRE) using the Graphbased framework of Krahmer et al. (2003). This work was carried out in the context of the First NLG Shared Task and Evaluation Challenge on Attribute Selection for Referring Expression Generation.
W05-1612,Explorations in Sentence Fusion,2005,20,69,2,1,32380,erwin marsi,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"Sentence fusion is a text-to-text (revision-like) generation task which takes related sentences as input and merges these into a single output sentence. In this paper we describe our ongoing work on developing a sentence fusion module for Dutch. We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences relate to each other. It is shown that human labelers can perform this task with a high agreement (Fscore of .95). We then describe and evaluate our adaptation of an existing automatic alignment algorithm, and use the resulting alignments, plus the semantic labels, in a generalized fusion and generation algorithm. A small-scale evaluation study reveals that most of the resulting sentences are adequate to good."
W05-1201,Classification of Semantic Relations by Humans and Machines,2005,11,25,2,1,32380,erwin marsi,Proceedings of the {ACL} Workshop on Empirical Modeling of Semantic Equivalence and Entailment,0,"This paper addresses the classification of semantic relations between pairs of sentences extracted from a Dutch parallel corpus at the word, phrase and sentence level. We first investigate the performance of human annotators on the task of manually aligning dependency analyses of the respective sentences and of assigning one of five semantic relations to the aligned phrases (equals, generalizes, specifies, restates and intersects). Results indicate that humans can perform this task well, with an F-score of .98 on alignment and an F-score of .95 on semantic relations (after correction). We then describe and evaluate a combined alignment and classification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification."
J05-1002,Squibs and Discussions: Real versus Template-Based Natural Language Generation: A False Opposition?,2005,18,110,2,0,5942,kees deemter,Computational Linguistics,0,"This article challenges the received wisdom that template-based approaches to the generation of language are necessarily inferior to other approaches as regards their maintainability, linguistic well-foundedness, and quality of output. Some recent NLG systems that call themselves ''template-based'' will illustrate our claims."
van-der-sluis-krahmer-2004-evaluating,Evaluating Multimodal {NLG} Using Production Experiments,2004,9,7,2,0,31286,ielka sluis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we report on an evaluation study for the generation of multimodal referring expressions. To test our algorithm, which allows for various gradations of preciseness in pointing, subjects performed an object identification task in a strict experimental setting. 20 subjects participated and were instructed to always use a pointing gesture (they were led to believe they were testing a new kind of xe2x80x98digital pointing devicexe2x80x99). The subjects performed their tasks on two distances: close (10 subjects) and at a distance of 2.5 meters (10 subjects). The assumption is that these conditions yield precise and imprecise pointing gestures respectively. In addition we varied the xe2x80x98typexe2x80x99 of target objects (geometrical figures versus pictures of persons). This study resulted in a corpus of 600 multimodal referring expressions. A statistical analysis (ANOVA) revealed a main effect of distance (subjects adapt their language to the kind of pointing gesture) and also a main effect of target (persons are more difficult to describe than objects). The advantages and disadvantages of this evaluation method are discussed."
W03-2710,Machine Learning for Shallow Interpretation of User Utterances in Spoken Dialogue Systems,2003,17,21,3,0,17846,piroska lendvai,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,"We investigate to what extent automatic learning techniques can be used for shallow interpretation of user utterances in spoken dialogue systems. This task involves dialogue act classification, shallow understanding and problem detection simultaneously. For this purpose we train both a rule-induction and a memory-based learning algorithm on a large set of surface features obtained by affordable means from an annotated corpus of human-machine dialogues. Using a pseudo-exhaustive search, the parameters of both algorithms are optimized. The shallow interpretation task turns out to be a difficult one, partly since there are 94 types of user answers. The best overall accuracy (exact match) obtained was 73.5%, which is a significant improvement over the baseline. The best average precision and recall for dialogue act classification was 91.2%, for classifying slot types 86.8% and for detecting communication problems 91.0%."
W03-2307,A New Model for Generating Multimodal Referring Expressions,2003,17,13,1,1,3389,emiel krahmer,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,"We present a new algorithm for the generation of multimodal referring expressions (combining language and deictic gestures).1 The approach differs from earlier work in that we allow for various gradations of preciseness in pointing, ranging from unambiguous to vague pointing gestures. The model predicts that linguistic properties realized in the generated expression are co-dependent on the kind of pointing gesture included. The decision to point is based on a tradeoff between the costs of pointing and the costs of linguistic properties, where both kinds of costs are computed in empirically motivated ways. The model has been implemented using a graph-based generation algorithm."
J03-1003,Graph-Based Generation of Referring Expressions,2003,39,179,1,1,3389,emiel krahmer,Computational Linguistics,0,"This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches."
W01-0805,A Meta-Algorithm for the Generation of Referring Expressions,2001,17,14,1,1,3389,emiel krahmer,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,This paper describes a new approach to the generation of referring expressions. We propose to formalize a scene as a labeled directed graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic --- and even improve--- a number of well-known algorithms.
P01-1012,Detecting Problematic Turns in Human-Machine Interactions: Rule-induction Versus Memory-based Learning Approaches,2001,15,26,2,0.450597,18116,antal bosch,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We address the issue of on-line detection of communication problems in spoken dialogue systems. The usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances. By applying both rule-induction and memory-based learning techniques to data obtained with a Dutch train time-table information system, the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline. The results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead. The results are interesting from a machine learning perspective, since they show that the rule-based method performs significantly better than the memory-based method, because the former is better capable of representing interactions between features."
swerts-krahmer-2000-use,On the Use of Prosody for On-line Evaluation of Spoken Dialogue Systems,2000,21,2,2,0,42549,marc swerts,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper focuses on the usersxe2x80x99 signaling of information status in human-machine interactions, and in particular looks at the role prosody may play in this respect. Using a corpus of interactions with two Dutch spoken dialogue systems, prosodic correlates of usersxe2x80x99 disconfirmations were investigated. In this corpus, disconfirmations may serve as a signal to xe2x80x98go onxe2x80x99 in one context and as a signal to xe2x80x98go backxe2x80x99 in another. With the data obtained from this corpus an acoustic and a perception experiment have been carried out. The acoustic analysis shows that the difference in signaling function is reflected in the distribution of the various types of disconfirmations as well as in different prosodic variables (pause, duration, intonation contour and pitch range). The perception experiment revealed that subjects are very good at classifying disconfirmations as positive or negative signals (without context), which strongly suggests that the acoustic features have communicative relevance. The implications of these results for human-machine interactions are discussed."
W98-0129,"Description theory, {LTAG}s and underspecified semantics",1998,9,8,2,0,55253,reinhard muskens,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,"An attractive way to modeln the relation between an underspecified syntactic representation andn its completions is to let the underspecified representation correspondn to a logical description and the completions to then models of that description. This approach, which underlies then Description Theory of (Marcus et al. 1983) has been integratedn in (Vijay-Shanker 1992) with a pure unification approach to n Lexicalized Tree-Adjoiningn Grammars (Joshi et al. 1975, Schabes 1990). We generalize n Description Theory by integrating semanticn information, that is, we propose to tackle both syntactic andn semantic underspecification using descriptions."
W97-0616,How to obey the 7 commandments for spoken dialogue?,1997,11,2,1,1,3389,emiel krahmer,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,"We describe the design and implementation of the dialogue management module in a voice operated car-driver information system. The literature on designing 'good' user interfaces involving natural language dialogue in general and speech in particular is abundant with useful guidelines for actual development. We have tried to summarize these guidelines in 7 'meta-guidelines', or commandments. Even though state-of-the-art Speech Recognition modules perform well, speech recognition errors cannot be precluded. For the current application, the fact that the car is an acoustically hostile environment is an extra complication. This means that special attention should be paid to effective methods to compensate for speech recognition errors. Moreover, this should be done in a way which is not disturbing for the driver. In this paper, we show how these constraints influence the design and subsequent implementation of the Dialogue Manager module, and how the additional requirements fit in with the 7 commandments."
