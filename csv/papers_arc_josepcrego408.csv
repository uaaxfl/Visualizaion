2021.tacl-1.2,Revisiting Multi-Domain Machine Translation,2021,-1,-1,2,1,835,minhquang pham,Transactions of the Association for Computational Linguistics,0,"When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises."
2020.wmt-1.63,Priming Neural Machine Translation,2020,0,28,3,0.961538,5485,minh pham,Proceedings of the Fifth Conference on Machine Translation,0,"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources."
2020.wmt-1.72,A Study of Residual Adapters for Multi-Domain Neural Machine Translation,2020,-1,-1,2,0.961538,5485,minh pham,Proceedings of the Fifth Conference on Machine Translation,0,"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors."
2020.ngt-1.25,Efficient and High-Quality Neural Machine Translation with {O}pen{NMT},2020,-1,-1,4,0.512821,16475,guillaume klein,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models."
2020.coling-main.348,Integrating Domain Terminology into Neural Machine Translation,2020,-1,-1,2,0,21441,elise michon,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper extends existing work on terminology integration into Neural Machine Translation, a common industrial practice to dynamically adapt translation to a specific domain. Our method, based on the use of placeholders complemented with morphosyntactic annotation, efficiently taps into the ability of the neural network to deal with symbolic knowledge to surpass the surface generalization shown by alternative techniques. We compare our approach to state-of-the-art systems and benchmark them through a well-defined evaluation framework, focusing on actual application of terminology and not just on the overall performance. Results indicate the suitability of our method in the use-case where terminology is used in a system trained on generic data only."
2020.acl-main.144,Boosting Neural Machine Translation with Similar Translations,2020,-1,-1,2,1,9989,jitao xu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with {``}copy{''} information while translations based on embedding similarities tend to extend the translation {``}context{''}. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation."
D19-5615,Enhanced Transformer Model for Data-to-Text Generation,2019,0,1,2,0,26548,li gong,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on database records. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model: First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics: BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper."
D19-5629,{SYSTRAN} @ {WNGT} 2019: {DGT} Task,2019,0,0,2,0,26548,li gong,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,This paper describes SYSTRAN participation to the Document-level Generation and Trans- lation (DGT) Shared Task of the 3rd Workshop on Neural Generation and Translation (WNGT 2019). We participate for the first time using a Transformer network enhanced with modified input embeddings and optimising an additional objective function that considers content selection. The network takes in structured data of basketball games and outputs a summary of the game in natural language.
D19-5225,{SYSTRAN} @ {WAT} 2019: {R}ussian-{J}apanese News Commentary task,2019,0,0,4,1,9989,jitao xu,Proceedings of the 6th Workshop on Asian Translation,0,"This paper describes Systran{'}s submissions to WAT 2019 Russian-Japanese News Commentary task. A challenging translation task due to the extremely low resources available and the distance of the language pair. We have used the neural Transformer architecture learned over the provided resources and we carried out synthetic data generation experiments which aim at alleviating the data scarcity problem. Results indicate the suitability of the data augmentation experiments, enabling our systems to rank first according to automatic evaluations."
W18-6485,{SYSTRAN} Participation to the {WMT}2018 Shared Task on Parallel Corpus Filtering,2018,0,0,2,1,835,minhquang pham,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,This paper describes the participation of SYSTRAN to the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018). We participate for the first time using a neural sentence similarity classifier which aims at predicting the relatedness of sentence pairs in a multilingual context. The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task.
W18-3914,Neural Network Architectures for {A}rabic Dialect Identification,2018,0,0,3,0,21441,elise michon,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"SYSTRAN competes this year for the first time to the DSL shared task, in the Arabic Dialect Identification subtask. We participate by training several Neural Network models showing that we can obtain competitive results despite the limited amount of training data available for learning. We report our experiments and detail the network architecture and parameters of our 3 runs: our best performing system consists in a Multi-Input CNN that learns separate embeddings for lexical, phonetic and acoustic input features (F1: 0.5289); we also built a CNN-biLSTM network aimed at capturing both spatial and sequential features directly from speech spectrograms (F1: 0.3894 at submission time, F1: 0.4235 with later found parameters); and finally a system relying on binary CNN-biLSTMs (F1: 0.4339)."
W18-2715,{O}pen{NMT} System Description for {WNMT} 2018: 800 words/sec on a single-core {CPU},2018,0,2,6,0,13889,jean senellart,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a high-performance CPU system. The final system uses a combination of four techniques, all of them lead to significant speed-ups in combination: (a) sequence distillation, (b) architecture modifications, (c) precomputation, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and available to the community."
D18-1328,Fixing Translation Divergences in Parallel Corpora for Neural {MT},2018,0,2,2,1,835,minhquang pham,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance."
W17-4722,{SYSTRAN} Purely Neural {MT} Engines for {WMT}2017,2017,6,0,9,0,19900,yongchao deng,Proceedings of the Second Conference on Machine Translation,0,"This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news translation task for English-German, in both translation directions. Our systems are built using OpenNMT, an open-source neural machine translation system, implementing sequence-to-sequence models with LSTM encoder/decoders and attention. We experimented using monolingual data automatically back-translated. Our resulting models are further hyper-specialised with an adaptation technique that finely tunes models according to the evaluation test sentences."
kobus-etal-2017-domain,Domain Control for Neural Machine Translation,2017,11,32,2,0,27334,catherine kobus,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Machine translation systems are very sensitive to the domains they were trained on. Several domain adaptation techniques have already been deeply studied. We propose a new technique for neural machine translation (NMT) that we call domain control which is performed at runtime using a unique neural network covering multiple domains. The presented approach shows quality improvements when compared to dedicated domains translating on any of the covered domains and even on out-of-domain data. In addition, model parameters do not need to be re-estimated for each domain, making this effective to real use cases. Evaluation is carried out on English-to-French translation for two different testing scenarios. We first consider the case where an end-user performs translations on a known domain. Secondly, we consider the scenario where the domain is not known and predicted at the sentence level before translating. Results show consistent accuracy improvements for both conditions."
I17-2046,Boosting Neural Machine Translation,2017,0,0,3,1,16476,dakun zhang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Training efficiency is one of the main problems for Neural Machine Translation (NMT). Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high computation cost, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on data boosting and bootstrap with no modifications to the neural network. It imitates the learning process of humans, which typically spend more time when learning {``}difficult{''} concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to 1.63 BLEU while saving 20{\%} of training time."
2017.jeptalnrecital-demo.6,Conception d{'}une solution de d{\\'e}tection d{'}{\\'e}v{\\'e}nements bas{\\'e}e sur {T}witter (Design of a solution for event detection from Tweeter),2017,-1,-1,8,0,5281,christophe servan,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Cet article pr{\'e}sente un syst{\`e}me d{'}alertes fond{\'e} sur la masse de donn{\'e}es issues de Tweeter. L{'}objectif de l{'}outil est de surveiller l{'}actualit{\'e}, autour de diff{\'e}rents domaines t{\'e}moin incluant les {\'e}v{\'e}nements sportifs ou les catastrophes naturelles. Cette surveillance est transmise {\`a} l{'}utilisateur sous forme d{'}une interface web contenant la liste d{'}{\'e}v{\'e}nements localis{\'e}s sur une carte."
2017.jeptalnrecital-court.27,Adaptation incr{\\'e}mentale de mod{\\`e}les de traduction neuronaux (Incremental adaptation of neural machine translation models),2017,-1,-1,2,0,5281,christophe servan,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"L{'}adaptation au domaine est un verrou scientifique en traduction automatique. Il englobe g{\'e}n{\'e}ralement l{'}adaptation de la terminologie et du style, en particulier pour la post-{\'e}dition humaine dans le cadre d{'}une traduction assist{\'e}e par ordinateur. Avec la traduction automatique neuronale, nous {\'e}tudions une nouvelle approche d{'}adaptation au domaine que nous appelons {``}sp{\'e}cialisation{''} et qui pr{\'e}sente des r{\'e}sultats prometteurs tant dans la vitesse d{'}apprentissage que dans les scores de traduction. Dans cet article, nous proposons d{'}explorer cette approche."
W12-3140,Joint {WMT} 2012 Submission of the {QUAERO} Project,2012,37,5,12,0.833333,3519,markus freitag,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system."
W11-2135,{LIMSI} @ {WMT}11,2011,19,17,8,1,5598,alexandre allauzen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the French-English and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up."
W11-2142,Joint {WMT} Submission of the {QUAERO} Project,2011,25,1,11,0.833333,3519,markus freitag,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach."
W11-2168,From n-gram-based to {CRF}-based Translation Models,2011,46,12,3,0,8590,thomas lavergne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure. Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. The tractability of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,4,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
W10-1704,{LIMSI}{'}s Statistical Translation Systems for {WMT}{'}10,2010,19,15,2,1,5598,alexandre allauzen,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes our Statistical Machine Translation systems for the WMT10 evaluation, where LIMSI participated for two language pairs (French-English and German-English, in both directions). For German-English, we concentrated on normalizing the German side through a proper preprocessing, aimed at reducing the lexical redundancy and at splitting complex compounds. For French-English, we studied two extensions of our in-house N-code decoder: firstly, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model. For both set of experiments, we report the improvements obtained on the development and test data."
max-etal-2010-contrastive,Contrastive Lexical Evaluation of Machine Translation,2010,15,10,2,0,28247,aurelien max,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper advocates a complementary measure of translation performance that focuses on the constrastive ability of two or more systems or system versions to adequately translate source words. This is motivated by three main reasons : 1) existing automatic metrics sometimes do not show significant differences that can be revealed by fine-grained focussed human evaluation, 2) these metrics are based on direct comparisons between system hypotheses with the corresponding reference translations, thus ignoring the input words that were actually translated, and 3) as these metrics do not take input hypotheses from several systems at once, fine-grained contrastive evaluation can only be done indirectly. This proposal is illustrated on a multi-source Machine Translation scenario where multiple translations of a source text are available. Significant gains (up to +1.3 BLEU point) are achieved on these experiments, and contrastive lexical evaluation is shown to provide new information that can help to better analyse a system's performance."
C10-2023,Improving Reordering with Linguistically Informed Bilingual n-grams,2010,18,16,1,1,836,josep crego,Coling 2010: Posters,0,"We present a new reordering model estimated as a standard n-gram language model with units built from morpho-syntactic information of the source and target languages. It can be seen as a model that translates the morpho-syntactic structure of the input sentence, in contrast to standard translation models which take care of the surface word forms. We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process, thus effectively accounting for mid-range reorderings. Empirical results on French-English and German-English translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model."
C10-1027,Local lexical adaptation in Machine Translation through triangulation: {SMT} helping {SMT},2010,23,11,1,1,836,josep crego,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We present a framework where auxiliary MT systems are used to provide lexical predictions to a main SMT system. In this work, predictions are obtained by means of pivoting via auxiliary languages, and introduced into the main SMT system in the form of a low order language model, which is estimated on a sentence-by-sentence basis. The linear combination of models implemented by the decoder is thus extended with this additional language model. Experiments are carried out over three different translation tasks using the European Parliament corpus. For each task, nine additional languages are used as auxiliary languages to obtain the triangulated predictions. Translation accuracy results show that improvements in translation quality are obtained, even for large data conditions."
2010.iwslt-papers.12,Multi-pivot translation by system combination,2010,28,10,3,0.419029,29312,gregor leusch,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"This paper describes a technique to exploit multiple pivot languages when using machine translation (MT) on language pairs with scarce bilingual resources, or where no translation system for a language pair is available. The principal idea is to generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using MT system combination techniques. Our technique can also be applied when a translation system for a language pair is available, but is limited in its translation accuracy because of scarce resources. Using statistical MT systems for the 11 different languages of Europarl, we show experimentally that a direct translation system can be replaced by this pivot approach without a loss in translation quality if about six pivot languages are available. Furthermore, we can already improve an existing MT system by adding two pivot systems to it. The maximum improvement was found to be 1.4{\%} abs. in BLEU in our experiments for 8 or more pivot languages."
2010.iwslt-evaluation.13,{LIMSI} @ {IWSLT} 2010,2010,30,0,2,1,5598,alexandre allauzen,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes LIMSI{'}s Statistical Machine Translation systems (SMT) for the IWSLT evaluation, where we participated in two tasks (Talk for English to French and BTEC for Turkish to English). For the Talk task, we studied an extension of our in-house n-code SMT system (the integration of a bilingual reordering model over generalized translation units), as well as the use of training data extracted from Wikipedia in order to adapt the target language model. For the BTEC task, we concentrated on pre-processing schemes on the Turkish side in order to reduce the morphological discrepancies with the English side. We also evaluated the use of two different continuous space language models for such a small size of training data."
W09-0417,{LIMSI}{`}s Statistical Translation Systems for {WMT}{`}09,2009,18,3,2,1,5598,alexandre allauzen,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus."
2009.jeptalnrecital-court.28,Plusieurs langues (bien choisies) valent mieux qu{'}une : traduction statistique multi-source par renforcement lexical,2009,17,3,1,1,836,josep crego,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les syst{\`e}mes de traduction statistiques int{\`e}grent diff{\'e}rents types de mod{\`e}les dont les pr{\'e}dictions sont combin{\'e}es, lors du d{\'e}codage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polys{\'e}miques, comme, par exemple, le mot avocat du fran{\c{c}}ais vers l{'}anglais (lawyer ou avocado), requiert l{'}utilisation de mod{\`e}les suppl{\'e}mentaires, dont l{'}estimation et l{'}int{\'e}gration s{'}av{\`e}rent complexes. Une alternative consiste {\`a} tirer parti de l{'}observation selon laquelle les ambigu{\""\i}t{\'e}s li{\'e}es {\`a} la polys{\'e}mie ne sont pas les m{\^e}mes selon les langues source consid{\'e}r{\'e}es. Si l{'}on dispose, par exemple, d{'}une traduction vers l{'}espagnol dans laquelle avocat a {\'e}t{\'e} traduit par aguacate, alors la traduction de ce mot vers l{'}anglais n{'}est plus ambigu{\""e}. Ainsi, la connaissance d{'}une traduction fran{\c{c}}ais!espagnol permet de renforcer la s{\'e}lection de la traduction avocado pour le syst{\`e}me fran{\c{c}}ais!anglais. Dans cet article, nous proposons d{'}utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectu{\'e}s par un syst{\`e}me de traduction automatique. En particulier, nous montrons une am{\'e}lioration des performances sur plusieurs m{\'e}triques lorsque les traductions auxiliaires utilis{\'e}es sont obtenues manuellement."
2009.eamt-1.10,Gappy Translation Units under Left-to-Right {SMT} Decoding,2009,16,12,1,1,836,josep crego,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper presents an extension for a bilingual n-gram statistical machine translation (SMT) system based on allowing translation units with gaps. Our gappy translation units can be seen as a first step towards introducing hierarchical units similar to those employed in hierarchical MT systems. Our goal is double. On the one hand we aim at capturing the benefits of the higher generalization power shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make dicult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system."
W08-0307,Using Shallow Syntax Information to Improve Word Alignment and Reordering for {SMT},2008,19,25,1,1,836,josep crego,Proceedings of the Third Workshop on Statistical Machine Translation,0,"We describe two methods to improve SMT accuracy using shallow syntax information. First, we use chunks to refine the set of word alignments typically used as a starting point in SMT systems. Second, we extend an N-gram-based SMT system with chunk tags to better account for long-distance reorderings. Experiments are reported on an Arabic-English task showing significant improvements. A human error analysis indicates that long-distance reorderings are captured effectively."
W08-0315,The {TALP}-{UPC} {N}gram-Based Statistical Machine Translation System for {ACL}-{WMT} 2008,2008,11,6,4,0,17603,maxim khalilov,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper reports on the participation of the TALP Research Center of the UPC (Universitat Politecnica de Catalunya) to the ACL WMT 2008 evaluation campaign.n n This year's system is the evolution of the one we employed for the 2007 campaign. Main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique. In addition, this system introduces a target language model, based on linguistic classes (Part-of-Speech), morphology reduction for an inflectional language (Spanish) and an improved optimization procedure.n n Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented."
W07-0720,Ngram-Based Statistical Machine Translation Enhanced with Multiple Weighted Reordering Hypotheses,2007,7,9,2,1,5326,marta costajussa,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Politecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlyghted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure.n n The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks."
P07-2054,Extending {MARIE}: an N-gram-based {SMT} decoder,2007,5,11,1,1,836,josep crego,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"In this paper we present several extensions of MARIE, a freely available N-gram-based statistical machine translation (SMT) decoder. The extensions mainly consist of the ability to accept and generate word graphs and the introduction of two new N-gram models in the loglinear combination of feature functions the decoder implements. Additionally, the decoder is enhanced with a caching strategy that reduces the number of N-gram calls improving the overall search efficiency. Experiments are carried out over the Eurpoean Parliament Spanish-English translation task."
N07-2022,Discriminative Alignment Training without Annotated Data for Machine Translation,2007,19,14,3,0,23604,patrik lambert,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"In present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model. Consequently, alignment model parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed."
N07-2035,Analysis and System Combination of Phrase- and {N}-Gram-Based Statistical Machine Translation Systems,2007,10,9,2,1,5326,marta costajussa,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"In the framework of the Tc-Star project, we analyze and propose a combination of two Statistical Machine Translation systems: a phrase-based and an N-gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system's output. Additionally, we combine both systems, showing accuracy improvements."
2007.mtsummit-papers.16,Syntax-enhanced n-gram-based {SMT},2007,-1,-1,1,1,836,josep crego,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.iwslt-1.26,The {TALP} ngram-based {SMT} system for {IWSLT} 2007,2007,-1,-1,3,0,23604,patrik lambert,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"This paper describes TALPtuples, the 2007 N-gram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years. Mainly, these include optimizing alignment parameters in function of translation metric scores and rescoring with a neural network language model. Results on two translation directions are reported, namely from Arabic and Chinese into English, thoroughly explaining all language-related preprocessing and translation schemes."
W06-3120,{TALP} Phrase-based statistical translation system for {E}uropean language pairs,2006,15,9,2,1,5326,marta costajussa,Proceedings on the Workshop on Statistical Machine Translation,0,This paper reports translation results for the Exploiting Parallel Texts for Statistical Machine Translation (HLT-NAACL Workshop on Parallel Texts 2006). We have studied different techniques to improve the standard Phrase-Based translation system. Mainly we introduce two reordering approaches and add morphological information.
W06-3125,N-gram-based {SMT} System Enhanced with Reordering Patterns,2006,13,20,1,1,836,josep crego,Proceedings on the Workshop on Statistical Machine Translation,0,"This work presents translation results for the three data sets made available in the shared task Exploiting Parallel Texts for Statistical Machine Translation of the HLT-NAACL 2006 Workshop on Statistical Machine Translation. All results presented were generated by using the N-gram-based statistical machine translation system which has been enhanced from the last year's evaluation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated source-side reorderings."
J06-4004,N-gram-based Machine Translation,2006,36,210,3,0.909091,40951,jose marino,Computational Linguistics,0,"This article describes in detail an n-gram approach to statistical machine translation. This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions. Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS)."
2006.iwslt-evaluation.17,The {TALP} Ngram-based {SMT} systems for {IWSLT} 2006,2006,37,25,1,1,836,josep crego,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes TALPtuples, the 2006 Ngrambased statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polit ecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years, being highlighted and empirically compared. Mainly, these include a novel and much more efcient word ordering strategy based on reordering patterns, a linguistically-guided tuple segmentation criterion and improved optimization procedures. The paper provides details of this system participation in the third International Workshop on Spoken Language Translation (IWSLT) held in Kyoto, Japan in November 2006. Results on four translation directions are reported, namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes."
2006.iwslt-evaluation.18,{TALP} phrase-based system and {TALP} system combination for {IWSLT} 2006,2006,14,7,2,1,5326,marta costajussa,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation."
2006.amta-papers.4,Integration of {POS}tag-based Source Reordering into {SMT} Decoding by an Extended Search Graph,2006,16,13,1,1,836,josep crego,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper presents a reordering framework for statistical machine translation (SMT) where source-side reorderings are integrated into SMT decoding, allowing for a highly constrained reordered search graph. The monotone search is extended by means of a set of reordering patterns (linguistically motivated rewrite patterns). Patterns are automatically learnt in training from word-to-word alignments and source-side Part-Of-Speech (POS) tags. Traversing the extended search graph, the decoder evaluates every hypothesis making use of a group of widely used SMT models and helped by an additional Ngram language model of source-side POS tags. Experiments are reported on the Euparl task (Spanish-to-English and English-to- Spanish). Results are presented regarding translation accuracy (using human and automatic evaluations) and computational efficiency, showing significant improvements in translation quality for both translation directions at a very low computational cost."
W05-0823,Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams,2005,10,18,2,0.833333,28438,rafael banchs,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,This work discusses translation results for the four Euparl data sets which were made available for the shared task Exploiting Parallel Texts for Statistical Machine Translation. All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.
2005.mtsummit-papers.36,Bilingual N-gram Statistical Machine Translation,2005,22,37,3,0.909091,40951,jose marino,Proceedings of Machine Translation Summit X: Papers,0,"This paper describes a statistical machine translation system that uses a translation model which is based on bilingual n-grams. When this translation model is log-linearly combined with four specific feature functions, state of the art translations are achieved for Spanish-to-English and English-to-Spanish translation tasks. Some specific results obtained for the EPPS (European Parliament Plenary Sessions) data are presented and discussed. Finally, future research issues are depicted."
2005.mtsummit-papers.37,"Reordered Search, and Tuple Unfolding for Ngram-based {SMT}",2005,-1,-1,1,1,836,josep crego,Proceedings of Machine Translation Summit X: Papers,0,"In Statistical Machine Translation, the use of reordering for certain language pairs can produce a significant improvement on translation accuracy. However, the search problem is shown to be NP-hard when arbitrary reorderings are allowed. This paper addresses the question of reordering for an Ngram-based SMT approach following two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we allow for an Ngram-based decoder (MARIE) to perform a reordered search over the source sentence, while combining a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its reordered search produces an improved translation. On the other hand, we allow for a modification of the translation units that unfolds the tuples, so that shorter units are learnt from a new parallel corpus, where the source sentences are reordered according to the target language. This tuple unfolding technique reduces data sparseness and, when combined with the reordered search, further boosts translation performance. Translation accuracy and efficency results are reported for the IWSLT 2004 Chinese to English task."
2005.iwslt-1.23,Ngram-based versus Phrase-based Statistical Machine Translation,2005,0,18,1,1,836,josep crego,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
2005.iwslt-1.25,The {TALP} Ngram-based {SMT} System for {IWSLT}{'}05,2005,-1,-1,1,1,836,josep crego,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
arranz-etal-2004-bilingual,Bilingual Connections for Trilingual Corpora: An {XML} Approach,2004,10,2,3,0,11078,victoria arranz,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
2004.iwslt-papers.3,Phrase-based alignment combining corpus cooccurrences and linguistic knowledge,2004,17,13,3,0,23877,adria gispert,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,"This paper introduces a phrase alignment strategy that seeks phrase and word links in two stages using cooccurrence measures and linguistic information. On a first stage, the algorithm finds high-precision links involving a linguistically-derived set of phrases, leaving word alignment to be performed in a second phase. Experiments have been carried out for an English-Spanish parallel corpus, and we show how phrase cooccurrence measures convey a complementary information to word cooccurrences, and a stronger evidence of a good alignment. Alignment Error Rate (AER) results are presented, being competitive with and even outperforming state-of-the-art alignment algorithms."
