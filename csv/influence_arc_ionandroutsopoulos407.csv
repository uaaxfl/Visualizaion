2020.acl-main.396,W18-5105,0,0.0278135,"Missing"
2020.acl-main.396,N19-1423,0,0.0231053,"toxicity probability. We fix the bias term of the T single output neuron to log N , where T and N are the numbers of toxic and non-toxic training comments, respectively, to counter-bias against the majority (non-toxic) class.7 This BILSTM-based model could, of course, be made more complex (e.g., by stacking more BILSTM layers, and including self-attention), but it is used here mainly to measure how much a relatively simple (by today’s standards) classifier benefits when a context mechanism is added (see below). At the other end of complexity, our second context-insensitive classifier is BERT (Devlin et al., 2019), fine-tuned on the training subset of each experiment, with a task-specific classifier on top, fed with BERT’s top-level embedding of the [CLS] token. We use BERT- BASE pre-trained on cased data, with 12 layers and 768 hidden units. We only unfreeze the top three layers during fine-tuning, with a small learning rate (2e-05) to avoid catastrophic forgetting. The task-specific classifier is the same FFNN as in the BILSTM classifier. Figure 3: Illustration of CA - BILSTM - BILSTM. Two BILSTM s, shown unidirectional for simplicity, encode the parent and target comment. The concatenation of the ve"
2020.acl-main.396,P18-1128,0,0.0281215,"Missing"
2020.acl-main.396,W17-3013,0,0.0406136,"Missing"
2020.acl-main.396,gao-huang-2017-detecting,0,0.34342,"ful language (van Aken et al., 2018). As is customary in natural language processing, we focus on aggregate results when hoping to answer our research questions, and leave largely unanswered the related epistemological questions when this does not preclude using classifiers in real-world applications. Table 3 lists all currently available public datasets for the various forms of toxic language that we are aware of. The two last columns show that 3 https://github.com/ipavlopoulos/ context_toxicity 4297 Dataset Name CCTK CWTK Davidson et al. (2017) Zampieri et al. (2019a) Waseem and Hovy (2016) Gao and Huang (2017) Wiegand et al. (2018) Ross et al. (2016) Pavlopoulos et al. (2017a) Mubarak et al. (2017) Source Civil Comments Toxicity Kaggle Wikipedia Toxicity Kaggle Twitter Twitter Twitter Fox News Twitter Twitter Gazzetta.gr Aljazeera.net Size 2M 223,549 24,783 14,100 1,607 1,528 8541 470 1,6M 31,633 Type Toxicity sub-types Toxicity sub-types Hate/Offense Offense Sexism/Racism Hate Insult/Abuse/Profanity Hate Rejection Obscene/Offense Lang. EN EN EN EN EN EN DE DE EL AR Ca 7 7 7 7 7 X 7 7 X X Ct Title Title Table 3: Publicly available datasets for toxicity detection. The Size column shows the number of"
2020.acl-main.396,P19-1267,0,0.052134,"Missing"
2020.acl-main.396,D18-1305,1,0.882925,"Missing"
2020.acl-main.396,W18-4401,0,0.0266325,"often context affects the perceived toxicity of online posts have not been published. Hence, in this paper we focus on the following two foundational research questions: • RQ 1: How often does context affect the toxicity of posts as perceived by humans in online conversations? And how often does context amplify or mitigate the perceived toxicity? 1 Following the work of Wulczyn et al. (2017) and Borkan et al. (2019), toxicity is defined as “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion” (Wulczyn et al., 2017). 2 For English, see for example TRAC (Kumar et al., 2018), O FFENS E VAL (Zampieri et al., 2019b), or the recent Workshops on Abusive Language Online (https://goo.gl/ 9HmSzc). For other languages, see for example the German G ERM E VAL (https://goo.gl/uZEerk). 4296 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4296–4305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics C OMMENT WITH TOXICITY AMPLIFIED IN CONTEXT PARENT But what if the user is a lesbian? Then what? TARGET “Pigs Are People Too”. “Avant-garde a clue” C OMMENT WITH TOXICITY MITIGATED IN CONTEXT PARENT Hmmm. The flame on"
2020.acl-main.396,malmasi-zampieri-2017-detecting,0,0.0192259,"rtant direction for further research is how to efficiently annotate larger corpora of comments in context. We make our code and data publicly available.3 2 Related Work Toxicity detection has attracted a lot of attention in recent years (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Here we use the term ‘toxic’ as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: ‘offensive’ (Zampieri et al., 2019a), ‘abusive’ (Pavlopoulos et al., 2017a), ‘hateful’ (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018), etc. There are also taxonomies for these phenomena based on their directness (e.g., whether the abuse was unambiguously implied/denoted or not), and their target (e.g., whether it was a general comment or targeting an individual/group) (Waseem et al., 2017). Other hierarchical taxonomies have also been defined (Zampieri et al., 2019a). While most previous work does not address toxicity in general, instead addressing particular subtypes, toxicity and its subtypes are strongly related, with systems trained to detect toxici"
2020.acl-main.396,W17-3008,0,0.379496,"focus on aggregate results when hoping to answer our research questions, and leave largely unanswered the related epistemological questions when this does not preclude using classifiers in real-world applications. Table 3 lists all currently available public datasets for the various forms of toxic language that we are aware of. The two last columns show that 3 https://github.com/ipavlopoulos/ context_toxicity 4297 Dataset Name CCTK CWTK Davidson et al. (2017) Zampieri et al. (2019a) Waseem and Hovy (2016) Gao and Huang (2017) Wiegand et al. (2018) Ross et al. (2016) Pavlopoulos et al. (2017a) Mubarak et al. (2017) Source Civil Comments Toxicity Kaggle Wikipedia Toxicity Kaggle Twitter Twitter Twitter Fox News Twitter Twitter Gazzetta.gr Aljazeera.net Size 2M 223,549 24,783 14,100 1,607 1,528 8541 470 1,6M 31,633 Type Toxicity sub-types Toxicity sub-types Hate/Offense Offense Sexism/Racism Hate Insult/Abuse/Profanity Hate Rejection Obscene/Offense Lang. EN EN EN EN EN EN DE DE EL AR Ca 7 7 7 7 7 X 7 7 X X Ct Title Title Table 3: Publicly available datasets for toxicity detection. The Size column shows the number of comments. Column Ca shows if annotation was context-aware or not. Column Ct shows the typ"
2020.acl-main.396,P02-1040,0,0.107497,"Missing"
2020.acl-main.396,W17-3006,0,0.0466775,"e tried a range of classifiers and mechanisms to make them context aware, and having also considered the effect of using gold labels obtained out of context or by showing context to the annotators. This finding is likely related to the small number of context-sensitive comments. In turn this suggests that an important direction for further research is how to efficiently annotate larger corpora of comments in context. We make our code and data publicly available.3 2 Related Work Toxicity detection has attracted a lot of attention in recent years (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Here we use the term ‘toxic’ as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: ‘offensive’ (Zampieri et al., 2019a), ‘abusive’ (Pavlopoulos et al., 2017a), ‘hateful’ (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018), etc. There are also taxonomies for these phenomena based on their directness (e.g., whether the abuse was unambiguously implied/denoted or not), and their target (e.g., whether it was a general comment or"
2020.acl-main.396,W17-3004,1,0.891397,"of toxicity classifiers. We tried a range of classifiers and mechanisms to make them context aware, and having also considered the effect of using gold labels obtained out of context or by showing context to the annotators. This finding is likely related to the small number of context-sensitive comments. In turn this suggests that an important direction for further research is how to efficiently annotate larger corpora of comments in context. We make our code and data publicly available.3 2 Related Work Toxicity detection has attracted a lot of attention in recent years (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Here we use the term ‘toxic’ as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: ‘offensive’ (Zampieri et al., 2019a), ‘abusive’ (Pavlopoulos et al., 2017a), ‘hateful’ (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018), etc. There are also taxonomies for these phenomena based on their directness (e.g., whether the abuse was unambiguously implied/denoted or not), and their target (e.g., whether it was"
2020.acl-main.396,D17-1117,1,0.914052,"of toxicity classifiers. We tried a range of classifiers and mechanisms to make them context aware, and having also considered the effect of using gold labels obtained out of context or by showing context to the annotators. This finding is likely related to the small number of context-sensitive comments. In turn this suggests that an important direction for further research is how to efficiently annotate larger corpora of comments in context. We make our code and data publicly available.3 2 Related Work Toxicity detection has attracted a lot of attention in recent years (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Here we use the term ‘toxic’ as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: ‘offensive’ (Zampieri et al., 2019a), ‘abusive’ (Pavlopoulos et al., 2017a), ‘hateful’ (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018), etc. There are also taxonomies for these phenomena based on their directness (e.g., whether the abuse was unambiguously implied/denoted or not), and their target (e.g., whether it was"
2020.acl-main.396,N15-1020,0,0.0700741,"Missing"
2020.acl-main.396,W17-3012,0,0.0392126,"use the term ‘toxic’ as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: ‘offensive’ (Zampieri et al., 2019a), ‘abusive’ (Pavlopoulos et al., 2017a), ‘hateful’ (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018), etc. There are also taxonomies for these phenomena based on their directness (e.g., whether the abuse was unambiguously implied/denoted or not), and their target (e.g., whether it was a general comment or targeting an individual/group) (Waseem et al., 2017). Other hierarchical taxonomies have also been defined (Zampieri et al., 2019a). While most previous work does not address toxicity in general, instead addressing particular subtypes, toxicity and its subtypes are strongly related, with systems trained to detect toxicity being effective also at subtypes, such as hateful language (van Aken et al., 2018). As is customary in natural language processing, we focus on aggregate results when hoping to answer our research questions, and leave largely unanswered the related epistemological questions when this does not preclude using classifiers in real"
2020.acl-main.396,N16-2013,0,0.371934,"subtypes, such as hateful language (van Aken et al., 2018). As is customary in natural language processing, we focus on aggregate results when hoping to answer our research questions, and leave largely unanswered the related epistemological questions when this does not preclude using classifiers in real-world applications. Table 3 lists all currently available public datasets for the various forms of toxic language that we are aware of. The two last columns show that 3 https://github.com/ipavlopoulos/ context_toxicity 4297 Dataset Name CCTK CWTK Davidson et al. (2017) Zampieri et al. (2019a) Waseem and Hovy (2016) Gao and Huang (2017) Wiegand et al. (2018) Ross et al. (2016) Pavlopoulos et al. (2017a) Mubarak et al. (2017) Source Civil Comments Toxicity Kaggle Wikipedia Toxicity Kaggle Twitter Twitter Twitter Fox News Twitter Twitter Gazzetta.gr Aljazeera.net Size 2M 223,549 24,783 14,100 1,607 1,528 8541 470 1,6M 31,633 Type Toxicity sub-types Toxicity sub-types Hate/Offense Offense Sexism/Racism Hate Insult/Abuse/Profanity Hate Rejection Obscene/Offense Lang. EN EN EN EN EN EN DE DE EL AR Ca 7 7 7 7 7 X 7 7 X X Ct Title Title Table 3: Publicly available datasets for toxicity detection. The Size colum"
2020.acl-main.396,N19-1144,0,0.130841,"toxicity of online posts have not been published. Hence, in this paper we focus on the following two foundational research questions: • RQ 1: How often does context affect the toxicity of posts as perceived by humans in online conversations? And how often does context amplify or mitigate the perceived toxicity? 1 Following the work of Wulczyn et al. (2017) and Borkan et al. (2019), toxicity is defined as “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion” (Wulczyn et al., 2017). 2 For English, see for example TRAC (Kumar et al., 2018), O FFENS E VAL (Zampieri et al., 2019b), or the recent Workshops on Abusive Language Online (https://goo.gl/ 9HmSzc). For other languages, see for example the German G ERM E VAL (https://goo.gl/uZEerk). 4296 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4296–4305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics C OMMENT WITH TOXICITY AMPLIFIED IN CONTEXT PARENT But what if the user is a lesbian? Then what? TARGET “Pigs Are People Too”. “Avant-garde a clue” C OMMENT WITH TOXICITY MITIGATED IN CONTEXT PARENT Hmmm. The flame on top of the gay pride emblem can probab"
2020.acl-main.396,S19-2010,0,0.129829,"toxicity of online posts have not been published. Hence, in this paper we focus on the following two foundational research questions: • RQ 1: How often does context affect the toxicity of posts as perceived by humans in online conversations? And how often does context amplify or mitigate the perceived toxicity? 1 Following the work of Wulczyn et al. (2017) and Borkan et al. (2019), toxicity is defined as “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion” (Wulczyn et al., 2017). 2 For English, see for example TRAC (Kumar et al., 2018), O FFENS E VAL (Zampieri et al., 2019b), or the recent Workshops on Abusive Language Online (https://goo.gl/ 9HmSzc). For other languages, see for example the German G ERM E VAL (https://goo.gl/uZEerk). 4296 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4296–4305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics C OMMENT WITH TOXICITY AMPLIFIED IN CONTEXT PARENT But what if the user is a lesbian? Then what? TARGET “Pigs Are People Too”. “Avant-garde a clue” C OMMENT WITH TOXICITY MITIGATED IN CONTEXT PARENT Hmmm. The flame on top of the gay pride emblem can probab"
2020.bionlp-1.15,D19-1371,0,0.0398295,"Missing"
2020.bionlp-1.15,W19-5039,0,0.0251323,"names they had to rely more on the text of the passage and question, which they had trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the"
2020.bionlp-1.15,P16-1223,0,0.0479938,"Missing"
2020.bionlp-1.15,P17-1171,0,0.0198668,"with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented with the Stanford Reader (Chen et al., 2017) and the GatedAttention Reader (Dhingra et al., 2017), which perform worse than AOA - READER (Cui et al., 2017). The QA dataset of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts. The gold answers comprise multiple relevant documents per question, relevant snippets from the documents, exact answers in the form of entities, as well as reference summaries, written by the ex13 https://casereports.bmj.com/ Kappa 70.23 65.61 72.30 47.22 perts. Creating data of this kind, however, requires significant expertise and time. In the eight years of BIOASQ, only 3,243 que"
2020.bionlp-1.15,P17-1055,0,0.120762,"en n-grams from the passage that include an entity identifier (@entityN ), and all the n-grams from the question that include the placeholder (XXXX).7 Then for each candidate answer (entity identifier), it counts the tokens shared between the n-grams that include the candidate and the n-grams that include the placeholder. The candidate with the most shared tokens is selected. These baselines are used to check that the questions cannot be answered by simplistic heuristics (Chen et al., 2016). Neural baselines: We use the same implementations of AS - READER (Kadlec et al., 2016) and AOA READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space. The hyper-parameters of both methods were tuned on the development set of BIOMRC LITE. BERT -based Experiments and Results We experimented only on BIOMRC LITE and TINY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on BIOMRC LARGE, and we make our cod"
2020.bionlp-1.15,N19-1423,0,0.171015,"NY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on BIOMRC LARGE, and we make our code available, as already noted. 3.1 Methods We experimented with the four basic baselines (BASE 1–4) that Pappas et al. (2018) used in BIOREAD , the two neural MRC models used by the same authors, AS - READER (Kadlec et al., 2016) and AOA - READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed. Basic baselines: BASE 1, 2, 3 return the first, last, and the entity that occurs most frequently in the passage (or randomly one of the entities with the same highest frequency, if multiple exist), respectively. Since in BIOREAD the correct answer is never (by construction) the most frequent entity of the passage, unless there are multiple entities with the same highest frequency, BASE 3 performs poorly. Hence, we also include a variant, BASE 3+, which randomly selects one of the entities of the 143 model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin e"
2020.bionlp-1.15,P17-1168,0,0.0189158,"articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented with the Stanford Reader (Chen et al., 2017) and the GatedAttention Reader (Dhingra et al., 2017), which perform worse than AOA - READER (Cui et al., 2017). The QA dataset of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts. The gold answers comprise multiple relevant documents per question, relevant snippets from the documents, exact answers in the form of entities, as well as reference summaries, written by the ex13 https://casereports.bmj.com/ Kappa 70.23 65.61 72.30 47.22 perts. Creating data of this kind, however, requires significant expertise and time. In the eight years of BIOASQ, only 3,243 questions and gold answers have been created. It would b"
2020.bionlp-1.15,P16-1086,0,0.0241129,"requency. BASE 4 extracts all the token n-grams from the passage that include an entity identifier (@entityN ), and all the n-grams from the question that include the placeholder (XXXX).7 Then for each candidate answer (entity identifier), it counts the tokens shared between the n-grams that include the candidate and the n-grams that include the placeholder. The candidate with the most shared tokens is selected. These baselines are used to check that the questions cannot be answered by simplistic heuristics (Chen et al., 2016). Neural baselines: We use the same implementations of AS - READER (Kadlec et al., 2016) and AOA READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space. The hyper-parameters of both methods were tuned on the development set of BIOMRC LITE. BERT -based Experiments and Results We experimented only on BIOMRC LITE and TINY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on"
2020.bionlp-1.15,D17-1082,0,0.0211979,"used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets. Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016; Hermann et al., 2015; Dunn et al., 2017; Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016). There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Nguyen et al., 2016; Lai et al., 2017). To our knowledge the biggest human annotated corpus is Google’s Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples. Datasets of this kind require extensive annotation effort, which for open-domain datasets is usually crowd-sourced. Crowd-sourcing, however, is much more difficult for biomedical datasets, because of the required expertise of the annotators. 5 Conclusions and Future Work We introduced BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et"
2020.bionlp-1.15,D18-1258,0,0.07324,"bly because without entity names they had to rely more on the text of the passage and question, which they had trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of impo"
2020.bionlp-1.15,L18-1439,1,0.855691,"them (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016). They require no human annotations, however, which is particularly important in biomedical question answering, where employing annotators with appropriate expertise is costly. For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators. To bypass the need for expert annotators and produce a biomedical MRC dataset large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted the cloze-style questions approach. They used the full text of unlabeled biomedical articles from PUBMED CENTRAL,1 and METAMAP (Aronson and Lang, 2010) to annotate the biomedical entities of the articles. They extracted sequences of 21 sentences from the articles. The first 20 sentences were used as a passage and the last sentence as a cloze-style question. A biomedical entity of the ‘question’ was replaced by a placeholder, and systems have to guess which biomedical entity of the passage can best fill the placeholder. This allowed Pappas et al. to produce a dataset, called BIOREAD, o"
2020.bionlp-1.15,P18-2124,0,0.0513082,"Missing"
2020.bionlp-1.15,D16-1264,0,0.350538,"ned (and evaluated for their ability) to read the passage and the smaller text, and guess the named entity that was replaced by the placeholder, which is typically one of the named entities of the passage. This kind of question answering (QA) is also known as cloze-type questions (Taylor, 1953). Several datasets have been created following this approach either using books (Hill et al., 2016; Bajgar et al., 2016) or news articles (Hermann et al., 2015). Datasets of this kind are noisier than MRC datasets containing human-authored questions and manually annotated passage spans that answer them (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016). They require no human annotations, however, which is particularly important in biomedical question answering, where employing annotators with appropriate expertise is costly. For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators. To bypass the need for expert annotators and produce a biomedical MRC dataset large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted"
2020.bionlp-1.15,N18-1140,0,0.0175582,"ad trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented"
2020.bionlp-1.15,W17-2623,0,0.0355402,"ated datasets like BIOMRC and CLICR could be used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets. Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016; Hermann et al., 2015; Dunn et al., 2017; Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016). There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Nguyen et al., 2016; Lai et al., 2017). To our knowledge the biggest human annotated corpus is Google’s Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples. Datasets of this kind require extensive annotation effort, which for open-domain datasets is usually crowd-sourced. Crowd-sourcing, however, is much more difficult for biomedical datasets, because of the required expertise of the annotators. 5 Conclusions and Future Work We introduced BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to th"
2020.bionlp-1.15,P18-1128,0,\N,Missing
2020.bionlp-1.15,Q19-1026,0,\N,Missing
2020.emnlp-main.607,P19-1424,1,0.907824,"Missing"
2020.emnlp-main.607,P19-1636,1,0.874433,"Missing"
2020.emnlp-main.607,N19-1423,0,0.462082,"ls in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider other confounding factors, such as using deeper neural networks at the same time, or alternative encodings of the hierarchy. Chalkidis et al. (2019b) also considered few and zero-shot learning, but"
2020.emnlp-main.607,W19-5032,1,0.844058,"o = tanh(Wo dl + bo )   pl = sigmoid (u3l )&gt; dlo (8) (9) DC - BIGRU - LWAN: The stack of GCN layers in GC BIGRU - LWAN (Eq. 5–6) can be turned into a plain two-layer Multi-Layer Perceptron (MLP), unaware of the label hierarchy, by setting Np,l = Nc,l = ∅. We call DC - BIGRU - LWAN the resulting (deeper than C - BIGRU - LWAN) variant of GC - BIGRU - LWAN. We use it as an ablation method to evaluate the impact of the GCN layers on performance. DN - BIGRU - LWAN: As an alternative approach to exploit the label hierarchy, we used a recent improvement of NODE 2 VEC (Grover and Leskovec, 2016) by Kotitsas et al. (2019) to obtain alternative hierarchy-aware label representations. NODE 2 VEC is similar to WORD 2 VEC (Mikolov et al., 2013), but pre-trains node embeddings instead of word embeddings, replacing WORD 2 VEC’s text windows by random walks on a graph (here the label hierarchy). 4 In a variant of DC - BIGRU - LWAN, dubbed DN - BIGRU - LWAN , we simply replace the initial centroid ul label representations of DC - BIGRU LWAN in Eq. 5 and 7 by the label representations gl generated by the NODE 2 VEC extension. DNC - BIGRU - LWAN: In another version of DC called DNC - BIGRU - LWAN, we replace the initial"
2020.emnlp-main.607,2021.ccl-1.108,0,0.0940112,"Missing"
2020.emnlp-main.607,D18-1211,1,0.90059,"Missing"
2020.emnlp-main.607,N18-1100,0,0.468757,"n MIMIC - III, only leaf nodes can be used, causing the label assignments to be much sparser (GAP: 0.27). In AMAZON 13 K, documents are tagged with leaf nodes, but it is assumed that all the parent nodes are also assigned, leading to dense label assignments (GAP: 0.86). Introduction Large-scale Multi-label Text Classification (LMTC) is the task of assigning a subset of labels from a large predefined set (typically thousands) to a given document. LMTC has a wide range of applications in Natural Language Processing (NLP), such as associating medical records with diagnostic and procedure labels (Mullenbach et al., 2018; Rios and Kavuluru, 2018), legislation with relevant legal concepts (Mencia and F¨urnkranzand, 2007; Chalkidis et al., 2019b), and products with categories (Lewis et al., 2004; Partalas et al., 2015). Apart from the large label space, LMTC datasets often have skewed label distributions (e.g., some labels have few or no training examples) and a label hierarchy with different labelling guidelines (e.g., they may require documents to be tagged only with leaf nodes, or they may allow both leaf and other nodes to be used). The latter affects graph-aware annotation proximity (GAP), i.e., the proxim"
2020.emnlp-main.607,D14-1162,0,0.087906,"Missing"
2020.emnlp-main.607,N18-1202,0,0.393403,"ely tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider other confounding factors, such as using deeper neural networks at the same time, or alternative encodings of the hierarchy. Chalkidis et al. (2019b) also considered few and zero-shot learning, but ignoring the label hierarchy."
2020.emnlp-main.607,D18-1352,0,0.267613,"nodes can be used, causing the label assignments to be much sparser (GAP: 0.27). In AMAZON 13 K, documents are tagged with leaf nodes, but it is assumed that all the parent nodes are also assigned, leading to dense label assignments (GAP: 0.86). Introduction Large-scale Multi-label Text Classification (LMTC) is the task of assigning a subset of labels from a large predefined set (typically thousands) to a given document. LMTC has a wide range of applications in Natural Language Processing (NLP), such as associating medical records with diagnostic and procedure labels (Mullenbach et al., 2018; Rios and Kavuluru, 2018), legislation with relevant legal concepts (Mencia and F¨urnkranzand, 2007; Chalkidis et al., 2019b), and products with categories (Lewis et al., 2004; Partalas et al., 2015). Apart from the large label space, LMTC datasets often have skewed label distributions (e.g., some labels have few or no training examples) and a label hierarchy with different labelling guidelines (e.g., they may require documents to be tagged only with leaf nodes, or they may allow both leaf and other nodes to be used). The latter affects graph-aware annotation proximity (GAP), i.e., the proximity of the gold labels in"
2020.emnlp-main.607,2020.tacl-1.54,0,0.018382,"esults in XMTC. Nonetheless, previous work has not thoroughly compared PLT-based methods to neural models in LMTC. In particular, only You et al. (2018) have compared PLT methods to neural models in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider othe"
2020.emnlp-main.607,N19-5004,0,0.0216006,"also achieving top results in XMTC. Nonetheless, previous work has not thoroughly compared PLT-based methods to neural models in LMTC. In particular, only You et al. (2018) have compared PLT methods to neural models in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they"
2020.findings-emnlp.261,W19-1909,0,0.0384325,"Missing"
2020.findings-emnlp.261,D19-1371,0,0.206451,"ang et al., 2018), SQUAD (Rajpurkar et al., 2016), and RACE (Lai et al., 2017). Typically, transfer learning with language models requires a computationally heavy step where the language model is pre-trained on a large corpus and a less expensive step where the model is finetuned for downstream tasks. When using BERT, the first step can be omitted as the pre-trained models are publicly available. Being pre-trained on generic corpora (e.g., Wikipedia, Children’s Books, etc.) BERT has been reported to under-perform in specialised domains, such as biomedical or scientific text (Lee et al., 2019; Beltagy et al., 2019). To overcome this limitation there are two possible strategies; either further pre-train (FP) BERT on domain specific corpora, or pre-train BERT from scratch (SC) on domain specific corpora. Consequently, to employ BERT in specialised domains one may consider three alternative strategies before fine-tuning for the downstream task (Figure 1): (a) use BERT out of the box, (b) further pre-train (FP) BERT on domain-specific corpora, and (c) pre-train BERT from scratch (SC) on domain specific corpora with a new vocabulary of sub-word units. In this paper, we systematically explore strategies (a)–("
2020.findings-emnlp.261,W19-2209,1,0.887592,"Missing"
2020.findings-emnlp.261,D17-1082,0,0.0218727,"hree alternatives when employing BERT for NLP tasks in specialised domains: (a) use BERT out of the box, (b) further pre-train BERT (FP), and (c) pre-train BERT from scratch (SC). All strategies have a final fine-tuning step. Introduction Pre-trained language models based on Transformers (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) and its variants (Liu et al., 2019; Yang et al., 2019; Lan et al., 2019), have achieved state-of-the-art results in several downstream NLP tasks on generic benchmark datasets, such as GLUE (Wang et al., 2018), SQUAD (Rajpurkar et al., 2016), and RACE (Lai et al., 2017). Typically, transfer learning with language models requires a computationally heavy step where the language model is pre-trained on a large corpus and a less expensive step where the model is finetuned for downstream tasks. When using BERT, the first step can be omitted as the pre-trained models are publicly available. Being pre-trained on generic corpora (e.g., Wikipedia, Children’s Books, etc.) BERT has been reported to under-perform in specialised domains, such as biomedical or scientific text (Lee et al., 2019; Beltagy et al., 2019). To overcome this limitation there are two possible stra"
2020.findings-emnlp.261,W18-5446,0,0.072335,"Missing"
2020.findings-emnlp.261,N16-1174,0,0.0685726,"Missing"
2020.findings-emnlp.278,D18-1217,0,0.0251027,"., 2020). This approach adds a computationally expensive step that requires unlabeled data from a specific source. By contrast, our method leverages out-of-domain data with only a small computational overhead and minimal changes to the fine-tuning process. Our work is compatible with the semi-supervised learning paradigm (Chapelle et al., 2010) that combines learning from both labeled and unlabeled data. In this setting, unlabeled data from the task domain is leveraged using a consistency loss which enforces invariance of the output given small perturbations of the input (Miyato et al., 2017; Clark et al., 2018). The adversarial loss term of AFTER can be interpreted as a consistency loss that ensures invariance of representations across domains. Recently, adversarial or trust region based approaches (Zhu et al., 2020; Jiang et al., 2020; Aghajanyan et al., 2020) have been proposed as an extension to the LM fine-tuning process. These methods introduce constraints that prevent aggressive updating of the pretrained parameters or enforce smoothness during fine-tuning. However, these approaches require additional forward and backward computations while our method is more computationally efficient and can"
2020.findings-emnlp.278,N18-2097,0,0.0216462,"-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The datasets used represent both high (SST-2) and lowresource (RTE, C O LA, MRPC) tasks, as well as single-sentence (C O LA, SST-2) and sentence-pair (MRPC, RTE) tasks. For Auxiliary data we select corpora from various domains. For the NEWS domain we use the AG NEWS dataset (Zhang et al., 2015) and for the REVIEWS domain we use a part of the Electronics reviews of He and McAuley (2016). For the LEGAL domain we use the English part of E UROPARL (Koehn, 2004) and for the MEDICAL domain we use papers from PubMed, provided by Cohan et al. (2018). We also use math questions from the dataset of Saxton et al. (2019) for the MATH domain. Table 1 summarizes all datasets. More details regarding the selection and processing of the datasets can be found in Appendix A.1. Baselines. We compare our approach (AFTER) with the standard fine-tuning (SFT) scheme of the DATASET Main C O LA SST-2 MRPC RTE Auxiliary AG NEWS EUROPARL AMAZON PUBMED MATHEMATICS D OMAIN Ntrain Miscellaneous Movie Reviews News News, Wikipedia 8.5K 67K 3.7K 2.5K Agricultural News (NEWS) Legal Documents (LEGAL) Electronics Reviews (REVIEWS) Medical Papers (MEDICAL) Mathematic"
2020.findings-emnlp.278,N19-1423,0,0.641081,"adversarial classifier acts as a regularizer which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning. 1 Introduction Current research in NLP focuses on transferring knowledge from a language model (LM), pretrained on large general-domain data, to a target task. The LM representations are transferred to the target task either as additional features of a task-specific model (Peters et al., 2018), or by finetuning (Howard and Ruder, 2018; Devlin et al., 2019; Yang et al., 2019). Standard fine-tuning involves initializing the target model with the pretrained LM and training it with the target data. Fine-tuning, however, can lead to catastrophic forgetting (Goodfellow et al., 2013), if the pretrained LM representations are adjusted to such an extent to the target task, that most generic knowledge, captured during pretraining, is in effect forgotten (Howard and Ruder, 2018). A related problem of fine-tuning is overfitting to the target task, that often occurs when only a small number of training examples is available (Dai and Le, 2015). Adversarial"
2020.findings-emnlp.278,I05-5002,0,0.0124902,"identity transform, but during backpropagation, GRL reverses the gradients. In effect, the pretrained LM parameters are updated towards the opposite direction of the gradient of LM ain and, adversarially, towards the direction of the gradient of LDomain . 4 Experiments Datasets. We experiment with four Main datasets from the GLUE benchmark (Wang et al., 2019a). The chosen datasets represent the broad variety of natural language understanding tasks, such as linguistic acceptability (C O LA) (Warstadt et al., 2019), sentiment analysis (SST-2) (Socher et al., 2013), paraphrase detection (MRPC) (Dolan and Brockett, 2005) and textual entailment (RTE) (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The datasets used represent both high (SST-2) and lowresource (RTE, C O LA, MRPC) tasks, as well as single-sentence (C O LA, SST-2) and sentence-pair (MRPC, RTE) tasks. For Auxiliary data we select corpora from various domains. For the NEWS domain we use the AG NEWS dataset (Zhang et al., 2015) and for the REVIEWS domain we use a part of the Electronics reviews of He and McAuley (2016). For the LEGAL domain we use the English part of E UROPARL (Koehn, 2004) and for the"
2020.findings-emnlp.278,W07-1401,0,0.0672772,"ned LM parameters are updated towards the opposite direction of the gradient of LM ain and, adversarially, towards the direction of the gradient of LDomain . 4 Experiments Datasets. We experiment with four Main datasets from the GLUE benchmark (Wang et al., 2019a). The chosen datasets represent the broad variety of natural language understanding tasks, such as linguistic acceptability (C O LA) (Warstadt et al., 2019), sentiment analysis (SST-2) (Socher et al., 2013), paraphrase detection (MRPC) (Dolan and Brockett, 2005) and textual entailment (RTE) (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The datasets used represent both high (SST-2) and lowresource (RTE, C O LA, MRPC) tasks, as well as single-sentence (C O LA, SST-2) and sentence-pair (MRPC, RTE) tasks. For Auxiliary data we select corpora from various domains. For the NEWS domain we use the AG NEWS dataset (Zhang et al., 2015) and for the REVIEWS domain we use a part of the Electronics reviews of He and McAuley (2016). For the LEGAL domain we use the English part of E UROPARL (Koehn, 2004) and for the MEDICAL domain we use papers from PubMed, provided by Cohan et al. (2018). We also use math questi"
2020.findings-emnlp.278,N18-1202,0,0.0547498,"ers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularizer which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning. 1 Introduction Current research in NLP focuses on transferring knowledge from a language model (LM), pretrained on large general-domain data, to a target task. The LM representations are transferred to the target task either as additional features of a task-specific model (Peters et al., 2018), or by finetuning (Howard and Ruder, 2018; Devlin et al., 2019; Yang et al., 2019). Standard fine-tuning involves initializing the target model with the pretrained LM and training it with the target data. Fine-tuning, however, can lead to catastrophic forgetting (Goodfellow et al., 2013), if the pretrained LM representations are adjusted to such an extent to the target task, that most generic knowledge, captured during pretraining, is in effect forgotten (Howard and Ruder, 2018). A related problem of fine-tuning is overfitting to the target task, that often occurs when only a small number of"
2021.acl-long.301,W18-5304,1,0.918644,"pets. Experimental results on biomedical data from (Tsatsaronis et al., 2015) show the joint models vastly outperform the corresponding pipelines in snippet extraction, with fewer trainable parameters. Although our joint architecture is engineered to favor retrieving good snippets (as a near-final stage of QA), results show that the joint models are also competitive in document retrieval. We also show that our joint version of PDRMM , which has the fewest parameters of all models and does not use BERT, is competitive to BERT -based models, while also outperforming the best system of BIOASQ 6 (Brokos et al., 2018) in both document and snippet retrieval. These claims are also supported by human evaluation on two test batches of BIOASQ 7 (2019). To test our key findings on another dataset, we modified Natural Questions (Kwiatkowski et al., 2019), which only includes questions and answer spans from a single document, so that it can be used for document and snippet retrieval. Again, our joint PDRMMbased model largely outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions, though it does not perform better than the pipeline in document retrieval, since the BIOASQ joint"
2021.acl-long.301,P17-1171,0,0.0258781,"on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available. 1 Introduction Question answering (QA) systems that search large document collections (Voorhees, 2001; Tsatsaronis et al., 2015; Chen et al., 2017) typically use pipelines operating at gradually finer text granularities. A fully-fledged pipeline includes components that (i) retrieve possibly relevant documents typically using conventional information retrieval (IR); (ii) re-rank the retrieved documents employing a computationally more expensive document ranker; (iii) rank the passages, sentences, or other ‘snippets’ of the top-ranked documents; and (iv) select spans of the top-ranked snippets as ‘exact’ answers. Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et"
2021.acl-long.301,P17-1055,0,0.0256812,"ost relevant snippets of the top-ranked documents to help users quickly identify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a). The top-ranked snippets can also be used as a starting point for multi-document query-focused summarization, as in the BIOASQ challenge (Tsatsaronis et al., 2015). Hence, methods that identify good snippets are useful in several other applications, apart from QA. We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension ( MRC ) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers 3896 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3896–3907 August 1–6, 2021. ©2021 Association for Computational Linguistics only in a particular, usually paragraph-sized snippet, which is given per question. For QA systems that search large document collections, stages (ii) and (iii) are also important, if not more important, but have been studied much less in recent years, and not in a single joint neural mo"
2021.acl-long.301,D18-1454,0,0.0426863,"Missing"
2021.acl-long.301,P16-1086,0,0.0283382,"engines display the most relevant snippets of the top-ranked documents to help users quickly identify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a). The top-ranked snippets can also be used as a starting point for multi-document query-focused summarization, as in the BIOASQ challenge (Tsatsaronis et al., 2015). Hence, methods that identify good snippets are useful in several other applications, apart from QA. We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension ( MRC ) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers 3896 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3896–3907 August 1–6, 2021. ©2021 Association for Computational Linguistics only in a particular, usually paragraph-sized snippet, which is given per question. For QA systems that search large document collections, stages (ii) and (iii) are also important, if not more important, but have been studied much less in recent years, and not in a sing"
2021.acl-long.301,D19-1281,0,0.0315539,"Missing"
2021.acl-long.301,Q19-1026,0,0.037334,"Missing"
2021.acl-long.301,D18-1053,0,0.0179309,"2017) typically use pipelines operating at gradually finer text granularities. A fully-fledged pipeline includes components that (i) retrieve possibly relevant documents typically using conventional information retrieval (IR); (ii) re-rank the retrieved documents employing a computationally more expensive document ranker; (iii) rank the passages, sentences, or other ‘snippets’ of the top-ranked documents; and (iv) select spans of the top-ranked snippets as ‘exact’ answers. Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekuli´c et al., 2020). Although pipelines are conceptually simple, errors propagate from one component to the next (Hosein et al., 2019), without later components being able to revise earlier decisions. For example, once a document has been assigned a low relevance score, finding a particularly relevant snippet cannot change the document’s score. We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good sn"
2021.acl-long.301,P19-1612,0,0.0150093,"ng BERT with PDRMM in BJPDRMM and JBERT. However, we retrieve both documents and snippets, whereas MacAvaney et al. (2019) retrieve only documents. Models that directly retrieve documents by indexing neural document representations, rather than re-ranking documents retrieved by conventional IR, have also been proposed (Fan et al., 2018; Ai et al., 2018; Khattab and Zaharia, 2020), but none addresses both document and snippet retrieval. Yang et al. (2019a) use BERT to encode, index, and directly retrieve snippets, but do not consider documents; indexing snippets is also computationally costly. Lee et al. (2019) propose a joint model for direct snippet retrieval (and indexing) and answer span selection, again without retrieving documents. No previous work combined document and snippet retrieval in a joint neural model. This may be due to existing datasets, which do not provide both gold documents and gold snippets, with the exception of BIOASQ, which is however small by today’s standards (2.7k training questions, Section 3.1). For example, Pang et al. (2017) used much larger clickthrough datasets from a Chinese search engine, as well as datasets from the 2007 and 2008 TREC Million Query tracks (Qin e"
2021.acl-long.301,D18-1211,1,0.398931,"se pipelines operating at gradually finer text granularities. A fully-fledged pipeline includes components that (i) retrieve possibly relevant documents typically using conventional information retrieval (IR); (ii) re-rank the retrieved documents employing a computationally more expensive document ranker; (iii) rank the passages, sentences, or other ‘snippets’ of the top-ranked documents; and (iv) select spans of the top-ranked snippets as ‘exact’ answers. Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekuli´c et al., 2020). Although pipelines are conceptually simple, errors propagate from one component to the next (Hosein et al., 2019), without later components being able to revise earlier decisions. For example, once a document has been assigned a low relevance score, finding a particularly relevant snippet cannot change the document’s score. We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from releva"
2021.acl-long.301,N18-1202,0,0.0565641,"Missing"
2021.acl-long.301,P18-2124,0,0.0204474,"etrieving documents. No previous work combined document and snippet retrieval in a joint neural model. This may be due to existing datasets, which do not provide both gold documents and gold snippets, with the exception of BIOASQ, which is however small by today’s standards (2.7k training questions, Section 3.1). For example, Pang et al. (2017) used much larger clickthrough datasets from a Chinese search engine, as well as datasets from the 2007 and 2008 TREC Million Query tracks (Qin et al., 2010), but these datasets do not contain gold snippets. SQUAD (Rajpurkar et al., 2016) and SQUAD v.2 (Rajpurkar et al., 2018) provide 100k and 150k questions, respectively, but for each question they require extracting an exact answer span from a single given Wikipedia paragraph; no snippet retrieval is performed, because the relevant (paragraph-sized) snippet is given. Ahmad et al. (2019) provide modified versions of SQUAD and Natural Questions, suitable for direct snippet retrieval, but do not consider document retrieval. SearchQA (Dunn et al., 2017) provides 140k questions, along with 50 snippets per question. The web pages the snippets were extracted from, however, are not included in the dataset, only their URL"
2021.acl-long.301,N19-4013,0,0.0786088,"o revise earlier decisions. For example, once a document has been assigned a low relevance score, finding a particularly relevant snippet cannot change the document’s score. We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. We note that modern web search engines display the most relevant snippets of the top-ranked documents to help users quickly identify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a). The top-ranked snippets can also be used as a starting point for multi-document query-focused summarization, as in the BIOASQ challenge (Tsatsaronis et al., 2015). Hence, methods that identify good snippets are useful in several other applications, apart from QA. We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension ( MRC ) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers 3896 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and"
2021.acl-long.301,D16-1264,0,0.0514198,"answer span selection, again without retrieving documents. No previous work combined document and snippet retrieval in a joint neural model. This may be due to existing datasets, which do not provide both gold documents and gold snippets, with the exception of BIOASQ, which is however small by today’s standards (2.7k training questions, Section 3.1). For example, Pang et al. (2017) used much larger clickthrough datasets from a Chinese search engine, as well as datasets from the 2007 and 2008 TREC Million Query tracks (Qin et al., 2010), but these datasets do not contain gold snippets. SQUAD (Rajpurkar et al., 2016) and SQUAD v.2 (Rajpurkar et al., 2018) provide 100k and 150k questions, respectively, but for each question they require extracting an exact answer span from a single given Wikipedia paragraph; no snippet retrieval is performed, because the relevant (paragraph-sized) snippet is given. Ahmad et al. (2019) provide modified versions of SQUAD and Natural Questions, suitable for direct snippet retrieval, but do not consider document retrieval. SearchQA (Dunn et al., 2017) provides 140k questions, along with 50 snippets per question. The web pages the snippets were extracted from, however, are not"
2021.acl-long.301,P79-1022,0,0.267635,"Missing"
2021.acl-long.301,D18-1259,0,0.0484077,"Missing"
2021.acl-long.301,2020.tacl-1.54,0,0.02754,"if BERT is finetuned when training JPDRMM, and BJPDRMM - NF if BERT is not fine-tuned. In another variant of BJP DRMM , called BJPDRMM - ADAPT , the input embedding of each token is a linear combination of all the embeddings that BERT produces for that token at its different Transformer layers. The weights of the linear combination are learned via backpropagation. This allows BJPDRMM - ADAPT to learn which BERT layers it should mostly rely on when obtaining token embeddings. Previous work has reported that representations from different BERT layers may be more appropriate for different tasks (Rogers et al., 2020). BJPDRMM - ADAPT- NF is the same as BJPDRMM - ADAPT, but BERT is not finetuned; the weights of the linear combination of embeddings from BERT layers are still learned. 2.4 to JPDRMM. Instead, in this subsection we use BERT as a ranker, replacing PDRMM . For document ranking alone (when not cosidering snippets), we feed BERT with pairs of questions and documents (Fig. 3). BERT’s top-layer embedding of the ‘classification’ token [ CLS ] is concatenated with external features (the same as when scoring documents with PDRMM, Section 2.1), and a dense layer again produces the document’s score. We f"
2021.acl-long.301,N16-1174,0,0.0377286,"cesses separately each sentence si of d, producing a relevance score r(q, si ) per sentence, as when PDRMM scores sentences in the PDRMM + PDRMM pipeline. The highest sentence score maxi r(q, si ) is concatenated (Fig. 2) with the extra features that are used when PDRMM ranks documents, and an MLP produces the document’s score.4 JPDRMM then revises the sentence scores, by concatenating the score of each sentence with the document score 4 We also tried alternative mechanisms to obtain the document score from the sentence scores, including average of k-max sentence scores and hierarchical RNNs (Yang et al., 2016), but they led to no improvement. 3898 and passing each pair of scores to a dense layer to compute a linear combination, which becomes the revised sentence score. Notice that JPDRMM is mostly based on scoring sentences, since the main goal for QA is to obtain good snippets (almost final answers). The document score is obtained from the score of the document’s best sentence (and external features), but the sentence scores are revised, once the document score has been obtained. We use sentence-sized snippets, for compatibility with BIOASQ, but other snippet granularities (e.g., paragraph-sized)"
2021.acl-long.301,2020.acl-main.412,0,0.046894,"Missing"
2021.acl-long.301,Q16-1019,0,0.078609,"Missing"
2021.acl-long.301,Q16-1009,0,0.0426363,"without later components being able to revise earlier decisions. For example, once a document has been assigned a low relevance score, finding a particularly relevant snippet cannot change the document’s score. We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. We note that modern web search engines display the most relevant snippets of the top-ranked documents to help users quickly identify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a). The top-ranked snippets can also be used as a starting point for multi-document query-focused summarization, as in the BIOASQ challenge (Tsatsaronis et al., 2015). Hence, methods that identify good snippets are useful in several other applications, apart from QA. We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension ( MRC ) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers 3896 Proceedings of the 59th Annual Meeting of the Associatio"
2021.econlp-1.2,W19-5504,0,0.226949,"ailable to date. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use json format. We use edgar-corpus to train and release edgarw2v, which are word2vec embeddings for the financial domain. We employ these embeddings in a battery of financial nlp tasks and showcase their superiority over generic glove embeddings and other existing financial word embeddings. We also open-source edgarcrawler, a toolkit that facilitates downloading and extracting future annual reports. 1 Filings Tokens Companies Händschke et al. (2018) Various 242M Daudert and Ahmadi (2019) Various 188M Lee et al. (2014) 8-K 27.9M Kogan et al. (2009) 10-K 247.7M Tsai et al. (2016) 10-K 359M 10-K 6.5B edgar-corpus (ours) 270 60 500 10,492 7,341 38,009 Table 1: Financial corpora derived from part) and other sources (upper part). Years 2000-2015 1995-2018 2002-2012 1996-2006 1996-2013 1993-2020 sec (lower 2019), and merger participants identification (Katsafados et al., 2021). However, there has not been an open-source, efficient tool to retrieve textual information from edgar. Researchers interested in economics and nlp often rely on heavily-paid subscription services or try to bu"
2021.econlp-1.2,N09-1031,0,0.455947,"orresponding items (sections), and provided in a clean, easy-to-use json format. We use edgar-corpus to train and release edgarw2v, which are word2vec embeddings for the financial domain. We employ these embeddings in a battery of financial nlp tasks and showcase their superiority over generic glove embeddings and other existing financial word embeddings. We also open-source edgarcrawler, a toolkit that facilitates downloading and extracting future annual reports. 1 Filings Tokens Companies Händschke et al. (2018) Various 242M Daudert and Ahmadi (2019) Various 188M Lee et al. (2014) 8-K 27.9M Kogan et al. (2009) 10-K 247.7M Tsai et al. (2016) 10-K 359M 10-K 6.5B edgar-corpus (ours) 270 60 500 10,492 7,341 38,009 Table 1: Financial corpora derived from part) and other sources (upper part). Years 2000-2015 1995-2018 2002-2012 1996-2006 1996-2013 1993-2020 sec (lower 2019), and merger participants identification (Katsafados et al., 2021). However, there has not been an open-source, efficient tool to retrieve textual information from edgar. Researchers interested in economics and nlp often rely on heavily-paid subscription services or try to build web crawlers from scratch, often unsuccessfully. In the l"
2021.econlp-1.2,lee-etal-2014-importance,0,0.219407,"wnloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use json format. We use edgar-corpus to train and release edgarw2v, which are word2vec embeddings for the financial domain. We employ these embeddings in a battery of financial nlp tasks and showcase their superiority over generic glove embeddings and other existing financial word embeddings. We also open-source edgarcrawler, a toolkit that facilitates downloading and extracting future annual reports. 1 Filings Tokens Companies Händschke et al. (2018) Various 242M Daudert and Ahmadi (2019) Various 188M Lee et al. (2014) 8-K 27.9M Kogan et al. (2009) 10-K 247.7M Tsai et al. (2016) 10-K 359M 10-K 6.5B edgar-corpus (ours) 270 60 500 10,492 7,341 38,009 Table 1: Financial corpora derived from part) and other sources (upper part). Years 2000-2015 1995-2018 2002-2012 1996-2006 1996-2013 1993-2020 sec (lower 2019), and merger participants identification (Katsafados et al., 2021). However, there has not been an open-source, efficient tool to retrieve textual information from edgar. Researchers interested in economics and nlp often rely on heavily-paid subscription services or try to build web crawlers from scratch,"
2021.econlp-1.2,D14-1162,0,0.0881219,"s containing all the us annual reports (10-K filings) from 1993 to 2020.2 Each report is provided in an easy-to-use json format containing all 20 sections and subsections (items) of a sec annual report; different items provide useful information for different tasks in financial nlp. To the best of our knowledge, edgar-corpus is the largest publicly available financial corpus (Table 1). In addition, we use edgar-corpus to train and release word2vec embeddings, dubbed edgar-w2v. We experimentally show that the new embeddings are more useful for financial nlp tasks than generic glove embeddings (Pennington et al., 2014) and other previously released financial word2vec embeddings (Tsai et al., 2016). Introduction Natural Language Processing (nlp) for economics and finance is a rapidly developing research area (Hahn et al., 2018, 2019; Chen et al., 2020; ElHaj et al., 2020). While financial data are usually reported in tables, much valuable information also lies in text. A prominent source of such textual data is the Electronic Data Gathering, Analysis, and Retrieval system (edgar) from the us Securities and Exchange (sec) website that hosts filings of publicly traded companies.1 In order to maintain transpare"
2021.emnlp-main.559,Q19-1038,0,0.0242247,"2020; Ruder et al., 2021), complement- transfer has not yet been explored in legal NLP. ing previous monolingual benchmarks (Wang et al., To facilitate research on cross-lingual transfer 2018). The initial paradigm of multilingual word for text classification and legal topic classificaembeddings (Ruder et al., 2017) was rapidly ex- tion in particular, we introduce a new multilingual panded to pretrained multilingual models (Con- dataset, MULTI - EURLEX, which includes 65k Euneau et al., 2018), including work on zero-shot ropean Union (EU) laws, officially translated in cross-lingual transfer (Artetxe and Schwenk, 2019). the 23 EU official languages (Fig. 1). Each docMultilingual models based on TRANSFORMERs ument is annotated with multiple labels from EU (Vaswani et al., 2017), jointly pretrained on large ROVOC , where concepts are organized hierarchi6974 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6974–6996 c November 7–11, 2021. 2021 Association for Computational Linguistics • Experiments with several adaptation strategies showing that adaptation is beneficial in zero-shot cross-lingual transfer, apart from task transfer. • Comparison of chronological vs."
2021.emnlp-main.559,2020.emnlp-main.607,1,0.820975,"Missing"
2021.emnlp-main.559,2020.findings-emnlp.261,1,0.804267,"Missing"
2021.emnlp-main.559,2020.coling-main.598,0,0.0610697,"Missing"
2021.emnlp-main.559,2020.acl-main.747,0,0.433496,"accelerate finetuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer, but their impact also depends on the pretrained model used and the size of the label set. Figure 1: MULTI - EURLEX covers 23 official EU languages (Table 1) from 7 families (illustrated per EU country in the map). The UK was an EU member until 2020. The map should not be taken to imply that no other languages are spoken in EU countries. corpora across multiple languages, have significantly advanced the state-of-the-art in cross-lingual tasks (Conneau et al., 2020; Xue et al., 2021). In another interesting direction, legal NLP (Aletras et al., 2019; Zhong et al., 2020) is an emerging field targeting tasks such as legal judgment pre1 Introduction diction (Aletras et al., 2016), legal topic classification (Chalkidis et al., 2019), legal question anMultilingual learning is an active field of research swering (Kim et al., 2015), contract understanding in NLP. Starting from neural machine translation (Hendrycks et al., 2021), to name a few. Generic (Stahlberg, 2020), multilingual neural models are pretrained language models for legal text in parincreasingly"
2021.emnlp-main.559,D18-1269,0,0.0485601,"Missing"
2021.emnlp-main.559,2020.findings-emnlp.292,0,0.0510949,"Missing"
2021.emnlp-main.559,N19-1423,0,0.147452,"ss-lingual transfer is a very active area of language (source) to classify documents in another wider NLP research, currently dominated by large language (target). This would allow, e.g., classimultilingually pretrained models (Conneau et al., fiers trained in resource-rich languages to be reused 2018; Eisenschlos et al., 2019; Liu et al., 2020; in languages with fewer or no training instances. Xue et al., 2021). Recent work explores adapter We experiment with monolingual and multilingual TRANSFORMER-based models, i.e., mono- modules (Houlsby et al., 2019) to transfer monolingual BERT models (Devlin et al., 2019), XLM - lingually pretrained (Artetxe et al., 2020) or multilingually pretrained (Pfeiffer et al., 2020) models ROBERTA (Conneau et al., 2020), and MT 5 (Xue to new (target) languages. We examine more adapet al., 2021). We find that fine-tuning a multilingual model in a single source language leads to catas- tation strategies, apart from adapter modules, in trophic forgetting of multilingual knowledge and, truly zero-shot cross-lingual transfer. Unlike Pfeifconsequently, poor zero-shot transfer to target lan- fer et al. (2020), we do not train language-specific guages. We show that adaptation"
2021.emnlp-main.559,D19-1572,0,0.0264708,"Nguyen et al., 2018; Angelidis et al., 2018; cally (Fig. 2). We use the dataset as a testbed for Luz de Araujo et al., 2020), cross-lingual transfer zero-shot cross-lingual transfer in cases where we has not been studied in the legal domain. wish to exploit labeled training documents in one Cross-lingual transfer is a very active area of language (source) to classify documents in another wider NLP research, currently dominated by large language (target). This would allow, e.g., classimultilingually pretrained models (Conneau et al., fiers trained in resource-rich languages to be reused 2018; Eisenschlos et al., 2019; Liu et al., 2020; in languages with fewer or no training instances. Xue et al., 2021). Recent work explores adapter We experiment with monolingual and multilingual TRANSFORMER-based models, i.e., mono- modules (Houlsby et al., 2019) to transfer monolingual BERT models (Devlin et al., 2019), XLM - lingually pretrained (Artetxe et al., 2020) or multilingually pretrained (Pfeiffer et al., 2020) models ROBERTA (Conneau et al., 2020), and MT 5 (Xue to new (target) languages. We examine more adapet al., 2021). We find that fine-tuning a multilingual model in a single source language leads to catas"
2021.emnlp-main.559,2020.coling-main.79,0,0.0779233,"Missing"
2021.emnlp-main.559,P19-1267,0,0.0150518,"uments is available (Table 1). Compared to EURLEX 57 K (Chalkidis et al., 2019), MULTI EURLEX is not only larger (8k more documents) and multilingual; it is also more challenging, as the chronological split leads to temporal real-world concept drift across the training, development, test subsets, i.e., differences in label distribution and phrasing, representing a realistic temporal generalization problem (Huang and Paul, 2019; Lazaridou et al., 2021). Recently, Søgaard et al. (2021) showed this setup is more realistic, as it does not over-estimate real performance, contrary to random splits (Gorman and Bedrick, 2019). Label Set Level 1 Level 2 Level 3 All Random train-dev train-test 0.00 0.00 0.00 0.00 0.01 0.01 0.20 0.20 Chronological train-dev train-test 0.03 0.04 0.12 0.16 0.21 0.32 1.09 1.67 Table 3: KL-divergence of label distributions between subsets, using a random or chronological split. To verify that the chronological split of MULTI EURLEX in training, development, test subsets leads to a temporal concept drift, we compare the KL -divergence between the label distributions of the subsets using the chronological vs. a random split. Table 3 shows a random split leads to almost zero divergence for"
2021.emnlp-main.559,P19-1403,0,0.0223661,"ges (Table 1).6 For the official languages of the seven oldest member countries, the same 55k training documents are available; for the other languages, only a subset of the 55k training documents is available (Table 1). Compared to EURLEX 57 K (Chalkidis et al., 2019), MULTI EURLEX is not only larger (8k more documents) and multilingual; it is also more challenging, as the chronological split leads to temporal real-world concept drift across the training, development, test subsets, i.e., differences in label distribution and phrasing, representing a realistic temporal generalization problem (Huang and Paul, 2019; Lazaridou et al., 2021). Recently, Søgaard et al. (2021) showed this setup is more realistic, as it does not over-estimate real performance, contrary to random splits (Gorman and Bedrick, 2019). Label Set Level 1 Level 2 Level 3 All Random train-dev train-test 0.00 0.00 0.00 0.00 0.01 0.01 0.20 0.20 Chronological train-dev train-test 0.03 0.04 0.12 0.16 0.21 0.32 1.09 1.67 Table 3: KL-divergence of label distributions between subsets, using a random or chronological split. To verify that the chronological split of MULTI EURLEX in training, development, test subsets leads to a temporal concep"
2021.emnlp-main.559,2021.eacl-main.156,0,0.0254052,"ldest member countries, the same 55k training documents are available; for the other languages, only a subset of the 55k training documents is available (Table 1). Compared to EURLEX 57 K (Chalkidis et al., 2019), MULTI EURLEX is not only larger (8k more documents) and multilingual; it is also more challenging, as the chronological split leads to temporal real-world concept drift across the training, development, test subsets, i.e., differences in label distribution and phrasing, representing a realistic temporal generalization problem (Huang and Paul, 2019; Lazaridou et al., 2021). Recently, Søgaard et al. (2021) showed this setup is more realistic, as it does not over-estimate real performance, contrary to random splits (Gorman and Bedrick, 2019). Label Set Level 1 Level 2 Level 3 All Random train-dev train-test 0.00 0.00 0.00 0.00 0.01 0.01 0.20 0.20 Chronological train-dev train-test 0.03 0.04 0.12 0.16 0.21 0.32 1.09 1.67 Table 3: KL-divergence of label distributions between subsets, using a random or chronological split. To verify that the chronological split of MULTI EURLEX in training, development, test subsets leads to a temporal concept drift, we compare the KL -divergence between the label d"
2021.naacl-main.22,P19-1424,1,0.498034,"ionales it selects, as opposed to inferring explaand there is a large scope for further research. nations from the model’s decisions in a post-hoc manner (Ribeiro et al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may"
2021.naacl-main.22,C18-1041,0,0.0188322,"all the ECHR articles in each case, which is not true. In reality, the Court considers only alleged violations of particular articles, argued by applicants. Establishing which articles are allegedly violated is an important preliminary task when preparing an EC t HR application. Instead of oversimplifying the overall judgment prediction task, we focus on the preliminary task and use it as a test-bed for generating paragraph-level rationales in a multi-label text classification task for the first time. Legal judgment prediction has also been studied in Chinese criminal cases (Luo et al., 2017; Hu et al., 2018; Zhong et al., 2018). Similarly to the literature on legal judgment prediction for ECtHR cases, the aforementioned approaches ignore the crucial aspect of justifying the models’ predictions. Given the gravity that legal outcomes have for individuals, explainability is essential to increase the trust of both legal professionals and laypersons on system decisions and promote the use of supportive tools (Barfield, 2020). To the best of our knowledge, our work is the first step towards this direction for the legal domain, but is also applicable in other domains (e.g., biomedical), where justifica"
2021.naacl-main.22,2020.findings-emnlp.7,0,0.0127884,"s are sensitive to specific language, e.g., they misuse (are easily fooled by) references to health issues and medical examinations as support for Article 3 alleged violations, or references to appeals in higher courts as support for Article 5, even when there is no concrete evidence.11 Manually inspecting the predicted rationales, we did not identify bias on demographics. Although such spurious features may be buried in the contextualized paragraph encodings (Pi[ CLS ] ). In general, de-biasing models could benefit rationale extraction and we aim to investigate this direction in future work (Huang et al., 2020). Plausibility: Plausibility refers to how convincing the interpretation is to humans (Jacovi and Goldberg, 2020). While the legal expert annotated all relevant facts with respect to allegations, according to his manual review, allegations can also be justified by sub-selections (parts) of rationales. Thus, although a method may fail to extract all the available rationales, the provided (incomplete) set of rationales may still be a convincing explanation. To properly estimate plausibility across methods, one has to perform a subjective human evaluation which we did not conduct due to lack of r"
2021.naacl-main.22,W19-4828,0,0.0416749,"Missing"
2021.naacl-main.22,N19-1357,0,0.0163759,"al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle"
2021.naacl-main.22,2020.acl-main.409,0,0.0538525,"aint through a minimax game, where two players, one using the predicted binary mask and another using the complement of this mask, aim to correctly classify the text. If the first player fails to outperform the second, the model is penalized. Chang et al. (2019) use a Generative Adversarial Network (GAN) (Goodfellow et al., 2014), where a generator producing factual rationales competes with a generator producing counterfactual rationales to trick a discriminator. The GAN was not designed to perform classification. Given a text and a label it produces a rationale supporting (or not) the label. Jain et al. (2020) decoupled the model’s predictor from the rationale extractor to produce inherently faithful explanations, ensuring that the predictor considers only the rationales and not other parts of the text. Faithfulness refers to how accurately an explanation reflects the true reasoning of a model (Lipton, 2018; Jacovi and Goldberg, 2020). All the aforementioned work conceives rationales as selections of words, targeting binary classification tasks even when this is inappropriate. For instance, DeYoung et al. (2020) and Jain et al. (2020) over-simplified the task of the multipassage reading comprehensi"
2021.naacl-main.22,D17-1289,0,0.0863497,"model’s decisions in a post-hoc manner (Ribeiro et al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems tha"
2021.naacl-main.22,D19-1445,0,0.0239455,"and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 226–241 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: A de"
2021.naacl-main.22,D16-1011,0,0.0585929,"Missing"
2021.naacl-main.22,2020.tacl-1.54,0,0.0172295,"rom a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 226–241 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: A depiction of the ECtHR p"
2021.naacl-main.22,D19-1002,0,0.0161259,"2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference o"
2021.naacl-main.22,N16-1174,0,0.0877746,"Missing"
2021.naacl-main.22,D19-1420,0,0.194451,"case study on European Court of Human Rights Cases Ilias Chalkidis † ‡ Manos Fergadiotis † ‡ Dimitrios Tsarapatsanis ?  ‡† Nikolaos Aletras Ion Androutsopoulos Prodromos Malakasiotis † ‡ † EY AI Centre of Excellence in Document Intelligence, NCSR “Demokritos” ‡ Department of Informatics, Athens University of Economics and Business  Computer Science Department, University of Sheffield ? Law School, University of York Abstract justification for their decisions, similar to those of humans, (Zaidan et al., 2007; Lei et al., 2016; Interpretability or explainability is an emergChang et al., 2019; Yu et al., 2019) by requiring the ing research field in NLP. From a user-centric models to satisfy additional constraints. point of view, the goal is to build models that provide proper justification for their deciHere we follow a user-centric approach to ratiosions, similar to those of humans, by requirnale extraction, where the model learns to select ing the models to satisfy additional constraints. a subset of the input that justifies its decision. To To this end, we introduce a new application this end, we introduce a new application on leon legal text where, contrary to mainstream gal text where, contrar"
2021.naacl-main.22,N07-1033,0,0.192157,"Missing"
2021.naacl-main.22,D16-1076,0,0.0493649,"Missing"
2021.naacl-main.22,2020.acl-main.466,0,0.0184695,"iction task, the goal is to predict the court’s decision; this is much more difficult and vastly relies on case law (precedent cases). Although the new task (alleged violation prediction) is simpler than legal judgment prediction, models that address it (and their rationales) can still be useful in the judicial process (Fig. 1). For example, they can help applicants (plaintiffs) identify alleged violations that are supported by the facts of a case. They can help judges identify more quickly facts that support the alleged violations, contributing towards more informed judicial decision making (Zhong et al., 2020). They can also help legal experts identify previous cases related to particular allegations, helping analyze case law (Katz, 2012). Our contributions are the following: • We introduce rationale extraction for alleged violation prediction in ECtHR cases, a more tractable task compared to legal judgment prediction. This is a multi-label classification task that requires paragraph-level rationales, unlike previous work on word-level rationales for binary classification. that continuity is not beneficial and requisite in paragraph-level rationale-extraction, while comprehensiveness needs to be re"
2021.naacl-main.22,D18-1390,0,0.0977148,"1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanatio"
2021.semeval-1.6,N19-4010,0,0.121964,"es, both of which are based on a RoBERTa model (Liu et al., 2019). The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets.6 For toxic span detection, RoBERTa’s subword representations from three different layers (1, 6, 12) are summed to produce the corresponding word embeddings. A binary classifier on top of RoBERTa, operating on the word embeddings, predicts whether a word belongs to a toxic span or not. For the first component of the ensemble, the word embeddings obtained from RoBERTa’s subword representations are concatenated with FLAIR (Akbik et al., 2019) and FastText (Bojanowski et al., 2017) embeddings.7 The resulting embeddings are passed on to a two-layer stacked BiLSTM with a CRF layer on top to generate a BIO tag per word. The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al., 2019a). RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets. The ensemble returns the intersection of the toxic spans identified by the two components"
2021.semeval-1.6,2021.semeval-1.116,0,0.722277,"IO tag per word. The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al., 2019a). RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets. The ensemble returns the intersection of the toxic spans identified by the two components. 4.3 model-agnostic approach of Pluci´nski and Klimczak (2021) combined SHAP (Lundberg and Lee, 2017) with a fine-tuned BERT model. Ding and Jurgens (2021) and Benlahbib et al. (2021) also experimented with model-agnostic approaches, but they combined LIME (Ribeiro et al., 2016) with a Logistic Regression (LR) or with a linear Support Vector Machine (SVM) toxicity classifier. All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words). Lexicon-based No team relied on a purely lexiconbased approach, but few experimented with lexiconbased baselines (Zhu et al., 2021; Palomino et al., 2021) or used such components in ensembles (Ranasinghe et al., 2021). Three kin"
2021.semeval-1.6,2021.ccl-1.108,0,0.106924,"Missing"
2021.semeval-1.6,Q17-1010,0,0.0886387,"oBERTa model (Liu et al., 2019). The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets.6 For toxic span detection, RoBERTa’s subword representations from three different layers (1, 6, 12) are summed to produce the corresponding word embeddings. A binary classifier on top of RoBERTa, operating on the word embeddings, predicts whether a word belongs to a toxic span or not. For the first component of the ensemble, the word embeddings obtained from RoBERTa’s subword representations are concatenated with FLAIR (Akbik et al., 2019) and FastText (Bojanowski et al., 2017) embeddings.7 The resulting embeddings are passed on to a two-layer stacked BiLSTM with a CRF layer on top to generate a BIO tag per word. The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al., 2019a). RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets. The ensemble returns the intersection of the toxic spans identified by the two components. 4.3 model-agnostic approach of Pluci´"
2021.semeval-1.6,2021.semeval-1.113,0,0.299262,") combined an LSTM classifier with a token-masking approach that we call Input Erasure (IE), due to its similarities to the method of Li et al. (2016). The 6 github.com/unitaryai/detoxify In the latter case, in-vocabulary word embeddings were imported to Word2Vec for efficiency, and out of vocabulary words were handled with BPEs (Sennrich et al., 2016). 7 8 63 github.com/Orthrus-Lexicon/Toxic 5.3 build lexicons (Rusert, 2021), to leverage unsupervised rationale extraction methods (Rusert, 2021; Pluci´nski and Klimczak, 2021; Ding and Jurgens, 2021; Benlahbib et al., 2021), or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier. Suman and Jain (2021) astutely produced silver data from external sources to augment the initial golden annotated dataset, training their model iteratively in a semi-supervised manner. 5 TEAM 1 2 HITSZ-HLT S-NLP B ENCHMARK I hitmi&t YNU-HPCC Cisco MedAI IITKDetox GHOST HLE-UPC UTNLP YoungSheldon Lone Pine sk WLV-RIT CSECUDSG LISAC FSDM USMBA UoT-UWF-PartAI uob The median score UAntwerp MIPT-NSU-UTMN NLRG HamiltonDinggg lz1904 UIT-E10dot3 UniParma hub GoldenWindPlymouth AStarTwice sefamerve arge UPB Entity B ENCHMARK II BennettNLP (Fuchs"
2021.semeval-1.6,D19-1565,0,0.230116,"Missing"
2021.semeval-1.6,2021.semeval-1.121,0,0.321401,"Missing"
2021.semeval-1.6,2020.acl-main.387,0,0.0288832,"ipants, even if they did not lead to high scores. Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier. The model-specific rationale mechanism of Rusert (2021) used the attention scores of an LSTM toxicity classifier to detect the toxic spans. Pluci´nski and Klimczak (2021) used the same approach, but also employed an orthogonalisation technique (Mohankumar et al., 2020). The model-agnostic rationale mechanism of Rusert (2021) combined an LSTM classifier with a token-masking approach that we call Input Erasure (IE), due to its similarities to the method of Li et al. (2016). The 6 github.com/unitaryai/detoxify In the latter case, in-vocabulary word embeddings were imported to Word2Vec for efficiency, and out of vocabulary words were handled with BPEs (Sennrich et al., 2016). 7 8 63 github.com/Orthrus-Lexicon/Toxic 5.3 build lexicons (Rusert, 2021), to leverage unsupervised rationale extraction methods (Rusert, 2021; Pluci´nski and Klimczak, 2021; Ding and Jurg"
2021.semeval-1.6,2021.semeval-1.137,0,0.306754,"Missing"
2021.semeval-1.6,N19-1423,0,0.0217228,"e 2: Examples of toxic test posts and their ground truth toxic spans (shown in red). The left column shows the character offsets of the toxic spans. The top three posts have no toxic spans, the next three have one each, while the remaining three posts have two toxic spans each. continuous, and there were submissions until the last day. Despite the decreasing total number of submissions per day, the top daily score increased, reaching its maximum on the last day (see Fig. 4). ing and span extraction (Zhu et al., 2021). For their token labeling approach, the team used two systems based on BERT (Devlin et al., 2019). Both systems had a Conditional Random Field (CRF) layer (Sutton and McCallum, 2006) on top, but one of the two also had an LSTM layer (Hochreiter and Schmidhuber, 1997) between BERT and the CRF layer. In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span). For their span extraction approach, the team also used BERT. Roughly speaking, in this case BERT produces probabilities indicating how likely it is for each token to be the beginning or end of a toxic span. Then a"
2021.semeval-1.6,2021.semeval-1.6,1,0.0541379,"sion per day during the evaluation period. 4 Participation overview We received 479 individual participation requests, 92 team formations, and 1,449 submissions. 91 teams submitted valid predictions (1,385 valid submissions in total) and were scored; out of these, only 36 submitted system descriptions. 4.1 4.2 The HITSZ-HLT submission The S-NLP submission The team with the second best performing system (S-NLP) consists of individual participants who grouped and submitted an ensemble of their sysThe best performing team (HITSZ-HLT) formulated the problem as a combination of token label62 tems (Nguyen et al., 2021). The ensemble combines two approaches, both of which are based on a RoBERTa model (Liu et al., 2019). The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets.6 For toxic span detection, RoBERTa’s subword representations from three different layers (1, 6, 12) are summed to produce the corresponding word embeddings. A binary classifier on top of RoBERTa, operating on the word embeddings, predicts whether a word belongs to a toxic span or not. For the first component of the ensemble, the word embeddings obtained from RoBERTa’s subword represent"
2021.semeval-1.6,2021.semeval-1.31,0,0.699007,"layer on top to generate a BIO tag per word. The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al., 2019a). RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets. The ensemble returns the intersection of the toxic spans identified by the two components. 4.3 model-agnostic approach of Pluci´nski and Klimczak (2021) combined SHAP (Lundberg and Lee, 2017) with a fine-tuned BERT model. Ding and Jurgens (2021) and Benlahbib et al. (2021) also experimented with model-agnostic approaches, but they combined LIME (Ribeiro et al., 2016) with a Logistic Regression (LR) or with a linear Support Vector Machine (SVM) toxicity classifier. All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words). Lexicon-based No team relied on a purely lexiconbased approach, but few experimented with lexiconbased baselines (Zhu et al., 2021; Palomino et al., 2021) or used such components in ensembles (Ranasin"
2021.semeval-1.6,D17-1117,1,0.852801,"proach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions. 1 Introduction Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a). Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017; Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible. In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic. This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems. As a first step along this direction, Task 5 of SemEval 2021 provided the parti"
2021.semeval-1.6,2021.semeval-1.29,0,0.536936,"ed to tag some spans and many authors noted that performance on posts with no tagged span was extremely poor compared to performance on posts with tagged spans. Many systems were also reluctant to tag function words like ‘of’ and ‘and’, which can be included in multi-word spans (e.g., ‘piece of crap’), leading to a decline in performance as measured by the chosen F1 measure. The overwhelming presence 66 of single word gold spans in the training set favors short spans. But the majority of the short spans comprises common cuss or clearly abusive words, which can be directly classified as toxic (Ghosh and Kumar, 2021); by contrast, the infrequent longer spans are rather context dependent and more challenging to detect. This probably also contributed to the performance of the best system (HITSZ-HLT), since one of the two components of that ensemble handled better long spans, as already discussed in Section 6.3. Other error analysis highlighted challenges intrinsic to the task. The strong dependency of toxicity on context makes it particularly difficult to solve with systems based on vocabulary. Toxicity, when expressed with subtle language, can appear through non-local text features: some comments are toxic"
2021.semeval-1.6,2021.semeval-1.114,0,0.284425,"Missing"
2021.semeval-1.6,2021.semeval-1.111,0,0.121973,"(2021) and Benlahbib et al. (2021) also experimented with model-agnostic approaches, but they combined LIME (Ribeiro et al., 2016) with a Logistic Regression (LR) or with a linear Support Vector Machine (SVM) toxicity classifier. All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words). Lexicon-based No team relied on a purely lexiconbased approach, but few experimented with lexiconbased baselines (Zhu et al., 2021; Palomino et al., 2021) or used such components in ensembles (Ranasinghe et al., 2021). Three kinds of lexiconbased methods were used. First, the lexicon was handcrafted by domain experts (Smedt et al., 2020) and it was simply employed as a list of toxic words for lookup operations (Palomino et al., 2021). Second, the lexicon was compiled using the set of tokens labeled as toxic in our span-annotated training set and it was used as a lookup table (Burtenshaw and Kestemont, 2021), possibly also storing the frequency of each lexicon token in the training set (Zhu et al., 2021). The former two were also combined (Ranasinghe et al., 2021). Third, the least supervised lexicons were"
2021.semeval-1.6,P82-1020,0,0.784405,"Missing"
2021.semeval-1.6,N16-3020,0,0.221454,"ble for the toxicity of the posts, when identifying such spans was possible. Note that a post may include no toxic span and still be marked as toxic. On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts. We provided a dataset of English posts with gold annotations of toxic spans, and evaluated participating systems on a held-out test subset using character-based F1. The task could be addressed as supervised sequence labeling, training on the provided posts with gold toxic spans. It could also be treated as rationale extraction (Li et al., 2016; Ribeiro et al., 2016), using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. There were almost 500 individual participants, and 36 out of the 92 teams that were formed submitted reports and results that we survey here. Most teams adopted the supervised sequence labeling approach. Hence, there is still scope for further work on the rationale extraction approach. We also discuss other possible improvements in the definition and data of the task. The Toxic Spans Detection task of SemEval2021 required participants to predict the spans of toxi"
2021.semeval-1.6,P19-1051,0,0.0507131,"t one of the two also had an LSTM layer (Hochreiter and Schmidhuber, 1997) between BERT and the CRF layer. In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span). For their span extraction approach, the team also used BERT. Roughly speaking, in this case BERT produces probabilities indicating how likely it is for each token to be the beginning or end of a toxic span. Then a heuristic search algorithm, originally developed for target extraction in sentiment analysis by Hu et al. (2019), selects the best combinations of candidate begin and end tokens, aiming to output the most likely set of toxic spans per post. The character predictions of the three systems described above were combined with majority voting per character. That is, if any two systems considered a character to be part of a toxic span, then the ensemble classified the character as toxic, otherwise the ensemble classified it as non-toxic. Figure 4: The evaluation score (character F1) of the best submission per day during the evaluation period. 4 Participation overview We received 479 individual participation re"
2021.semeval-1.6,2021.semeval-1.119,0,0.69917,"employed as a list of toxic words for lookup operations (Palomino et al., 2021). Second, the lexicon was compiled using the set of tokens labeled as toxic in our span-annotated training set and it was used as a lookup table (Burtenshaw and Kestemont, 2021), possibly also storing the frequency of each lexicon token in the training set (Zhu et al., 2021). The former two were also combined (Ranasinghe et al., 2021). Third, the least supervised lexicons were built with statistical analysis on the occurrences of tokens in a training set solely annotated at the comment level (toxic/nontoxic post) (Rusert, 2021). An added value of these approaches is that easy to use resources (toxicity lexicons) are built and shared publicly, such as the one suggested by Pluci´nski and Klimczak (2021).8 Custom losses Zhen Wang and Liu (2021) experimented with a new custom loss, which weighted false toxicity predictions based on their location in the text. If a false prediction was located near a ground truth toxic span, then it would contribute less to the overall loss for that post, compared to one located further away. The loss function used by Kuyumcu et al. (2021) to train their system is the Tversky Similarity"
2021.semeval-1.6,W17-1101,0,0.0754697,"rvised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions. 1 Introduction Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a). Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017; Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible. In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic. This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems. As a first step along this direction, Task 5 of SemEv"
2021.semeval-1.6,P16-1162,0,0.00826101,"sed the attention scores of an LSTM toxicity classifier to detect the toxic spans. Pluci´nski and Klimczak (2021) used the same approach, but also employed an orthogonalisation technique (Mohankumar et al., 2020). The model-agnostic rationale mechanism of Rusert (2021) combined an LSTM classifier with a token-masking approach that we call Input Erasure (IE), due to its similarities to the method of Li et al. (2016). The 6 github.com/unitaryai/detoxify In the latter case, in-vocabulary word embeddings were imported to Word2Vec for efficiency, and out of vocabulary words were handled with BPEs (Sennrich et al., 2016). 7 8 63 github.com/Orthrus-Lexicon/Toxic 5.3 build lexicons (Rusert, 2021), to leverage unsupervised rationale extraction methods (Rusert, 2021; Pluci´nski and Klimczak, 2021; Ding and Jurgens, 2021; Benlahbib et al., 2021), or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier. Suman and Jain (2021) astutely produced silver data from external sources to augment the initial golden annotated dataset, training their model iteratively in a semi-supervised manner. 5 TEAM 1 2 HITSZ-HLT S-NLP B ENCHMARK I hitmi&t YNU-HPCC Cisco MedAI IITKDetox GHOST HLE-U"
2021.semeval-1.6,2021.semeval-1.118,0,0.151624,"Erasure (IE), due to its similarities to the method of Li et al. (2016). The 6 github.com/unitaryai/detoxify In the latter case, in-vocabulary word embeddings were imported to Word2Vec for efficiency, and out of vocabulary words were handled with BPEs (Sennrich et al., 2016). 7 8 63 github.com/Orthrus-Lexicon/Toxic 5.3 build lexicons (Rusert, 2021), to leverage unsupervised rationale extraction methods (Rusert, 2021; Pluci´nski and Klimczak, 2021; Ding and Jurgens, 2021; Benlahbib et al., 2021), or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier. Suman and Jain (2021) astutely produced silver data from external sources to augment the initial golden annotated dataset, training their model iteratively in a semi-supervised manner. 5 TEAM 1 2 HITSZ-HLT S-NLP B ENCHMARK I hitmi&t YNU-HPCC Cisco MedAI IITKDetox GHOST HLE-UPC UTNLP YoungSheldon Lone Pine sk WLV-RIT CSECUDSG LISAC FSDM USMBA UoT-UWF-PartAI uob The median score UAntwerp MIPT-NSU-UTMN NLRG HamiltonDinggg lz1904 UIT-E10dot3 UniParma hub GoldenWindPlymouth AStarTwice sefamerve arge UPB Entity B ENCHMARK II BennettNLP (Fuchsia) TeamGriek UIT-ISE-NLP NLP UIowa B ENCHMARK III macech 3 5 7 8 9 13 14 15 16"
2021.semeval-1.6,S19-2010,0,0.0771481,"oses, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions. 1 Introduction Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a). Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017; Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible. In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic. This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems. As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previ"
2021.semeval-1.6,2021.semeval-1.30,0,0.170656,"lookup table (Burtenshaw and Kestemont, 2021), possibly also storing the frequency of each lexicon token in the training set (Zhu et al., 2021). The former two were also combined (Ranasinghe et al., 2021). Third, the least supervised lexicons were built with statistical analysis on the occurrences of tokens in a training set solely annotated at the comment level (toxic/nontoxic post) (Rusert, 2021). An added value of these approaches is that easy to use resources (toxicity lexicons) are built and shared publicly, such as the one suggested by Pluci´nski and Klimczak (2021).8 Custom losses Zhen Wang and Liu (2021) experimented with a new custom loss, which weighted false toxicity predictions based on their location in the text. If a false prediction was located near a ground truth toxic span, then it would contribute less to the overall loss for that post, compared to one located further away. The loss function used by Kuyumcu et al. (2021) to train their system is the Tversky Similarity Index (Tversky, 1977), a generalisation of the Sørensen–Dice coefficient and the Jaccard index, which was adjusted by the authors to weigh up false negatives. Data augmentation The vast majority of the participating te"
2021.woah-1.15,W19-3508,0,0.019667,"c (p). By using this binary ground truth, AUPR and AUC ver143 ified that BERTr outperforms the rest of the models, even when the models are used as classifiers. 4 Related Work Following the work of Borkan et al. (2019), this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way. Toxicity detection is a popular task that has been addressed by machine learning approaches (Davidson et al., 2017; Waseem and Hovy, 2016; Djuric et al., 2015), including deep learning approaches (Park and Fung, 2017; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Chiril et al., 2020; Ibrohim and Budi, 2018; Ross et al.,"
2021.woah-1.15,2020.lrec-1.175,0,0.0295355,"17; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Chiril et al., 2020; Ibrohim and Budi, 2018; Ross et al., 2016; Wiegand et al., 2018). We started to investigate context-sensitivity in toxicity detection in our previous work (Pavlopoulos et al., 2020) using existing toxicity detection datasets and a much smaller dataset (250 posts) we constructed with both IC and OC labels. Comparing to our previous work, here we constructed and released a much larger dataset (10k posts) with IC and OC labels, we introduced the new task of context-sensitivity estimation, and we reported experimental results indicating that the new task is feasible. 5 post, which is more costly"
2021.woah-1.15,N19-1423,0,0.00566148,"uch posts are rare (Fig. 4) and thus, they are hard to collect and annotate. This observation motivated the experiments of the next section, where we train context-sensitivity detectors, which allow us to collect posts that are likely to be context-sensitive. These posts can then be used to train toxicity detectors on datasets richer in context-sensitive posts. 3.2 Context Sensitivity Estimation We trained and assessed four regressors on the new CCC dataset, to predict the context-sensitivity δ. We used Linear Regression, Support Vector Regression, a Random Forest regressor, and a BERT-based (Devlin et al., 2019) regression model (BERTr). The first three regressors use TF - IDF features. In the case of BERTr, we add a feed-forward neural network (FFNN) on top of the top-level embedding of the [CLS] token. The FFNN consists of a dense layer (128 neurons) and a tanh activation function, followed by another dense layer. The last dense layer has a single output neuron, with no activation function, that produces the context sensitivity score. Preliminary experiments showed that adding simplistic context-mechanisms (e.g., concatenating the parent post) to the context sensitivity regressors does not lead to"
2021.woah-1.15,2021.naacl-main.204,0,0.057855,"Missing"
2021.woah-1.15,2020.osact-1.12,0,0.0150309,"AUC ver143 ified that BERTr outperforms the rest of the models, even when the models are used as classifiers. 4 Related Work Following the work of Borkan et al. (2019), this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way. Toxicity detection is a popular task that has been addressed by machine learning approaches (Davidson et al., 2017; Waseem and Hovy, 2016; Djuric et al., 2015), including deep learning approaches (Park and Fung, 2017; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Chiril et al., 2020; Ibrohim and Budi, 2018; Ross et al., 2016; Wiegand et al., 2018). We started to i"
2021.woah-1.15,W17-3008,0,0.0178849,"hes (Park and Fung, 2017; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Chiril et al., 2020; Ibrohim and Budi, 2018; Ross et al., 2016; Wiegand et al., 2018). We started to investigate context-sensitivity in toxicity detection in our previous work (Pavlopoulos et al., 2020) using existing toxicity detection datasets and a much smaller dataset (250 posts) we constructed with both IC and OC labels. Comparing to our previous work, here we constructed and released a much larger dataset (10k posts) with IC and OC labels, we introduced the new task of context-sensitivity estimation, and we reported experimental results indicating that the new task is feasible. 5 post,"
2021.woah-1.15,2020.alw-1.4,0,0.0246135,"t BERTr outperforms the rest of the models, even when the models are used as classifiers. 4 Related Work Following the work of Borkan et al. (2019), this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way. Toxicity detection is a popular task that has been addressed by machine learning approaches (Davidson et al., 2017; Waseem and Hovy, 2016; Djuric et al., 2015), including deep learning approaches (Park and Fung, 2017; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Chiril et al., 2020; Ibrohim and Budi, 2018; Ross et al., 2016; Wiegand et al., 2018). We started to investigate context-se"
2021.woah-1.15,W17-3006,0,0.0137123,"rs for that specific post: t(p) = SEMoc (p) + SEMic (p). By using this binary ground truth, AUPR and AUC ver143 ified that BERTr outperforms the rest of the models, even when the models are used as classifiers. 4 Related Work Following the work of Borkan et al. (2019), this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way. Toxicity detection is a popular task that has been addressed by machine learning approaches (Davidson et al., 2017; Waseem and Hovy, 2016; Djuric et al., 2015), including deep learning approaches (Park and Fung, 2017; Pavlopoulos et al., 2017b,c; Chakrabarty et al., 2019; Badjatiya et al., 2017; Haddad et al., 2020; Ozler et al., 2020). Despite the plethora of computational approaches, what most of these have in common is that they disregard context, such as the parent post in discussions. The reason for this weakness is that datasets are developed while annotators ignore the context (Nobata et al., 2016; Wulczyn et al., 2017; Waseem and Hovy, 2016). Most of the datasets in the field are in English, but datasets in other languages have the same weakness (Pavlopoulos et al., 2017a; Mubarak et al., 2017; Ch"
2021.woah-1.15,W17-3004,1,0.902593,"Missing"
2021.woah-1.15,D17-1117,1,0.934315,"ataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce an additional cost. 1 Introduction Online fora are used to facilitate discussions, but hateful, insulting, identity-attacking, profane, or otherwise abusive posts may also occur. These posts are called toxic (Borkan et al., 2019) or abusive (Thylstrup and Waseem, 2020), and systems detecting them (Waseem and Hovy, 2016; Pavlopoulos et al., 2017b; Badjatiya et al., 2017) are called toxicity (or abusive language) detection systems. What most of these systems have in common, besides aiming to promote healthy discussions online (Zhang et al., 2018), is that they disregard the conversational context (e.g., the parent post in the discussion), making the detection of context-sensitive toxicity a lot harder. For instance, the post “Keep the hell out” may be considered as ∗ Corresponding author. toxic by a moderator, if the previous (parent) post “What was the title of that ‘hell out’ movie?” is ignored. Although toxicity datasets that inclu"
2021.woah-1.15,W17-4209,1,0.921337,"ataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce an additional cost. 1 Introduction Online fora are used to facilitate discussions, but hateful, insulting, identity-attacking, profane, or otherwise abusive posts may also occur. These posts are called toxic (Borkan et al., 2019) or abusive (Thylstrup and Waseem, 2020), and systems detecting them (Waseem and Hovy, 2016; Pavlopoulos et al., 2017b; Badjatiya et al., 2017) are called toxicity (or abusive language) detection systems. What most of these systems have in common, besides aiming to promote healthy discussions online (Zhang et al., 2018), is that they disregard the conversational context (e.g., the parent post in the discussion), making the detection of context-sensitive toxicity a lot harder. For instance, the post “Keep the hell out” may be considered as ∗ Corresponding author. toxic by a moderator, if the previous (parent) post “What was the title of that ‘hell out’ movie?” is ignored. Although toxicity datasets that inclu"
2021.woah-1.15,N16-2013,0,0.222489,"idered. Using the new dataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce an additional cost. 1 Introduction Online fora are used to facilitate discussions, but hateful, insulting, identity-attacking, profane, or otherwise abusive posts may also occur. These posts are called toxic (Borkan et al., 2019) or abusive (Thylstrup and Waseem, 2020), and systems detecting them (Waseem and Hovy, 2016; Pavlopoulos et al., 2017b; Badjatiya et al., 2017) are called toxicity (or abusive language) detection systems. What most of these systems have in common, besides aiming to promote healthy discussions online (Zhang et al., 2018), is that they disregard the conversational context (e.g., the parent post in the discussion), making the detection of context-sensitive toxicity a lot harder. For instance, the post “Keep the hell out” may be considered as ∗ Corresponding author. toxic by a moderator, if the previous (parent) post “What was the title of that ‘hell out’ movie?” is ignored. Although to"
2021.woah-1.15,P18-1125,0,0.0176201,"he parent posts, which may not always be necessary and may introduce an additional cost. 1 Introduction Online fora are used to facilitate discussions, but hateful, insulting, identity-attacking, profane, or otherwise abusive posts may also occur. These posts are called toxic (Borkan et al., 2019) or abusive (Thylstrup and Waseem, 2020), and systems detecting them (Waseem and Hovy, 2016; Pavlopoulos et al., 2017b; Badjatiya et al., 2017) are called toxicity (or abusive language) detection systems. What most of these systems have in common, besides aiming to promote healthy discussions online (Zhang et al., 2018), is that they disregard the conversational context (e.g., the parent post in the discussion), making the detection of context-sensitive toxicity a lot harder. For instance, the post “Keep the hell out” may be considered as ∗ Corresponding author. toxic by a moderator, if the previous (parent) post “What was the title of that ‘hell out’ movie?” is ignored. Although toxicity datasets that include conversational context have recently started to appear, in previous work we showed that context-sensitive posts are still too few in those datasets (Pavlopoulos et al., 2020), which does not allow mode"
C04-1199,H01-1045,0,0.468031,"Missing"
C04-1199,W01-0509,0,0.0788192,"Missing"
C04-1199,H01-1006,0,0.31225,"teresting and of practical use. For example, a list of single-snippet definitions accompanied by their source URLs can be a good starting point for users of search engines wishing to find definitions. Single-snippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions; see Radev and McKeown (1997). Experiments indicate that our method clearly outperforms the techniques it builds upon in the task we considered. We sketch in section 6 how we plan to adapt our method to the post-2003 TREC task. 2 Previous techniques Prager et al. (2001, 2002) observe that definition questions can often be answered by hypernyms; for example, “schizophrenia” is a “mental illness”, where the latter is a hypernym of the former in WordNet. Deciding which hypernym to report, however, is not trivial. To use an example of Prager et al., in “What is a meerkat?” WordNet provides hypernym synsets such as {“viverrine”, “viverrine mammal”} (level 1), {“carnivore”} (level 2), {“placental”, …} (level 3), {“mammal”} (level 4), up to {“entity”, “something”} (level 9). In a neutral context, the most natural response is arguably “mammal” or “animal”. A hypern"
C04-1199,A97-1033,0,0.0204082,"Missing"
C04-1199,N03-2029,0,0.02676,"the training vectors that correspond to definitions are much fewer than the vectors for non-definitions (3004 vs. 15469 in our dataset of section 4). As a result, the induced classifier is biased towards non-definitions, and, hence, most unseen vectors receive higher confidence scores for the category of non-definitions than for the category of definitions. We do not compare the two scores. We pick the five vectors whose confidence score for the category of definitions is highest, and report the corresponding snippets; in effect, we use the SVM as a ranker, rather than a classifier; see also Ravichandran et al. (2003). The imbalance between the two categories can be reduced by considering (during both training and classification) only the first three snippets of each document, which discards mostly non-definitions. 3.2.1 Configuration 1: attributes of Joho at al. In the first configuration of our learning-based approach, the attributes of the vectors are roughly those of Joho et al.: two numeric attributes for SN and WC (section 2), and a binary attribute for each one of patterns (1)–(9) showing if the pattern is satisfied. We have also added binary attributes for the following manually crafted patterns, a"
C04-1199,N03-2037,0,0.170866,"Missing"
C04-1199,N03-1004,0,\N,Missing
C04-1199,A00-1021,0,\N,Missing
C04-1199,W03-1209,0,\N,Missing
C12-1056,P11-1049,0,0.625763,"rge collection and then using each cluster as a set of documents to be summarized. Although evaluations with human judges also examine the coherence, referential clarity, grammaticality, and readability of the summaries (Dang, 2005, 2006; Dang and Owczarzak, 2008), and some of these factors have also been considered in recent summarization algorithms (Nishikawa et al., 2010b; Woodsend and Lapata, 2012), most current multi-document summarization systems consider only the importance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selected sentences may only marginally improve or even reduce the perceived quality of the resulting summaries (Gillick and Favre, 2009), though recent work has produced abstractive summarization methods that perform better than extractive ones (Berg-Kirkpatri"
C12-1056,P10-1084,0,0.0697404,"Missing"
C12-1056,W09-1802,0,0.73687,"ance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selected sentences may only marginally improve or even reduce the perceived quality of the resulting summaries (Gillick and Favre, 2009), though recent work has produced abstractive summarization methods that perform better than extractive ones (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). Nevertheless, the difference in the performance of extractive and abstractive summarizers is often small, and abstractive summarizers typically require more processing time, as well as tools and resources (e.g., reliable large coverage parsers, paraphrasing rules) that are often not available in less widely spoken languages. Hence, it is still worth trying to improve extractive summarizers, at least from a practical, applicatio"
C12-1056,N09-1041,0,0.211971,"Missing"
C12-1056,W04-1013,0,0.374442,"ti-document summarization. Inspired by BergKirkpatrick et al.’s work, we include in the objective function of our ILP model the number of distinct word bigrams (of the input documents) that occur in the summary, but we use that number to measure diversity, unlike Berg-Kirkpatrick et al.’s work, where bigrams are weighted to measure importance. To obtain an importance score for each sentence, we use a Support Vector Regression (SVR) model (Vapnik, 1998), which has no direct counter-part 912 in Berg-Kirkpatrick et al.’s method. We show that our ILP method achieves state of the art ROUGE scores (Lin, 2004) on widely used benchmark datasets, when compared to BergKirkpatrick’s and other competitive extractive summarizers, also outperforming two greedy baselines that use only the importance scores of the SVR. For completeness, we also discuss and compare against the abstractive version of Berg-Kirkpatrick et al.’s summarizer, and the state of the art abstractive summarizer of Woodsend and Lapata (2012). Section 2 below discusses previous work on ILP methods for summarization. Section 3 presents our own ILP model, after first introducing the SVR model of sentence importance and the greedy baselines"
C12-1056,P11-1052,0,0.158149,"ach cluster as a set of documents to be summarized. Although evaluations with human judges also examine the coherence, referential clarity, grammaticality, and readability of the summaries (Dang, 2005, 2006; Dang and Owczarzak, 2008), and some of these factors have also been considered in recent summarization algorithms (Nishikawa et al., 2010b; Woodsend and Lapata, 2012), most current multi-document summarization systems consider only the importance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selected sentences may only marginally improve or even reduce the perceived quality of the resulting summaries (Gillick and Favre, 2009), though recent work has produced abstractive summarization methods that perform better than extractive ones (Berg-Kirkpatrick et al., 2011; Woodse"
C12-1056,C10-2105,0,0.0554503,"ingle summary from an input set of documents. The input documents may have been obtained, for example, by submitting a query to an information retrieval engine and retaining the most highly ranked documents, or by clustering the documents of a large collection and then using each cluster as a set of documents to be summarized. Although evaluations with human judges also examine the coherence, referential clarity, grammaticality, and readability of the summaries (Dang, 2005, 2006; Dang and Owczarzak, 2008), and some of these factors have also been considered in recent summarization algorithms (Nishikawa et al., 2010b; Woodsend and Lapata, 2012), most current multi-document summarization systems consider only the importance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selecte"
C12-1056,P10-2060,0,0.114311,"ingle summary from an input set of documents. The input documents may have been obtained, for example, by submitting a query to an information retrieval engine and retaining the most highly ranked documents, or by clustering the documents of a large collection and then using each cluster as a set of documents to be summarized. Although evaluations with human judges also examine the coherence, referential clarity, grammaticality, and readability of the summaries (Dang, 2005, 2006; Dang and Owczarzak, 2008), and some of these factors have also been considered in recent summarization algorithms (Nishikawa et al., 2010b; Woodsend and Lapata, 2012), most current multi-document summarization systems consider only the importance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selecte"
C12-1056,C10-1111,0,0.0289988,"We are already experimenting with an extended version of our method that also performs sentence compression. In future work, we hope to extend our ILP model to consider discourse coherence, sentence aggregation, and referring expression generation. Acknowledgements This research was funded by the Research Centre of the Athens University of Economics and Business. 923 system ROUGE -2 ROUGE - SU 4 ILP 2 0.11168 0.14413 Woodsend and Lapata 2012 (with QSTG) Woodsend and Lapata 2012 (without QSTG) Berg-Kirkpatrick et al. 2011 (with subtree cuts) Berg-Kirkpatrick et al. 2011 (without subtree cuts) Shen and Li 2010 Gillick and Favre 2009 (with sentence compression) Gillick and Favre 2009 (without sentence compression) Gillick et al. 2008 (run 43 in TAC 2008) Gillick et al. 2008 (run 13 in TAC 2008) Conroy and Schlesinger 2008 (run 60 in TAC 2008) Conroy and Schlesinger 2008 (run 37 in TAC 2008) Conroy and Schlesinger 2008 (run 06 in TAC 2008) Galanis and Malakasiotis 2008 (run 02 in TAC 2008) 0.11370 0.10320 0.11700 0.11050 0.09012 0.11100 0.11000 0.11140− 0.11044− 0.10379− 0.10338− 0.10133+ 0.10012+ 0.14470 0.13680 0.14380 0.13860 0.12094 N/A N/A 0.14298− 0.13985− 0.14200− 0.14277− 0.13977− 0.13694− Ta"
C12-1056,D12-1022,0,0.583487,"ut set of documents. The input documents may have been obtained, for example, by submitting a query to an information retrieval engine and retaining the most highly ranked documents, or by clustering the documents of a large collection and then using each cluster as a set of documents to be summarized. Although evaluations with human judges also examine the coherence, referential clarity, grammaticality, and readability of the summaries (Dang, 2005, 2006; Dang and Owczarzak, 2008), and some of these factors have also been considered in recent summarization algorithms (Nishikawa et al., 2010b; Woodsend and Lapata, 2012), most current multi-document summarization systems consider only the importance of the summary’s sentences, their non-redundancy (also called diversity), and the summary length (McDonald, 2007; Berg-Kirkpatrick et al., 2011; Lin and Bilmes, 2011). An extractive multi-document summarizer forms summaries by extracting (selecting) sentences from the input documents, without modifying the selected sentences. By contrast, an abstractive summarizer may also shorten or, more generally, rephrase the selected sentences. In practice, the additional processing of the selected sentences may only marginal"
C12-1056,P08-2052,0,\N,Missing
D09-1132,H05-1041,1,0.672522,"ing single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K > 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects r"
D09-1132,2000.eamt-1.1,0,0.0870498,"Missing"
D09-1132,voorhees-tice-2000-trec,0,0.0893042,"Missing"
D09-1132,N03-2037,0,0.0606493,"Missing"
D09-1132,E09-2005,1,0.836701,"Missing"
D09-1132,N04-1007,0,0.031939,"Missing"
D09-1132,H01-1045,0,0.041976,"Missing"
D09-1132,W04-1013,0,0.0175006,"ssifier is trained on Web windows, not directly on encyclopedia definitions, which allows it to avoid relying excessively on phrasings that are common in encyclopedia definitions, but uncommon in more indirect definitions of arbitrary Web pages. Fur1271 thermore, training the classifier directly on encyclopedia definitions would not provide negative examples. In our previous work with ATTW (Androutsopoulos and Galanis, 2005) we used a measure constructed by ourselves to assess the similarity between Web windows and encyclopedia definitions. Here, we use the more established ROUGE - W measure (Lin, 2004) instead. ROUGE W and other versions of ROUGE have been used in summarization to measure how close a machineauthored summary is to multiple human summaries of the same input. We use ROUGE - W in a similar setting, to measure how close a training window is to multiple encyclopedia definitions of the same term. A further difference from our previous work is that we also use ROUGE - W when computing the features of the windows to be classified. Previously, the SVM relied, among others, on Boolean features indicating if the target term was preceded or followed in the window to be classified by a p"
D09-1132,C04-1199,1,0.906839,"as Murasaki Shikibu. It also serves as a kind of travel guide to the world (...) Target term: Jacques Lacan (...) who is Jacques Lacan? John Haber in New York city a primer for pre-post-structuralists Jacques Lacan is a Parisian psychoanalyst who has influenced literary criticism and feminism. He began work in the 1950s, in the Freudian society there. It was a (...) Table 1: Definitions found by our system. descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets. The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor, 2000) was used to separate acceptable windows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows. On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al. (2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004). To train the SVM, however, thousands of training windows were required, each tagged as a positive or negative example. Obtaining large numbers of t"
D09-1132,H01-1006,0,0.0149636,"und by our system. descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets. The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor, 2000) was used to separate acceptable windows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows. On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al. (2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004). To train the SVM, however, thousands of training windows were required, each tagged as a positive or negative example. Obtaining large numbers of training windows is easy, but manually tagging them is very timeconsuming. In the TREC 2000 and 2001 datasets, it was possible to tag the training windows automatically by using training target terms and accompanying regular expression patterns provided by the TREC organizers. The regular expressions covered all the known acceptable definitions of the corresponding terms tha"
D11-1009,P05-1074,0,0.0779952,"les were extracted from a parallel English-Chinese corpus, based on the assumption that two English phrases e1 and e2 that are often aligned to the same Chinese phrase c are likely to be paraphrases and, hence, they can be treated as a paraphrasing rule e1 ↔ e2 .2 Zhao et al.’s method actually operates on slotted English phrases, obtained from parse trees, where slots correspond to part of speech (POS) tags. Hence, rules like the following three may be obtained, where NNi indicates a noun slot and NNPi a proper name slot. 2 This pivot-based paraphrase extraction approach was first proposed by Bannard and Callison-Burch (2005). It underlies several other paraphrase extraction methods (Riezler et al., 2007; Callison-Burch, 2008; Kok and Brockett, 2010). (1) (2) (3) a lot of NN1 ↔ plenty of NN1 NNP1 area ↔ NNP1 region NNP1 wrote NNP2 ↔ NNP2 was written by NNP1 In the basic form of their method, called Model 1, Zhao et al. (2009b) use a log-linear ranker to assign scores to candidate English paraphrase pairs he1 , e2 i; the ranker uses the alignment probabilities P (c|e1 ) and P (e2 |c) as features, along with features that assess the quality of the corresponding alignments. In an extension of their method, Model 2, Z"
D11-1009,N06-1003,0,0.0320915,"aluations of this kind. Further experiments indicate that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes"
D11-1009,D08-1021,0,0.385418,"paraphrasing rules (e.g., “X wrote Y ” “↔ Y was authored by X”) or similar patterns from corpora. Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entailment recognition (Dagan et al., 2009), where the goal is to decide whether or not the information of a given text is entailed by that of another. Significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules from multilingual parallel corpora (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Zhao et al., 2009a; Zhao et al., 2009b; Kok and Brockett, 2010). In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories. There are currently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters"
D11-1009,J96-2004,0,0.0215259,"Missing"
D11-1009,J05-1003,0,0.0415827,"surface form differs from that of the input; we call the latter factor diversity. The intuition is that a good paraphrase is grammatical, preserves the meaning of the original sentence, while also being as different as possible. Experimental results show that including in the ranking (or classification) component features from an existing paraphrase recognizer leads to improved results. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammatical1 See, for example, Collins and Koo (2005). 97 ity, meaning preservation, and diversity. The paper is accompanied by a new publicly available paraphrasing dataset we constructed for evaluations of this kind. Further experiments indicate that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s querie"
D11-1009,N06-2009,0,0.16513,"e used as monolingual phrase tables in a Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96–106, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics phrase-based SMT systems (Zhao et al., 2008; Zhao et al., 2009a); in both cases, paraphrases can then be generated by invoking an SMT system’s decoder (Koehn, 2009). A second paraphrase generation approach is to treat existing machine translation engines as black boxes, and translate each input sentence to a pivot language and then back to the original language (Duboue and Chu-Carroll, 2006). An extension of this approach uses multiple translation engines and pivot languages (Zhao et al., 2010). In this paper, we investigate a different paraphrase generation approach, which does not produce paraphrases by invoking machine translation system(s). We use an existing collection of monolingual paraphrasing rules extracted from multilingual parallel corpora (Zhao et al., 2009b); each rule is accompanied by one or more scores, intended to indicate the rule’s overall quality without considering particular contexts where the rule may be applied. Instead of using the rules as a monolingual"
D11-1009,N06-1058,0,0.0233695,"s multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candidate paraphrases We use the approximately one million English paraphrasing rules of Zhao et al. (2009b). Roughly speaking, the rules were extracted from a parallel English-Chinese c"
D11-1009,P09-5002,0,0.0141831,"allel corpora may perform poorly on comparable corpora (Nelken and Shieber, 2006); alternatively, large collections of paraphrasing rules obtained via paraphrase extraction from multilingual parallel corpora can be used as monolingual phrase tables in a Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96–106, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics phrase-based SMT systems (Zhao et al., 2008; Zhao et al., 2009a); in both cases, paraphrases can then be generated by invoking an SMT system’s decoder (Koehn, 2009). A second paraphrase generation approach is to treat existing machine translation engines as black boxes, and translate each input sentence to a pivot language and then back to the original language (Duboue and Chu-Carroll, 2006). An extension of this approach uses multiple translation engines and pivot languages (Zhao et al., 2010). In this paper, we investigate a different paraphrase generation approach, which does not produce paraphrases by invoking machine translation system(s). We use an existing collection of monolingual paraphrasing rules extracted from multilingual parallel corpora (Z"
D11-1009,N10-1017,0,0.0488784,"ns from corpora. Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entailment recognition (Dagan et al., 2009), where the goal is to decide whether or not the information of a given text is entailed by that of another. Significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules from multilingual parallel corpora (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Zhao et al., 2009a; Zhao et al., 2009b; Kok and Brockett, 2010). In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories. There are currently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles referring to the same event (Quirk et al., 2004) or other similar m"
D11-1009,I05-5008,0,0.0280164,"tate of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candidate paraphrases We use the approximately one million English paraphrasing rules of Zhao et al. (2009b). Roughly speaking, the rules w"
D11-1009,J10-3003,0,0.0346641,"ur overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind. 1 Introduction In recent years, significant effort has been devoted to research on paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The methods that have been proposed can be roughly classified into three categories: (i) recognition methods, i.e., methods that detect whether or not two in96 put sentences or other texts are paraphrases; (ii) generation methods, where the aim is to produce paraphrases of a given input sentence; and (iii) extraction methods, which aim to extract paraphrasing rules (e.g., “X wrote Y ” “↔ Y was authored by X”) or similar patterns from corpora. Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entail"
D11-1009,W07-0716,0,0.0614983,"sing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candidate paraphrases We use the approximately one"
D11-1009,P09-3004,1,0.896603,"sing rules, and we then rank them. Unfortunately, we did not have access to an implementation of ZHAO - RUL to compare against, but below we compare against another paraphraser proposed by Zhao et al. (2010), hereafter called ZHAO - ENG, which uses multiple machine translation engines and pivot languages, instead of paraphrasing rules, and which Zhao et al. found to outperform ZHAO - RUL. To further help the ranking component assess the degree to which C preserves the meaning of S, we also optionally include in the vectors of the hS, Ci pairs the features of an existing paraphrase recognizer (Malakasiotis, 2009) that obtained the best published results (Androutsopoulos and Malakasiotis, 2010) on the widely used MSR paraphrasing corpus.11 Most of the recognizer’s features are computed by using nine similarity measures: Levenshtein, Jaro-Winkler, Manhattan, Euclidean, and ngram (n = 3) distance, cosine similarity, Dice, Jaccard, and matching coefficients, all computed on tokens; consult Malakasiotis (2009) for details. For each hS, Ci pair, the nine similarity measures are ap9 We use SRILM; see http://www-speech.sri.com/. Application-specific features are also included, which can be used, for example,"
D11-1009,D09-1040,0,0.0187742,"er experiments indicate that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candid"
D11-1009,P09-1089,0,0.0124118,"te that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candidate paraphrases We us"
D11-1009,E06-1021,0,0.0162408,"rently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles referring to the same event (Quirk et al., 2004) or other similar monolingual comparable corpora can be used, though sentence alignment methods for parallel corpora may perform poorly on comparable corpora (Nelken and Shieber, 2006); alternatively, large collections of paraphrasing rules obtained via paraphrase extraction from multilingual parallel corpora can be used as monolingual phrase tables in a Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96–106, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics phrase-based SMT systems (Zhao et al., 2008; Zhao et al., 2009a); in both cases, paraphrases can then be generated by invoking an SMT system’s decoder (Koehn, 2009). A second paraphrase generation approach is to treat existing machin"
D11-1009,P09-1034,0,0.0494729,"Missing"
D11-1009,P02-1040,0,0.0825153,"Missing"
D11-1009,W04-3219,0,0.0790112,"et al., 2009b; Kok and Brockett, 2010). In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories. There are currently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles referring to the same event (Quirk et al., 2004) or other similar monolingual comparable corpora can be used, though sentence alignment methods for parallel corpora may perform poorly on comparable corpora (Nelken and Shieber, 2006); alternatively, large collections of paraphrasing rules obtained via paraphrase extraction from multilingual parallel corpora can be used as monolingual phrase tables in a Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96–106, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics phrase-based SMT systems (Zhao et al., 2008; Zhao"
D11-1009,J10-3010,0,0.0139289,"nd diversity. The paper is accompanied by a new publicly available paraphrasing dataset we constructed for evaluations of this kind. Further experiments indicate that when paraphrasing rules apply to the input sentences, our paraphrasing method is competitive to a state of the art paraphrase generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are"
D11-1009,P07-1059,0,0.0651173,"nglish phrases e1 and e2 that are often aligned to the same Chinese phrase c are likely to be paraphrases and, hence, they can be treated as a paraphrasing rule e1 ↔ e2 .2 Zhao et al.’s method actually operates on slotted English phrases, obtained from parse trees, where slots correspond to part of speech (POS) tags. Hence, rules like the following three may be obtained, where NNi indicates a noun slot and NNPi a proper name slot. 2 This pivot-based paraphrase extraction approach was first proposed by Bannard and Callison-Burch (2005). It underlies several other paraphrase extraction methods (Riezler et al., 2007; Callison-Burch, 2008; Kok and Brockett, 2010). (1) (2) (3) a lot of NN1 ↔ plenty of NN1 NNP1 area ↔ NNP1 region NNP1 wrote NNP2 ↔ NNP2 was written by NNP1 In the basic form of their method, called Model 1, Zhao et al. (2009b) use a log-linear ranker to assign scores to candidate English paraphrase pairs he1 , e2 i; the ranker uses the alignment probabilities P (c|e1 ) and P (e2 |c) as features, along with features that assess the quality of the corresponding alignments. In an extension of their method, Model 2, Zhao et al. consider two English phrases e1 and e2 as paraphrases, if they are of"
D11-1009,P08-1078,0,0.0311253,"would be to recursively apply the same process to the resulting Cs. 98 apply to S) with the highest scores. Zhao et al. actually associate each rule with three scores. The first one, hereafter called r1 , is the Model 1 score, and the other two, r2 and r3 , are the forward and backward alignment probabilities of Model 3; see Zhao et al. (2009b) for details. We use the average of the three scores, hereafter r4 , when generating candidates. Unfortunately, Zhao et al.’s scores reflect the overall quality of each rule, without considering the context of the particular S where the rule is applied. Szpektor et al. (2008) point out that, for example, a rule like “X acquire Y ” ↔ “X buy Y ” may work well in many contexts, but not in “Children acquire language quickly”. Similarly, “X charged Y with” ↔ “X accused Y of” should not be applied to sentences about charging batteries. Szpektor et al. propose, roughly speaking, to associate each rule with a model of the contexts where the rule is applicable, as well as models of the expressions that typically fill its slots, in order to be able to assess the applicability of each rule in specific contexts. The rules that we use do not have associated models of this kind"
D11-1009,P08-1089,0,0.101685,"g., “X wrote Y ” “↔ Y was authored by X”) or similar patterns from corpora. Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entailment recognition (Dagan et al., 2009), where the goal is to decide whether or not the information of a given text is entailed by that of another. Significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules from multilingual parallel corpora (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Zhao et al., 2009a; Zhao et al., 2009b; Kok and Brockett, 2010). In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories. There are currently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles re"
D11-1009,P09-1094,0,0.0737946,"Y was authored by X”) or similar patterns from corpora. Most of the methods that have been proposed belong in the first category, possibly because of the thrust provided by related research on textual entailment recognition (Dagan et al., 2009), where the goal is to decide whether or not the information of a given text is entailed by that of another. Significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules from multilingual parallel corpora (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Zhao et al., 2009a; Zhao et al., 2009b; Kok and Brockett, 2010). In this paper, we are concerned with paraphrase generation, which has received less attention than the other two categories. There are currently two main approaches to paraphrase generation. The first one treats paraphrase generation as a machine translation problem, with the peculiarity that the target language is the same as the source one. To bypass the lack of large monolingual parallel corpora, which are needed to train statistical machine translation (SMT) systems for paraphrasing, monolingual clusters of news articles referring to the same"
D11-1009,C10-1149,0,0.140968,"Processing, pages 96–106, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics phrase-based SMT systems (Zhao et al., 2008; Zhao et al., 2009a); in both cases, paraphrases can then be generated by invoking an SMT system’s decoder (Koehn, 2009). A second paraphrase generation approach is to treat existing machine translation engines as black boxes, and translate each input sentence to a pivot language and then back to the original language (Duboue and Chu-Carroll, 2006). An extension of this approach uses multiple translation engines and pivot languages (Zhao et al., 2010). In this paper, we investigate a different paraphrase generation approach, which does not produce paraphrases by invoking machine translation system(s). We use an existing collection of monolingual paraphrasing rules extracted from multilingual parallel corpora (Zhao et al., 2009b); each rule is accompanied by one or more scores, intended to indicate the rule’s overall quality without considering particular contexts where the rule may be applied. Instead of using the rules as a monolingual phrase table and invoking an SMT system’s decoder, we follow a generate and rank approach, which is incr"
D11-1009,W06-1610,0,0.0203228,"generator that uses multiple translation engines and pivot languages (Zhao et al., 2010). We note that paraphrase generation is useful in several language processing tasks. In question answering, for example, paraphrase generators can be used to paraphrase the user’s queries (Duboue and Chu-Carroll, 2006; Riezler and Liu, 2010); and in machine translation, paraphrase generation can help improve the translations (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Madnani et al., 2007), or it can be used when evaluating machine translation systems (Lepage and Denoual, 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009). The remainder of this paper is structured as follows: Section 2 explains how our method generates candidate paraphrases; Section 3 introduces the dataset we constructed, which is also used in subsequent sections; Section 4 discusses how candidate paraphrases are ranked; Section 5 compares our overall method to a state of the art paraphrase generator; and Section 6 concludes. 2 Generating candidate paraphrases We use the approximately one million English paraphrasing rules of Zhao et al. (2009b). Roughly speaking, the rules were extracted from"
D17-1117,D14-1179,0,0.00687933,"Missing"
D17-1117,D14-1181,0,0.0150067,"g evaluates on the same held-out subsets. For Gazzetta, word embeddings are initialized to the WORD 2 VEC embeddings we provide (Section 2.1). For Wikipedia, they are initialized to GLOVE embeddings (Pennington et al., 2014).15 In both cases, the embeddings are updated during backpropagation. Out of vocabulary (OOV) words, meaning words for which we have no initial embeddings, are mapped to a single randomly initialized embedding, also updated. 3.3 CNN We also compare against a vanilla CNN operating on word embeddings. We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015). For Wikipedia comments, we use a ‘narrow’ convolution layer, with kernels sliding (stride 1) over (entire) embeddings of word n-grams of sizes n = 1, . . . , 4. We use 300 kernels for each n value, a total of 1,200 kernels. The outputs of each kernel, obtained by applying the kernel to the different n-grams of a comment c, are then 13 For experiments with additional variants of a-RNN, consult Pavlopoulos et al. (2017). 14 We implemented the methods of this sub-section using Keras (keras.io) and TensorFlow (te"
D17-1117,D15-1166,0,0.0476107,"asures for the semi-automatic scenario. On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of Wulczyn et al. (2017), the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams. We also propose an attention mechanism that improves the overall performance of the RNN. Our attention mechanism differs from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al., 2014). In that sense, our attention is similar to that of of Yang et al. (2016), but our attention mechanism is a deeper MLP and it is only applied to words, whereas Yang et al. also have a second attention mechanism that assigns attention scores to entire sentences. In effect, our attention detects the words of a comment that affect most the classification decision (accept, reject), by examining them in the con"
D17-1117,W16-3638,0,0.0304377,"Missing"
D17-1117,N13-1090,0,0.0325321,"Missing"
D17-1117,W17-3004,1,0.824833,"NN operating on word embeddings. We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015). For Wikipedia comments, we use a ‘narrow’ convolution layer, with kernels sliding (stride 1) over (entire) embeddings of word n-grams of sizes n = 1, . . . , 4. We use 300 kernels for each n value, a total of 1,200 kernels. The outputs of each kernel, obtained by applying the kernel to the different n-grams of a comment c, are then 13 For experiments with additional variants of a-RNN, consult Pavlopoulos et al. (2017). 14 We implemented the methods of this sub-section using Keras (keras.io) and TensorFlow (tensorflow.org). 15 See https://nlp.stanford.edu/projects/ glove/. We use ‘Common Crawl’ (840B tokens). 1129 0.0 accept gray ta : accept threshold reject 1.0 tr : reject threshold Figure 4: Illustration of threshold tuning. max-pooled, leading to a single output per kernel. The resulting feature vector (1,200 maxpooled outputs) goes through a dropout layer (Hinton et al., 2012) (p = 0.5), and then to an LR layer, which provides PCNN (reject|c). For Gazzetta, the CNN is the same, except that n = 1, . . ."
D17-1117,D14-1162,0,0.124372,"Missing"
D17-1117,P94-1013,0,0.021687,"C, NBSVM). Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best results were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of at most 5 tokens, examining the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) aimed to predict which users would be banned from on-line communities. Their best system used a random forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Sood et al. (2012a; 2012b) experimented with 6.5K comments from Yahoo Buzz, moderated via crowdsourcing. They showed that a linear SVM, representing each comment as a bag of word bigrams and stems, performs better than word lists. Their best results were obtained by combining the SVM with a word"
D17-1117,W12-2103,0,0.552699,"and token n-grams. An LR classifier operating on DOC 2 VEC-like comment embeddings (Le and Mikolov, 2014) also performed worse than NBSVM . To surpass NBSVM , Mehdad et al. used an SVM to combine features from their three other methods (RNNLMs, LR with DOC 2 VEC, NBSVM). Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best results were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of at most 5 tokens, examining the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) aimed to predict which users would be banned from on-line communities. Their best system used a random forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Sood et al. (2012a; 2012b) experimented with 6.5"
D17-1117,N16-2013,0,0.124141,"Missing"
D17-1117,N16-1174,0,0.0446224,"t for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams. We also propose an attention mechanism that improves the overall performance of the RNN. Our attention mechanism differs from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al., 2014). In that sense, our attention is similar to that of of Yang et al. (2016), but our attention mechanism is a deeper MLP and it is only applied to words, whereas Yang et al. also have a second attention mechanism that assigns attention scores to entire sentences. In effect, our attention detects the words of a comment that affect most the classification decision (accept, reject), by examining them in the context of the particular comment. Although our attention mechanism does not always improve the performance of the RNN, it has the additional advantage of allowing the RNN to highlight suspicious words that a moderator could consider to decide more quickly if a comme"
D18-1211,P18-1128,0,0.0166753,"using the TREC ad-hoc retrieval evaluation script10 focusing on MAP, Precision@20 and nDCG@20 (Manning et al., 2008). We trained each model five times with different random seeds and report the mean and standard deviation for each metric on test data; in each run, the model selected had the highest MAP on the development data. We also report results for an oracle, which re-ranks the N documents returned by BM 25 placing all human-annotated relevant documents at the top. To test for statistical significance between two systems, we employed twotailed stratified shuffling (Smucker et al., 2007; Dror et al., 2018) using the model with the highest development MAP over the five runs per method. 4.2 BioASQ Experiments Our first experiment used the dataset of the document ranking task of BIOASQ (Tsatsaronis et al., 2015), years 1–5.11 It contains 2,251 English biomedical questions, each formulated by a biomedical expert, who searched (via PubMed12 ) for, and annotated relevant documents. Not all relevant documents were necessarily annotated, but the data includes additional expert relevance judgments made during the official evaluation.13 The document collection consists of approx. 28M ‘articles’ (titles a"
D18-1211,D17-1110,0,0.174069,"Missing"
D18-1211,N16-1030,0,0.00546428,"the terms, here their IDFs, to upper layers) applied before scoring the q-terms. By contrast, in DRMM (Fig. 1) term-gating is applied after q-term scoring, and operates on [e(qi ); idf(qi )]. 3.2 Context-sensitive Term Encodings In their original incarnations, DRMM and PACRR use pre-trained word embeddings that are insensitive to the context of a particular query or document where a term occurs. This contrasts with the plethora of systems that use context-sensitive word encodings (for each particular occurrence of a word) in virtually all NLP tasks (Bahdanau et al., 2014; Plank et al., 2016; Lample et al., 2016). In general, this is achieved via RNNs, e.g., LSTMs (Gers et al., 2000), or CNNs (Bai et al., 2018). In the IR literature, context-sensitivity is typically viewed through two lenses: term proximity (B¨uttcher et al., 2006) and term dependency (Metzler and Croft, 2005). The former assumes that the context around a term match is also relevant, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an encoder produces the co"
D18-1211,W17-2328,0,0.0279019,"that the interaction-based DRMM outperforms previous representation-based methods. On the other hand, interaction-based models are less efficient, since one cannot index a document representation independently of the query. This is less important, though, when relevance ranking methods rerank the top documents returned by a conventional IR engine, which is the scenario we consider here. One set of our experiments ranks biomedical texts. Several methods have been proposed for the BIOASQ challenge (Tsatsaronis et al., 2015), mostly based on traditional IR techniques. The most related work is of Mohan et al. (2017), who use a deep learning architecture. Unlike our work, they focus on user click data as a supervised signal, and they use context-insensitive representations of document-query interactions. The other dataset we experiment with, TREC ROBUST 2004 (Voorhees, 2005), has been used extensively to evaluate traditional and deep learning IR methods. Document relevance ranking is also related to other NLP tasks. Passage scoring for question answering (Surdeanu et al., 2008) ranks passages by their relevance to the question; several deep networks have been proposed, e.g., Tan et al. (2015). Short-text"
D18-1211,P08-1082,0,0.0298003,"proposed for the BIOASQ challenge (Tsatsaronis et al., 2015), mostly based on traditional IR techniques. The most related work is of Mohan et al. (2017), who use a deep learning architecture. Unlike our work, they focus on user click data as a supervised signal, and they use context-insensitive representations of document-query interactions. The other dataset we experiment with, TREC ROBUST 2004 (Voorhees, 2005), has been used extensively to evaluate traditional and deep learning IR methods. Document relevance ranking is also related to other NLP tasks. Passage scoring for question answering (Surdeanu et al., 2008) ranks passages by their relevance to the question; several deep networks have been proposed, e.g., Tan et al. (2015). Short-text matching/ranking is also related and has seen recent deep learning solutions (Lu and Li, 2013; Hu et al., 2014; Severyn and Moschitti, 2015). In document relevance ranking, though, documents are typically much longer than queries, which makes methods from other tasks that consider pairs of short texts not directly applicable. Our starting point is DRMM, to which we add richer representations inspired by PACRR. Hence, we first discuss DRMM and PACRR further. 2.1 DRMM"
D18-1211,N18-1202,0,0.0170916,"t, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an encoder produces the context-sensitive encoding of each qterm or d-term from the pre-trained embeddings. To compute this we use a standard BILSTM encoding scheme and set the context-sentence encoding as the concatenation of the last layer’s hidden states of the forward and backward LSTMs at each position. As is common for CNNs and even recent RNN term encodings (Peters et al., 2018), we use the original term embedding e(ti ) as a residual and combine it with the BILSTM encodings. → − ← − Specifically, if h (ti ) and h (ti ) are the last layer’s hidden states of the left-to-right and right-to-left LSTM s for term ti , respectively, then we set the context-sensitive term encoding as: → − ← − c(ti ) = [ h (ti ) + e(ti ); h (ti ) + e(ti )] (1) Since we are adding the original term embedding to each LSTM hidden state, we require the dimensionality of the hidden layers to be equal to that of the original embedding. Other methods were tried, including passing all representation"
D18-1211,P16-2067,0,0.0169478,"on information about the terms, here their IDFs, to upper layers) applied before scoring the q-terms. By contrast, in DRMM (Fig. 1) term-gating is applied after q-term scoring, and operates on [e(qi ); idf(qi )]. 3.2 Context-sensitive Term Encodings In their original incarnations, DRMM and PACRR use pre-trained word embeddings that are insensitive to the context of a particular query or document where a term occurs. This contrasts with the plethora of systems that use context-sensitive word encodings (for each particular occurrence of a word) in virtually all NLP tasks (Bahdanau et al., 2014; Plank et al., 2016; Lample et al., 2016). In general, this is achieved via RNNs, e.g., LSTMs (Gers et al., 2000), or CNNs (Bai et al., 2018). In the IR literature, context-sensitivity is typically viewed through two lenses: term proximity (B¨uttcher et al., 2006) and term dependency (Metzler and Croft, 2005). The former assumes that the context around a term match is also relevant, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an e"
D19-1618,W12-3102,0,0.045691,"Missing"
D19-1618,W06-0707,0,0.260888,"aries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared Figure 1: S UM -QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories. tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. S UM -QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form.1 2 Related Work Summarization evaluation metrics like Pyramid (Nenkova and Passonneau, 2004) and ROUGE (Lin and Hovy, 2003; Lin, 2004) are recalloriented; they basically measure"
D19-1618,W14-3348,0,0.0121202,"d but poorly with linguistic qualities of summaries. Louis and Nenkova (2013) proposed a regression model for measuring summary quality without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly.2 Quality Estimation is well established in MT (Callison-Burch et al., 2012; Bojar et al., 2016, 2017; Martins et al., 2017; Specia et al., 2018). QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics (Papineni et al., 2002; Denkowski and Lavie, 2014). QE models for MT make use of large postedited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no postedited datasets – like the ones used in MT – are available for summarization, we use instead the ratings assigned by human annotators with respect to a set of linguistic quality criteria. Our proposed models achieve high correlation with human judgments, showing that it is possible to estimate su"
D19-1618,N19-1423,0,0.0368882,"man references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared Figure 1: S UM -QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories. tasks, and compare the performance of our model to a"
D19-1618,W04-1013,0,0.632253,"tic aspects. Predictions of the S UM -QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are give"
D19-1618,N03-1020,0,0.485946,"Missing"
D19-1618,J13-2002,0,0.24798,"ence on Natural Language Processing, pages 6005–6011, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2019). ROUGE is the most commonly used evaluation metric (Nenkova and McKeown, 2012; Allahyari et al., 2017; Gambhir and Gupta, 2017). Inspired by BLEU (Papineni et al., 2002), it relies on common n-grams or subsequences between peer and model summaries. Many ROUGE versions are available, but it remains hard to decide which one to use (Graham, 2015). Being recall-based, ROUGE correlates well with Pyramid but poorly with linguistic qualities of summaries. Louis and Nenkova (2013) proposed a regression model for measuring summary quality without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly.2 Quality Estimation is well established in MT (Callison-Burch et al., 2012; Bojar et al., 2016, 2017; Martins et al., 2017; Specia et al., 2018). QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics (Papineni et al., 2002; Denkowski and Lavie, 2014). QE models for MT make use of large postedited dat"
D19-1618,N04-1019,0,0.875899,"used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly availabl"
D19-1618,P02-1040,0,0.104385,"S UM -QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evalua"
D19-1618,P13-2026,0,0.0194252,"different aspects of linguistic quality. S UM -QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form.1 2 Related Work Summarization evaluation metrics like Pyramid (Nenkova and Passonneau, 2004) and ROUGE (Lin and Hovy, 2003; Lin, 2004) are recalloriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings (Passonneau et al., 2013) and a lightweight crowdsourcing scheme (Shapira et al., 1 Our code is available at https://github.com/ nlpaueb/SumQE 6005 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6005–6011, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2019). ROUGE is the most commonly used evaluation metric (Nenkova and McKeown, 2012; Allahyari et al., 2017; Gambhir and Gupta, 2017). Inspired by BLEU (Papineni et al., 2002), it relies on common n-grams or sub"
D19-1618,N19-1072,0,0.0876233,"Missing"
E09-2005,W07-2322,1,0.625918,"s to the user type (e.g., child vs. adult) and the interaction history (e.g., by avoiding repetitions, or by comparing to previous objects). In project XENIOS (Vogiatzis et al., 2008), NaturalOWL was embedded in a mobile robot acting as a museum guide, and in project INDIGO it is being integrated in a more advanced robotic guide that includes a multimodal dialogue manager, facial animation, and mechanisms to recognize and express emotions (Konstantopoulos et al., 2009). Here, we demonstrate a similar application, where NaturalOWL is embedded in a robotic avatar acting Introduction NaturalOWL (Galanis and Androutsopoulos, 2007; Androutsopoulos and Galanis, 2008) is a natural language generation engine that produces descriptions of entitities (e.g., items for sale, museum exhibits) and classes (e.g., types of exhibits) in English and Greek from OWL DL ontologies; the ontologies must have been annotated with linguistic and user modeling annotations expressed in RDF.1 An accompanying plug-in for the well known Prot´eg´e ontology editor is available, which can be used to create the linguistic and user modeling annotations while editing an ontology, as well as to generate previews of the resulting texts by invoking the"
E09-2005,E09-2010,1,\N,Missing
E09-2010,E09-2005,1,\N,Missing
E14-1009,N09-1003,0,0.0532053,"Missing"
E14-1009,N07-1043,0,0.14944,"d problem, and that research should focus on the first phase. Although we experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailore"
E14-1009,S13-1005,0,0.194914,"imilarity DS (t, t0 ) of two aspect terms t, t0 is the cosine similarity of ~v (t), ~v (t0 ).10 Finally, we tried combinations of the similarity measures: AVG is the average of all five; WN is the average of the first four, which employ WordNet; and WNDS is the average of WN and DS ; all the scores range in [0, 1]. We also tried regression (e.g., SVR), but there was no improvement. 3.3 To get a better view of the performance of WNDS with sense pruning, i.e., the best overall measure of Table 2, we compared it to two state of the art semantic similarity systems. First, we applied the system of Han et al. (2013), one of the best systems of the recent *Sem 2013 semantic text similarity competition, to our Phase A data. The performance (Pearson correlation with gold similarities) of the same system on the widely used WordSim353 word similarity dataset (Agirre et al., 2009) is 0.73, much higher than the same system’s performance on our Phase A data (see Table 3), Phase A experimental results Each similarity measure was evaluated by computing its Pearson correlation with the scores of the gold similarity matrix. Table 2 shows the results. Our sense pruning consistently improves all four WordNet-based mea"
E14-1009,O97-1002,0,0.306295,"Missing"
E14-1009,N10-1122,0,0.212557,"cable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They"
E14-1009,J06-1003,0,0.030102,"ot sense-tagged, we follow the common approach of treating each occurrence of a word as an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of Lin’s measure, one where the ic(s) scores were estimated from the Brown corpus (Marcus et al., 1993), and one where they were estimated from the (restaurant or laptop) reviews of our datasets. The fourth measure is Jiang and Conrath’s (1997), defined below. Again, we experimented with two variants of ic(s), as above. 3.2 Phase A methods We employed five term similarity measures. The first two are WordNet-based (Budanitsky and Hirst, 2006). The next two combine WordNet with statistics from corpora. The fifth one is a corpusbased distributional similarity measure. The first measure is Wu and Palmer’s (1994). It is actually a sense similarity measure (a term may have multiple senses). Given two senses sij , si0 j 0 of terms ti , ti0 , the measure is defined as follows: WP (sij , si0 j 0 ) = 2 · 2 · ic(lcs(sij , si0 j 0 )) , ic(sij ) + ic(si0 j 0 ) depth(lcs(sij , si0 j 0 )) , depth(sij ) + depth(sij ) where lcs(sij , si0 j 0 ) is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and"
E14-1009,N10-1010,0,0.0210369,"lar phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, much as in our Phase B, but not in the context of ABSA , and without trying to learn a similarity matrix first. They also label the hierarchy’s concepts, a task we do not consider. Klapaftis and Manandhar (2010) show how word sense induction can be combined with agglomerative clustering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 2 We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 3 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspec"
E14-1009,D07-1114,0,0.0354151,"), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering,"
E14-1009,P06-1127,0,0.0135673,"n is almost a solved problem, and that research should focus on the first phase. Although we experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et a"
E14-1009,P09-1116,0,0.230115,"rch should focus on the first phase. Although we experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an exis"
E14-1009,J93-2004,0,0.049105,"s). The third measure is Lin’s (1998), defined as: LIN (sij , si0 j 0 ) = where sij , si0 j 0 are senses of terms ti , ti0 , lcs(sij , si0 j 0 ) is the least common subsumer of sij , si0 j 0 in WordNet, and ic(s) = − log P (s) is the information content of sense s (Pedersen et al., 2004), estimated from a corpus. When the corpus is not sense-tagged, we follow the common approach of treating each occurrence of a word as an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of Lin’s measure, one where the ic(s) scores were estimated from the Brown corpus (Marcus et al., 1993), and one where they were estimated from the (restaurant or laptop) reviews of our datasets. The fourth measure is Jiang and Conrath’s (1997), defined below. Again, we experimented with two variants of ic(s), as above. 3.2 Phase A methods We employed five term similarity measures. The first two are WordNet-based (Budanitsky and Hirst, 2006). The next two combine WordNet with statistics from corpora. The fifth one is a corpusbased distributional similarity measure. The first measure is Wu and Palmer’s (1994). It is actually a sense similarity measure (a term may have multiple senses). Given two"
E14-1009,N12-1051,0,0.0190541,"ilar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, much as in our Phase B, but not in the context of ABSA , and without trying to learn a similarity matrix first. They also label the hierarchy’s concepts, a task we do not consider. Klapaftis and Manandhar (2010) show how word sense induction can be combined with agglomerative clustering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 2 We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 3 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most comPhase A 2 Topic models are typically also used to perform aspect extraction, apart from aspect aggregation, but simple heuristics (e.g., most frequent nouns) often outperform them in aspect extraction (Liu,"
E14-1009,J07-2002,0,0.0641782,"Missing"
E14-1009,N04-3012,0,0.0892007,"cts (0.25 and 0.11).6 In other preliminary experiments, we asked human judges to rank alternative aspect hierarchies that had been produced by applying agglomerative clustering with different linkage criteria to 20 aspect terms, but we obtained very poor inter-annotator agreement (Pearson score −0.83 for restaurants and 0 for laptops). The third measure is Lin’s (1998), defined as: LIN (sij , si0 j 0 ) = where sij , si0 j 0 are senses of terms ti , ti0 , lcs(sij , si0 j 0 ) is the least common subsumer of sij , si0 j 0 in WordNet, and ic(s) = − log P (s) is the information content of sense s (Pedersen et al., 2004), estimated from a corpus. When the corpus is not sense-tagged, we follow the common approach of treating each occurrence of a word as an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of Lin’s measure, one where the ic(s) scores were estimated from the Brown corpus (Marcus et al., 1993), and one where they were estimated from the (restaurant or laptop) reviews of our datasets. The fourth measure is Jiang and Conrath’s (1997), defined below. Again, we experimented with two variants of ic(s), as above. 3.2 Phase A methods We employed five term similar"
E14-1009,P08-1036,0,0.10122,"icular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between"
E14-1009,D11-1013,0,0.0271632,"., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task"
E14-1009,C10-1143,0,0.0197004,"experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. mon approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-"
E14-1009,P94-1019,0,\N,Missing
H05-1041,C04-1093,0,0.292045,"stigate this in future work, along with an investigation of how the performance of DEFQAs relates to q, the number of training target terms. Finally, note that the scores of both baselines are very poor, indicating that DEFQAs performs significantly better than picking the first, or a random snippet among those returned by the search engine. 5 Related work Definition questions have recently attracted several QA researchers. Many of the proposed approaches, however, rely on manually crafted patterns or heuristics to identify definitions, and do not employ learning algorithms (Liu et al., 2003; Fujii and Ishikawa, 2004; Hildebrandt et al., 2004; Xu et al., 2004). 329 Ng et al. (2001) use machine learning (C5 with boosting) to classify and rank candidate answers in a general QA system, but they do not treat definition questions in any special way; consequently, their worst results are for “What. . . ?” questions, that presumably include definition questions. Ittycheriah and Roukos (2002) employ a maximum entropy model to rank candidate answers in a generalpurpose QA system. Their maximum entropy model uses a very rich set of attributes, that includes 8,500 n-gram patterns. Unlike our work, their n-grams are"
H05-1041,N04-1007,0,0.173544,"n locating singlesnippet definitions. We believe this task is still interesting and of practical use. For example, a list of single-snippet definitions accompanied by their source URLs is a good starting point for users of search engines wishing to obtain definitions. Singlesnippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions. We also note that the post-2003 TREC task has encountered evaluation problems, because it is difficult to agree on which nuggets should be included in the multi-snippet definitions (Hildebrandt et al., 2004). In contrast, our experimental results of Section 4 indicate strong inter-assessor agreement for single-snippet answers, suggesting that it is easier to agree upon what constitutes an acceptable single-snippet definition. DEFQA relies on an SVM , which is trained to classify 250-character snippets that have the target term at their centre, hereafter called windows, as acceptable definitions or non-definitions.4 To train the SVM , a collection of q training target terms is used; M & A used the target terms of definition questions from TREC-2000 and TREC-2001. The terms are submitted to an IR s"
H05-1041,C04-1199,1,0.639843,"Missing"
H05-1041,W01-0509,0,0.0156076,"mance of DEFQAs relates to q, the number of training target terms. Finally, note that the scores of both baselines are very poor, indicating that DEFQAs performs significantly better than picking the first, or a random snippet among those returned by the search engine. 5 Related work Definition questions have recently attracted several QA researchers. Many of the proposed approaches, however, rely on manually crafted patterns or heuristics to identify definitions, and do not employ learning algorithms (Liu et al., 2003; Fujii and Ishikawa, 2004; Hildebrandt et al., 2004; Xu et al., 2004). 329 Ng et al. (2001) use machine learning (C5 with boosting) to classify and rank candidate answers in a general QA system, but they do not treat definition questions in any special way; consequently, their worst results are for “What. . . ?” questions, that presumably include definition questions. Ittycheriah and Roukos (2002) employ a maximum entropy model to rank candidate answers in a generalpurpose QA system. Their maximum entropy model uses a very rich set of attributes, that includes 8,500 n-gram patterns. Unlike our work, their n-grams are five or more words long, they are coupled to twoword question pref"
H05-1041,N03-2037,0,0.147142,"identifies the term to be defined, called the target term.2 The input to DEFQA is a (possibly multi-word) target term, along with the r most highly ranked documents that an IR system returned for that term. The output is a list of k 250-character snippets from the r documents, at least one of which must contain an acceptable short definition of the target term, much as in the QA track of TREC-2000 and TREC-2001.3 We note that since 2003, TREC requires definition questions to be answered by lists of complementary snippets, jointly providing a range of information nuggets about the target term (Voorhees, 2003). In contrast, here we focus on locating singlesnippet definitions. We believe this task is still interesting and of practical use. For example, a list of single-snippet definitions accompanied by their source URLs is a good starting point for users of search engines wishing to obtain definitions. Singlesnippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions. We also note that the post-2003 TREC task has encountered evaluation problems, because it is difficult to agree on which nuggets should be included in t"
L18-1439,P16-1223,0,0.0532971,"groups with fewer computational resources. We re-implemented (in PyTorch2 ), trained, and tested on BioReadLite two well-known MRC methods, AS Reader (Kadlec et al., 2016) and AOA Reader (Cui et al., 2017). We report their performance, along with the performance of four simpler baselines, as a first step towards a BioRead (and BioReadLite) leaderboard. We open-source the reimplementations to make it easier to replicate our experiments and build upon previous MRC methods.3 Automatically generated cloze-style MRC datasets are of lower quality compared to manually constructed ones. For example, Chen et al. (2016) reported that the CNN and Daily Mail datasets contain both questions that are too easy 1 Consult https://www.ncbi.nlm.nih.gov/pmc/. See http://pytorch.org/. 3 BioRead and the re-implementations will be made available at http://nlp.cs.aueb.gr/software.html. The original implementation of AS Reader (in Theano) is available at https://github.com/rkadlec/asreader/. The original implementation of AOA Reader does not appear to be online. 2771 2 Instances Avg candidates Max candidates Min candidates Avg context len. Max context len. Min context len. Avg question len. Max question len. Min question l"
L18-1439,P17-1171,0,0.0283584,"015) were produced in a similar manner. They comprise news articles and cloze-style questions constructed by removing words from sentences summarising the articles; they contain approx. 380k and 880k instances, respectively. Apart from constituting a testbed for natural language understanding algorithms, MRC is also useful as a component of larger systems. We are interested in a setting where an Information Retrieval engine retrieves document passages that may be relevant to a question, and then MRC is used to identify exact answers (e.g., named entities) in the passages (Sultan et al., 2016; Chen et al., 2017). We focus on the biomedical domain, where this setting is included in the BioASQ challenges (Tsatsaronis et al., 2015). There is currently, however, no sufficiently large publicly available biomedical MRC dataset to train deep learning models. We, therefore, constructed and provide a new biomedical MRC dataset, called BioRead, with approx. 16.4 million clozestyle questions, each paired to a passage and candidate answers. BioRead was constructed in the same manner as CBTest and BookTest, using randomly selected biomedical articles from PubMed Central.1 To the best of our knowledge, it is curre"
L18-1439,D14-1179,0,0.0469729,"Missing"
L18-1439,P17-1055,0,0.283001,"n clozestyle questions, each paired to a passage and candidate answers. BioRead was constructed in the same manner as CBTest and BookTest, using randomly selected biomedical articles from PubMed Central.1 To the best of our knowledge, it is currently one of the largest MRC datasets, and the largest one in the biomedical domain. We also provide a subset of BioRead, called BioReadLite, with 900k instances, for groups with fewer computational resources. We re-implemented (in PyTorch2 ), trained, and tested on BioReadLite two well-known MRC methods, AS Reader (Kadlec et al., 2016) and AOA Reader (Cui et al., 2017). We report their performance, along with the performance of four simpler baselines, as a first step towards a BioRead (and BioReadLite) leaderboard. We open-source the reimplementations to make it easier to replicate our experiments and build upon previous MRC methods.3 Automatically generated cloze-style MRC datasets are of lower quality compared to manually constructed ones. For example, Chen et al. (2016) reported that the CNN and Daily Mail datasets contain both questions that are too easy 1 Consult https://www.ncbi.nlm.nih.gov/pmc/. See http://pytorch.org/. 3 BioRead and the re-implement"
L18-1439,P16-1086,0,0.16126,"led BioRead, with approx. 16.4 million clozestyle questions, each paired to a passage and candidate answers. BioRead was constructed in the same manner as CBTest and BookTest, using randomly selected biomedical articles from PubMed Central.1 To the best of our knowledge, it is currently one of the largest MRC datasets, and the largest one in the biomedical domain. We also provide a subset of BioRead, called BioReadLite, with 900k instances, for groups with fewer computational resources. We re-implemented (in PyTorch2 ), trained, and tested on BioReadLite two well-known MRC methods, AS Reader (Kadlec et al., 2016) and AOA Reader (Cui et al., 2017). We report their performance, along with the performance of four simpler baselines, as a first step towards a BioRead (and BioReadLite) leaderboard. We open-source the reimplementations to make it easier to replicate our experiments and build upon previous MRC methods.3 Automatically generated cloze-style MRC datasets are of lower quality compared to manually constructed ones. For example, Chen et al. (2016) reported that the CNN and Daily Mail datasets contain both questions that are too easy 1 Consult https://www.ncbi.nlm.nih.gov/pmc/. See http://pytorch.or"
L18-1439,D16-1264,0,0.106544,"Missing"
L18-1439,Q16-1009,0,0.0308509,"ts (Hermann et al., 2015) were produced in a similar manner. They comprise news articles and cloze-style questions constructed by removing words from sentences summarising the articles; they contain approx. 380k and 880k instances, respectively. Apart from constituting a testbed for natural language understanding algorithms, MRC is also useful as a component of larger systems. We are interested in a setting where an Information Retrieval engine retrieves document passages that may be relevant to a question, and then MRC is used to identify exact answers (e.g., named entities) in the passages (Sultan et al., 2016; Chen et al., 2017). We focus on the biomedical domain, where this setting is included in the BioASQ challenges (Tsatsaronis et al., 2015). There is currently, however, no sufficiently large publicly available biomedical MRC dataset to train deep learning models. We, therefore, constructed and provide a new biomedical MRC dataset, called BioRead, with approx. 16.4 million clozestyle questions, each paired to a passage and candidate answers. BioRead was constructed in the same manner as CBTest and BookTest, using randomly selected biomedical articles from PubMed Central.1 To the best of our kn"
L18-1439,D16-1013,0,0.0126077,"ere used during their construction. 3. Re-implemented Methods and Baselines We re-implemented and experimented with AS Reader (Kadlec et al., 2016), because it is one of the simplest and most well-known deep learning MRC methods. It has also been shown (Bajgar et al., 2016) that increasing the size of the training set of AS Reader (using BookTest instead of CBTest) leads to much larger performance gains than training more complex MRC methods, like AOA Reader 8 We configured MetaMap for high precision, by setting its minimum score of recognised concepts to 10. (Cui et al., 2017) and EpiReader (Trischler et al., 2016), on the original training set (CBTest). We also reimplemented and experimented with AOA Reader (Cui et al., 2017), an extension of AS Reader that uses a more complex attention mechanism, because it is one of the best performing methods on CBTest (Bajgar et al., 2016). We make both re-implementations publicly available, as already noted. AS Reader (Kadlec et al., 2016) uses a bidirectional recurrent neural network (biRNN) (Schuster and Paliwal, 1997; Seo et al., 2016) with GRU units (Cho et al., 2014) to process the passage (context) and another one to process the question. The states of the f"
N10-1131,P06-1048,0,0.514993,"them were deleted in ci . For every POS tag label, we use two features, one that shows how many POS tags of that label are contained in s and one that shows how many of these POS tags were deleted in ci . To assign a regression score yi to each training vector xi , we experimented with the following functions that measure how similar ci is to the gold compression g, and how grammatical ci is. • Grammatical relations overlap: In this case, yi is the F1 -score of the dependencies of ci against those of the gold compression g. This measure has been shown to correlate well with human judgements (Clarke and Lapata, 2006). As in the ranking function of section 3.2.1, we add a compression rate penalty factor. yi = F1 (d(ci )), d(g)) − α · CR(ci |s) (5) d(·) denotes the set of dependencies. We call SVR-F1 the configuration of our system that uses equation 5 to rank the candidates. • Tokens accuracy and grammaticality: Tokens accuracy, T okAcc(ci |s, g), is the percentage of tokens of s that were correctly retained or removed in ci ; a token was correctly retained or removed, if it was also retained (or removed) in the gold compression g. To calculate T okAcc(ci |s, g), we need the word-toword alignment of s to g"
N10-1131,D07-1008,0,0.0436291,"how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P (y) and a channel model P (x|y), where x 1 An implementation of our method will be freely available from"
N10-1131,C08-1018,0,0.199032,"tences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best amon"
N10-1131,J05-1003,0,0.0325316,"the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P (y) and a channel model P (x|y), where x 1 An implementation of our method will be freely available from http://nlp.cs.aueb.gr/software.html 885 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics is the source sentence and y the compressed one. P (x|y) is calculated as the product of the probabilities of the parse tree tranform"
N10-1131,W09-2805,0,0.0275454,"hat the search was. c3 : Last week a second note informed Mrs Allan the search was on the wrong side of the bridge. c4 : Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge. Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1 , . . . , c4 . guage model trained on a large background corpus. However, language models tend to assign smaller probabilities to longer sentences; therefore they favor short sentences, but not necessarily the most appropriate compressions. To overcome this problem, we follow Cordeiro et al. (2009) and normalize the score of a trigram language model as shown below, where w1 , . . . , wm are the words of candidate ci . Gramm(ci ) = log PLM (ci )1/m = m Y (1/m) · log( P (wj |wj−1 , wj−2 )) (1) j=1 The importance rate ImpRate(ci |s), defined below, estimates how much information of the original sentence s is retained in candidate ci . tf (wi ) is the term frequency of wi in the document that contained ξ (ξ = ci , s), and idf (wi ) is the inverse document frequency of wi in a background corpus. We actually compute idf (wi ) only for nouns and verbs, and set idf (wi ) = 0 for other words. Im"
N10-1131,de-marneffe-etal-2006-generating,0,0.005009,"Missing"
N10-1131,E09-2005,1,0.724552,"uses no hand-crafted rules. In future work, we plan to support more complex tranformations, instead of only removing words and experiment with different sizes of training data. The work reported in this paper was carried out in the context of project INDIGO, where an autonomous robotic guide for museum collections is being developed. The guide engages the museum’s visitors in spoken dialogues, and it describes the exhibits that the visitors select by generating textual descriptions, which are passed on to a speech synthesizer. The texts are generated from logical facts stored in an ontology (Galanis et al., 2009) and from canned texts; the latter are used when the corresponding information is difficult to encode in symbolic form (e.g., to store short stories about the exhibits). The descriptions of the exhibits are tailored depending on the type of the visitor (e.g., child vs. adult), and an important tailoring aspect is the generation of shorter or longer descriptions. The parts of the descriptions that are generated from logical facts can be easily made shorter or longer, by conveying fewer or more facts. The methods of this paper are used to automatically shorten the parts of the descriptions that"
N10-1131,P07-2049,0,0.0195757,"s of the edges that have the same head as ei in Ts (one feature for each possible dependency label). • Two binary features that show if the subtree rooted at the modifier of ei or ei ’s uptree (the rest of the tree, when ei ’s subtree is removed) contain an important word. A word is considered important if it appears in the document s was drawn from significantly more often than in a background corpus. In summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio λ of each word (Lin and Hovy, 2000; Gupta et al., 2007). For each dependency edge ei of a source training sentence s, we create a training vector V with the above features. If ei is retained in the dependency tree of the corresponding compressed sentence g in the corpus, V is assigned the category not del. If ei is not retained, it is assigned the category del l or del u, depending on whether the head (as in the ccomp of “said” in Figure 1) or the modifier (as in the dobj of “attend”) of ei has also been removed. When the modifier of an edge is removed, the entire subtree rooted at the modifier is removed, and similarly for the uptree, when the he"
N10-1131,A00-1043,0,0.530637,"first stage, it generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier. In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive"
N10-1131,C00-1072,0,0.0369048,"not) among the labels of the edges that have the same head as ei in Ts (one feature for each possible dependency label). • Two binary features that show if the subtree rooted at the modifier of ei or ei ’s uptree (the rest of the tree, when ei ’s subtree is removed) contain an important word. A word is considered important if it appears in the document s was drawn from significantly more often than in a background corpus. In summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio λ of each word (Lin and Hovy, 2000; Gupta et al., 2007). For each dependency edge ei of a source training sentence s, we create a training vector V with the above features. If ei is retained in the dependency tree of the corresponding compressed sentence g in the corpus, V is assigned the category not del. If ei is not retained, it is assigned the category del l or del u, depending on whether the head (as in the ccomp of “said” in Figure 1) or the modifier (as in the dobj of “attend”) of ei has also been removed. When the modifier of an edge is removed, the entire subtree rooted at the modifier is removed, and similarly for th"
N10-1131,N03-5008,0,0.00805029,") q (7) where score(q; w) is the dot product hΨ(q), wi. Ψ(q) is a vector-valued feature function, and w is a vector of weights learnt using a Structured Support Vector Machine (Tsochantaridis et al., 2005). Ψ(q) consists of: (i) the log-probability of the resulting candidate, as returned by a tri-gram language model; and (ii) features that describe how the operators of q are applied, for example the number of the terminals in each operator’s α and γ subtrees, the POS tags of the X and Y roots of α and γ etc. 5 Experiments We used Stanford’s parser (de Marneffe et al., 2006) and ME classifier (Manning et al., 2003).5 For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002).6 The language model was trained on approximately 4.5 million sentences of the TIPSTER corpus. To obtain idf (wi ) values, we used approximately 19.5 million verbs and nouns from the TIPSTER corpus. T3 requires the syntax trees of the source-gold pairs in Penn Treebank format, as well as a trigram language model. We obtained T3’s trees using Stanford’s parser, as in our system, unlike Cohn and Lapata (2009) that use Bikel’s (2002) parser. The language models in T3 and our system are trained on"
N10-1131,E06-1038,0,0.474985,"andeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source s"
N10-1131,D09-1041,0,0.350415,"pressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our"
N10-1131,P05-1008,0,0.0127563,"cond stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P (y) and a channel model P (x|y), where x 1 An implementation of our method will be freely available from http://nlp.cs.aueb.gr/software.html 885 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics is the source sentence and y the compressed one. P (x|y) is calculated as the product of the probabilities of"
N10-1131,W04-1015,0,0.00787647,"it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and"
N10-1131,J96-1002,0,\N,Missing
N19-1071,P82-1020,0,0.808165,"Missing"
N19-1071,K16-1002,0,0.104267,"Missing"
N19-1071,N16-1012,0,0.115087,"Missing"
N19-1071,D16-1140,0,0.0298938,"on of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softmax(uct ) (3) Wo , bo , Wv , bv are learned. ct is also used when updating the state hct of the decoder, along with the embedding ect of yt and a countdown argument M − t (scaled by a learnable wd ) indicating the number of the remaining words of the summary (Fevry and Phang, 2018; Kikuchi et al., 2016). −−→ hct+1 = RNNc (hct , ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al.,"
N19-1071,W18-2706,0,0.0200439,"ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al., 2016) and summarization (Fan et al., 2018). Proposed Model 2.1 Compressor 2.2 Differentiable Word Sampling The bottom left part of Fig. 2 illustrates the internals of the compressor C. An embedding layer projects the source sequence x to the word embeddings es = ⟨es1 , . . . , esN ⟩, which are then enTo generate the summary, we need to sample its words yt from the categorical distributions p(yt |y<t , x), which is a non-differentiable process. 674 Soft-Argmax Instead of sampling yt , a simple workaround during training is to pass as input to the next timestep of C’s decoder and to the corresponding timestep of R’s encoder a weighted s"
N19-1071,K18-1040,0,0.62226,"equire parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily ex"
N19-1071,W17-3204,0,0.0356547,", . . . , xN ⟩ of N words, and generates a summary y = ⟨y1 , . . . , yM ⟩ of M words (M<N), y being a latent variable. R and C communicate only through the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation"
N19-1071,N18-2081,0,0.0329404,"Missing"
N19-1071,D15-1044,0,0.821236,"s the input text. ai hsi i=1 The matrix Wa is learned. We obtain a probability distribution for yt over the vocabulary V by combining ct and the current state hct of the decoder. oct = tanh(Wo [ct ; hct ] + bo ) (1) uct (2) = p(yt |y<t , x) = ˆ) ing to minimize a reconstruction loss LR = (x, x (§2.5). A pretrained language model acts as a prior on y, introducing an additional loss LP (x, y) that encourages SEQ3 to produce human-readable summaries. A third loss LT (x, y) rewards summaries y with similar topic-indicating words as x. Experiments (§3) on the Gigaword sentence compression dataset (Rush et al., 2015) and the DUC -2003 and DUC -2004 shared tasks (Over et al., 2007) produce promising results. Our contributions are: (1) a fully differentiable sequence-to-sequence-to-sequence (SEQ3 ) autoencoder that can be trained without parallel data via gradient optimization; (2) an application of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softma"
N19-1071,D15-1166,0,0.063871,"rallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (est , h st+1 )] Reconstructor ? ℎ? ? ?? To generate the summary y, we employ the attentional RNN decoder of Luong et al. (2015), with their global attention and input feeding. Concretely, at each timestep (t ∈ {1, . . . , M}) we compute a probability distribution ai over all the states hs1 , . . . , hsN of the source encoder conditioned on the current state hct of the compressor’s decoder to produce a context vector ct . ?Μ LM prior Loss Compressor ℎ1? ℎ2? … ? ℎ? ℎ0? ℎ1? … ? ℎ?−1 ?1? ?2? … ??? ?0? ?1? … ? ??−1 ?1 ?2 ?? Topic Loss ai = softmax(hsi ⊺ Wa hct ), ct = 3 Figure 2: More detailed illustration of SEQ . The compressor (C) produces a summary from the input text, and the reconstructor (R) tries to reproduce the i"
N19-1071,P17-1099,0,0.260948,"Missing"
N19-1071,P16-1009,0,0.0512056,"gh the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (e"
N19-1071,D18-1267,0,0.0184866,"of the sampled words yt in the forward pass, approximate differentiable embeddings in the backward pass). (5) i where uct is the unnormalized score in Eq. 2 (i.e., the logit) of each word wi and τ ∈ (0, ∞) is the temperature. As τ → 0 most of the probability mass in Eq. 5 goes to the most probable word, hence the operation approaches the arg max. 2.4 Decoder Initialization We initialize the hidden state of each decoder us−→ ← − ing a transformation of the concatenation [hsN ; hs1 ] of the last hidden states (from the two directions) of its bidirectional encoder and a length vector, following Mallinson et al. (2018). The length vector for the decoder of the compressor C consists of the target summary length M, scaled by a learnable parameter wv , and the compression ratio M N. Gumbel-Softmax We still want to be able to perform sampling, though, as it has the benefit of adding stochasticity and facilitating exploration of the parameter space. Hence, we use the GumbelSoftmax (GS) reparametrization trick (Maddison et al., 2017; Jang et al., 2017) as a low variance approximation of sampling from categorical distributions. Sampling a specific word yt from the softmax (Eq. 3) is equivalent to adding (element-w"
N19-1071,D16-1031,0,0.606807,"ssion on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks, e.g., machine translation and semantic parsing via task-specific losses. I"
N19-1071,D18-1451,0,0.715263,"mmary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks"
N19-1071,K16-1028,0,0.103221,"Missing"
N19-1071,D10-1050,0,0.0733195,"Missing"
N19-1071,D14-1162,0,0.0937538,"et. Length Penalty A fourth loss LL (not shown in Fig. 1) helps the (decoder of the) compressor to predict the end-of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards corr"
N19-1071,E17-2025,0,0.0312917,"of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards correct word order. We also tried removing the topic loss, but the model failed to converge and results were extrem"
N19-1071,P17-1101,0,0.0405545,"Missing"
P13-2100,E09-2005,1,0.838462,"o a maximum allowed number Bmax , in effect limiting the maximum length of an aggregated sentence. We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. The section of a fact fi = hSi , Ri , Oi i is the section of its relation Ri . Constraint 9 ensures that facts from different sections will not be placed in the same subset sj , to avoid unnatural aggregations. 4 We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4 For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL. The original system, called PIPELINE, assumes that each relation has been mapped to a topical s"
P13-2100,C12-1056,1,0.83526,"sed to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text gene"
P13-2100,P04-1051,0,0.0381876,"ss by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface real"
P13-2100,P12-1039,0,0.0346431,"to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 sentence plan pik specifies how to express fi = hSi , Ri , Oi i as an alternative single sentence. In our work, a sentence plan is a sequence of slots, along with instructions specify"
P13-2100,N12-1093,0,0.0349982,"to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 sentence plan pik specifies how to express fi = hSi , Ri , Oi i as an alternative single sentence. In our work, a sentence plan is a sequence of slots, along with instructions specify"
P13-2100,P09-1011,0,0.0278862,"o maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 sentence plan pik specifies how to express fi = hSi , Ri , Oi i as an alternative single sentence. In our work, a sentence plan is a sequence of slots, along"
P13-2100,H05-1042,0,0.0530748,"s arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content 2 Related work Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed a"
P13-2100,N06-1046,0,0.0146433,"ulti-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalen"
P13-2100,W05-0618,0,0.241157,"lization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content 2 Related work Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of"
P13-2100,P11-1049,0,0.0312411,"ger sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentenc"
P13-2100,C10-2128,0,0.0137974,"ion, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface r"
P13-2100,D12-1022,0,0.0229738,"ioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first"
P13-2100,P84-1107,0,\N,Missing
P18-2041,I17-1102,0,0.0183458,"ns the words of each sentence to a sentence embedding, and a second one turns the sentence embeddings to a document embedding, which is fed to an LR layer. Yang et al. use selfattention in both RNNs, to assign attention scores to words and sentences. We classify sentences (or clauses), not entire texts, hence our second BIL STM does not produce a document embedding and does not use self-attention. Also, Yang et al. experimented with reviews and community question answering logs, whereas we considered legal texts. Hierarchical RNNs have also been developed for multilingual text classification (Pappas and Popescu-Belis, 2017), language modeling (Lin et al., 2015), and dialogue breakdown detection (Xie and Ling, 2017). 6 tion), which converts each sentence to an embedding, and then processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers the surrounding sentences, because the hierarchical model has a broader view of the discourse. Further performance improvements may be possible by considering deeper self-attention mechanisms (Pavlopoulos et al., 2017), stacking BILSTM s (Irsoy and Cardie, 2014), o"
P18-2041,D17-1117,1,0.846023,"ed for multilingual text classification (Pappas and Popescu-Belis, 2017), language modeling (Lin et al., 2015), and dialogue breakdown detection (Xie and Ling, 2017). 6 tion), which converts each sentence to an embedding, and then processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers the surrounding sentences, because the hierarchical model has a broader view of the discourse. Further performance improvements may be possible by considering deeper self-attention mechanisms (Pavlopoulos et al., 2017), stacking BILSTM s (Irsoy and Cardie, 2014), or pre-training the BILSTMs with auxiliary tasks (Ramachandran et al., 2017). The hierarchical BILSTM with attention of this paper may also be useful in other sentence, clause, or utterance classification tasks, for example in dialogue turn classification (Xie and Ling, 2017), detecting abusive user comments in on-line discussions (Pavlopoulos et al., 2017), and discourse segmentation (Hearst, 1997). We would also like to investigate replacing its BILSTMs with sequence-labeling CNNs (Bai et al., 2018), which may lead to efficiency improvements. Ack"
P18-2041,D17-1039,0,0.0340578,"gue breakdown detection (Xie and Ling, 2017). 6 tion), which converts each sentence to an embedding, and then processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers the surrounding sentences, because the hierarchical model has a broader view of the discourse. Further performance improvements may be possible by considering deeper self-attention mechanisms (Pavlopoulos et al., 2017), stacking BILSTM s (Irsoy and Cardie, 2014), or pre-training the BILSTMs with auxiliary tasks (Ramachandran et al., 2017). The hierarchical BILSTM with attention of this paper may also be useful in other sentence, clause, or utterance classification tasks, for example in dialogue turn classification (Xie and Ling, 2017), detecting abusive user comments in on-line discussions (Pavlopoulos et al., 2017), and discourse segmentation (Hearst, 1997). We would also like to investigate replacing its BILSTMs with sequence-labeling CNNs (Bai et al., 2018), which may lead to efficiency improvements. Acknowledgments We are grateful to the members of AUEB’s Natural Language Processing Group, for several suggestions that help"
P18-2041,J97-1003,0,0.271197,"Missing"
P18-2041,N16-1174,0,0.524609,"ng similar classes, O’ Neill et al. (2017) reported that a bidirectional LSTM ( BILSTM ) classifier (Graves et al., 2013) outperformed several others (including logistic regression, SVM, AdaBoost, Random Forests) in legal sentence classification, possibly because longterm dependencies (e.g., modal verbs or negations interacting with distant dependents) are common and crucial in legal texts, and LSTMs can cope with long-term dependencies better than methods relying on fixed-size context windows. We improve upon the work of O’ Neill et al. (2017) in four ways. First, we show that selfattention (Yang et al., 2016) improves the performance of the BILSTM classifier, by allowing the system to focus on indicative words (Fig 1). Second, we introduce a hierarchical BILSTM, where a first BILSTM processes each sentence word by Introduction Legal text processing (Ashley, 2017) is a growing research area, comprising tasks such as legal question answering (Kim and Goebel, 2017), contract element extraction (Chalkidis et al., 2017), and legal text generation (Alschnerd and Skougarevskiy, 2017). We consider obligation and prohibition extraction from contracts, i.e., detecting sentences (or clauses) that specify wha"
P18-2041,D15-1106,0,0.0328914,"g, and a second one turns the sentence embeddings to a document embedding, which is fed to an LR layer. Yang et al. use selfattention in both RNNs, to assign attention scores to words and sentences. We classify sentences (or clauses), not entire texts, hence our second BIL STM does not produce a document embedding and does not use self-attention. Also, Yang et al. experimented with reviews and community question answering logs, whereas we considered legal texts. Hierarchical RNNs have also been developed for multilingual text classification (Pappas and Popescu-Belis, 2017), language modeling (Lin et al., 2015), and dialogue breakdown detection (Xie and Ling, 2017). 6 tion), which converts each sentence to an embedding, and then processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers the surrounding sentences, because the hierarchical model has a broader view of the discourse. Further performance improvements may be possible by considering deeper self-attention mechanisms (Pavlopoulos et al., 2017), stacking BILSTM s (Irsoy and Cardie, 2014), or pre-training the BILSTMs with auxili"
P18-2041,P16-1101,0,0.0247318,"egulations. Asooja et al. (2015) employed SVMs with ngram and manually crafted features to classify paragraphs of money laundering regulations into five classes (e.g., enforcement, monitoring, reporting), experimenting with 212 paragraphs. In previous work (Chalkidis et al., 2017; Chalkidis and Androutsopoulos, 2017) we focused on extracting contract elements (e.g., contractor names, legislation references, start and end dates, amounts), a task which is similar to named entity recognition. The best results were obtained by stacked BILSTMs (Irsoy and Cardie, 2014) or stacked BILSTM-CRF models (Ma and Hovy, 2016); hierarchical BILSTMs were not considered. By contrast, in this paper we considered obligation and prohibition extraction, treating it as a sentence (or clause) classification task, and showing the benefits of employing a hierarchical BILSTM model that considers both the sequence of words in each sentence and the sequence of sentences. Yang et al. (2016) proposed a hierarchical RNN with self-attention to classify texts. A first bidirectional RNN turns the words of each sentence to a sentence embedding, and a second one turns the sentence embeddings to a document embedding, which is fed to an"
P19-1424,E17-2041,0,0.148215,"iased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights"
P19-1424,P18-2041,1,0.816977,"tance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent a"
P19-1424,N19-1423,0,0.455709,"d by Aletras et al. (2016); (2) multi-label classification (type of violation, if any); (3) case importance detection. In all tasks, neural models outperform an SVM with bag-of-words (Aletras et al., 2016; Medvedeva et al., 2018), the only method tested in English legal judgment prediction so far. As a third contribution, we use an approach based on data anonymization to study, for the first time, whether the legal predictive models are biased towards demographic information or factual information relevant to human rights. Finally, as a side-product, we propose a hierarchical version of BERT (Devlin et al., 2019), which bypasses BERT ’s length limitation and leads to the best results. 2 ECHR Dataset ECHR hears allegations that a state has breached human rights provisions of the European Conven1 The dataset is submitted at https://archive. org/details/ECHR-ACL2019. 4317 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317–4323 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion of Human Rights.2 Our dataset contains approx. 11.5k cases from ECHR’s public database.3 For each case, the dataset provides a list of f"
P19-1424,D17-1289,0,0.282592,"., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly availa"
P19-1424,N18-1100,0,0.0384572,"o the output layer using a sigmoid for binary violation, softmax for multi-label violation, or no activation for case importance regression. HAN: The Hierarchical Attention Network (Yang et al., 2016) is a state-of-the-art model for text classification. We use a slightly modified version where a BIGRU with self-attention reads the words of each fact, as in BIGRU - ATT, producing fact embeddings. A second-level BIGRU with selfattention reads the fact embeddings, producing a single case embedding that goes through a similar output layer as in BIGRU - ATT. LWAN: The Label-Wise Attention Network (Mullenbach et al., 2018) has been shown to be robust in multi-label classification. Instead of a single attention mechanism, LWAN employs L attentions, one for each possible label. This produces P L case embeddings (h(l) = i al,i hi ) per case, each one specialized to predict the corresponding label. Each of the case embeddings goes through a separate linear layer (L linear layers in total), each with a sigmoid, to decide if the corresponding label should be assigned. Since this is a multi-label model, we use it only in multi-label violation. BERT and HIER-BERT: BERT (Devlin et al., 2019) is a language model based on"
P19-1424,D08-1046,0,0.630188,"g strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlo"
P19-1424,D14-1162,0,0.0852768,"em to BERT’s maximum length, which affects its performance. This also highlights an important limitation of BERT in processing long documents, a common characteristic in legal text processing. To surpass BERT’s maximum length limitation, we also propose a hierarchical version of BERT ( HIER - BERT ). Firstly BERT- BASE reads the words of each fact, producing fact embeddings. Then a self-attention mechanism reads fact embeddings, producing a single case embedding that goes through a similar output layer as in HAN. 5 Experiments 5.1 Experimental Setup Hyper-parameters: We use pre-trained GLOVE (Pennington et al., 2014) embeddings (d = 200) for all experiments. Hyper-parameters are tuned by random sampling 50 combinations and selecting the values with the best development loss in each task.6 Given the best hyper-parameters, we perform five runs for each model reporting mean scores and standard deviations. We use categorical cross-entropy loss for the classification tasks and mean absolute error for the regression task, Glorot initialization (Glorot and Bengio, 2010), Adam (Kingma and Ba, 2015) with default learning rate 0.001, and early stopping on the development loss. Baselines: A majority-class (MAJORITY)"
P19-1424,C18-1041,0,0.224557,"ce, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly available English legal judgment prediction"
P19-1424,sulea-etal-2017-predicting,0,0.0773369,"Missing"
P19-1424,N19-1357,0,0.0316993,"ction and annotation of these attributes are manually crafted and dependent to the court. Zhong et al. (2018) decompose the problem of charge prediction into different subtasks that are tailored to the Chinese criminal court using multitask learning. 7 Limitations and Future Work The neural models we considered outperform previous feature-based models, but provide no justification for their predictions. Attention scores (Fig. 1) provide some indications of which parts of the texts affect the predictions most, but are far from being justifications that legal practitioners could trust; see also Jain and Wallace (2019). Providing valid justifications is an important priority for future work and an emerging topic in the NLP community.8 In this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions. We also plan to apply neural methods to data from other courts, e.g., the European Court of Justice, the US Supreme Court, and multiple languages, to gain a broader perspective of their potentia"
P19-1424,P12-1078,0,0.202689,"on; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the lik"
P19-1424,N16-1174,0,0.0436725,"Missing"
P19-1424,N18-1168,0,0.0677892,") binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges"
P19-1424,D18-1390,0,0.229121,"ntracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly available English legal ju"
P19-1424,W19-2209,1,\N,Missing
P19-1636,P18-2041,1,0.830861,"ll the relevant labels from a large set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), assigning diagnostic and procedure labels to medical records (Mullenbach et al., 2018; Rios and Kavuluru, 2018). We focus on legal text processing, an emerging NLP field with many applications (e.g., legal judgment (Nallapati and Manning, 2008; Aletras et al., 2016), contract element extraction (Chalkidis et al., 2017), obligation extraction (Chalkidis et al., 2018)), but limited publicly available resources. Our first contribution is a new publicly available legal LMTC dataset, dubbed EURLEX 57 K, containing 57k English EU legislative documents from the EUR - LEX portal, tagged with ∼4.3k labels (concepts) from the European Vocabulary (EUROVOC).1 EUROVOC contains approx. 7k labels, but most of them are rarely used, hence they are under-represented (or absent) in EURLEX 57 K, making the dataset also appropriate for few- and zero-shot learning. EURLEX 57 K can be viewed as an improved version of the dataset released by Mencia and F¨urnkranzand (2007), whi"
P19-1636,D08-1046,0,\N,Missing
P19-1636,D14-1162,0,\N,Missing
P19-1636,N16-1174,0,\N,Missing
P19-1636,N18-1100,0,\N,Missing
P19-1636,P18-1128,0,\N,Missing
P19-1636,D18-1352,0,\N,Missing
P19-1636,N19-1423,0,\N,Missing
petasis-etal-2002-ellogon,W00-1501,0,\N,Missing
S14-2004,E12-2021,0,0.136575,"Missing"
S14-2004,N10-1122,0,0.0570403,"still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, “The restaurant was expensive, but the menu was great.” would be assigned the aspect PRICE"
S14-2004,P08-1036,0,0.0512405,"Informatics Athens University of Economics and Business ion@aueb.gr Abstract Suresh Manandhar Dept. of Computer Science, University of York suresh@cs.york.ac.uk laptop”), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., “Its performance is ideal, I wish I could say the same about the price”) or objective information (e.g., “This one still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt dif"
S14-2004,P02-1053,0,0.0623976,"k provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams. 1 John Pavlopoulos Dept. of Informatics, Athens University of Economics and Business annis@aueb.gr Introduction With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002; Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or ‘feature-based’) sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., “This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland,"
S14-2004,C10-2088,0,0.00353612,"nt aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, “The restaurant was expensive, but the menu was great.” would be assigned the aspect PRICE with negative polarity and FOO"
S14-2004,W02-1011,0,0.0452772,"ach aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams. 1 John Pavlopoulos Dept. of Informatics, Athens University of Economics and Business annis@aueb.gr Introduction With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002; Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or ‘feature-based’) sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., “This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, D"
S14-2004,W14-1306,1,0.655294,"ils: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August 23-24, 2014. ity (positive, negative, conflict, or neutral) of each aspect category discussed in each sentence. Subtasks SB1 and SB2 are useful in cases where no predefined inventory of aspect categories is available. In these cases, frequently discussed aspect terms of the entity can be identified together with their overall sentiment polarities. We hope to include an additional aspect term aggregation subtask in future (Pavlopoulos and Androutsopoulos, 2014b) to cluster near-synonymous (e.g., ‘money’, ‘price’, ‘cost’) or related aspect terms (e.g., ‘design’, ‘color’, ‘feeling’) together with their averaged sentiment scores as shown in Fig. 1. vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al. The SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice. 2 Task Description For the first two subtasks (SB1, SB2), datasets on both domains (r"
S14-2004,E14-1009,1,0.616343,"ils: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August 23-24, 2014. ity (positive, negative, conflict, or neutral) of each aspect category discussed in each sentence. Subtasks SB1 and SB2 are useful in cases where no predefined inventory of aspect categories is available. In these cases, frequently discussed aspect terms of the entity can be identified together with their overall sentiment polarities. We hope to include an additional aspect term aggregation subtask in future (Pavlopoulos and Androutsopoulos, 2014b) to cluster near-synonymous (e.g., ‘money’, ‘price’, ‘cost’) or related aspect terms (e.g., ‘design’, ‘color’, ‘feeling’) together with their averaged sentiment scores as shown in Fig. 1. vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al. The SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice. 2 Task Description For the first two subtasks (SB1, SB2), datasets on both domains (r"
S14-2004,piperidis-2012-meta,0,0.00755085,"al Train Test 90 31 10 1 20 3 23 8 357 51 500 94 Total Train Test 1232 418 321 83 597 172 431 118 1132 234 3713 1025 Table 3: Aspect categories distribution per sentiment class. general views about a restaurant, without explicitly referring to its atmosphere or environment. 3.3 the F1 measure, defined as usually: F1 = Format and Availability of the Datasets The datasets of the ABSA task were provided in an XML format (see Fig. 3). They are available with a non commercial, no redistribution license through META-SHARE, a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012).5 4 2·P ·R P +R (1) where precision (P ) and recall (R) are defined as: P = |S ∩ G| |S ∩ G| ,R = |S| |G| (2) Here S is the set of aspect term or aspect category annotations (in SB1 and SB3, respectively) that a system returned for all the test sentences (of a domain), and G is the set of the gold (correct) aspect term or aspect category annotations. To evaluate aspect term polarity (SB2) and aspect category polarity (SB4) detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted aspect term or aspect category polarity labels, respectively, d"
S14-2004,H05-1043,0,0.147955,"nd Business ion@aueb.gr Abstract Suresh Manandhar Dept. of Computer Science, University of York suresh@cs.york.ac.uk laptop”), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., “Its performance is ideal, I wish I could say the same about the price”) or objective information (e.g., “This one still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different task"
S14-2004,H05-2017,0,\N,Missing
S15-2082,N10-1122,0,0.210315,"al-2015 Task 12: Aspect Based Sentiment Analysis Maria Pontiki*, Dimitrios Galanis*, Haris Papageorgiou*, Suresh Manandhar±, Ion Androutsopoulos◊* *Institute for Language and Speech Processing, Athena R.C., Athens, Greece ± Dept. of Computer Science, University of York, UK ◊ Dept. of Informatics, Athens University of Economics and Business, Greece {mpontiki, galanisd, xaris} @ilsp.gr suresh@cs.york.ac.uk ion@aueb.gr and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Abstract SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams. 1 Introduction and Related Wor"
S15-2082,piperidis-2012-meta,0,0.00812028,"Missing"
S15-2082,S14-2004,1,0.440354,"ers has also been addressed in the context of the Multilingual Opinion Analysis Task (Seki et al., 2007; Seki et al., 2008; Seki et al., 2010) and the Sentiment Slot Filling2 Task of the Knowledge Base Population Track (Mitchell, 2013). However, these tasks deal with the identification of opinion targets in general, not in the context of ABSA. SemEval-2014 Task 4 (SE-ABSA14) provided datasets annotated with aspect terms (e.g., “hard disk”, “pizza”) and their polarity for laptop and restaurant reviews, as well as coarser aspect categories (e.g., PRICE) and their polarity only for restaurants3 (Pontiki et al., 2014). The task attracted 165 submissions from 32 teams that experimented with a variety of features (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has bee"
S15-2082,W14-2605,0,0.0327647,"s (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has been adopted for the creation of aspect-level sentiment datasets in other languages, like Czech (Steinberger et al., 2014). SemEval-2015 Task 12 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks (aspect category extraction, aspect term extraction, polarity classification) into a principled unified framework (described in Section 2). In addition, SE-ABSA15 included an aspect level polarity classification subtask for the hotels domain in which no training data were provided (out-of-domain ABSA). The annotation schema and the provided datasets are described in Section 3. The evaluation measures and the baseline methods are described in Section 4, while the evaluation scores and the 2 http://www.nist.gov"
S15-2082,E12-2021,0,0.0529217,"Missing"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
S16-1012,N16-1162,0,0.0473811,"Missing"
S16-1012,S14-2015,1,0.924637,"lable. 1 Data – TWtrain16 : train data for SemEval-2016 Task 4, – TWdev16 : development data for SemEval-2016 Task 4, – TWdevtest16 : dev-test data for SemEval-2016 Task 4, Introduction This paper describes the system with which we participated in SemEval-2016 Task 4 (Sentiment Analysis in Twitter) and specifically the Message Polarity Classification subtask (Nakov et al., 2016). In this subtask, each tweet is classified as expressing a positive, negative, or no opinion (neutral). Our system is a weighted ensemble of two systems. The first one is based on a previous sentiment analysis system (Karampatsis et al., 2014) and uses manually crafted features. The second system of our ensemble uses features based on word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Our ensemble was ranked 5th among 34 teams. Section 2 discusses the datasets we used to train and tune our ensemble. Sections 3 and 4 describe our ensemble and its performance respectively. Finally, Section 5 concludes and discusses future work. – TWtrain13 : train data for SemEval-2013 Task 2, – TWdev13 : development data for SemEval-2013 Task 2. The organisers also provided 6,908 tweets from old SemEval data, to allow system evaluation"
S16-1012,S16-1001,0,0.0251756,"a previous sentiment analysis system and uses manually crafted features. The second system of our ensemble uses features based on word embeddings. Our ensemble was ranked 5th among 34 teams. The source code of our system is publicly available. 1 Data – TWtrain16 : train data for SemEval-2016 Task 4, – TWdev16 : development data for SemEval-2016 Task 4, – TWdevtest16 : dev-test data for SemEval-2016 Task 4, Introduction This paper describes the system with which we participated in SemEval-2016 Task 4 (Sentiment Analysis in Twitter) and specifically the Message Polarity Classification subtask (Nakov et al., 2016). In this subtask, each tweet is classified as expressing a positive, negative, or no opinion (neutral). Our system is a weighted ensemble of two systems. The first one is based on a previous sentiment analysis system (Karampatsis et al., 2014) and uses manually crafted features. The second system of our ensemble uses features based on word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Our ensemble was ranked 5th among 34 teams. Section 2 discusses the datasets we used to train and tune our ensemble. Sections 3 and 4 describe our ensemble and its performance respectively. Finally"
S16-1012,N13-1039,0,0.100559,"Missing"
S16-1012,D14-1162,0,0.0853034,"Missing"
S16-1050,S14-2015,1,0.940398,"this to be a classification task. 2 Sentences including opinions that can not be described by the SE - ABSA 16 annotation schema, are “Out of Scope”. 3 https://github.com/nlpaueb/aueb-absa 313 Constrained ACD system One Support Vector Machine (SVM) classifier was trained for each predefined E and A, based on lexicons created from the training data.4 5 The lexicons assigned scores to unigrams (stemmed and unstemmed) and bigrams (stemmed, unstemmed or using only POS tag bigrams). For each unigram or bigram, we computed its Precision, Recall and F 1 over the training data, following the work of Karampatsis et al. (2014). We used the average, median, maximum, and minimum values for each score (Precision, Recall, F 1) and for each lexicon (stemmed unigrams, unstemmed unigrams, stemmed, unstemmed, POS tag bigrams), thus, yielding 12 features per lexicon and 60 features overall. One Support Vector Machine (SVM) classifier was trained for each E and A label, yielding 11 classifiers for Restaurants and 31 for Laptops. Given a new text, the confidence scores of all the classifiers were examined and the Es and As of the classifiers whose confidence exceeded a threshold were used to form the E#A aspects.6 All the pos"
S16-1050,S14-2004,1,0.780862,"ification task. 2 Sentences including opinions that can not be described by the SE - ABSA 16 annotation schema, are “Out of Scope”. 3 https://github.com/nlpaueb/aueb-absa 313 Constrained ACD system One Support Vector Machine (SVM) classifier was trained for each predefined E and A, based on lexicons created from the training data.4 5 The lexicons assigned scores to unigrams (stemmed and unstemmed) and bigrams (stemmed, unstemmed or using only POS tag bigrams). For each unigram or bigram, we computed its Precision, Recall and F 1 over the training data, following the work of Karampatsis et al. (2014). We used the average, median, maximum, and minimum values for each score (Precision, Recall, F 1) and for each lexicon (stemmed unigrams, unstemmed unigrams, stemmed, unstemmed, POS tag bigrams), thus, yielding 12 features per lexicon and 60 features overall. One Support Vector Machine (SVM) classifier was trained for each E and A label, yielding 11 classifiers for Restaurants and 31 for Laptops. Given a new text, the confidence scores of all the classifiers were examined and the Es and As of the classifiers whose confidence exceeded a threshold were used to form the E#A aspects.6 All the pos"
S16-1050,S15-2082,1,0.832146,"on word embeddings. Our systems were ranked in the top 6 positions in all the tasks we participated. The source code of our systems is publicly available. 1 Introduction The amount of user-generated content on the web has grown rapidly in recent years, leading to increased interest in sentiment analysis and, more generally, opinion mining. The task of Aspect Based Sentiment Analysis of SemEval-2014 (SE - ABSA 14) and SemEval-2015 (SE - ABSA 15) was concerned with identifying the aspects of given target entities and extracting the sentiment expressed towards each aspect (Pontiki et al., 2014; Pontiki et al., 2015). The task of Aspect Based Sentiment Analysis of SemEval-2016 (SE - ABSA 16) is a continuation of those tasks (Pontiki et al., 2016). We participated in Aspect Category Detection (ACD, Subtask1/Slot1), Opinion Target Expression (OTE, Subtask1/Slot2), and Polarity Detection (PD, Subtask1/Slot3). In ACD, we participated in the English language, for both Laptops and Restaurants, submitting both constrained and unconstrained systems. Our constrained system used only the provided training data for the corresponding domain. Features were extracted from lexicons created from the training data. One Su"
S16-1050,S14-2038,0,0.0177408,"the training data. One Support Vector Machine (SVM) classifier (Vapnik and Vapnik, 1998) was trained for each Entity and Attribute category (called E and A respectively). Our unconstrained system used word embeddings as additional resources (Mikolov et al., 2013). For each category (E or A), we used an ensemble of two systems: our constrained system and one new system, which was based on word embeddings. In OTE, we participated with both a constrained and an unconstrained system.1 The task is to identify aspects of given target entities. We addressed the problem as a sequential labeling task (Toh and Wang, 2014), assigning one label to each word in a sentence, indicating whether the word was an aspect term or not. In this task, we used Conditional Random Fields (Lafferty et al., 2001). Similarly to ACD, our unconstrained system differed in that it also used word embeddings as features. In PD, we participated only with an unconstrained system, in both domains, in the English language. We used an ensemble of two classifiers. The first classifier used hand crafted features and sentiment lexicons with scores. The second classifier was based on IDF-weighted centroids of the word embeddings of each sentenc"
S16-1050,S14-2041,0,0.0363008,"Missing"
S16-1050,S16-1002,1,\N,Missing
S19-2102,W18-4401,0,0.0306196,"l., 2017). Apart from the growing volume of popular press concerning 2 3 See, for example, https://goo.gl/VQNDNX. See also https://goo.gl/v7kA1K. https://www.perspectiveapi.com/ 571 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 571–576 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 3 toxicity online, the increased interest in research into offensive language is partly due to the recent Workshops on Abusive Language Online,4 as well as other fora, such as GermEval for German texts,5 or TA-COS6 and TRAC (Kumar et al., 2018),7 . The literature contains many terms for different kinds of offensive language: toxic, abusive, hateful, attacking, etc. Largely, these are defined by different survey methods. In (Waseem et al., 2017), abusive language is divided into explicit vs. implicit, and directed vs. generalized. However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2). Data The S EM E VAL-2019 O FFENS E VAL dataset that is available to participants contains 13240 tweets; the counts of the labels are shown in Table 1. The O FFENS E VAL task consists of three subtask"
S19-2102,malmasi-zampieri-2017-detecting,0,0.0600409,"(UNT). Subtask C aims to identify whether the target of an offensive post is an individual (IND), a group (GRP), or unknown (OTH). Table 1 also shows the size of the vocabulary per class (label), which, unsurprisingly, is proportional to the class size. It is worth noting that offensive tweets targeting a group are the lengthier texts, with 28 tokens on average (see Table 1, column C, GRP column). Although some previous research has considered several types of abuse and their relations (Malmasi and Zampieri, 2018), detecting varieties of hate has attracted more attention (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). The first publicly available dataset for hate speech detection was that of Waseem and Hovy (2016). It contained 1607 English tweets annotated for sexism and racism. A larger dataset was published by Davidson et al. (2017), containing approx. 25K tweets collected by using a hate lexicon. Despite the popularity of hate speech detection in literature, no larger publicly available hate speech datasets seem to exist. For recent overviews of hate speech detection, consult Schmidt and Wiegand (2017) and Fortuna and Nunes (2018)"
S19-2102,W17-3008,0,0.0242906,"ve language), such as the offensive tweets of O FFENS E VAL. For offensive language categorization in Subtask B, we employed other experimental models, also available via the Perspective API, which detect various abuse types including those of Table 2. Research into the various kinds of offensive language detection is mainly focused on English, but some work in other languages also exists. Work on a large dataset of Greek moderated news portal comments is presented by Pavlopoulos et al. (2017a). A dataset of obscene and offensive user comments and words in Arabic social media was presented by Mubarak et al. (2017). Previous work includes a system to detect and rephrase profanity in Chinese (Su et al., 2017), and an annotation schema for unacceptable social media content in Slovene (Fiˇser et al., 2017). 4.2 BERT B ERT (Devlin et al., 2018) is a deep bidirectional network built using Transformers (Vaswani et al., 4 https://goo.gl/9HmSzc https://goo.gl/uZEerk 6 http://ta-cos.org/ 7 https://goo.gl/DTZquU 8 5 https://conversationai.github.io/ https://goo.gl/yN196H 10 https://goo.gl/rHYMqt 9 572 Subtask Label Number of Tweets Class specific vocabulary size Average number of tokens / Tweet A NOT 8840 29.2K 2"
S19-2102,W17-3006,0,0.0269611,"17b), which could be targeted (at an individual or group) or not (Waseem et al., 2017). These computational approaches are often used by moderators who face an increasing volume of abusive content and would like assistance in managing it efficiently.1 Although offensive language detection is not a new task (Dinakar et al., 2011; Dadvar et al., 2013; Kwok and Wang, 2013; Burnap and Williams, 2015; Tulkens et al., 2016), the creation of large 1 2 Related Work Various forms of offensive language detection have recently attracted a lot of attention (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Apart from the growing volume of popular press concerning 2 3 See, for example, https://goo.gl/VQNDNX. See also https://goo.gl/v7kA1K. https://www.perspectiveapi.com/ 571 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 571–576 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 3 toxicity online, the increased interest in research into offensive language is partly due to the recent Workshops on Abusive Language Online,4 as well as other fora, such as GermEval for German texts,5 or TA-C"
S19-2102,W17-3004,1,0.903955,"egorizing the offensive type. Both baselines were ranked surprisingly high in the S EM E VAL-2019 O FFENS EVAL competition, Perspective in detecting an offensive post (12th) and B ERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (Perspective) and the categorization (B ERT) of offensive language with little or no additional training data. 1 Introduction Offensive language detection refers to computational approaches for detecting abusive language, such as threats, insults, calumniation, discrimination, swearing (Pavlopoulos et al., 2017b), which could be targeted (at an individual or group) or not (Waseem et al., 2017). These computational approaches are often used by moderators who face an increasing volume of abusive content and would like assistance in managing it efficiently.1 Although offensive language detection is not a new task (Dinakar et al., 2011; Dadvar et al., 2013; Kwok and Wang, 2013; Burnap and Williams, 2015; Tulkens et al., 2016), the creation of large 1 2 Related Work Various forms of offensive language detection have recently attracted a lot of attention (Nobata et al., 2016; Pavlopoulos et al., 2017b; Pa"
S19-2102,D17-1117,1,0.948116,"egorizing the offensive type. Both baselines were ranked surprisingly high in the S EM E VAL-2019 O FFENS EVAL competition, Perspective in detecting an offensive post (12th) and B ERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (Perspective) and the categorization (B ERT) of offensive language with little or no additional training data. 1 Introduction Offensive language detection refers to computational approaches for detecting abusive language, such as threats, insults, calumniation, discrimination, swearing (Pavlopoulos et al., 2017b), which could be targeted (at an individual or group) or not (Waseem et al., 2017). These computational approaches are often used by moderators who face an increasing volume of abusive content and would like assistance in managing it efficiently.1 Although offensive language detection is not a new task (Dinakar et al., 2011; Dadvar et al., 2013; Kwok and Wang, 2013; Burnap and Williams, 2015; Tulkens et al., 2016), the creation of large 1 2 Related Work Various forms of offensive language detection have recently attracted a lot of attention (Nobata et al., 2016; Pavlopoulos et al., 2017b; Pa"
S19-2102,W17-1101,0,0.0380086,"ention (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). The first publicly available dataset for hate speech detection was that of Waseem and Hovy (2016). It contained 1607 English tweets annotated for sexism and racism. A larger dataset was published by Davidson et al. (2017), containing approx. 25K tweets collected by using a hate lexicon. Despite the popularity of hate speech detection in literature, no larger publicly available hate speech datasets seem to exist. For recent overviews of hate speech detection, consult Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). 4 Baselines We now describe the two baselines (Perspective, B ERT) that we implemented and evaluated. 4.1 Perspective We employed the Perspective API, which was created by Jigsaw and Google’s Counter Abuse Technology team in Conversation-AI,8 to facilitate better conversations online and protect voices in conversations (Hosseini et al., 2017). Although opensource code is available,9 we chose to use pretrained models, accessible through the API. For offensive language detection in Subtask A, we used the Toxicity model, which is a CNN based on G LOV E word embeddin"
S19-2102,W17-3003,0,0.0134558,"n Subtask B, we employed other experimental models, also available via the Perspective API, which detect various abuse types including those of Table 2. Research into the various kinds of offensive language detection is mainly focused on English, but some work in other languages also exists. Work on a large dataset of Greek moderated news portal comments is presented by Pavlopoulos et al. (2017a). A dataset of obscene and offensive user comments and words in Arabic social media was presented by Mubarak et al. (2017). Previous work includes a system to detect and rephrase profanity in Chinese (Su et al., 2017), and an annotation schema for unacceptable social media content in Slovene (Fiˇser et al., 2017). 4.2 BERT B ERT (Devlin et al., 2018) is a deep bidirectional network built using Transformers (Vaswani et al., 4 https://goo.gl/9HmSzc https://goo.gl/uZEerk 6 http://ta-cos.org/ 7 https://goo.gl/DTZquU 8 5 https://conversationai.github.io/ https://goo.gl/yN196H 10 https://goo.gl/rHYMqt 9 572 Subtask Label Number of Tweets Class specific vocabulary size Average number of tokens / Tweet A NOT 8840 29.2K 22 B OFF 4400 18.6K 24 UNT 524 3.5K 19 TIN 3876 17.3K 24 IND 2407 11.5K 22 C GRP 1074 7.8K 28 OT"
S19-2102,W17-3007,0,0.0742365,"Missing"
S19-2102,W17-3013,0,0.0489786,"Missing"
S19-2102,W17-3012,0,0.122236,"VAL-2019 O FFENS EVAL competition, Perspective in detecting an offensive post (12th) and B ERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (Perspective) and the categorization (B ERT) of offensive language with little or no additional training data. 1 Introduction Offensive language detection refers to computational approaches for detecting abusive language, such as threats, insults, calumniation, discrimination, swearing (Pavlopoulos et al., 2017b), which could be targeted (at an individual or group) or not (Waseem et al., 2017). These computational approaches are often used by moderators who face an increasing volume of abusive content and would like assistance in managing it efficiently.1 Although offensive language detection is not a new task (Dinakar et al., 2011; Dadvar et al., 2013; Kwok and Wang, 2013; Burnap and Williams, 2015; Tulkens et al., 2016), the creation of large 1 2 Related Work Various forms of offensive language detection have recently attracted a lot of attention (Nobata et al., 2016; Pavlopoulos et al., 2017b; Park and Fung, 2017; Wulczyn et al., 2017). Apart from the growing volume of popular p"
S19-2102,N16-2013,0,0.0473106,"y per class (label), which, unsurprisingly, is proportional to the class size. It is worth noting that offensive tweets targeting a group are the lengthier texts, with 28 tokens on average (see Table 1, column C, GRP column). Although some previous research has considered several types of abuse and their relations (Malmasi and Zampieri, 2018), detecting varieties of hate has attracted more attention (Djuric et al., 2015; Malmasi and Zampieri, 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). The first publicly available dataset for hate speech detection was that of Waseem and Hovy (2016). It contained 1607 English tweets annotated for sexism and racism. A larger dataset was published by Davidson et al. (2017), containing approx. 25K tweets collected by using a hate lexicon. Despite the popularity of hate speech detection in literature, no larger publicly available hate speech datasets seem to exist. For recent overviews of hate speech detection, consult Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). 4 Baselines We now describe the two baselines (Perspective, B ERT) that we implemented and evaluated. 4.1 Perspective We employed the Perspective API, which was created"
S19-2102,N19-1144,0,0.0363973,"ns many terms for different kinds of offensive language: toxic, abusive, hateful, attacking, etc. Largely, these are defined by different survey methods. In (Waseem et al., 2017), abusive language is divided into explicit vs. implicit, and directed vs. generalized. However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2). Data The S EM E VAL-2019 O FFENS E VAL dataset that is available to participants contains 13240 tweets; the counts of the labels are shown in Table 1. The O FFENS E VAL task consists of three subtasks, described in detail by Zampieri et al. (2019b). Subtask A aims at the detection of offensive language (OFF or NOT in Table 3). Subtask B aims at categorizing offensive language as targeting a specific entity (TIN) or not (UNT). Subtask C aims to identify whether the target of an offensive post is an individual (IND), a group (GRP), or unknown (OTH). Table 1 also shows the size of the vocabulary per class (label), which, unsurprisingly, is proportional to the class size. It is worth noting that offensive tweets targeting a group are the lengthier texts, with 28 tokens on average (see Table 1, column C, GRP column). Although some previous"
S19-2102,S19-2010,0,0.0331888,"ns many terms for different kinds of offensive language: toxic, abusive, hateful, attacking, etc. Largely, these are defined by different survey methods. In (Waseem et al., 2017), abusive language is divided into explicit vs. implicit, and directed vs. generalized. However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2). Data The S EM E VAL-2019 O FFENS E VAL dataset that is available to participants contains 13240 tweets; the counts of the labels are shown in Table 1. The O FFENS E VAL task consists of three subtasks, described in detail by Zampieri et al. (2019b). Subtask A aims at the detection of offensive language (OFF or NOT in Table 3). Subtask B aims at categorizing offensive language as targeting a specific entity (TIN) or not (UNT). Subtask C aims to identify whether the target of an offensive post is an individual (IND), a group (GRP), or unknown (OTH). Table 1 also shows the size of the vocabulary per class (label), which, unsurprisingly, is proportional to the class size. It is worth noting that offensive tweets targeting a group are the lengthier texts, with 28 tokens on average (see Table 1, column C, GRP column). Although some previous"
W01-0506,W00-0719,0,0.0233111,"of machine learning techniques in text categorization (Sebastiani, 2001) has recently led to alternative, learning-based approaches (Sahami, et al. 1998; Pantel & Lin, 1998; Drucker, et al. 1999). A classifier capable of distinguishing between spam and non-spam, hereafter legitimate, messages is induced from a manually categorized learning collection of messages, and is then used to identify incoming spam e-mail. Initial results have been promising, and experiments are becoming more systematic, by exploiting recently introduced benchmark corpora, and cost-sensitive evaluation measures (Gomez Hidalgo, et al. 2000; Androutsopoulos, et al. 2000a, b, c). Stacked generalization (Wolpert, 1992), or stacking, is an approach for constructing classifier ensembles. A classifier ensemble, or committee, is a set of classifiers whose individual decisions are combined in some way to classify new instances (Dietterich, 1997). Stacking combines multiple classifiers to induce a higher-level classifier with improved performance. The latter can be thought of as the president of a committee with the ground-level classifiers as members. Each unseen incoming message is first given to the members; the president then decide"
W03-2304,W98-1411,0,\N,Missing
W03-2304,W00-0306,0,\N,Missing
W03-2304,N01-1001,0,\N,Missing
W03-2304,C00-1007,0,\N,Missing
W03-2304,N01-1003,0,\N,Missing
W03-2304,P00-1012,0,\N,Missing
W03-2304,P98-1116,0,\N,Missing
W03-2304,C98-1112,0,\N,Missing
W03-2304,P99-1018,0,\N,Missing
W03-2304,W02-2101,0,\N,Missing
W03-2304,P01-1023,0,\N,Missing
W03-2304,A00-2026,0,\N,Missing
W03-2304,W90-0112,0,\N,Missing
W03-2304,W02-2113,0,\N,Missing
W03-2304,W02-2111,0,\N,Missing
W03-2304,W02-2112,0,\N,Missing
W05-1617,W96-0505,0,0.0136899,"Missing"
W07-1407,W01-0509,0,0.0187542,"ng T Total features 10 10 +10 +10 +1 +1 +1 +1 +10 +10 +10 +10 +2 +1 +10 +10 +10 +10 +1 128 IE IR QA SUM X X X X X X X X X X X X X X X X X X X X X X X X X X 64 31 X 23 54 Table 1: Feature sets considered and chosen in each subtask. systems used, but QA systems typically return T s that contain the expected answer type of the input question; for instance, if the question is “When did Charles de Gaulle die?”, T will typically contain a temporal expression. Furthermore, QA systems typically prefer T s that contain many words of the question, preferably in the same order, etc. (Radev et al., 2000; Ng et al., 2001; Harabagiu et al., 2003). Hence, if the answers are sought in a document collection with high redundancy (e.g., the Web), i.e., a collection where each answer can be found with many different phrasings, the T s (or parts of them) that most QA systems return are often very similar, in terms of phrasings, to the questions, provided that the required answers exist in the collection. In the QA datasets of the challenge, for each T , which was a snippet returned by a QA system for a question (e.g., “When did Charle de Gaulle die?”), an H was formed by “plugging into” the question an expression of"
W07-1407,A00-1021,0,0.0161132,"erb stems + short/long T Total features 10 10 +10 +10 +1 +1 +1 +1 +10 +10 +10 +10 +2 +1 +10 +10 +10 +10 +1 128 IE IR QA SUM X X X X X X X X X X X X X X X X X X X X X X X X X X 64 31 X 23 54 Table 1: Feature sets considered and chosen in each subtask. systems used, but QA systems typically return T s that contain the expected answer type of the input question; for instance, if the question is “When did Charles de Gaulle die?”, T will typically contain a temporal expression. Furthermore, QA systems typically prefer T s that contain many words of the question, preferably in the same order, etc. (Radev et al., 2000; Ng et al., 2001; Harabagiu et al., 2003). Hence, if the answers are sought in a document collection with high redundancy (e.g., the Web), i.e., a collection where each answer can be found with many different phrasings, the T s (or parts of them) that most QA systems return are often very similar, in terms of phrasings, to the questions, provided that the required answers exist in the collection. In the QA datasets of the challenge, for each T , which was a snippet returned by a QA system for a question (e.g., “When did Charle de Gaulle die?”), an H was formed by “plugging into” the question"
W07-1407,W07-1401,0,\N,Missing
W07-2322,bontcheva-2004-open,0,0.0652474,"</owlnl:singularForms> ... </owlnl:NP> 4 Source authoring Naturalowl is supported by m-piro’s authoring tool (Androutsopoulos et al., 2007), which has been extended by ncsr “Demokritos” to be compatible with owl dl. The tool helps “authors” port Naturalowl to new application domains, including the tasks of ontology construction, defining micro-plans, creating the domain-dependent lexicon, etc. Naturalowl is also accompanied by a plug-in for Prot´eg´e, an ontology editor most sw researchers are familiar with.8 The plug-in provides the same functionality as m-piro’s authoring tool. Consult also Bontcheva (2004) for related work on authoring tools. 5 Conclusions and further work We introduced Naturalowl, an open-source natural language generator for owl dl ontologies that currently supports English and Greek. The system is intended to demonstrate the benefits of adopting nlg techniques in the Semantic Web, and to contribute towards a discussion in the nlg community on relevant annotation standards. Naturalowl was partly developed and is being extended in project Xenios, where it is used by mobile robots acting as museum guides, an application that requires, among others, extensions to generate spatia"
W07-2322,W05-1613,0,0.017855,"ts manufacturer is Toshiba, etc.5 Naturalowl may be instructed to include facts that are further away in a graph representation of the ontology, up to a maximum (configurable) distance; setting the distance to two when describing a statue, for example, would also include in the selected facts information about the statue’s sculptor (e.g., the country and year they were born in). This is very similar 5 To save space, we restrict the discussion to descriptions of instances. Naturalowl can also describe classes, but it conveys only information that is explicit in the ontology, unlike the work of Mellish and Sun (2005), where class descriptions also convey inferred facts. There are also separate stand-off annotations that specify how interesting each type of fact is per user type, and other user modelling information, much as in ilex and m-piro. The ordering annotations could also be made sensitive to user type and target language. to ilex’s content selection, but without employing rhetorical relations. The selected facts of distance one are then ordered by consulting ordering annotations (see owlnl:order below), which specify a partial order of properties (e.g., that the manufacturer should be mentioned fi"
W07-2322,W05-1617,1,\N,Missing
W11-2701,P05-1074,0,0.0429171,"entences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. si  ei1  ai1.1  ai1.1.1  ··· ' ei2 ' ai1.2 & ai1.1.2 ,e ik ··· , ··· ··· , ai1.mi1 ai1.1.mi1.1 , ,a i2.1 -a i2.2 ai1.2.1 ··· ··· .a i2.mi2 ··· -a ik.1 ··· .a ik.mik Figure 1: Generating candidate extractive (eij ) and abstractive (aij... ) compressions from a source sentence (si ). likely to be paraphrases and, hence, can be treated as a paraphrasing rule φ1 ↔ φ2 . This pivoting was used, for example, by Bannard and Callison-Burch (2005), and it underlies several other paraphrase extraction methods (Riezler et al., 2007; CallisonBurch, 2008; Kok and Brockett, 2010). Zhao et al. (2009b) provide approximately one million rules, but we use only approximately half of them, because we use only rules that can shorten a sentence, and only in the direction that shortens the sentence. From each extractive candidate eij , we produced abstractive candidates aij.1 , aij.2 , . . . , aij.mij (Figure 1) by applying a single (each time different) applicable paraphrasing rule to eij . From each of the resulting abstractive candidates aij.l ,"
W11-2701,E06-1032,0,0.020347,"owever, a large number of gold compressions would be necessary to capture all (or at least most) of the acceptable shorter rephras1 Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1–11, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Computational Linguistics ings of the source sentences, and it is questionable if human judges could provide (or even think of) all the acceptable rephrasings. In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they cannot cope sufficiently well with paraphrases (Callison-Burch et al., 2006), which play a central role in abstractive sentence compression (Zhao et al., 2009a).1 Although it is difficult to construct datasets for end-to-end automatic evaluation of abstractive sentence compression methods, it is possible to construct datasets to evaluate the ranking components of generate-and-rank abstractive sentence compressors, i.e., compressors that first generate a large set of candidate abstractive (and possibly also extractive) compressions of the source and then rank them to select the best one. In previous work (Galanis and Androutsopoulos, 2010), we presented a generateand-r"
W11-2701,D08-1021,0,0.0352869,"Missing"
W11-2701,P06-2019,0,0.0218296,"uce compressions by only removing words, whereas abstractive methods may additionally rephrase expressions of the source sentence. Extractive methods are generally simpler and have dominated the sentence compression literature (Jing, When evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, because it has been shown that measuring the similarity (as F1 -measure of dependencies) between the dependency tree of the gold compression and that of a machine-generated compression correlates well with human judgements (Riezler et al., 2003; Clarke and Lapata, 2006a). With abstractive methods, however, there is a much wider range of acceptable abstractive compressions of each source sentence, to the extent that a single gold compression per source is insufficient. Indeed, to the best of our knowledge no measure to compare a machine-generated abstractive compression to a single human gold compression has been shown to correlate well with human judgements. One might attempt to provide multiple human gold abstractive compressions per source sentence and employ measures from machine translation, for example BLEU (Papineni et al., 2002), to compare each mach"
W11-2701,P06-1048,0,0.445548,"uce compressions by only removing words, whereas abstractive methods may additionally rephrase expressions of the source sentence. Extractive methods are generally simpler and have dominated the sentence compression literature (Jing, When evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, because it has been shown that measuring the similarity (as F1 -measure of dependencies) between the dependency tree of the gold compression and that of a machine-generated compression correlates well with human judgements (Riezler et al., 2003; Clarke and Lapata, 2006a). With abstractive methods, however, there is a much wider range of acceptable abstractive compressions of each source sentence, to the extent that a single gold compression per source is insufficient. Indeed, to the best of our knowledge no measure to compare a machine-generated abstractive compression to a single human gold compression has been shown to correlate well with human judgements. One might attempt to provide multiple human gold abstractive compressions per source sentence and employ measures from machine translation, for example BLEU (Papineni et al., 2002), to compare each mach"
W11-2701,D07-1008,0,0.423952,"Missing"
W11-2701,C08-1018,0,0.532531,"pressor ∗ Dimitrios Galanis∗ and Ion Androutsopoulos∗+ Department of Informatics, Athens University of Economics and Business, Greece + Digital Curation Unit – IMIS, Research Center “Athena”, Greece Abstract 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2007; Clarke and Lapata, 2008; Cohn and Lapata, 2009; Nomoto, 2009; Galanis and Androutsopoulos, 2010; Yamangil and Shieber, 2010). Abstractive methods, however, can in principle produce shorter compressions that convey the same information as longer extractive ones. Furthermore, humans produce mostly abstractive compressions (Cohn and Lapata, 2008); hence, abstractive compressors may generate more natural outputs. Sentence compression has attracted much interest in recent years, but most sentence compressors are extractive, i.e., they only delete words. There is a lack of appropriate datasets to train and evaluate abstractive sentence compressors, i.e., methods that apart from deleting words can also rephrase expressions. We present a new dataset that contains candidate extractive and abstractive compressions of source sentences. The candidate compressions are annotated with human judgements for grammaticality and meaning preservation."
W11-2701,N10-1131,1,0.932661,"sufficiently well with paraphrases (Callison-Burch et al., 2006), which play a central role in abstractive sentence compression (Zhao et al., 2009a).1 Although it is difficult to construct datasets for end-to-end automatic evaluation of abstractive sentence compression methods, it is possible to construct datasets to evaluate the ranking components of generate-and-rank abstractive sentence compressors, i.e., compressors that first generate a large set of candidate abstractive (and possibly also extractive) compressions of the source and then rank them to select the best one. In previous work (Galanis and Androutsopoulos, 2010), we presented a generateand-rank extractive sentence compressor, hereafter called GA - EXTR, which achieved state-of-the art results. We aim to construct a similar abstractive generate-and-rank sentence compressor. As part of this endeavour, we needed a dataset to automatically test (and train) several alternative ranking components. In this paper, we introduce a dataset of this kind, which we also make publicly available.2 The dataset consists of pairs of source sentences and candidate extractive or abstractive compressions. The candidate compressions were generated by first using GA - EXTR"
W11-2701,A00-1043,0,0.266858,"ctive compressions of source sentences. The candidate compressions are annotated with human judgements for grammaticality and meaning preservation. We discuss how the dataset was created, and how it can be used in generate-and-rank abstractive sentence compressors. We also report experimental results with a novel abstractive sentence compressor that uses the dataset. 1 Introduction Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). Sentence compression is useful in many applications, such as text summarization (Madnani et al., 2007) and subtitle generation (Corston-Oliver, 2001). Methods for sentence compression can be divided in two categories: extractive methods produce compressions by only removing words, whereas abstractive methods may additionally rephrase expressions of the source sentence. Extractive methods are generally simpler and have dominated the sentence compression literature (Jing, When evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, bec"
W11-2701,N06-1058,0,0.03277,"compressions. The candidate compressions were generated by first using GA - EXTR and then applying existing paraphrasing rules (Zhao et al., 2009b) to the best extractive compressions of GA - EXTR. Each pair (source and candidate compression) was then scored by a human judge for grammaticality and meaning preservation. We discuss how the dataset was constructed and how we established upper and lower performance boundaries for ranking components of compressors that may use it. We also present the 1 Ways to extend n-gram measures to account for paraphrases have been proposed (Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009), but they require accurate paraphrase recognizers (Androutsopoulos and Malakasiotis, 2010), which are not yet available; or they assume that the same paraphrase generation resources (Madnani and Dorr, 2010), for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below. 2 The new dataset and GA - EXTR are freely available from http://nlp.cs.aueb.gr/software.html. 2 current version of our abstractive sentence compressor, and we discuss how its ranking component"
W11-2701,P09-5002,0,0.0228573,"Missing"
W11-2701,N10-1017,0,0.0124925,"which we drew source sentences for our dataset. si  ei1  ai1.1  ai1.1.1  ··· ' ei2 ' ai1.2 & ai1.1.2 ,e ik ··· , ··· ··· , ai1.mi1 ai1.1.mi1.1 , ,a i2.1 -a i2.2 ai1.2.1 ··· ··· .a i2.mi2 ··· -a ik.1 ··· .a ik.mik Figure 1: Generating candidate extractive (eij ) and abstractive (aij... ) compressions from a source sentence (si ). likely to be paraphrases and, hence, can be treated as a paraphrasing rule φ1 ↔ φ2 . This pivoting was used, for example, by Bannard and Callison-Burch (2005), and it underlies several other paraphrase extraction methods (Riezler et al., 2007; CallisonBurch, 2008; Kok and Brockett, 2010). Zhao et al. (2009b) provide approximately one million rules, but we use only approximately half of them, because we use only rules that can shorten a sentence, and only in the direction that shortens the sentence. From each extractive candidate eij , we produced abstractive candidates aij.1 , aij.2 , . . . , aij.mij (Figure 1) by applying a single (each time different) applicable paraphrasing rule to eij . From each of the resulting abstractive candidates aij.l , we produced further abstractive candidates aij.l.1 , aij.l.2 , . . . , aij.l.mij.l by applying again a single (each time different"
W11-2701,J10-3003,0,0.0215629,"ompression) was then scored by a human judge for grammaticality and meaning preservation. We discuss how the dataset was constructed and how we established upper and lower performance boundaries for ranking components of compressors that may use it. We also present the 1 Ways to extend n-gram measures to account for paraphrases have been proposed (Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009), but they require accurate paraphrase recognizers (Androutsopoulos and Malakasiotis, 2010), which are not yet available; or they assume that the same paraphrase generation resources (Madnani and Dorr, 2010), for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below. 2 The new dataset and GA - EXTR are freely available from http://nlp.cs.aueb.gr/software.html. 2 current version of our abstractive sentence compressor, and we discuss how its ranking component was improved by performing experiments on the dataset. Section 2 below summarizes prior work on abstractive sentence compression. Section 3 discusses the dataset we constructed. Section 4 describes our abstractive sentence comp"
W11-2701,J00-2011,0,0.0456162,"tent words wi , wj that co-occur in σ at a maximum distance of 10 tokens; below N is the number of such pairs. PMI (σ) = 1 X · PMI (wi , wj ) N i,j In our second SVR-based ranking component, SVR PMI , we compute PMI (si ), PMI (e), and PMI (cij ), and we include them as three additional features; otherwise SVR - PMI is identical to SVR - BASE. 14 We used texts from TIPSTER and AQUAINT, a total of 953 million tokens, to estimate PMI(w1 , w2 ). 15 A problem with PMI is that two frequent and completely dependent words receive lower scores than two other, less frequent completely dependent words (Manning and Schutze, 2000). Pecina (2005), however, found PMI to be the best collocation extraction measure; and Newman et al. (2010) found it to be the best measure of ‘topical coherence’ for sets of words. 4.4 Additional LDA-based features Our third SVR-based ranking component includes features from a Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Roughly speaking, LDA models assume that each document d of |d |words w1 , . . . , w|d |is generated by iteratively (for r = 1, . . . , |d|) selecting a topic tr from a documentspecific multinomial distribution P (t|d) over K topics, and then (for each r) sele"
W11-2701,E06-1038,0,0.432944,"Missing"
W11-2701,N10-1012,0,0.0384425,"I (σ) = 1 X · PMI (wi , wj ) N i,j In our second SVR-based ranking component, SVR PMI , we compute PMI (si ), PMI (e), and PMI (cij ), and we include them as three additional features; otherwise SVR - PMI is identical to SVR - BASE. 14 We used texts from TIPSTER and AQUAINT, a total of 953 million tokens, to estimate PMI(w1 , w2 ). 15 A problem with PMI is that two frequent and completely dependent words receive lower scores than two other, less frequent completely dependent words (Manning and Schutze, 2000). Pecina (2005), however, found PMI to be the best collocation extraction measure; and Newman et al. (2010) found it to be the best measure of ‘topical coherence’ for sets of words. 4.4 Additional LDA-based features Our third SVR-based ranking component includes features from a Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Roughly speaking, LDA models assume that each document d of |d |words w1 , . . . , w|d |is generated by iteratively (for r = 1, . . . , |d|) selecting a topic tr from a documentspecific multinomial distribution P (t|d) over K topics, and then (for each r) selecting a word wr from a topic-specific multinomial distribution P (w|t) over the vocabulary.16 The probabili"
W11-2701,D09-1041,0,0.445717,"Missing"
W11-2701,P03-1021,0,0.0070779,"Missing"
W11-2701,P09-1034,0,0.0241482,"Missing"
W11-2701,P02-1040,0,0.0820462,"iezler et al., 2003; Clarke and Lapata, 2006a). With abstractive methods, however, there is a much wider range of acceptable abstractive compressions of each source sentence, to the extent that a single gold compression per source is insufficient. Indeed, to the best of our knowledge no measure to compare a machine-generated abstractive compression to a single human gold compression has been shown to correlate well with human judgements. One might attempt to provide multiple human gold abstractive compressions per source sentence and employ measures from machine translation, for example BLEU (Papineni et al., 2002), to compare each machine-generated compression to all the corresponding gold ones. However, a large number of gold compressions would be necessary to capture all (or at least most) of the acceptable shorter rephras1 Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1–11, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Computational Linguistics ings of the source sentences, and it is questionable if human judges could provide (or even think of) all the acceptable rephrasings. In machine translation, n-gram-based evaluation measures like BLEU have"
W11-2701,P05-2003,0,0.0188367,"ccur in σ at a maximum distance of 10 tokens; below N is the number of such pairs. PMI (σ) = 1 X · PMI (wi , wj ) N i,j In our second SVR-based ranking component, SVR PMI , we compute PMI (si ), PMI (e), and PMI (cij ), and we include them as three additional features; otherwise SVR - PMI is identical to SVR - BASE. 14 We used texts from TIPSTER and AQUAINT, a total of 953 million tokens, to estimate PMI(w1 , w2 ). 15 A problem with PMI is that two frequent and completely dependent words receive lower scores than two other, less frequent completely dependent words (Manning and Schutze, 2000). Pecina (2005), however, found PMI to be the best collocation extraction measure; and Newman et al. (2010) found it to be the best measure of ‘topical coherence’ for sets of words. 4.4 Additional LDA-based features Our third SVR-based ranking component includes features from a Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Roughly speaking, LDA models assume that each document d of |d |words w1 , . . . , w|d |is generated by iteratively (for r = 1, . . . , |d|) selecting a topic tr from a documentspecific multinomial distribution P (t|d) over K topics, and then (for each r) selecting a word wr"
W11-2701,N03-1026,0,0.198548,"xtractive methods produce compressions by only removing words, whereas abstractive methods may additionally rephrase expressions of the source sentence. Extractive methods are generally simpler and have dominated the sentence compression literature (Jing, When evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, because it has been shown that measuring the similarity (as F1 -measure of dependencies) between the dependency tree of the gold compression and that of a machine-generated compression correlates well with human judgements (Riezler et al., 2003; Clarke and Lapata, 2006a). With abstractive methods, however, there is a much wider range of acceptable abstractive compressions of each source sentence, to the extent that a single gold compression per source is insufficient. Indeed, to the best of our knowledge no measure to compare a machine-generated abstractive compression to a single human gold compression has been shown to correlate well with human judgements. One might attempt to provide multiple human gold abstractive compressions per source sentence and employ measures from machine translation, for example BLEU (Papineni et al., 20"
W11-2701,P07-1059,0,0.0230264,"heir abstractive dataset (Section 2), from which we drew source sentences for our dataset. si  ei1  ai1.1  ai1.1.1  ··· ' ei2 ' ai1.2 & ai1.1.2 ,e ik ··· , ··· ··· , ai1.mi1 ai1.1.mi1.1 , ,a i2.1 -a i2.2 ai1.2.1 ··· ··· .a i2.mi2 ··· -a ik.1 ··· .a ik.mik Figure 1: Generating candidate extractive (eij ) and abstractive (aij... ) compressions from a source sentence (si ). likely to be paraphrases and, hence, can be treated as a paraphrasing rule φ1 ↔ φ2 . This pivoting was used, for example, by Bannard and Callison-Burch (2005), and it underlies several other paraphrase extraction methods (Riezler et al., 2007; CallisonBurch, 2008; Kok and Brockett, 2010). Zhao et al. (2009b) provide approximately one million rules, but we use only approximately half of them, because we use only rules that can shorten a sentence, and only in the direction that shortens the sentence. From each extractive candidate eij , we produced abstractive candidates aij.1 , aij.2 , . . . , aij.mij (Figure 1) by applying a single (each time different) applicable paraphrasing rule to eij . From each of the resulting abstractive candidates aij.l , we produced further abstractive candidates aij.l.1 , aij.l.2 , . . . , aij.l.mij.l b"
W11-2701,P08-1078,0,0.0234068,"cality and meaning preservation scores. Table 1 shows the distribution of GM scores in the 3,072 pairs. Low GM scores (2– 5) are less frequent than higher scores (6–10), but this is not surprising given that we selected pairs whose cij had high language model scores, that we used the kmax extractive compressions of each si that GA - EXTR considered best, and that we assigned higher preference to applying paraphrasing rules with higher scores. We note, however, that applying a paraphrasing rule does not necessarily preserve neither grammaticality nor meaning, even if the rule has a high score. Szpektor et al. (2008) point out that, for example, a rule like “X acquire Y ” ↔ “X buy Y ” may work well in many contexts, but not in “Children acquire language quickly”. Similarly, “X charged Y with” ↔ “X accused Y of” should not be applied to sentences about batteries. Many (but not all) inappropriate rule applications 7 6 Each rule is actually associated with three scores. We use the ‘Model 1’ score; see Zhao et al. (2009b) for details. 4 We used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002). We trained the language model on approximately 4.5 million sentences from the TIPSTER corpus. GM score 2 3 4"
W11-2701,P10-1096,0,0.0227824,"Missing"
W11-2701,P08-1116,0,0.0601459,"Missing"
W11-2701,P09-1094,0,0.248244,"st) of the acceptable shorter rephras1 Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1–11, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Computational Linguistics ings of the source sentences, and it is questionable if human judges could provide (or even think of) all the acceptable rephrasings. In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they cannot cope sufficiently well with paraphrases (Callison-Burch et al., 2006), which play a central role in abstractive sentence compression (Zhao et al., 2009a).1 Although it is difficult to construct datasets for end-to-end automatic evaluation of abstractive sentence compression methods, it is possible to construct datasets to evaluate the ranking components of generate-and-rank abstractive sentence compressors, i.e., compressors that first generate a large set of candidate abstractive (and possibly also extractive) compressions of the source and then rank them to select the best one. In previous work (Galanis and Androutsopoulos, 2010), we presented a generateand-rank extractive sentence compressor, hereafter called GA - EXTR, which achieved sta"
W11-2701,C10-1149,0,0.0304371,"Missing"
W11-2701,W06-1610,0,0.0174313,"ive or abstractive compressions. The candidate compressions were generated by first using GA - EXTR and then applying existing paraphrasing rules (Zhao et al., 2009b) to the best extractive compressions of GA - EXTR. Each pair (source and candidate compression) was then scored by a human judge for grammaticality and meaning preservation. We discuss how the dataset was constructed and how we established upper and lower performance boundaries for ranking components of compressors that may use it. We also present the 1 Ways to extend n-gram measures to account for paraphrases have been proposed (Zhou et al., 2006; Kauchak and Barzilay, 2006; Pad´o et al., 2009), but they require accurate paraphrase recognizers (Androutsopoulos and Malakasiotis, 2010), which are not yet available; or they assume that the same paraphrase generation resources (Madnani and Dorr, 2010), for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below. 2 The new dataset and GA - EXTR are freely available from http://nlp.cs.aueb.gr/software.html. 2 current version of our abstractive sentence compressor, and we discu"
W13-2106,P11-1049,0,0.0435913,"Missing"
W13-2106,P84-1107,0,0.749456,"ization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation may then combine shorter sentences to form longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, decisions made during content selection may maximize importance measures, but may produce facts that are difficult to turn into a coherent text; also, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. Some of these problems can be addressed by overgenerating at each stage (e.g., producing several alternative sets of facts at the end of content selection, several alternative lexicalizations etc.) and employing a final ranking component to select the best combination (Walker et al., 2001). Th"
W13-2106,W10-4202,0,0.157428,"ndividual or class, we aim to produce a compact text that expresses as many facts in as few words as possible. This is desirable when space is limited or expensive, e.g., when displaying product descriptions on smartphones, or when including advertisements in Web search results. If an importance score is available for each fact, our model can take it into account to prefer expressing important facts, again using as few words as possible. The model itself, however, does not produce importance scores, i.e., we assume that the scores are produced by a separate process (Barzilay and Lapata, 2005; Demir et al., 2010), not included in our content selection. In the experiments of this article, we treat all the facts as equally important. Although the search space of our model is very large and ILP problems are in general NP-hard, offthe-shelf ILP solvers can be used, which can be very fast in practice and guarantee finding a global optimum. Experiments with two ontologies show that our ILP model outperforms, in terms of expressed facts per word, an NLG system that uses the same components connected in a pipeline, with no deterioration in perceived text quality; the ILP model may actually lead to texts of hi"
W13-2106,W07-2322,1,0.917549,"mation of our model, which allows longer texts to be generated efficiently. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Berners-Lee et al., 2001; Shadbolt et al., 2006; Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008), a form of description logic (Baader et al., 2002), in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts (axioms, in the case of an OWL ontology) to be expressed. The purpose of the next stage, text planning, ranges from simply ordering the facts to be expressed to making more complex decisions about the rhetorical structure of the text. Lexical51 Proceedings of the 14th European Workshop on Natural Language Generation, pages 51–60, c Sofia, Bulgaria, August 8-9 2013. 2013 Association f"
W13-2106,P04-1051,0,0.032262,") consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts of the same subset are aggregated to form a longer sentence. The optimal partitioning maximizes the pairwise similarity of the facts in each subset, subject to constraints that limit the number of subsets and the number of facts in each subset. A Maximum Entropy classifier predicts the semantic similarity of each pair of facts, and an ILP model is used to find the optimal partitioning. Althaus et al. (2004) show that ordering a set of sentences to maximize local coherence is equivalent to the traveling salesman problem and, hence, NP -complete. They also show an ILP formulation of the problem, which can be solved efficiently in practice using branch-and-cut with cutting planes. Kuznetsova et al. (2012) use ILP to generate image captions. They train classifiers to detect the objects in each image. Having identified the objects of a given image, they retrieve phrases from the captions of a corpus of images, focusing on the captions of objects that are similar (color, texture, shape) to the ones in"
W13-2106,E09-2005,1,0.90943,"Missing"
W13-2106,P12-1039,0,0.0177998,"optimization problem similar to energy minimization. The problem is solved by applying a minimal cut partition algorithm to a graph representing the Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et 52 al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records, as opposed to ontologies. Our method is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence conceptto-text generation. 3 made in Bancroft. ⇒ Bancroft Chardonnay is a kind of Chardonnay made in Bancroft. Let s1 , . . . , sm be disjoint subsets of F , each containing 0 to n facts, with m &lt; n. A single sentence is generated for each subset sj by aggregating the sentences (more precise"
W13-2106,H05-1042,0,0.371315,"facts (axioms) about the individual or class, we aim to produce a compact text that expresses as many facts in as few words as possible. This is desirable when space is limited or expensive, e.g., when displaying product descriptions on smartphones, or when including advertisements in Web search results. If an importance score is available for each fact, our model can take it into account to prefer expressing important facts, again using as few words as possible. The model itself, however, does not produce importance scores, i.e., we assume that the scores are produced by a separate process (Barzilay and Lapata, 2005; Demir et al., 2010), not included in our content selection. In the experiments of this article, we treat all the facts as equally important. Although the search space of our model is very large and ILP problems are in general NP-hard, offthe-shelf ILP solvers can be used, which can be very fast in practice and guarantee finding a global optimum. Experiments with two ontologies show that our ILP model outperforms, in terms of expressed facts per word, an NLG system that uses the same components connected in a pipeline, with no deterioration in perceived text quality; the ILP model may actuall"
W13-2106,N12-1093,0,0.018432,"optimization problem similar to energy minimization. The problem is solved by applying a minimal cut partition algorithm to a graph representing the Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et 52 al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records, as opposed to ontologies. Our method is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence conceptto-text generation. 3 made in Bancroft. ⇒ Bancroft Chardonnay is a kind of Chardonnay made in Bancroft. Let s1 , . . . , sm be disjoint subsets of F , each containing 0 to n facts, with m &lt; n. A single sentence is generated for each subset sj by aggregating the sentences (more precise"
W13-2106,N06-1046,0,0.020037,"that our ILP model outperforms, in terms of expressed facts per word, an NLG system that uses the same components connected in a pipeline, with no deterioration in perceived text quality; the ILP model may actually lead to texts of higher quality, compared to those of the pipeline, when there are many facts to express. We also present an approximation of our ILP model, which is more efficient when larger numbers of facts need to be expressed. Section 2 discusses previous related work. Section 3 defines our ILP model. Section 4 presents our experimentals. Section 5 concludes. 2 In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts of the same subset are aggregated to form a longer sentence. The optimal partitioning maximizes the pairwise similarity of the facts in each subset, subject to constraints that limit the number of subsets and the number of facts in each subset. A Maximum Entropy classifier predicts the semantic similarity of each pair of facts, and an ILP model is used to find the optimal partitioning. A"
W13-2106,P12-1038,0,0.0191385,"aximizes the pairwise similarity of the facts in each subset, subject to constraints that limit the number of subsets and the number of facts in each subset. A Maximum Entropy classifier predicts the semantic similarity of each pair of facts, and an ILP model is used to find the optimal partitioning. Althaus et al. (2004) show that ordering a set of sentences to maximize local coherence is equivalent to the traveling salesman problem and, hence, NP -complete. They also show an ILP formulation of the problem, which can be solved efficiently in practice using branch-and-cut with cutting planes. Kuznetsova et al. (2012) use ILP to generate image captions. They train classifiers to detect the objects in each image. Having identified the objects of a given image, they retrieve phrases from the captions of a corpus of images, focusing on the captions of objects that are similar (color, texture, shape) to the ones in the given image. To select which objects of the image to report and in what order, Kuznetsova et al. maximize (via ILP) the mean of the confidence scores of the object detection classifiers and the sum of the cooccurrence probabilities of the objects that will be reported in adjacent positions in th"
W13-2106,W05-0618,0,0.157388,"selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation may then combine shorter sentences to form longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, decisions made during content selection may maximize importance measures, but may produce facts that are difficult to turn into a coherent text; also, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. Some of these problems can be addressed by overgenerating at each stage (e.g., producing several alternative sets of facts at the end of content selection, several alternative lexicalizations etc.) and employing a final ranking component to select the best combination (Walker et al., 2001). This overgenerate and rank app"
W13-2106,C10-2128,0,0.0129214,"oduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Berners-Lee et al., 2001; Shadbolt et al., 2006; Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008), a form of description logic (Baader et al., 2002), in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts (axioms, in the case of an OWL ontology) to be expressed. The purpose of the next stage, text planning, ranges from simply ordering the facts to be expressed to making more complex decisions about the rhetorical structure of the text. Lexical51 Proceedings of the 14th European Workshop on Natural Language Generation, pages 51–60, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics pool of facts and the importance sc"
W13-2106,N01-1003,0,0.0446872,"-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, decisions made during content selection may maximize importance measures, but may produce facts that are difficult to turn into a coherent text; also, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. Some of these problems can be addressed by overgenerating at each stage (e.g., producing several alternative sets of facts at the end of content selection, several alternative lexicalizations etc.) and employing a final ranking component to select the best combination (Walker et al., 2001). This overgenerate and rank approach, however, may also fail to find an optimal solution, and it generates an exponentially large number of candidate solutions when several components are pipelined. In this paper, we present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. Our model does not consider directly text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and an external referring expression generation component; we also do n"
W13-2106,D12-1022,0,\N,Missing
W13-2106,P09-1011,0,\N,Missing
W13-2106,C12-1056,1,\N,Missing
W14-1306,N13-1090,0,0.0484456,"ect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do. We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies. We also compute the average weighted precision over several weighted recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and 1 Each aspect term occurrence is also annotated with a sentiment score. We do not discuss these scores here, since we focus on ATE. The same comment applies to the dataset of Ganu et al. (2009) and our datasets. 45 tences from online customer reviews of 30 hotels. We used three annotators. Among the 3,600 hotel sentences, 1,326 contain exactly one aspect term, 652 more than one, and 1,622 none. There are 199 distinct multi-word aspect terms and 262 distinct single-word aspect terms, of which 24 and 120, respectively, were"
W14-1306,P07-1056,0,0.0156304,"‘food’, ‘service’, ‘price’, ‘ambience’, ‘anecdotes’, or ‘miscellaneous’). For example, “The restaurant was expensive, but the menu was great” would be tagged with the coarse aspects ‘price’ and ‘food’. The coarse aspects, however, are not necessarily terms occurring in the sentence, and it is unclear how they were obtained. By contrast, we asked human annotators to mark the explicit aspect terms of each sentence, leaving the task of clustering the terms to produce coarser aspects for an aspect aggregation stage. The ‘Concept-Level Sentiment Analysis Challenge’ of ESWC 2014 uses the dataset of Blitzer et al. (2007), which contains customer reviews of In this paper, we focus on aspect term extraction (ATE). Our contribution is threefold. Firstly, we argue (Section 2) that previous ATE datasets are not entirely satisfactory, mostly because they contain reviews from a particular domain only (e.g., consumer electronics), or they contain reviews for very few target entities, or they do not contain annotations for aspect terms. We constructed and make publicly available three new ATE datasets with customer reviews for a much larger number of target entities from three domains (restaurants, laptops, hotels), w"
W14-1306,P05-1015,0,0.0752148,"er, we consider free text customer reviews of products and services; ABSA, however, is also applicable to texts about other kinds of entities (e.g., politicians, organizations). We assume that a search engine retrieves customer reviews about a particular target entity (product or Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 44–52, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics evaluation measures, we demonstrate (Section 5) that the extended method performs better. ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. 2 Datasets Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘c"
W14-1306,H05-1043,0,0.831199,"er granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be replaced by ‘food’) (Liu, 2012; Long et al., 2010; Zhai et al., 2010; Zhai et al., 2011). A polarity (and intensity) score can then be computed for each coarser aspect (e.g., ‘food’) by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect. We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g., Nikon Coolpix 4300). Each sentence is annotated with aspect terms, but inter-annotator agreement has not been reported.1 All the sentences appear to have been selected to express clear positive or negative opinions. There are no sentences expressing conflicting opinions about aspect terms (e.g., “The screen is clear but small”), nor are there any sentences that do not express opinions about their aspect terms (e.g., “It has a 4.8-inch scre"
W14-1306,N07-1038,0,0.0252198,"a table like the one of Fig. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Introduction Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Before buying a product or service, consumers often search the Web for expert reviews, but increasingly also for opinions of other consumers, expressed in blogs, social networks etc. Many useful opinions are expressed in text-only form (e.g., in tweets). It is then desirable to extract aspects (e.g., screen, battery) from the texts that discuss a particular entity (e.g., a smartphone), i.e., figure out"
W14-1306,W06-0301,0,0.0121126,"or low, respectively. Furthermore, including in Am a term not in Gm should be penalized more or less heavily, depending on whether the term was placed towards the beginning or the end of Am , i.e., depending on the prominence that was assigned to the term. To address the issues discussed above, we introduce weighted variants of precision and recall. Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F -measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect terms (no duplicates) or aspect term occurrences. In the former case, each method is expected to return a set A of distinct aspect terms, to be compared to the set G of distinct aspect terms the human annotators identified in the texts. TP (true positives) is |A∩G|, FP (false positives) is |AG|, FN (false negatives) is |G  A|, and precision (P ), ·R recall (R), F = 2·P P +R are defined as usually: P = TP TP , R= TP + FP TP + FN (1) 47 (AWP ) of each m"
W14-1306,D07-1114,0,0.0136662,"m.6 In a system like the one of Fig. 1, for example, if we ignore aspect aggregation, each row will report the average sentiment score of a single frequent distinct aspect term, and m will be the number of rows, which may depend on the display size or user preferences. Figure 2 shows the percentage of distinct multiword aspect terms among the m most frequent distinct aspect terms, for different values of m, in 2 5 Cohen’s Kappa cannot be used here, because the annotators may tag any word sequence of any sentence, which leads to a very large set of categories. A similar problem was reported by Kobayashi et al. (2007). 6 A more general definition of prominence might also consider the average sentiment score of each distinct aspect term. See http://2014.eswc-conferences.org/. Our datasets are available upon request. The datasets of the ABSA task of SemEval 2014 (http://alt.qcri. org/semeval2014/task4/) are based on our datasets. 4 The original dataset of Ganu et al. contains 3,400 sentences, but some of the sentences had not been properly split. 3 46 This way, however, precision, recall, and F measure assign the same importance to all the distinct aspect terms, whereas missing, for example, a more frequent"
W14-1306,P11-1150,0,0.012225,"g. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Introduction Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Before buying a product or service, consumers often search the Web for expert reviews, but increasingly also for opinions of other consumers, expressed in blogs, social networks etc. Many useful opinions are expressed in text-only form (e.g., in tweets). It is then desirable to extract aspects (e.g., screen, battery) from the texts that discuss a particular entity (e.g., a smartphone), i.e., figure out what is being dis"
W14-1306,C10-1143,0,0.00923538,"ril 26-30 2014. 2014 Association for Computational Linguistics evaluation measures, we demonstrate (Section 5) that the extended method performs better. ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. 2 Datasets Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be replaced by ‘food’) (Liu, 2012; Long et al., 2010; Zhai et al., 2010; Zhai et al., 2011). A polarity (and intensity) score can then be computed for each coarser aspect (e.g., ‘food’) by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect. We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews o"
W14-1306,C10-2088,0,0.14285,"goal is to produce a table like the one of Fig. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Introduction Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Before buying a product or service, consumers often search the Web for expert reviews, but increasingly also for opinions of other consumers, expressed in blogs, social networks etc. Many useful opinions are expressed in text-only form (e.g., in tweets). It is then desirable to extract aspects (e.g., screen, battery) from the texts that discuss a particular entity (e.g., a sm"
W14-1306,H05-2017,0,\N,Missing
W16-2915,J84-3009,0,0.274404,"Missing"
W16-2915,D15-1075,0,0.022253,"Missing"
W16-2915,D14-1162,0,0.0843778,"4 34 Athens, Greece http://nlp.cs.aueb.gr 2 Institute for Language and Speech Processing, Research Center ‘Athena’, Greece Artemidos 6 & Epidavrou, GR-151 25 Maroussi, Athens, Greece http://www.ilsp.gr Abstract and aim to produce more concise answers. Document retrieval is particularly important in biomedical QA, since most of the information sought resides in documents and is essential in later stages. We propose a new document retrieval method. Instead of representing documents and questions as bags of words, we represent them as the centroids of their word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and retrieve the documents whose centroids are closer to the centroid of the question. This allows retrieving relevant documents that may have no common terms with the question without query expansion. Using biomedical questions from the BIOASQ competition (Tsatsaronis et al., 2015), we show that our method combined with a relaxation of the recently proposed Word Mover’s Distance (WMD) (Kusner et al., 2015) is competitive with PUBMED. We also show that with a top-k approximation, our method is particularly fast, with no significant decrease in effectiveness. Given that it does not require ont"
W16-2915,N16-1162,0,0.0308945,"sifier that represents articles as IDF-weighted centroids (Eq. 1) of 200-dimensional word embeddings (200 features) is as good at assigning semantic labels (MeSH headings) to biomedical articles as when using millions of bag-of-word features, reducing significantly the training and classification times. To our knowledge, our work is the first attempt to use IDF-weighted centroids of word embeddings in information retrieval, and the first to use WMD to rerank the retrieved documents. More elaborate methods to encode texts as vectors have been proposed (Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) and they could be used as alternatives to centroids of word embeddings, though the latter are simpler and faster to compute. The OHSUMED dataset (Hersh et al., 1994) is often used in biomedical information retrieval experiments. It is much smaller (101 queries, approx. 350K documents) than the BIOASQ dataset that we used, but we plan to experiment with OHSUMED in future work for completeness. nDCG@100 Figure 4: nDCG@k, for k = 20 and k = 100. System C ent IDF - RWMD - Q ANN - C ent IDF - RWMD - Q Search 47.41 (±1.22) 0.36 (±0.04) Reranking 14.45 (±6.15) 14.24 (±6.06) Other related work Total"
W17-3004,D15-1176,0,0.0822551,"Missing"
W17-3004,W13-1104,0,0.0170055,"e LSTM, starting from random embeddings. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of up to 5 tokens, the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) predict which users would be banned from on-line communities. Their best system uses a Random Forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Lukin and Walker (2013) experimented with 5.5K utterances from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.9K utterances from the same corpus annotated for sarcasm.18 In a bootstrapping manner, they manually identified cue words and phrases (indicative of nastiness or sarcasm), used the cue words to obtain training comments, and extracted patterns from the training comments. Xiang et al. (2012) also employed bootstrapping to identify users whose tweets frequently or never contain profane words, and collected 381M tweets from the two user types. They"
W17-3004,P11-2102,0,0.0345616,"d TF - IDF , sentiment, and context features (e.g., similarity to other posts in a thread).19 Our methods might also benefit by considering threads, rather than individual comments. Yin et al. point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose. Spam is unlikely in Wikipedia discussions and extremely rare so far in Gazzetta comments. Mihaylov and Nakov (2016) identify comments posted by opinion manipulation trolls. Dinakar et 18 For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al. (2016). 19 Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content. 16 According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not. 17 See http://hunch.net/˜vw/. 32 Acknowledgments al. (2011) and Dadvar et al. (2013) detect cyberbullying. Chandrinos et al. (2000) detect pornographic web pages, using a Naive Bayes classifier with text and image features. Spertus (1997) flag flame messages in Web"
W17-3004,D15-1166,0,0.289561,"en Gazzetta did not employ moderators. Table 1: Statistics of the datasets used. measures for the semi-automatic scenario. On both Gazzetta and Wikipedia comments and for both scenarios (automatic, semi-automatic), we show that a recursive neural network (RNN) outperforms the system of Wulczyn et al. (2017), the previous state of the art for comment moderation, which employed logistic regression (LR) or a multi-layered Perceptron (MLP). We also propose an attention mechanism that improves the overall performance of the RNN. Our attention differs from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in text classification, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al., 2014). In effect, our attention mechanism detects the words of a comment that affect mostly the classification decision (accept, reject), by examining them in the context of the particular comment. Our main contributions are: (i) We release a new dataset of 1.6M moderated user comments. (ii) We are among the first to apply deep learning to user comment moderation, and we show that an RNN with a novel classification-s"
W17-3004,P16-2065,0,0.0245176,"distance. Yin et al. (2009) used posts from chat rooms and discussion fora (<15K posts in total) to train an SVM to detect online harassment. They used TF - IDF , sentiment, and context features (e.g., similarity to other posts in a thread).19 Our methods might also benefit by considering threads, rather than individual comments. Yin et al. point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose. Spam is unlikely in Wikipedia discussions and extremely rare so far in Gazzetta comments. Mihaylov and Nakov (2016) identify comments posted by opinion manipulation trolls. Dinakar et 18 For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al. (2016). 19 Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content. 16 According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not. 17 See http://hunch.net/˜vw/. 32 Acknowledgments al. (2011) and Dadvar et al. (2013) detect cyberbullying. Chandrinos et al."
W17-3004,N13-1090,0,0.0230629,"Missing"
W17-3004,P15-2124,0,0.0178057,"ment, and context features (e.g., similarity to other posts in a thread).19 Our methods might also benefit by considering threads, rather than individual comments. Yin et al. point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose. Spam is unlikely in Wikipedia discussions and extremely rare so far in Gazzetta comments. Mihaylov and Nakov (2016) identify comments posted by opinion manipulation trolls. Dinakar et 18 For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al. (2016). 19 Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content. 16 According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not. 17 See http://hunch.net/˜vw/. 32 Acknowledgments al. (2011) and Dadvar et al. (2013) detect cyberbullying. Chandrinos et al. (2000) detect pornographic web pages, using a Naive Bayes classifier with text and image features. Spertus (1997) flag flame messages in Web feedback forms, using"
W17-3004,E17-2068,0,0.0317211,"ymous comments, how often personal attacks were followed by moderation actions). Our methods could replace DETOX in studies of this kind, since they perform better. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best method was an LR classifier with character n-grams (n = 1, . . . , 4) and a gender feature. Badjatiya et al. (2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.3), LSTM (Greff et al., 2015), FastText (Joulin et al., 2017). They also considered alternative feature sets: character n-grams, tfidf vectors, word embeddings, averaged word embeddings. Their best results were obained using GBDT with averaged word embeddings learned by the LSTM, starting from random embeddings. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of up to 5 tokens, the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) predict which users would be banned from on-line communities."
W17-3004,D14-1181,0,0.012628,"100 comments (for G - TRAIN - L ) in the training set, along with the precision of w, i.e., the ratio of rejected training comments containing w divided by the total number of training comments containing w. The resulting lists contain 10,423, 11,360, 16,864, and 21,940 word types, when using W- ATT- TRAIN, W- TOX TRAIN , G - TRAIN - S , G - TRAIN - L , respectively. For a comment c, PLIST (reject|c) is the maximum precision of all the words in c. CNN We also compare against a vanilla CNN operating on word embeddings. We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015). For Wikipedia comments, we use a ‘narrow’ convolution layer, with kernels sliding (stride 1) over (entire) embeddings of word n-grams of sizes n = 1, . . . , 4. We use 300 kernels for each n value, a total of 1,200 kernels. The outputs of each kernel, obtained by applying the kernel to the different n-grams of a comment c, are then max-pooled, leading to a single output per kernel. The resulting feature vector (1,200 max3.5 Tuning thresholds All methods produce a p = P (reject|c) per comment c. In semi-automa"
W17-3004,W17-0802,0,0.136717,"Missing"
W17-3004,N16-1174,0,0.048123,"he attention weights at sum to 1. Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.g., partly generated translation) to drive the attention (e.g., assign more weight to source words to translate next), unlike seq2seq models (Sutskever et al., 2014). It assigns larger weights at to hidden states ht corresponding to positions where there is more evidence that the comment should be accepted or rejected. Yang et al. (2016) use a similar attention mechanism, but ours is deeper. In effect they always set l = 2, whereas we allow l to be larger (tuning selects l = 4).7 On the other hand, the attention (1) at = ReLU(W (1,x) xt + b(1) ) (3) Intuitively, the attention of a-RNN considers each word embedding xt in its (left) context, modelled by ht , whereas the attention of da-RNN considers directly xt without its context, but hsum is still the weighted sum of the hidden states (Eq. 1). eq-RNN: In another variant of a-RNN, called eqRNN , we assign equal attention to all the hidden states. The feature vector of the LR l"
W17-3004,W16-3604,0,0.0116178,"tures (e.g., similarity to other posts in a thread).19 Our methods might also benefit by considering threads, rather than individual comments. Yin et al. point out that unlike other abusive content, spam in comments or discussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose. Spam is unlikely in Wikipedia discussions and extremely rare so far in Gazzetta comments. Mihaylov and Nakov (2016) identify comments posted by opinion manipulation trolls. Dinakar et 18 For sarcasm, see Davidov et al. (2010), Gonzalez-Ibanez et al. (2011), Joshi et al. (2015), Oraby et al. (2016). 19 Sentiment features have been used by several methods, but sentiment analysis (Pang and Lee, 2008; Liu, 2015) is typically not directly concerned with abusive content. 16 According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not. 17 See http://hunch.net/˜vw/. 32 Acknowledgments al. (2011) and Dadvar et al. (2013) detect cyberbullying. Chandrinos et al. (2000) detect pornographic web pages, using a Naive Bayes classifier with text and image features. Spertus (1997) flag flame messages in Web feedback forms, using decision trees and h"
W17-3004,P94-1013,0,0.0170091,"), CNN (similar to that of Section 3.3), LSTM (Greff et al., 2015), FastText (Joulin et al., 2017). They also considered alternative feature sets: character n-grams, tfidf vectors, word embeddings, averaged word embeddings. Their best results were obained using GBDT with averaged word embeddings learned by the LSTM, starting from random embeddings. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of up to 5 tokens, the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) predict which users would be banned from on-line communities. Their best system uses a Random Forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Lukin and Walker (2013) experimented with 5.5K utterances from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.9K utterances from the same corpus annotated for sarcasm.18 In a bootstrapping manner, they manually identified cue words and phras"
W17-3004,D14-1162,0,0.0801858,"Missing"
W17-3004,walker-etal-2012-corpus,0,0.0322223,"mitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of up to 5 tokens, the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) predict which users would be banned from on-line communities. Their best system uses a Random Forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Lukin and Walker (2013) experimented with 5.5K utterances from the Internet Argument Corpus (Walker et al., 2012; Abbott et al., 2016) annotated with nastiness scores, and 9.9K utterances from the same corpus annotated for sarcasm.18 In a bootstrapping manner, they manually identified cue words and phrases (indicative of nastiness or sarcasm), used the cue words to obtain training comments, and extracted patterns from the training comments. Xiang et al. (2012) also employed bootstrapping to identify users whose tweets frequently or never contain profane words, and collected 381M tweets from the two user types. They trained decision tree, Random Forest, or LR classifiers to distinguish between tweets fro"
W17-3004,W12-2103,0,0.166396,"er with character n-grams (n = 1, . . . , 4) and a gender feature. Badjatiya et al. (2017) experimented with the same dataset using LR, SVMs (Cortes and Vapnik, 1995), Random Forests (Ho, 1995), Gradient Boosted Decision Trees (GBDT) (Friedman, 2002), CNN (similar to that of Section 3.3), LSTM (Greff et al., 2015), FastText (Joulin et al., 2017). They also considered alternative feature sets: character n-grams, tfidf vectors, word embeddings, averaged word embeddings. Their best results were obained using GBDT with averaged word embeddings learned by the LSTM, starting from random embeddings. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of up to 5 tokens, the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994). Cheng et al. (2015) predict which users would be banned from on-line communities. Their best system uses a Random Forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Lukin and Walker (2013) experimented with 5.5K utterances from the In"
W17-3004,N16-2013,0,0.167388,"Missing"
W17-3004,L16-1704,0,\N,Missing
W17-4209,P16-1094,0,0.0353321,"to an MLP that classifies the tweet as sarcastic or not. This method outperforms a previous state of the art sarcasm detection method (Bamman and Smith, 2015) that relies on an LR classifier with handcrafted content and user-specific features. We use an RNN instead of a CNN, and we feed the comment and user embeddings to a simpler LR layer (Eq. 2), instead of an MLP. Amir et al. discard unknown users, unlike our experiments, and consider only sarcasm, whereas moderation also involves profanity, hate speech, bullying, threats etc. User embeddings have also been used in: conversational agents (Li et al., 2016); sentiment analysis (Chen et al., 2016); retweet prediction (Zhang et al., 2016); predicting which topics a user is likely to tweet about, the accounts a user may want to follow, and the age, gender, political affiliation of Twitter users (Benton et al., 2016). Our previous work (Pavlopoulos et al., 2017a) also discussed how machine learning can be used in semi-automatic moderation, by letting moderators focus on ‘difficult’ comments and automatically handling comments that are easier to accept or reject. In more recent work (Pavlopoulos et al., 2017b) we also explored how an attention mechan"
W17-4209,K16-1017,0,0.0196597,"t of tbRNN average bu of ubRNN −0.471 (±0.007) −0.180 (±0.024) 0.198 (±0.015) 0.058 (±0.022) 0.256 (±0.021) 0.312 (±0.011) 1.151 (±0.013) 0.387 (±0.023) features examining the average readability and sentiment of each user’s past posts, the past activity of each user (e.g., number of posts daily, proportion of posts that are replies), and the reactions of the community to the past actions of each user (e.g., up-votes, number of posts rejected). Lee et al. (2014) and Napoles et al. (2017) include similar user-specific features in classifiers intended to detect high quality on-line discussions. Amir et al. (2016) detect sarcasm in tweets. Their best system uses a word-based Convolutional Neural Network (CNN). The feature vector produced by the CNN (representing the content of the tweet) is concatenated with the user embedding of the author, and passed on to an MLP that classifies the tweet as sarcastic or not. This method outperforms a previous state of the art sarcasm detection method (Bamman and Smith, 2015) that relies on an LR classifier with handcrafted content and user-specific features. We use an RNN instead of a CNN, and we feed the comment and user embeddings to a simpler LR layer (Eq. 2), in"
W17-4209,W17-3004,1,0.651686,"oyalty. User comments, however, can also be abusive (e.g., bullying, profanity, hate speech), damaging the reputation of news portals, making them liable to fines (e.g., when hosting comments encouraging illegal actions), and putting off readers. Large news portals often employ moderators, who are frequently overwhelmed by the volume and abusiveness of comments.1 Readers are disappointed when nonabusive comments do not appear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments.2 In previous work (Pavlopoulos et al., 2017a), we introduced a new dataset of approx. 1.6M manually moderated user comments from a Greek sports news portal, called Gazzetta, which we made publicly available.3 Experimenting on that dataset and the datasets of Wulczyn et al. (2017), which contain moderated English Wikipedia comments, we showed that a method based on a Recurrent Neural Network (RNN) outperforms DETOX 2 Dataset We first discuss the dataset we used, to help acquaint the reader with the problem. The dataset contains Greek comments from Gazzetta (Pavlopoulos et al., 2017a). There are approximately 1.45M training comments (cov"
W17-4209,D17-1117,1,0.740014,"oyalty. User comments, however, can also be abusive (e.g., bullying, profanity, hate speech), damaging the reputation of news portals, making them liable to fines (e.g., when hosting comments encouraging illegal actions), and putting off readers. Large news portals often employ moderators, who are frequently overwhelmed by the volume and abusiveness of comments.1 Readers are disappointed when nonabusive comments do not appear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments.2 In previous work (Pavlopoulos et al., 2017a), we introduced a new dataset of approx. 1.6M manually moderated user comments from a Greek sports news portal, called Gazzetta, which we made publicly available.3 Experimenting on that dataset and the datasets of Wulczyn et al. (2017), which contain moderated English Wikipedia comments, we showed that a method based on a Recurrent Neural Network (RNN) outperforms DETOX 2 Dataset We first discuss the dataset we used, to help acquaint the reader with the problem. The dataset contains Greek comments from Gazzetta (Pavlopoulos et al., 2017a). There are approximately 1.45M training comments (cov"
W17-4209,P16-2003,0,0.0227739,"N instead of a CNN, and we feed the comment and user embeddings to a simpler LR layer (Eq. 2), instead of an MLP. Amir et al. discard unknown users, unlike our experiments, and consider only sarcasm, whereas moderation also involves profanity, hate speech, bullying, threats etc. User embeddings have also been used in: conversational agents (Li et al., 2016); sentiment analysis (Chen et al., 2016); retweet prediction (Zhang et al., 2016); predicting which topics a user is likely to tweet about, the accounts a user may want to follow, and the age, gender, political affiliation of Twitter users (Benton et al., 2016). Our previous work (Pavlopoulos et al., 2017a) also discussed how machine learning can be used in semi-automatic moderation, by letting moderators focus on ‘difficult’ comments and automatically handling comments that are easier to accept or reject. In more recent work (Pavlopoulos et al., 2017b) we also explored how an attention mechanism can be used to highlight possibly abusive words or phrases when showing ‘difficult’ comments to moderators. Table 4: Biases learned and standard error. countered a comment saying just “Ooooh, down to Pireaus. . . ” (translated from Greek), which the moderat"
W17-4209,D14-1162,0,0.084501,"Missing"
W17-4209,D16-1171,0,0.0615016,"Missing"
W17-4209,N16-2013,0,0.140029,"Missing"
W17-4209,D14-1179,0,0.0168802,"Missing"
W18-5304,D18-1211,1,0.799843,"RMM uses pretrained word embeddings for q-terms and d-terms, and (bucketed) cosine similarity histograms (outputs of ⊗ nodes in Fig. 2). Each histogram captures the similarity of a q-term to all the d-terms of a particular document. The histograms, which in this model are the document-aware q-term encodings, are fed to an MLP (dense layers of Fig. 2) that produces the (document-aware) score of each q-term. Each q-term score is then weighted using 5 Hui et al. (2017) used an additional LSTM, which was later replaced by the final concatenation (Hui et al., 2018). 6 In the related publication of McDonald et al. (2018) TERM - PACRR is identical to the PACRR - DRMM model. 31 Document Terms Query Term ... ContextSensitive Term Encodings Residual sions of the pre-trained embedding, for residuals to be summed without transformation. Specifically, let e(ti ) be the pre-trained embedding for a q-term or d-term term ti . We compute the context-sensitive encoding of ti as:    (1)  φH (qi ) = dqi c(qi ) X ai,j c(dj ) (e.g., Hadamard Product) (4) The ⊗ nodes and lower parts of the DRMM network of Fig. 2 are now replaced by (multiple copies of) the sub-network of Fig. 4 (one copy per q-term), with the nodes replac"
W18-5304,P16-1177,0,0.0428002,"Missing"
W18-5304,W17-2328,0,0.118904,"the linear layer that combines the q-term scores (Fig. 1). In ABEL - DRMM, an additional linear layer is used that concatenates the deep learning document relevance score with the traditional IR features. In BCNN, the additional features are included in the final linear layer (Fig. 5). The additional features we used were the BM 25 score of the document (the document the snippet came from, in snippet retrieval), word overlap (binary and IDF weighted) between the query and the document or snippet; bigram overlap between the query and the document or snippet. The latter features were taken from Mohan et al. (2017). The additional features improved the performance of all models. A technique that seems to improve our results in snippet retrieval is to retain only the top Ks snippets with the best BCNN scores for each query, and then re-rank the Ks snippets by the relevance scores of the documents they came from; if two snippets came from the same document, they are subsequently ranked by their BCNN score. This is a proxy for more sophisticated models that would jointly consider document and snippet retrieval. This is important as the snippet retrieval model is trained under the condition that it only see"
W18-5304,D17-1110,0,0.0671104,"Missing"
W18-5304,W17-2348,0,0.112541,"Missing"
W18-5304,P15-2116,0,0.0717408,"Missing"
W18-5304,Q16-1019,0,0.335347,"Missing"
W18-5304,D14-1179,0,\N,Missing
W18-5304,N16-1174,0,\N,Missing
W18-5304,N18-1202,0,\N,Missing
W19-1803,W05-0909,0,0.103396,"ongest common subsequence between the machine-generated description and the reference human description, to the size of the reference (ROUGE - L recall); or to the generated description (ROUGE - L precision); or a combination of the two (ROUGE - L F-measure). We note that several ROUGE variants exist, based on different ngram lengths, stemming, stopword removal, etc., but ROUGE - L is the most commonly used variant in biomedical image captioning so far. Evaluation The most common evaluation measures in biomedical image captioning are BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), which originate from machine translation and summarization. The more recent CIDER measure (Vedantam et al., 2015), which was designed for general image captioning (Kilickaya et al., 2016), has been used in only two biomedical image captioning works (Zhang et al., 2017b; Jing et al., 2018). SPICE (Anderson et al., 2016), which was also designed for general image captioning (Kilickaya et al., 2016), has not been used in any biomedical image captioning work we are aware of. Below, we describe each measure separately and discuss its advantages and limitations with respect to biomedical image cap"
W19-1803,W04-1013,0,0.106713,"of the length of the longest common subsequence between the machine-generated description and the reference human description, to the size of the reference (ROUGE - L recall); or to the generated description (ROUGE - L precision); or a combination of the two (ROUGE - L F-measure). We note that several ROUGE variants exist, based on different ngram lengths, stemming, stopword removal, etc., but ROUGE - L is the most commonly used variant in biomedical image captioning so far. Evaluation The most common evaluation measures in biomedical image captioning are BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), which originate from machine translation and summarization. The more recent CIDER measure (Vedantam et al., 2015), which was designed for general image captioning (Kilickaya et al., 2016), has been used in only two biomedical image captioning works (Zhang et al., 2017b; Jing et al., 2018). SPICE (Anderson et al., 2016), which was also designed for general image captioning (Kilickaya et al., 2016), has not been used in any biomedical image captioning work we are aware of. Below, we describe each measure separately and discuss its advantages and limitation"
W19-1803,P18-1240,0,0.0608486,"st survey of biomedical image captioning, discussing datasets, evaluation measures, and state of the art methods. Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets. 1 (a) General image caption. Introduction Radiologists or other physicians may need to examine many biomedical images daily, e.g. PET / CT scans or radiology images, and write their findings as medical reports (Figure 1b). Methods assisting physicians to focus on interesting image regions (Shin et al., 2016) or to describe findings (Jing et al., 2018) can reduce medical errors (e.g., suggesting findings to inexperienced physicians) and benefit medical departments by reducing the cost per exam (Bates et al., 2001; Lee et al., 2017). Despite the importance of biomedical image captioning, related resources are not easily accessible, hindering the emergence of new methods. The publicly available datasets are only three and not always directly available.1 Also, there is currently no assessment of simple baselines to determine the lower performance boundary and estimate the difficulty of the task. By contrast, complex (typically deep learning) s"
W19-1803,P02-1040,0,0.108769,"An LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) was used as the RNN decoder to generate image descriptions from the image encodings. In a second training phase, the mean of the RNNs state vectors (obtained while describing each image) was used as an improved representation of each training image. The original 17 classes that had been used to pretrain the CNN were replaced by 57 finer classes, by applying k-means clustering to the improved vector representations of the training images. The CNN was then retrained to predict the 57 new classes and this led to improved BLEU (Papineni et al., 2002) scores for the overall CNN - RNN system. The gen12 Zhang et al. had introduced earlier TandemNet (Zhang et al., 2017a), which also used attention, but for biomedical image classification. TandemNet could perform captioning, but the authors considered this task as future work, that was addressed with MDNET. 29 LSTM and each one of the visual feature vectors of the image) assigns attention scores to the visual feature vectors, and the weighted sum of the visual feature vectors (weighted by their attention scores) becomes a visual ‘context’ vector, specifying which patches of the image to expres"
W19-1803,varges-etal-2012-semscribe,0,0.0165552,"me-steps, the sentencelevel LSTM of Jing et al. examines both the visual and the semantic feature vectors of the image. Following previous work on image captioning, that added attention to encoder-decoder approaches (Xu et al., 2015; You et al., 2016; Zhang et al., 2017b), an attention mechanism (an MLP fed with the current state of the sentence-level ciated concepts and there are images associated with even thousands of concepts. The organizers observe the existence of noise and note that irrelevant concepts have been extracted, mainly due to the fully automatic extraction process. 3 Methods Varges et al. (2012) employed Natural Language Generation to assist medical professionals turn cardiological findings (e.g., from diagnostic imaging procedures) into fluent and readable textual descriptions. From a different perspective, Schlegl et al. (2015) used both the image and the textual report as input to a CNN, trained to classify images with the help of automatically extracted semantic concepts from the textual report. Kisilev et al. (2015a,b) employed a radiologist to mark an image lesion, and a semi-automatic segmentation approach to define the boundaries of that lesion. Then, they used structured Sup"
W19-1803,D14-1179,0,\N,Missing
W19-1803,E17-1019,0,\N,Missing
W19-2209,D14-1181,0,0.00590848,"enbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results on several benchmark datasets, most notably: RCV 1 (Lewis et al., 2004), containing news articles; EUR - LEX (Mencia and Frnkranz, 2007), containing legal documents; Amazon-12K (McAuley and Leskovec, 2013), containing product descriptions; and Wiki-30K (Zubiaga, 2012), containing Wikipedia articles. Their proposed method outperformed both tree-based methods (e.g., FASTXML, (Prabhu and Varma, 2014)) and target-embedding methods (e.g., SLEEC (Bhatia et al., 2015), FASTTEXT (Bojanowski et al., 2016)). RNNs with self-attention have been employed in a wide variety o"
W19-2209,P19-1285,0,0.0161146,"TT (left) and BIGRU - LWAN (right). Gold labels (concepts) are shown at the top of each sub-figure, while the top 5 predicted labels are shown at the bottom. Correct predictions are shown in bold. BIGRU - LWAN’s label-wise attentions are depicted in different colors. ing heat-maps include only one color. 6 consider the structure (sections) of the documents. The best methods of this work rely on GRUs and thus are computationally expensive. The length of the documents further affects the training time of these methods. Hence, we plan to investigate the use of Transformers (Vaswani et al., 2017; Dai et al., 2019) and dilated CNNs (Kalchbrenner et al., 2017) as alternative document encoders. Conclusions and Future Work We compared various neural methods on a new legal XMTC dataset, EURLEX 57 K, also investigating few-shot and zero-shot learning. We showed that BIGRU - ATT is a strong baseline for this XMTC dataset, outperforming CNN - LWAN (Mullenbach et al., 2018), which was especially designed for XMTC , but that replacing the vanilla CNN of CNN LWAN by a BIGRU encoder ( BIGRU - LWAN ) leads to the best overall results, except for zero-shot labels. For the latter, the zero-shot version of CNN LWAN of"
W19-2209,N19-1423,0,0.0552829,"Missing"
W19-2209,P18-1128,0,0.0395065,"Missing"
W19-2209,N18-1100,0,0.319881,"onomics and Business, Greece ** Computer Science Department, University of Sheffield, UK [ihalk,fergadiotis,rulller,ion]@aueb.gr, n.aletras@sheffield.ac.uk Abstract Publications Office of the European Union. Although EUROVOC contains more than 7,000 concepts, most of them are rarely used in practice. Consequently, they are under-represented in EU RLEX 57 K , making the dataset also appropriate for few-shot and zero-shot learning. Experimenting on EURLEX 57 K, we explore the use of various RNN-based and CNN-based neural classifiers, including the state of the art LabelWise Attention Network of Mullenbach et al. (2018), called CNN - LWAN here. We show that both a simpler BIGRU with self-attention (Xu et al., 2015) and the Hierarchical Attention Network (HAN) of Yang et al. (2016) outperform CNN LWAN by a wide margin. Replacing the CNN encoder of CNN - LWAN with a BIGRU, which leads to a method we call BIGRU - LWAN, further improves performance. Similar findings are observed in the zero-shot setting where Z - BIGRU - LWAN outperforms Z - CNN - LWAN. We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR - LEX,"
W19-2209,P18-1031,0,0.0567814,"Missing"
W19-2209,D08-1046,0,0.645838,"ification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results on several benchmark datasets, most notably: RCV 1 (Lewis et al., 2004), containing news articles; EUR - LEX (Mencia and Frnkranz, 2007), containing legal documents; Amazo"
W19-2209,N19-1357,0,0.0614205,"Missing"
W19-2209,D14-1162,0,0.0824988,"Missing"
W19-2209,N18-1202,0,0.107142,"Missing"
W19-2209,P16-2034,0,0.0763691,"Missing"
W19-2209,N18-1189,0,0.108532,"labelwise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance. 1 Introduction Extreme multi-label text classification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results o"
W19-2209,D18-1352,0,0.314619,"labelwise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance. 1 Introduction Extreme multi-label text classification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results o"
W19-2209,N16-1174,0,0.809742,"cations Office of the European Union. Although EUROVOC contains more than 7,000 concepts, most of them are rarely used in practice. Consequently, they are under-represented in EU RLEX 57 K , making the dataset also appropriate for few-shot and zero-shot learning. Experimenting on EURLEX 57 K, we explore the use of various RNN-based and CNN-based neural classifiers, including the state of the art LabelWise Attention Network of Mullenbach et al. (2018), called CNN - LWAN here. We show that both a simpler BIGRU with self-attention (Xu et al., 2015) and the Hierarchical Attention Network (HAN) of Yang et al. (2016) outperform CNN LWAN by a wide margin. Replacing the CNN encoder of CNN - LWAN with a BIGRU, which leads to a method we call BIGRU - LWAN, further improves performance. Similar findings are observed in the zero-shot setting where Z - BIGRU - LWAN outperforms Z - CNN - LWAN. We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR - LEX, the European Union’s public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus. The dataset is substantially larger than previo"
W19-5031,P18-1128,0,0.0389488,"Missing"
W19-5031,W09-2415,0,0.301027,"Missing"
W19-5031,D18-1307,0,0.10534,"by carrying highly controlled randomized controlled trials, but it is also possible to mine evidence from observational studies and meta-analyses (Ward and Johnson, 2008), where information is often expressed in natural language (e.g., journal articles or clinical study reports). In natural language processing (NLP), causality detection is often viewed as a type of relation extraction, where the goal is to determine which relations (e.g., part-whole, content-container, causeeffect), if any, hold between two entities in a text (Hendrickx et al., 2009), using deep learning in most recent works (Bekoulis et al., 2018; Zhang 1 We cannot provide the entire biomedical dataset, because it is used to develop commercial products. We report, however, results for both the entire biomedical dataset and the publicly available subset. 292 Proceedings of the BioNLP 2019 workshop, pages 292–297 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics helps only in datasets with hundreds of training instances. When a few thousands of training instances are available, BIGRUATT reaches a performance plateau (both in generic and biomedical texts), then increasing the size of the dataset or employi"
W19-5031,D18-1244,0,0.0690036,"Missing"
W19-5031,D18-1211,1,0.836808,"del. Methods passed to a logistic regression (LR) layer to estimate the probability p that a sentence is causal: BIGRUATT : Our baseline model is a bidirectional GRU ( BIGRU ) with self-attention ( BIGRUATT ) (Cho et al., 2014; Bahdanau et al., 2015), a classifier that has been reported to perform well in short text classification (Pavlopoulos et al., 2017; Chalkidis et al., 2019). The model views each sentence as the sequence he1 , . . . , en i of its word embeddings (Fig. 1). We use WORD 2 VEC embeddings (Mikolov et al., 2013) pre-trained on approx. (a) 3.5 billion tokens from PUBMED texts (McDonald et al., 2018)2 or (b) 100 billion tokens from Google News.3 The BIGRU computes two lists Hf , Hb of hidden states, reading the word embeddings left to right and right to left, respectively. The corresponding elements of Hf , Hb are then concatenated to form the output H of the BIGRU: D E Hf = hf1 , . . . , hfn = GRUf (e1 , . . . , en ) D E Hb = hb1 , . . . , hbn = GRUb (e1 , . . . , en ) D E H = [hf1 ; hb1 ], . . . , [hfn ; hbn ] s= ai hi , p = σ(up · s + bp ) i=1 where up ∈ R2×dh , bp ∈ R, and σ is the sigmoid function. We use cross-entropy loss, the Adam optimizer (Kingma and Ba, 2015), and dropout layer"
W19-5031,D17-1004,0,0.0255266,"T are statistically significant (stars in Table 1), except for BIGRUATT + ELMO in EventSL.10 However, in the other three datasets which contain thousands of instances, the AUC differences between transfer learning and plain BIGRUATT are small, with no statistical significance in most cases. Also, the AUC scores of all methods on BioCausal-Large are close to those on BioCausal-Small, despite the fact that BioCausalLarge is approx. seven times larger. Similar observations can be made by looking at the F 1 scores. 5 Related and Future Work Recent work on (causal) relation extraction uses LSTM s (Zhang et al., 2017) or CNN s (Li and Mao, 2019), assuming however that the spans of the two entities (cause, effect) are known. A notable exception is the model of Bekoulis et al. (2018), which jointly infers the spans of the entities and their relationships. Such finer relation extraction methods, however, are computationally more expensive than our causal sentence detection methods, especially when they involve parsing (Zhang et al., 2018). We plan to consider pipelines where computationally cheaper causal sentence detection components will first detect sentences likely to express causality, and then finer rel"
W19-5031,W14-0702,0,0.0736339,"Missing"
W19-5031,D17-1117,1,0.834561,"rst work to (a) focus on causal sentence detection as a binary classification task, (b) consider causal sentence detection in both generic and biomedical texts, and (c) explore the effect of transfer learning in this task. 2 Figure 1: Illustration of the BIGRUATT model. Methods passed to a logistic regression (LR) layer to estimate the probability p that a sentence is causal: BIGRUATT : Our baseline model is a bidirectional GRU ( BIGRU ) with self-attention ( BIGRUATT ) (Cho et al., 2014; Bahdanau et al., 2015), a classifier that has been reported to perform well in short text classification (Pavlopoulos et al., 2017; Chalkidis et al., 2019). The model views each sentence as the sequence he1 , . . . , en i of its word embeddings (Fig. 1). We use WORD 2 VEC embeddings (Mikolov et al., 2013) pre-trained on approx. (a) 3.5 billion tokens from PUBMED texts (McDonald et al., 2018)2 or (b) 100 billion tokens from Google News.3 The BIGRU computes two lists Hf , Hb of hidden states, reading the word embeddings left to right and right to left, respectively. The corresponding elements of Hf , Hb are then concatenated to form the output H of the BIGRU: D E Hf = hf1 , . . . , hfn = GRUf (e1 , . . . , en ) D E Hb = hb"
W19-5031,N18-1202,0,0.485058,"relation extraction datasets contain sentences from generic, not biomedical documents. In this paper, we focus on detecting causal sentences, i.e., sentences conveying at least one causal relation. This is a first step towards mining causal relations from texts. Once causal sentences have been detected, computationally more intensive relation extraction methods can be used to identify the exact entities that participate in the causal relations and their roles (cause, effect). To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO (Peters et al., 2018) and BERT (Devlin et al., 2018), comparing against a bidirectional GRU with self-attention (Cho et al., 2014; Bahdanau et al., 2015). We experiment with generic public relation extraction datasets and a new larger biomedical causal sentence detection dataset, a subset of which we make publicly available.1 Unlike recently reported results in other NLP tasks (Peters et al., 2018; Devlin et al., 2018; Peters et al., 2019), we find that transfer learning We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts. To bypass the scarcity"
W19-5031,W19-4302,0,0.261524,"e in the causal relations and their roles (cause, effect). To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO (Peters et al., 2018) and BERT (Devlin et al., 2018), comparing against a bidirectional GRU with self-attention (Cho et al., 2014; Bahdanau et al., 2015). We experiment with generic public relation extraction datasets and a new larger biomedical causal sentence detection dataset, a subset of which we make publicly available.1 Unlike recently reported results in other NLP tasks (Peters et al., 2018; Devlin et al., 2018; Peters et al., 2019), we find that transfer learning We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts. To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention ( BIGRUATT ) as a baseline. We experiment with both generic public relation extraction datasets and a new biomedical causal sentence detection dataset, a subset of which we make publicly available. We find that transfer learning helps only in very small datasets. With la"
W19-5031,W17-2711,0,\N,Missing
W19-5031,W19-2209,1,\N,Missing
W19-5032,D14-1179,0,0.0078856,"Missing"
W19-5032,D17-1070,0,0.0136659,". . . , wn i, a bidirectional GRU (BIGRU) computes two sets of n hidden state vectors, one for each direction. These two sets are then added to form the output H of the BIGRU: Hf = GRUf (e(w1 ), . . . , e(wn )) (4) Hb = GRUb (e(w1 ), . . . , e(wn )) (5) H = Hf + Hb (6) where f , b denote the forward and backward directions, and + indicates component-wise addition. We add residual connections (He et al., 2015) from each word embedding e(wt ) to the corresponding hidden state ht of H. Instead of using the final forward and backward states of H, we apply max-pooling (Collobert and Weston, 2008; Conneau et al., 2017) over the state vectors ht of H. The output of the max pooling is the node embedding f (v). Figure 3 illustrates this method. Additional experiments were conducted with several variants of the last encoder. A unidirectional GRU instead of a BIGRU, and a BIGRU with 301 Node Embedding f(v): X X ..... X ..... X Max-Pooling X X X X + + + + h1 h2 h3 h4 h1 h2 h3 h4 e1 e2 e3 e4 lumen of arterial trunk Node: Statistics Nodes Edges Training true positive edges Training true negative edges Test true positive edges Test true negative edges Avg. descriptor length Max. descriptor length 16,894 19,436 16,89"
W19-5032,D18-1211,1,0.879754,"Missing"
W19-5032,N19-1221,0,0.0218948,"embeddings from texts and network structure; and unlike WANE, we do not align the descriptors of different nodes. We generate the embedding of each node from the word embeddings of its descriptor via the RNN (Fig. 1), but the parameters of the RNN, the word embeddings, hence also the node embeddings are updated during training to predict NODE 2 VEC’s neighborhoods. Although we use NODE 2 VEC to incorporate network context in the node embeddings, other neighborhood embedding methods, such as GCNs, could easily be used too. Similarly, text encoders other than RNNs could be applied. For example, Mishra et al. (2019) try to detect abusive language in tweets with a semi-supervised learning approach based on GCNs. They exploit the network structure and also the labels associated with the tweets, taking into account the linguistic behavior of the authors. 3 Proposed Node Embedding Approach Consider a network (graph) G = hV, E, Si, where V is the set of nodes (vertices); E ⊆ V × V is the set of edges (links) between nodes; and S is a function that maps each node v ∈ V to its textual descriptor S(v) = hw1 , w2 , . . . , wn i, where n is the word length of the descriptor, and each word wi comes from a vocabular"
W19-5032,D18-1209,0,0.0655153,"inoma acute lymphocytic leukemia Figure 1: Example network with nodes associated with textual descriptors. a) A model where each node is represented by a vector (node embedding) from a look-up table. b) A model where each node embedding is generated compositionally from the word embeddings of its descriptor via an RNN. The latter model can learn node embeddings from both the network structure and the word sequences of the textual descriptors. as textual descriptors (labels) or other meta-data associated with the nodes. More recent NE methods, e.g., CANE (Tu et al., 1 Introduction 2017), WANE (Shen et al., 2018), produce embeddings by combining the network structure and the Network Embedding (NE) methods map each text associated with the nodes. These contentnode of a network to an embedding, meaning a oriented methods embed networks whose nodes low-dimensional feature vector. They are highly are rich textual objects (often whole documents). effective in network analysis tasks involving preThey aim to capture the compositionality and sedictions over nodes and edges, for example link mantic similarities in the text, encoding them with prediction (Lu and Zhou, 2010), and node classideep learning methods"
W19-5032,P17-1158,0,0.0516818,"al. (2003) strengthen Content-Aware-N2V 299 the argument of compositionality by observing that many GO terms contain other GO terms. Also, they argue that substrings that are not GO terms appear frequently and often indicate semantic relationships. Ogren et al. (2004) use finite state automata to represent GO terms and demonstrate how small conceptual changes can create biologically meaningful candidate terms. In other work on NE methods, CENE (Sun et al., 2016) treats textual descriptors as a special kind of node, and uses bidirectional recurrent neural networks (RNNs) to encode them. CANE (Tu et al., 2017) learns two embeddings per node, a textbased one and an embedding based on network structure. The text-based one changes when interacting with different neighbors, using a mutual attention mechanism. WANE (Shen et al., 2018) also uses two types of node embeddings, text-based and structure-based. For the text-based embeddings, it matches important words across the textual descriptors of different nodes, and aggregates the resulting alignment features. In spite of performance improvements over structure-oriented approaches, these content-aware methods do not thoroughly explore the network struct"
